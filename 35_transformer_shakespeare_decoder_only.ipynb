{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV-xnLHgr49O"
      },
      "source": [
        "# Transformer - Decoder only\n",
        "\n",
        "Coding a transformer from scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gsoI4vFmr49W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RdH8aRr49X"
      },
      "source": [
        "### Decoder-Only Models (e.g., GPT, LLaMA)\n",
        "**Best for generative tasks:**\n",
        "- Text generation and completion\n",
        "- Creative writing and storytelling  \n",
        "- Conversational AI and chatbots\n",
        "- Code generation\n",
        "- Open-ended question answering\n",
        "- Text summarization (though encoder-decoder can be better for longer texts)\n",
        "- Language modeling tasks\n",
        "\n",
        "**Why they excel here:** They're trained to predict the next token in a sequence, making them naturally suited for generating coherent text step-by-step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wwFI83cr49Y"
      },
      "source": [
        "## Data Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v9w5_183r49Y",
        "outputId": "3565afed-6232-4264-e3d1-3ea5ecaa57a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Get Shakespeares work from Andrej Karpathy's website\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", url)\n",
        "\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1xAaHnOCr49Y",
        "outputId": "2186f410-5cd0-47e3-f85c-a4a432e216ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the first few characters\n",
        "print(shakespeare_text[:148])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mF0ssBThr49Y",
        "outputId": "978b6097-24eb-47b0-fe1a-fc111ed5f372",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in vocabulary: 65\n",
            "Total length of text dataset: 1115394\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(shakespeare_text))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
        "\n",
        "# How many number of distinct characters has the vocabulary:\n",
        "tokens_len = len(vocab)\n",
        "print(f\"Number of tokens in vocabulary: {tokens_len}\")\n",
        "\n",
        "# How many characters has the dataset:\n",
        "text_length = len(shakespeare_text)\n",
        "print(f\"Total length of text dataset: {text_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Bn8K6xsbr49Z"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95BlR0Wsr49Z"
      },
      "source": [
        "![alt text](https://github.com/ieadoboe/effective-waffle/blob/main/image.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J28M6Mesr49Z",
        "outputId": "cbe2ba2d-da96-4a81-eae0-1a9c7a17e8e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18 47 56 ... 45  8  0]\n",
            "1115394\n"
          ]
        }
      ],
      "source": [
        "def text_to_indices(text):\n",
        "    return [char_to_idx[char] for char in text]\n",
        "\n",
        "\n",
        "text_as_indices = text_to_indices(shakespeare_text)\n",
        "print(np.array(text_as_indices))\n",
        "print(len(text_to_indices(shakespeare_text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yQxX7clfr49Z",
        "outputId": "0181a232-9d40-4202-ad37-440b968a0304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Z'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def indices_to_text(indices):\n",
        "    return \"\".join([idx_to_char[idx] for idx in indices])\n",
        "\n",
        "\n",
        "indices_to_text([np.random.randint(65)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z898QYwDr49Z"
      },
      "outputs": [],
      "source": [
        "def create_training_data(text, seq_length=100, batch_size=64):\n",
        "    text_as_indices = text_to_indices(text)\n",
        "    total_seq = len(text_as_indices) - seq_length - 1  # -1 to make room for shifting\n",
        "\n",
        "    # Convert to numpy array for efficient indexing\n",
        "    text_as_indices_np = np.array(text_as_indices)\n",
        "\n",
        "    # Create combined sequences (input+target together)\n",
        "    sequences = []\n",
        "    for i in range(0, total_seq, seq_length // 4):  # Use stride for more efficient data usage\n",
        "        if i + seq_length + 1 >= len(text_as_indices_np):\n",
        "            break\n",
        "        # Include an extra token for the target shift\n",
        "        sequences.append(text_as_indices_np[i : i + seq_length + 1])\n",
        "\n",
        "    # Convert to TensorFlow dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
        "\n",
        "    # Batch and shuffle\n",
        "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
        "    return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Create dataset\n",
        "dataset = create_training_data(shakespeare_text, seq_length=max_seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBU6iRz3r49a"
      },
      "source": [
        "## Define Architecture Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw_34zzqr49a"
      },
      "source": [
        "### Embedding Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p2Ihvlj6r49a",
        "outputId": "47bedf07-c9ee-436a-c5da-92587c90c08e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 747ms/step\n",
            "(1, 1, 10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m650\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "embedding_dim = 10\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(tokens_len, embedding_dim))\n",
        "\n",
        "input_array = np.random.randint(tokens_len, size=(1, 1))\n",
        "model.compile(\"rmsprop\", \"sparse_categorical_crossentropy\")\n",
        "\n",
        "output_array = model.predict(input_array)\n",
        "print(output_array.shape)\n",
        "\n",
        "model.summary()\n",
        "# (1, 1, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU5BnhKIr49a"
      },
      "source": [
        "### PositionalEncoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1qYnQVir49a"
      },
      "source": [
        "$$PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}$$\n",
        "$$PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XSK2RS3wr49a"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_seq_len, embedding_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_seq_len = (\n",
        "            max_seq_len  # maximum sequence length that the model can handle\n",
        "        )\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Create the positional encodings\n",
        "        position = np.arange(max_seq_len)[:, np.newaxis]\n",
        "        div_term = np.exp(\n",
        "            np.arange(0, embedding_dim, 2) * -(np.log(10000.0) / embedding_dim)\n",
        "        )\n",
        "        pe = np.zeros((max_seq_len, embedding_dim))\n",
        "        pe[:, 0::2] = np.sin(position * div_term)\n",
        "        pe[:, 1::2] = np.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension e.g. (max_seq_len,embedding_dim) -> (1,max_seq_len,embedding_dim)\n",
        "        self.pe = tf.constant(pe, dtype=tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Get the sequence length from the input shape\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Slice the positional encoding to match the sequence length of the input\n",
        "        positional_encoding = self.pe[:seq_len, :]\n",
        "\n",
        "        # Add the positional encoding to the input embeddings\n",
        "        return inputs + positional_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FX10SHmzr49b",
        "outputId": "441c45cd-830f-4f3c-a65b-c6a49698b21d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10, 10)\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 10  # Embedding dimension\n",
        "max_len = 50  # Maximum sequence length\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Embedding(tokens_len, embedding_dim),\n",
        "        PositionalEncoding(max_seq_len=max_len, embedding_dim=embedding_dim),\n",
        "    ]\n",
        ")\n",
        "\n",
        "input_array = np.random.randint(tokens_len, size=(1, 10))\n",
        "output_array = model(input_array)\n",
        "\n",
        "print(output_array.shape)  # Should be (1, 10, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "B6hL_LKQr49b",
        "outputId": "9f37ecab-4650-463b-afc8-1aef22220257",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[45 17  9 54 31 42 61 17 26  7]]\n",
            "tf.Tensor(\n",
            "[[[ 0.01708723  1.0047415   0.03995876  1.0349113  -0.03562125\n",
            "    1.0057732  -0.03511788  1.0304842  -0.001729    0.9925268 ]\n",
            "  [ 0.8374536   0.51935464  0.15791216  1.010191    0.00574581\n",
            "    1.0231787  -0.04414752  1.0123152   0.0104405   0.9504049 ]\n",
            "  [ 0.8712817  -0.4155519   0.3378681   0.9801491   0.03226474\n",
            "    1.000618   -0.0360381   0.972897    0.01103339  1.0200636 ]\n",
            "  [ 0.18640877 -0.9403332   0.504849    0.883165    0.03454895\n",
            "    1.008708    0.02019216  0.9988739  -0.00773865  0.9573477 ]\n",
            "  [-0.708929   -0.6434521   0.5555581   0.81013054  0.1216993\n",
            "    0.9742444   0.02617961  1.0129876   0.05073103  0.96029496]\n",
            "  [-0.9359168   0.265682    0.70644814  0.6530318   0.08545735\n",
            "    0.9904365   0.03816169  1.0293933  -0.0165331   0.9715244 ]\n",
            "  [-0.23400123  0.9268773   0.8245525   0.54013526  0.12874472\n",
            "    1.0382214   0.0572193   1.0437919   0.00791903  0.96194965]\n",
            "  [ 0.65296924  0.7329546   0.8955285   0.4679004   0.155557\n",
            "    1.0080756  -0.02026469  1.0119348   0.01422623  0.9503953 ]\n",
            "  [ 0.97827524 -0.13632555  0.9873859   0.34236524  0.18182477\n",
            "    0.9885104   0.00440619  0.96499264  0.04916766  0.9870891 ]\n",
            "  [ 0.3993986  -0.9437372   0.9742381   0.09543389  0.22551504\n",
            "    0.9652145   0.0820051   0.9858475  -0.01215817  1.0068048 ]]], shape=(1, 10, 10), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(input_array[:3])\n",
        "print(output_array[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nh4y5CIlr49b",
        "outputId": "04264c4f-a9ae-4a38-c064-cad8a56cd92f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m)                 │             \u001b[38;5;34m650\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ positional_encoding                  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)                 │                             │                 │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ positional_encoding                  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)                 │                             │                 │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TykB25Zzr49b"
      },
      "source": [
        "### ScaledDotProductAttention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0evl75Hsr49b"
      },
      "source": [
        "![ScaledDotProductAttention](https://github.com/ieadoboe/effective-waffle/blob/main/image-1.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EuNa7Pe-r49b"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        # dot product attention\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (bs, q_len, k_len)\n",
        "\n",
        "        # scale dot product\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        # apply mask when necessary\n",
        "        if mask is not None:\n",
        "            # adding very large negative values\n",
        "            # so they go to zero after softmax\n",
        "            scaled_attention_logits += mask * -1e9\n",
        "\n",
        "        # apply softmax to attention weights (scores)\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "        # multiply by V (values)\n",
        "        out = tf.matmul(attention_weights, v)\n",
        "\n",
        "        return out, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWHf1r-sr49b"
      },
      "source": [
        "### MultiHeadAttention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIQA2XAHr49b"
      },
      "source": [
        "![MultiHeadAttention](https://github.com/ieadoboe/effective-waffle/blob/main/image-2.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DhNjz6Qnr49c"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.embedding_dim = embedding_dim\n",
        "        assert (\n",
        "            embedding_dim % num_heads == 0\n",
        "        ), \"embedding_dim must be divisible by num_heads\"\n",
        "        self.depth = embedding_dim // num_heads  # depth per head\n",
        "\n",
        "        # linear projection layers\n",
        "        self.wq = tf.keras.layers.Dense(embedding_dim)\n",
        "        self.wk = tf.keras.layers.Dense(embedding_dim)\n",
        "        self.wv = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "        # output projection\n",
        "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # linear projections\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        # reshaping q, k, v\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = self.attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(\n",
        "            scaled_attention, (batch_size, -1, self.embedding_dim)\n",
        "        )\n",
        "\n",
        "        out = self.dense(concat_attention)\n",
        "\n",
        "        return out, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "clLatZ9Vr49c",
        "outputId": "a74af0ac-8879-4c5a-8732-0ae60f4dd1d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Shape:  (2, 100, 64)\n",
            "Key Shape:  (2, 100, 64)\n",
            "Value Shape:  (2, 100, 64)\n",
            "Output Shape:  (2, 100, 64)\n",
            "Attention Weights Shape:  (2, 8, 100, 100)\n",
            "✅ MultiHeadAttention test passed!\n"
          ]
        }
      ],
      "source": [
        "# Define dummy parameters\n",
        "num_heads = 8\n",
        "embedding_dim = 64\n",
        "batch_size = 2\n",
        "seq_length = 100  # Sequence length\n",
        "\n",
        "# Instantiate MultiHeadAttention\n",
        "mha = MultiHeadAttention(num_heads=num_heads, embedding_dim=embedding_dim)\n",
        "\n",
        "# Create dummy input tensors\n",
        "q = tf.random.uniform((batch_size, seq_length, embedding_dim))  # Queries\n",
        "k = tf.random.uniform((batch_size, seq_length, embedding_dim))  # Keys\n",
        "v = tf.random.uniform((batch_size, seq_length, embedding_dim))  # Values\n",
        "\n",
        "# Run the attention layer\n",
        "output, attention_weights = mha(q, k, v)\n",
        "\n",
        "# Print output shapes\n",
        "print(\"Query Shape: \", q.shape)\n",
        "print(\"Key Shape: \", k.shape)\n",
        "print(\"Value Shape: \", v.shape)\n",
        "print(\"Output Shape: \", output.shape)\n",
        "print(\"Attention Weights Shape: \", attention_weights.shape)\n",
        "\n",
        "# Assertions to check correctness\n",
        "assert output.shape == (\n",
        "    batch_size,\n",
        "    seq_length,\n",
        "    embedding_dim,\n",
        "), \"Output shape is incorrect\"\n",
        "assert attention_weights.shape == (\n",
        "    batch_size,\n",
        "    num_heads,\n",
        "    seq_length,\n",
        "    seq_length,\n",
        "), \"Attention weights shape is incorrect\"\n",
        "print(\"✅ MultiHeadAttention test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToS_pZLpr49c"
      },
      "source": [
        "### Position-wise Feed-Forward Network\n",
        "\n",
        "$$\\text{FFN(x)} = \\text{max}(0,~ xW_1 + b_1)W_2 + b_2$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0p_ctLqOr49c"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, hidden_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # hidden_dim (dff) - feed forward network hidden\n",
        "        # layer dimension a.k.a inner layer dimensionality\n",
        "\n",
        "        self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")\n",
        "        self.dense2 = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        x = self.dense1(inputs)\n",
        "\n",
        "        return self.dense2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLXID12qr49c"
      },
      "source": [
        "### DecoderLayer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SWaQN0Zkr49c"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self, embedding_dim, num_heads, hidden_dim, dropout_rate=0.1, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(embedding_dim, num_heads)\n",
        "        self.ffn = PositionwiseFeedForward(embedding_dim, hidden_dim)\n",
        "\n",
        "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(\n",
        "        self, x, training=False, mask=None,\n",
        "    ):\n",
        "        # Self attention with look-ahead mask\n",
        "        self_attn_output, _ = self.self_attention(q=x, v=x, k=x, mask=mask)\n",
        "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
        "        out1 = self.layer_norm1(x + self_attn_output) # residual connection\n",
        "\n",
        "        # Feed forward\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layer_norm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB9DVEuHr49d"
      },
      "source": [
        "### Decoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av8vrnn0r49d"
      },
      "source": [
        "In a decoder-only architecture, we need both:\n",
        "\n",
        "1. A causal mask to prevent attending to future tokens\n",
        "2. A padding mask to handle variable-length sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mSE50Ebhr49d"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers,\n",
        "        embedding_dim,\n",
        "        hidden_dim,\n",
        "        num_heads,\n",
        "        vocab_size,\n",
        "        max_seq_len,\n",
        "        dropout_rate=0.1,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoding = PositionalEncoding(max_seq_len, embedding_dim)\n",
        "\n",
        "        self.decoder_layers = [\n",
        "            DecoderLayer(embedding_dim, num_heads, hidden_dim, dropout_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(\n",
        "        self, x, training=False, mask=None\n",
        "    ):\n",
        "        # Create combined mask (causal + padding)\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        causal_mask = 1 - tf.linalg.band_part(\n",
        "            tf.ones((seq_len, seq_len)), -1, 0\n",
        "        )\n",
        "\n",
        "        # Add padding mask if sequences have padding\n",
        "        if mask is not None:\n",
        "            # Convert padding mask to float32 to match causal mask type\n",
        "            padding_mask = tf.cast(mask, tf.float32)\n",
        "            # Reshape padding mask for broadcasting\n",
        "            padding_mask = padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "            # Combine with causal mask\n",
        "            combined_mask = tf.maximum(causal_mask, padding_mask)\n",
        "        else:\n",
        "            combined_mask = causal_mask\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # decoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.decoder_layers[i](\n",
        "                x,\n",
        "                training=training,\n",
        "                mask=combined_mask,\n",
        "            )\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiXkrVb_r49d"
      },
      "source": [
        "## Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi88fY3-r49d"
      },
      "source": [
        "![alt text](https://github.com/ieadoboe/effective-waffle/blob/main/image.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-Ns3Szxxr49e"
      },
      "outputs": [],
      "source": [
        "# Create look-ahead mask for decoder\n",
        "def create_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers,\n",
        "        embedding_dim,\n",
        "        hidden_dim,\n",
        "        num_heads,\n",
        "        vocab_size,\n",
        "        max_seq_len,\n",
        "        dropout_rate=0.1,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            num_layers,\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_heads,\n",
        "            vocab_size,\n",
        "            max_seq_len,\n",
        "            dropout_rate,\n",
        "        )\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        x,\n",
        "        mask=None,\n",
        "        training=False,\n",
        "    ):\n",
        "\n",
        "        # Decoder output\n",
        "        dec_output = self.decoder(\n",
        "            x=x,\n",
        "            training=training,\n",
        "            mask=mask\n",
        "        )\n",
        "\n",
        "        # Final output\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "i1bkO5Rur49e",
        "outputId": "6eecf963-dfe0-4b33-e3bb-36443b4cf485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)                  │ ?                           │         \u001b[38;5;34m801,408\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m65\u001b[0m)                 │           \u001b[38;5;34m8,385\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)                  │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">801,408</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,385</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m809,793\u001b[0m (3.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">809,793</span> (3.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m809,793\u001b[0m (3.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">809,793</span> (3.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# Architecture parameters\n",
        "num_layers = 4\n",
        "embedding_dim = 128\n",
        "hidden_dim = 512\n",
        "num_heads = 8\n",
        "max_seq_len = 100\n",
        "\n",
        "# Training parameters\n",
        "dropout_rate = 0.1\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create model\n",
        "model = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_heads=num_heads,\n",
        "    hidden_dim=hidden_dim,\n",
        "    vocab_size=tokens_len,\n",
        "    max_seq_len=max_seq_len,\n",
        "    dropout_rate=dropout_rate,\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Sample input\n",
        "inp = tf.random.uniform((1, 10), maxval=tokens_len, dtype=tf.int32)\n",
        "tar = tf.random.uniform((1, 10), maxval=tokens_len, dtype=tf.int32)\n",
        "\n",
        "model(inp, tar)\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46GQp_jIr49e"
      },
      "source": [
        "## Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uIbFT7yAr49f"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "def generate_text(model, start_string, num_generate=1000, temperature=1.0):\n",
        "    # Convert start string to indices\n",
        "    input_indices = text_to_indices(start_string)\n",
        "    input_tensor = tf.expand_dims(input_indices, 0)\n",
        "\n",
        "    # Empty result string\n",
        "    result = start_string\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        # Create mask for padding\n",
        "        mask = None # for single sequence generation, we typically won't use padding mask\n",
        "\n",
        "        # Call model with keyword arguments\n",
        "        output = model(\n",
        "            x=input_tensor,\n",
        "            training=False,\n",
        "            mask=mask\n",
        "        )\n",
        "\n",
        "        # Select the last token from the output\n",
        "        output = output[:, -1, :]  # (batch_size, vocab_size)\n",
        "\n",
        "        # Apply temperature\n",
        "        if temperature != 1.0:\n",
        "            output = output / temperature\n",
        "\n",
        "        # Sample from the output distribution\n",
        "        predicted_id = tf.random.categorical(output, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Concatenate the predicted character to the output text\n",
        "        result += idx_to_char[predicted_id]\n",
        "\n",
        "        # Update the input tensor to the decoder\n",
        "        input_indices.append(predicted_id)\n",
        "        input_tensor = tf.expand_dims(input_indices[-max_seq_len:], 0)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUC62t0-r49f"
      },
      "source": [
        "### Generate text before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EMsJTy52r49f",
        "outputId": "8d34ca48-fb6f-49a6-d812-13bda4d42d59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Text generated before training -----\n",
            "\n",
            "ROMEO: Kon\n",
            "GW'jnsXE?WUT!neZY 'CjNGH?.uUrG?UIMjrAjTl'TQDxnMe-&j;-epD;N,TilUoVnTcZ,;Qjnu;NjuUnxzyjNeUkVUZnqS ;vFq;MQ\n",
            " .V nVU nDkkEMgMz yguUDHoMM.IDEAt!U.e:$c MM lsMsUMUMVntU\n",
            "MM;MjRbl QBMV.MEcO&fUTU?;ez  ; 'UDU?l.UFZnRizDkfM slajZxUgAM 'kckEPKzU.kU NHUV'jNcM M\n"
          ]
        }
      ],
      "source": [
        "# Generate text before training\n",
        "print(\"\\n----- Text generated before training -----\\n\")\n",
        "start_string = \"ROMEO: \"\n",
        "generated_text = generate_text(model, start_string, num_generate=250, temperature=1.0)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ByU3a8er49f"
      },
      "source": [
        "### Define training loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5RfvEGSKr49f"
      },
      "outputs": [],
      "source": [
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "# Loss function\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Use built-in metrics instead of custom class\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp):\n",
        "    input = inp[:, :-1]\n",
        "    target = inp[:, 1:]  # predict next token (here, character)\n",
        "\n",
        "    # Always create a padding mask, without conditional logic\n",
        "    padding_mask = tf.math.equal(input, 0)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(\n",
        "            x=input,\n",
        "            training=True,\n",
        "            mask=padding_mask,\n",
        "        )\n",
        "\n",
        "        # Apply mask for padding if needed\n",
        "        loss_mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "        loss = loss_object(\n",
        "            target, predictions, sample_weight=tf.cast(loss_mask, dtype=tf.float32)\n",
        "        )\n",
        "\n",
        "    # Get gradients and update weights\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Update metrics\n",
        "    train_loss.update_state(loss)\n",
        "    train_accuracy.update_state(\n",
        "        target, predictions, sample_weight=tf.cast(loss_mask, dtype=tf.float32)\n",
        "    )\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBorFoGSr49f"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "F2bZO3JPr49f",
        "outputId": "e13ea708-8378-45dc-c1b9-d0aa26da40bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 4.4851 Accuracy 0.0264\n",
            "Epoch 1 Batch 50 Loss 3.2831 Accuracy 0.1408\n",
            "Epoch 1 Batch 100 Loss 3.1455 Accuracy 0.1631\n",
            "Epoch 1 Batch 150 Loss 2.9241 Accuracy 0.1957\n",
            "Epoch 1 Batch 200 Loss 2.7854 Accuracy 0.2177\n",
            "Epoch 1 Batch 250 Loss 2.6837 Accuracy 0.2363\n",
            "Epoch 1 Batch 300 Loss 2.6033 Accuracy 0.2524\n",
            "Epoch 1 Batch 350 Loss 2.5364 Accuracy 0.2665\n",
            "Epoch 1 Batch 400 Loss 2.4800 Accuracy 0.2789\n",
            "Epoch 1 Batch 450 Loss 2.4291 Accuracy 0.2902\n",
            "Epoch 1 Batch 500 Loss 2.3844 Accuracy 0.3004\n",
            "Epoch 1 Batch 550 Loss 2.3440 Accuracy 0.3097\n",
            "Epoch 1 Batch 600 Loss 2.3059 Accuracy 0.3187\n",
            "Epoch 1 Batch 650 Loss 2.2705 Accuracy 0.3271\n",
            "Epoch 1 Loss 2.2411 Accuracy 0.3342\n",
            "Time taken for 1 epoch: 40.95 secs\n",
            "Epoch 2 Batch 0 Loss 1.8983 Accuracy 0.4105\n",
            "Epoch 2 Batch 50 Loss 1.8162 Accuracy 0.4388\n",
            "Epoch 2 Batch 100 Loss 1.7994 Accuracy 0.4450\n",
            "Epoch 2 Batch 150 Loss 1.7854 Accuracy 0.4488\n",
            "Epoch 2 Batch 200 Loss 1.7748 Accuracy 0.4518\n",
            "Epoch 2 Batch 250 Loss 1.7652 Accuracy 0.4545\n",
            "Epoch 2 Batch 300 Loss 1.7550 Accuracy 0.4574\n",
            "Epoch 2 Batch 350 Loss 1.7471 Accuracy 0.4597\n",
            "Epoch 2 Batch 400 Loss 1.7411 Accuracy 0.4611\n",
            "Epoch 2 Batch 450 Loss 1.7332 Accuracy 0.4631\n",
            "Epoch 2 Batch 500 Loss 1.7260 Accuracy 0.4648\n",
            "Epoch 2 Batch 550 Loss 1.7189 Accuracy 0.4665\n",
            "Epoch 2 Batch 600 Loss 1.7118 Accuracy 0.4683\n",
            "Epoch 2 Batch 650 Loss 1.7046 Accuracy 0.4701\n",
            "Epoch 2 Loss 1.6983 Accuracy 0.4717\n",
            "Time taken for 1 epoch: 27.89 secs\n",
            "Epoch 3 Batch 0 Loss 1.6341 Accuracy 0.4731\n",
            "Epoch 3 Batch 50 Loss 1.5982 Accuracy 0.4983\n",
            "Epoch 3 Batch 100 Loss 1.5896 Accuracy 0.5014\n",
            "Epoch 3 Batch 150 Loss 1.5814 Accuracy 0.5038\n",
            "Epoch 3 Batch 200 Loss 1.5793 Accuracy 0.5044\n",
            "Epoch 3 Batch 250 Loss 1.5770 Accuracy 0.5051\n",
            "Epoch 3 Batch 300 Loss 1.5723 Accuracy 0.5067\n",
            "Epoch 3 Batch 350 Loss 1.5708 Accuracy 0.5070\n",
            "Epoch 3 Batch 400 Loss 1.5698 Accuracy 0.5073\n",
            "Epoch 3 Batch 450 Loss 1.5670 Accuracy 0.5078\n",
            "Epoch 3 Batch 500 Loss 1.5648 Accuracy 0.5083\n",
            "Epoch 3 Batch 550 Loss 1.5630 Accuracy 0.5086\n",
            "Epoch 3 Batch 600 Loss 1.5606 Accuracy 0.5091\n",
            "Epoch 3 Batch 650 Loss 1.5570 Accuracy 0.5099\n",
            "Epoch 3 Loss 1.5543 Accuracy 0.5105\n",
            "Time taken for 1 epoch: 27.24 secs\n",
            "Epoch 4 Batch 0 Loss 1.4889 Accuracy 0.5185\n",
            "Epoch 4 Batch 50 Loss 1.4966 Accuracy 0.5257\n",
            "Epoch 4 Batch 100 Loss 1.4912 Accuracy 0.5283\n",
            "Epoch 4 Batch 150 Loss 1.4881 Accuracy 0.5289\n",
            "Epoch 4 Batch 200 Loss 1.4899 Accuracy 0.5286\n",
            "Epoch 4 Batch 250 Loss 1.4888 Accuracy 0.5288\n",
            "Epoch 4 Batch 300 Loss 1.4867 Accuracy 0.5294\n",
            "Epoch 4 Batch 350 Loss 1.4874 Accuracy 0.5292\n",
            "Epoch 4 Batch 400 Loss 1.4882 Accuracy 0.5290\n",
            "Epoch 4 Batch 450 Loss 1.4873 Accuracy 0.5291\n",
            "Epoch 4 Batch 500 Loss 1.4869 Accuracy 0.5292\n",
            "Epoch 4 Batch 550 Loss 1.4859 Accuracy 0.5293\n",
            "Epoch 4 Batch 600 Loss 1.4841 Accuracy 0.5296\n",
            "Epoch 4 Batch 650 Loss 1.4823 Accuracy 0.5300\n",
            "Epoch 4 Loss 1.4807 Accuracy 0.5303\n",
            "Time taken for 1 epoch: 27.44 secs\n",
            "Epoch 5 Batch 0 Loss 1.4762 Accuracy 0.5290\n",
            "Epoch 5 Batch 50 Loss 1.4408 Accuracy 0.5419\n",
            "Epoch 5 Batch 100 Loss 1.4364 Accuracy 0.5428\n",
            "Epoch 5 Batch 150 Loss 1.4339 Accuracy 0.5436\n",
            "Epoch 5 Batch 200 Loss 1.4350 Accuracy 0.5431\n",
            "Epoch 5 Batch 250 Loss 1.4368 Accuracy 0.5426\n",
            "Epoch 5 Batch 300 Loss 1.4357 Accuracy 0.5430\n",
            "Epoch 5 Batch 350 Loss 1.4355 Accuracy 0.5431\n",
            "Epoch 5 Batch 400 Loss 1.4369 Accuracy 0.5425\n",
            "Epoch 5 Batch 450 Loss 1.4370 Accuracy 0.5424\n",
            "Epoch 5 Batch 500 Loss 1.4367 Accuracy 0.5426\n",
            "Epoch 5 Batch 550 Loss 1.4369 Accuracy 0.5424\n",
            "Epoch 5 Batch 600 Loss 1.4362 Accuracy 0.5424\n",
            "Epoch 5 Batch 650 Loss 1.4348 Accuracy 0.5426\n",
            "Epoch 5 Loss 1.4337 Accuracy 0.5428\n",
            "Time taken for 1 epoch: 27.49 secs\n",
            "\n",
            "----- Text generated after epoch 5 -----\n",
            "\n",
            "ROMEO: an in a mape, and but darder the young sense my death? And I should here's by the weak desperate? There is a faults of it! my father life, against a did man the ancientry of sleep. Yet Trieuto, the torman was of your content and not drunk, the trumpp\n",
            "\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.4762 Accuracy 0.5253\n",
            "Epoch 6 Batch 50 Loss 1.4034 Accuracy 0.5516\n",
            "Epoch 6 Batch 100 Loss 1.3942 Accuracy 0.5548\n",
            "Epoch 6 Batch 150 Loss 1.3946 Accuracy 0.5546\n",
            "Epoch 6 Batch 200 Loss 1.3979 Accuracy 0.5532\n",
            "Epoch 6 Batch 250 Loss 1.3997 Accuracy 0.5526\n",
            "Epoch 6 Batch 300 Loss 1.3989 Accuracy 0.5530\n",
            "Epoch 6 Batch 350 Loss 1.4005 Accuracy 0.5527\n",
            "Epoch 6 Batch 400 Loss 1.4025 Accuracy 0.5521\n",
            "Epoch 6 Batch 450 Loss 1.4027 Accuracy 0.5519\n",
            "Epoch 6 Batch 500 Loss 1.4037 Accuracy 0.5516\n",
            "Epoch 6 Batch 550 Loss 1.4038 Accuracy 0.5515\n",
            "Epoch 6 Batch 600 Loss 1.4035 Accuracy 0.5515\n",
            "Epoch 6 Batch 650 Loss 1.4028 Accuracy 0.5517\n",
            "Epoch 6 Loss 1.4018 Accuracy 0.5518\n",
            "Time taken for 1 epoch: 27.58 secs\n",
            "Epoch 7 Batch 0 Loss 1.4313 Accuracy 0.5427\n",
            "Epoch 7 Batch 50 Loss 1.3751 Accuracy 0.5596\n",
            "Epoch 7 Batch 100 Loss 1.3679 Accuracy 0.5620\n",
            "Epoch 7 Batch 150 Loss 1.3676 Accuracy 0.5617\n",
            "Epoch 7 Batch 200 Loss 1.3728 Accuracy 0.5600\n",
            "Epoch 7 Batch 250 Loss 1.3739 Accuracy 0.5596\n",
            "Epoch 7 Batch 300 Loss 1.3732 Accuracy 0.5598\n",
            "Epoch 7 Batch 350 Loss 1.3737 Accuracy 0.5595\n",
            "Epoch 7 Batch 400 Loss 1.3761 Accuracy 0.5588\n",
            "Epoch 7 Batch 450 Loss 1.3771 Accuracy 0.5586\n",
            "Epoch 7 Batch 500 Loss 1.3775 Accuracy 0.5584\n",
            "Epoch 7 Batch 550 Loss 1.3770 Accuracy 0.5585\n",
            "Epoch 7 Batch 600 Loss 1.3774 Accuracy 0.5583\n",
            "Epoch 7 Batch 650 Loss 1.3772 Accuracy 0.5582\n",
            "Epoch 7 Loss 1.3767 Accuracy 0.5583\n",
            "Time taken for 1 epoch: 27.76 secs\n",
            "Epoch 8 Batch 0 Loss 1.4246 Accuracy 0.5387\n",
            "Epoch 8 Batch 50 Loss 1.3560 Accuracy 0.5643\n",
            "Epoch 8 Batch 100 Loss 1.3489 Accuracy 0.5666\n",
            "Epoch 8 Batch 150 Loss 1.3462 Accuracy 0.5673\n",
            "Epoch 8 Batch 200 Loss 1.3496 Accuracy 0.5661\n",
            "Epoch 8 Batch 250 Loss 1.3515 Accuracy 0.5654\n",
            "Epoch 8 Batch 300 Loss 1.3514 Accuracy 0.5654\n",
            "Epoch 8 Batch 350 Loss 1.3528 Accuracy 0.5653\n",
            "Epoch 8 Batch 400 Loss 1.3554 Accuracy 0.5645\n",
            "Epoch 8 Batch 450 Loss 1.3564 Accuracy 0.5642\n",
            "Epoch 8 Batch 500 Loss 1.3577 Accuracy 0.5638\n",
            "Epoch 8 Batch 550 Loss 1.3579 Accuracy 0.5636\n",
            "Epoch 8 Batch 600 Loss 1.3578 Accuracy 0.5636\n",
            "Epoch 8 Batch 650 Loss 1.3576 Accuracy 0.5636\n",
            "Epoch 8 Loss 1.3573 Accuracy 0.5637\n",
            "Time taken for 1 epoch: 40.95 secs\n",
            "Epoch 9 Batch 0 Loss 1.3804 Accuracy 0.5616\n",
            "Epoch 9 Batch 50 Loss 1.3328 Accuracy 0.5711\n",
            "Epoch 9 Batch 100 Loss 1.3276 Accuracy 0.5725\n",
            "Epoch 9 Batch 150 Loss 1.3287 Accuracy 0.5722\n",
            "Epoch 9 Batch 200 Loss 1.3331 Accuracy 0.5707\n",
            "Epoch 9 Batch 250 Loss 1.3362 Accuracy 0.5696\n",
            "Epoch 9 Batch 300 Loss 1.3362 Accuracy 0.5696\n",
            "Epoch 9 Batch 350 Loss 1.3380 Accuracy 0.5691\n",
            "Epoch 9 Batch 400 Loss 1.3399 Accuracy 0.5686\n",
            "Epoch 9 Batch 450 Loss 1.3407 Accuracy 0.5685\n",
            "Epoch 9 Batch 500 Loss 1.3419 Accuracy 0.5681\n",
            "Epoch 9 Batch 550 Loss 1.3429 Accuracy 0.5676\n",
            "Epoch 9 Batch 600 Loss 1.3429 Accuracy 0.5675\n",
            "Epoch 9 Batch 650 Loss 1.3427 Accuracy 0.5675\n",
            "Epoch 9 Loss 1.3423 Accuracy 0.5676\n",
            "Time taken for 1 epoch: 27.47 secs\n",
            "Epoch 10 Batch 0 Loss 1.3974 Accuracy 0.5580\n",
            "Epoch 10 Batch 50 Loss 1.3209 Accuracy 0.5740\n",
            "Epoch 10 Batch 100 Loss 1.3147 Accuracy 0.5759\n",
            "Epoch 10 Batch 150 Loss 1.3146 Accuracy 0.5760\n",
            "Epoch 10 Batch 200 Loss 1.3195 Accuracy 0.5744\n",
            "Epoch 10 Batch 250 Loss 1.3213 Accuracy 0.5736\n",
            "Epoch 10 Batch 300 Loss 1.3219 Accuracy 0.5736\n",
            "Epoch 10 Batch 350 Loss 1.3232 Accuracy 0.5733\n",
            "Epoch 10 Batch 400 Loss 1.3261 Accuracy 0.5725\n",
            "Epoch 10 Batch 450 Loss 1.3273 Accuracy 0.5721\n",
            "Epoch 10 Batch 500 Loss 1.3284 Accuracy 0.5716\n",
            "Epoch 10 Batch 550 Loss 1.3295 Accuracy 0.5713\n",
            "Epoch 10 Batch 600 Loss 1.3292 Accuracy 0.5713\n",
            "Epoch 10 Batch 650 Loss 1.3292 Accuracy 0.5712\n",
            "Epoch 10 Loss 1.3288 Accuracy 0.5713\n",
            "Time taken for 1 epoch: 27.80 secs\n",
            "\n",
            "----- Text generated after epoch 10 -----\n",
            "\n",
            "ROMEO: what are little lies of his horn! Farewell, Angelo? what's you fill? thou may'st a charging o' this. My fortune, the match'd eat; here come hath heart as my thing should it were brave. No; I will not be gone at honesty. Nay, marry, sound and thus I h\n",
            "\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.3630 Accuracy 0.5589\n",
            "Epoch 11 Batch 50 Loss 1.3095 Accuracy 0.5760\n",
            "Epoch 11 Batch 100 Loss 1.3039 Accuracy 0.5783\n",
            "Epoch 11 Batch 150 Loss 1.3051 Accuracy 0.5780\n",
            "Epoch 11 Batch 200 Loss 1.3100 Accuracy 0.5763\n",
            "Epoch 11 Batch 250 Loss 1.3128 Accuracy 0.5756\n",
            "Epoch 11 Batch 300 Loss 1.3113 Accuracy 0.5764\n",
            "Epoch 11 Batch 350 Loss 1.3128 Accuracy 0.5760\n",
            "Epoch 11 Batch 400 Loss 1.3156 Accuracy 0.5749\n",
            "Epoch 11 Batch 450 Loss 1.3164 Accuracy 0.5747\n",
            "Epoch 11 Batch 500 Loss 1.3181 Accuracy 0.5743\n",
            "Epoch 11 Batch 550 Loss 1.3183 Accuracy 0.5742\n",
            "Epoch 11 Batch 600 Loss 1.3188 Accuracy 0.5740\n",
            "Epoch 11 Batch 650 Loss 1.3187 Accuracy 0.5740\n",
            "Epoch 11 Loss 1.3180 Accuracy 0.5741\n",
            "Time taken for 1 epoch: 27.28 secs\n",
            "Epoch 12 Batch 0 Loss 1.3802 Accuracy 0.5568\n",
            "Epoch 12 Batch 50 Loss 1.2974 Accuracy 0.5801\n",
            "Epoch 12 Batch 100 Loss 1.2955 Accuracy 0.5814\n",
            "Epoch 12 Batch 150 Loss 1.2945 Accuracy 0.5812\n",
            "Epoch 12 Batch 200 Loss 1.2984 Accuracy 0.5799\n",
            "Epoch 12 Batch 250 Loss 1.3012 Accuracy 0.5790\n",
            "Epoch 12 Batch 300 Loss 1.3008 Accuracy 0.5795\n",
            "Epoch 12 Batch 350 Loss 1.3024 Accuracy 0.5790\n",
            "Epoch 12 Batch 400 Loss 1.3053 Accuracy 0.5781\n",
            "Epoch 12 Batch 450 Loss 1.3064 Accuracy 0.5777\n",
            "Epoch 12 Batch 500 Loss 1.3076 Accuracy 0.5774\n",
            "Epoch 12 Batch 550 Loss 1.3088 Accuracy 0.5770\n",
            "Epoch 12 Batch 600 Loss 1.3091 Accuracy 0.5768\n",
            "Epoch 12 Batch 650 Loss 1.3089 Accuracy 0.5769\n",
            "Epoch 12 Loss 1.3085 Accuracy 0.5768\n",
            "Time taken for 1 epoch: 27.88 secs\n",
            "Epoch 13 Batch 0 Loss 1.3351 Accuracy 0.5721\n",
            "Epoch 13 Batch 50 Loss 1.2880 Accuracy 0.5819\n",
            "Epoch 13 Batch 100 Loss 1.2825 Accuracy 0.5843\n",
            "Epoch 13 Batch 150 Loss 1.2825 Accuracy 0.5845\n",
            "Epoch 13 Batch 200 Loss 1.2881 Accuracy 0.5829\n",
            "Epoch 13 Batch 250 Loss 1.2910 Accuracy 0.5819\n",
            "Epoch 13 Batch 300 Loss 1.2920 Accuracy 0.5816\n",
            "Epoch 13 Batch 350 Loss 1.2940 Accuracy 0.5811\n",
            "Epoch 13 Batch 400 Loss 1.2971 Accuracy 0.5802\n",
            "Epoch 13 Batch 450 Loss 1.2978 Accuracy 0.5800\n",
            "Epoch 13 Batch 500 Loss 1.2983 Accuracy 0.5797\n",
            "Epoch 13 Batch 550 Loss 1.2995 Accuracy 0.5794\n",
            "Epoch 13 Batch 600 Loss 1.2998 Accuracy 0.5793\n",
            "Epoch 13 Batch 650 Loss 1.3000 Accuracy 0.5792\n",
            "Epoch 13 Loss 1.2998 Accuracy 0.5792\n",
            "Time taken for 1 epoch: 27.14 secs\n",
            "Epoch 14 Batch 0 Loss 1.3619 Accuracy 0.5564\n",
            "Epoch 14 Batch 50 Loss 1.2845 Accuracy 0.5842\n",
            "Epoch 14 Batch 100 Loss 1.2787 Accuracy 0.5859\n",
            "Epoch 14 Batch 150 Loss 1.2791 Accuracy 0.5858\n",
            "Epoch 14 Batch 200 Loss 1.2829 Accuracy 0.5844\n",
            "Epoch 14 Batch 250 Loss 1.2837 Accuracy 0.5840\n",
            "Epoch 14 Batch 300 Loss 1.2845 Accuracy 0.5839\n",
            "Epoch 14 Batch 350 Loss 1.2867 Accuracy 0.5833\n",
            "Epoch 14 Batch 400 Loss 1.2897 Accuracy 0.5822\n",
            "Epoch 14 Batch 450 Loss 1.2909 Accuracy 0.5819\n",
            "Epoch 14 Batch 500 Loss 1.2918 Accuracy 0.5816\n",
            "Epoch 14 Batch 550 Loss 1.2925 Accuracy 0.5816\n",
            "Epoch 14 Batch 600 Loss 1.2926 Accuracy 0.5813\n",
            "Epoch 14 Batch 650 Loss 1.2930 Accuracy 0.5811\n",
            "Epoch 14 Loss 1.2928 Accuracy 0.5811\n",
            "Time taken for 1 epoch: 27.19 secs\n",
            "Epoch 15 Batch 0 Loss 1.3111 Accuracy 0.5762\n",
            "Epoch 15 Batch 50 Loss 1.2783 Accuracy 0.5862\n",
            "Epoch 15 Batch 100 Loss 1.2698 Accuracy 0.5890\n",
            "Epoch 15 Batch 150 Loss 1.2699 Accuracy 0.5888\n",
            "Epoch 15 Batch 200 Loss 1.2742 Accuracy 0.5870\n",
            "Epoch 15 Batch 250 Loss 1.2773 Accuracy 0.5858\n",
            "Epoch 15 Batch 300 Loss 1.2776 Accuracy 0.5857\n",
            "Epoch 15 Batch 350 Loss 1.2805 Accuracy 0.5849\n",
            "Epoch 15 Batch 400 Loss 1.2826 Accuracy 0.5843\n",
            "Epoch 15 Batch 450 Loss 1.2836 Accuracy 0.5841\n",
            "Epoch 15 Batch 500 Loss 1.2844 Accuracy 0.5838\n",
            "Epoch 15 Batch 550 Loss 1.2858 Accuracy 0.5833\n",
            "Epoch 15 Batch 600 Loss 1.2861 Accuracy 0.5831\n",
            "Epoch 15 Batch 650 Loss 1.2864 Accuracy 0.5830\n",
            "Epoch 15 Loss 1.2859 Accuracy 0.5830\n",
            "Time taken for 1 epoch: 27.51 secs\n",
            "\n",
            "----- Text generated after epoch 15 -----\n",
            "\n",
            "ROMEO: if you have a due at your honour father I do know: the sir, if you may do so the moon had me of my purpose to his face, sir; and being she, as I am not honour for no; I do not thee, but sight: a patient, that from the prince and prosperous will with \n",
            "\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.3321 Accuracy 0.5611\n",
            "Epoch 16 Batch 50 Loss 1.2691 Accuracy 0.5882\n",
            "Epoch 16 Batch 100 Loss 1.2627 Accuracy 0.5899\n",
            "Epoch 16 Batch 150 Loss 1.2617 Accuracy 0.5903\n",
            "Epoch 16 Batch 200 Loss 1.2665 Accuracy 0.5891\n",
            "Epoch 16 Batch 250 Loss 1.2696 Accuracy 0.5880\n",
            "Epoch 16 Batch 300 Loss 1.2707 Accuracy 0.5878\n",
            "Epoch 16 Batch 350 Loss 1.2733 Accuracy 0.5872\n",
            "Epoch 16 Batch 400 Loss 1.2762 Accuracy 0.5863\n",
            "Epoch 16 Batch 450 Loss 1.2777 Accuracy 0.5858\n",
            "Epoch 16 Batch 500 Loss 1.2793 Accuracy 0.5852\n",
            "Epoch 16 Batch 550 Loss 1.2802 Accuracy 0.5849\n",
            "Epoch 16 Batch 600 Loss 1.2805 Accuracy 0.5847\n",
            "Epoch 16 Batch 650 Loss 1.2805 Accuracy 0.5845\n",
            "Epoch 16 Loss 1.2800 Accuracy 0.5846\n",
            "Time taken for 1 epoch: 27.57 secs\n",
            "Epoch 17 Batch 0 Loss 1.3313 Accuracy 0.5727\n",
            "Epoch 17 Batch 50 Loss 1.2632 Accuracy 0.5898\n",
            "Epoch 17 Batch 100 Loss 1.2609 Accuracy 0.5905\n",
            "Epoch 17 Batch 150 Loss 1.2598 Accuracy 0.5907\n",
            "Epoch 17 Batch 200 Loss 1.2642 Accuracy 0.5892\n",
            "Epoch 17 Batch 250 Loss 1.2663 Accuracy 0.5886\n",
            "Epoch 17 Batch 300 Loss 1.2658 Accuracy 0.5888\n",
            "Epoch 17 Batch 350 Loss 1.2676 Accuracy 0.5882\n",
            "Epoch 17 Batch 400 Loss 1.2709 Accuracy 0.5873\n",
            "Epoch 17 Batch 450 Loss 1.2720 Accuracy 0.5870\n",
            "Epoch 17 Batch 500 Loss 1.2726 Accuracy 0.5868\n",
            "Epoch 17 Batch 550 Loss 1.2734 Accuracy 0.5865\n",
            "Epoch 17 Batch 600 Loss 1.2738 Accuracy 0.5864\n",
            "Epoch 17 Batch 650 Loss 1.2739 Accuracy 0.5863\n",
            "Epoch 17 Loss 1.2739 Accuracy 0.5862\n",
            "Time taken for 1 epoch: 27.73 secs\n",
            "Epoch 18 Batch 0 Loss 1.3135 Accuracy 0.5664\n",
            "Epoch 18 Batch 50 Loss 1.2601 Accuracy 0.5905\n",
            "Epoch 18 Batch 100 Loss 1.2531 Accuracy 0.5931\n",
            "Epoch 18 Batch 150 Loss 1.2530 Accuracy 0.5932\n",
            "Epoch 18 Batch 200 Loss 1.2582 Accuracy 0.5913\n",
            "Epoch 18 Batch 250 Loss 1.2612 Accuracy 0.5904\n",
            "Epoch 18 Batch 300 Loss 1.2616 Accuracy 0.5904\n",
            "Epoch 18 Batch 350 Loss 1.2624 Accuracy 0.5902\n",
            "Epoch 18 Batch 400 Loss 1.2652 Accuracy 0.5893\n",
            "Epoch 18 Batch 450 Loss 1.2666 Accuracy 0.5888\n",
            "Epoch 18 Batch 500 Loss 1.2677 Accuracy 0.5885\n",
            "Epoch 18 Batch 550 Loss 1.2684 Accuracy 0.5881\n",
            "Epoch 18 Batch 600 Loss 1.2687 Accuracy 0.5880\n",
            "Epoch 18 Batch 650 Loss 1.2691 Accuracy 0.5877\n",
            "Epoch 18 Loss 1.2690 Accuracy 0.5876\n",
            "Time taken for 1 epoch: 27.09 secs\n",
            "Epoch 19 Batch 0 Loss 1.2630 Accuracy 0.5905\n",
            "Epoch 19 Batch 50 Loss 1.2594 Accuracy 0.5907\n",
            "Epoch 19 Batch 100 Loss 1.2526 Accuracy 0.5932\n",
            "Epoch 19 Batch 150 Loss 1.2508 Accuracy 0.5936\n",
            "Epoch 19 Batch 200 Loss 1.2545 Accuracy 0.5923\n",
            "Epoch 19 Batch 250 Loss 1.2566 Accuracy 0.5916\n",
            "Epoch 19 Batch 300 Loss 1.2567 Accuracy 0.5915\n",
            "Epoch 19 Batch 350 Loss 1.2589 Accuracy 0.5909\n",
            "Epoch 19 Batch 400 Loss 1.2616 Accuracy 0.5902\n",
            "Epoch 19 Batch 450 Loss 1.2629 Accuracy 0.5898\n",
            "Epoch 19 Batch 500 Loss 1.2634 Accuracy 0.5896\n",
            "Epoch 19 Batch 550 Loss 1.2637 Accuracy 0.5895\n",
            "Epoch 19 Batch 600 Loss 1.2643 Accuracy 0.5892\n",
            "Epoch 19 Batch 650 Loss 1.2645 Accuracy 0.5890\n",
            "Epoch 19 Loss 1.2644 Accuracy 0.5889\n",
            "Time taken for 1 epoch: 27.30 secs\n",
            "Epoch 20 Batch 0 Loss 1.3015 Accuracy 0.5735\n",
            "Epoch 20 Batch 50 Loss 1.2524 Accuracy 0.5925\n",
            "Epoch 20 Batch 100 Loss 1.2450 Accuracy 0.5948\n",
            "Epoch 20 Batch 150 Loss 1.2437 Accuracy 0.5952\n",
            "Epoch 20 Batch 200 Loss 1.2487 Accuracy 0.5935\n",
            "Epoch 20 Batch 250 Loss 1.2516 Accuracy 0.5926\n",
            "Epoch 20 Batch 300 Loss 1.2514 Accuracy 0.5929\n",
            "Epoch 20 Batch 350 Loss 1.2540 Accuracy 0.5922\n",
            "Epoch 20 Batch 400 Loss 1.2564 Accuracy 0.5917\n",
            "Epoch 20 Batch 450 Loss 1.2577 Accuracy 0.5912\n",
            "Epoch 20 Batch 500 Loss 1.2586 Accuracy 0.5910\n",
            "Epoch 20 Batch 550 Loss 1.2597 Accuracy 0.5906\n",
            "Epoch 20 Batch 600 Loss 1.2605 Accuracy 0.5902\n",
            "Epoch 20 Batch 650 Loss 1.2608 Accuracy 0.5900\n",
            "Epoch 20 Loss 1.2604 Accuracy 0.5901\n",
            "Time taken for 1 epoch: 27.46 secs\n",
            "\n",
            "----- Text generated after epoch 20 -----\n",
            "\n",
            "ROMEO: Here is to go. I was described and more discontentious burthen did follow my knave. O, thus not with my breast daughter! let me enforce excellence go to ship; and let her love to the sea-on the right of her, but what loved for worthy daughters, as la\n",
            "\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.3004 Accuracy 0.5731\n",
            "Epoch 21 Batch 50 Loss 1.2437 Accuracy 0.5951\n",
            "Epoch 21 Batch 100 Loss 1.2394 Accuracy 0.5966\n",
            "Epoch 21 Batch 150 Loss 1.2397 Accuracy 0.5964\n",
            "Epoch 21 Batch 200 Loss 1.2431 Accuracy 0.5953\n",
            "Epoch 21 Batch 250 Loss 1.2461 Accuracy 0.5943\n",
            "Epoch 21 Batch 300 Loss 1.2466 Accuracy 0.5943\n",
            "Epoch 21 Batch 350 Loss 1.2490 Accuracy 0.5936\n",
            "Epoch 21 Batch 400 Loss 1.2520 Accuracy 0.5928\n",
            "Epoch 21 Batch 450 Loss 1.2537 Accuracy 0.5924\n",
            "Epoch 21 Batch 500 Loss 1.2548 Accuracy 0.5921\n",
            "Epoch 21 Batch 550 Loss 1.2556 Accuracy 0.5918\n",
            "Epoch 21 Batch 600 Loss 1.2560 Accuracy 0.5915\n",
            "Epoch 21 Batch 650 Loss 1.2561 Accuracy 0.5913\n",
            "Epoch 21 Loss 1.2559 Accuracy 0.5913\n",
            "Time taken for 1 epoch: 27.47 secs\n",
            "Epoch 22 Batch 0 Loss 1.2537 Accuracy 0.5882\n",
            "Epoch 22 Batch 50 Loss 1.2448 Accuracy 0.5947\n",
            "Epoch 22 Batch 100 Loss 1.2375 Accuracy 0.5976\n",
            "Epoch 22 Batch 150 Loss 1.2378 Accuracy 0.5974\n",
            "Epoch 22 Batch 200 Loss 1.2410 Accuracy 0.5962\n",
            "Epoch 22 Batch 250 Loss 1.2440 Accuracy 0.5952\n",
            "Epoch 22 Batch 300 Loss 1.2437 Accuracy 0.5953\n",
            "Epoch 22 Batch 350 Loss 1.2459 Accuracy 0.5947\n",
            "Epoch 22 Batch 400 Loss 1.2486 Accuracy 0.5938\n",
            "Epoch 22 Batch 450 Loss 1.2499 Accuracy 0.5933\n",
            "Epoch 22 Batch 500 Loss 1.2515 Accuracy 0.5928\n",
            "Epoch 22 Batch 550 Loss 1.2522 Accuracy 0.5926\n",
            "Epoch 22 Batch 600 Loss 1.2525 Accuracy 0.5924\n",
            "Epoch 22 Batch 650 Loss 1.2529 Accuracy 0.5922\n",
            "Epoch 22 Loss 1.2524 Accuracy 0.5922\n",
            "Time taken for 1 epoch: 27.80 secs\n",
            "Epoch 23 Batch 0 Loss 1.2658 Accuracy 0.5812\n",
            "Epoch 23 Batch 50 Loss 1.2423 Accuracy 0.5955\n",
            "Epoch 23 Batch 100 Loss 1.2347 Accuracy 0.5981\n",
            "Epoch 23 Batch 150 Loss 1.2352 Accuracy 0.5977\n",
            "Epoch 23 Batch 200 Loss 1.2380 Accuracy 0.5968\n",
            "Epoch 23 Batch 250 Loss 1.2400 Accuracy 0.5961\n",
            "Epoch 23 Batch 300 Loss 1.2404 Accuracy 0.5960\n",
            "Epoch 23 Batch 350 Loss 1.2422 Accuracy 0.5955\n",
            "Epoch 23 Batch 400 Loss 1.2455 Accuracy 0.5946\n",
            "Epoch 23 Batch 450 Loss 1.2467 Accuracy 0.5942\n",
            "Epoch 23 Batch 500 Loss 1.2473 Accuracy 0.5939\n",
            "Epoch 23 Batch 550 Loss 1.2481 Accuracy 0.5937\n",
            "Epoch 23 Batch 600 Loss 1.2487 Accuracy 0.5934\n",
            "Epoch 23 Batch 650 Loss 1.2490 Accuracy 0.5933\n",
            "Epoch 23 Loss 1.2490 Accuracy 0.5932\n",
            "Time taken for 1 epoch: 27.06 secs\n",
            "Epoch 24 Batch 0 Loss 1.2696 Accuracy 0.5870\n",
            "Epoch 24 Batch 50 Loss 1.2335 Accuracy 0.5980\n",
            "Epoch 24 Batch 100 Loss 1.2303 Accuracy 0.6003\n",
            "Epoch 24 Batch 150 Loss 1.2290 Accuracy 0.6001\n",
            "Epoch 24 Batch 200 Loss 1.2324 Accuracy 0.5987\n",
            "Epoch 24 Batch 250 Loss 1.2357 Accuracy 0.5975\n",
            "Epoch 24 Batch 300 Loss 1.2358 Accuracy 0.5975\n",
            "Epoch 24 Batch 350 Loss 1.2379 Accuracy 0.5969\n",
            "Epoch 24 Batch 400 Loss 1.2406 Accuracy 0.5959\n",
            "Epoch 24 Batch 450 Loss 1.2425 Accuracy 0.5954\n",
            "Epoch 24 Batch 500 Loss 1.2436 Accuracy 0.5950\n",
            "Epoch 24 Batch 550 Loss 1.2444 Accuracy 0.5948\n",
            "Epoch 24 Batch 600 Loss 1.2450 Accuracy 0.5945\n",
            "Epoch 24 Batch 650 Loss 1.2451 Accuracy 0.5944\n",
            "Epoch 24 Loss 1.2450 Accuracy 0.5944\n",
            "Time taken for 1 epoch: 27.21 secs\n",
            "Epoch 25 Batch 0 Loss 1.2844 Accuracy 0.5819\n",
            "Epoch 25 Batch 50 Loss 1.2290 Accuracy 0.6000\n",
            "Epoch 25 Batch 100 Loss 1.2234 Accuracy 0.6021\n",
            "Epoch 25 Batch 150 Loss 1.2247 Accuracy 0.6012\n",
            "Epoch 25 Batch 200 Loss 1.2290 Accuracy 0.5997\n",
            "Epoch 25 Batch 250 Loss 1.2324 Accuracy 0.5984\n",
            "Epoch 25 Batch 300 Loss 1.2333 Accuracy 0.5983\n",
            "Epoch 25 Batch 350 Loss 1.2354 Accuracy 0.5978\n",
            "Epoch 25 Batch 400 Loss 1.2384 Accuracy 0.5968\n",
            "Epoch 25 Batch 450 Loss 1.2400 Accuracy 0.5962\n",
            "Epoch 25 Batch 500 Loss 1.2412 Accuracy 0.5958\n",
            "Epoch 25 Batch 550 Loss 1.2422 Accuracy 0.5955\n",
            "Epoch 25 Batch 600 Loss 1.2424 Accuracy 0.5954\n",
            "Epoch 25 Batch 650 Loss 1.2422 Accuracy 0.5954\n",
            "Epoch 25 Loss 1.2422 Accuracy 0.5952\n",
            "Time taken for 1 epoch: 27.46 secs\n",
            "\n",
            "----- Text generated after epoch 25 -----\n",
            "\n",
            "ROMEO: he hath touch'd his prophet, by heaven and false the worse; boldly, they are partial company, which he should die on his body would for mine: I have round out in our pardon? For this barber known, you shall please you to come to this bones, is appear\n",
            "\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.2878 Accuracy 0.5768\n",
            "Epoch 26 Batch 50 Loss 1.2300 Accuracy 0.5988\n",
            "Epoch 26 Batch 100 Loss 1.2219 Accuracy 0.6017\n",
            "Epoch 26 Batch 150 Loss 1.2224 Accuracy 0.6014\n",
            "Epoch 26 Batch 200 Loss 1.2262 Accuracy 0.6000\n",
            "Epoch 26 Batch 250 Loss 1.2292 Accuracy 0.5994\n",
            "Epoch 26 Batch 300 Loss 1.2298 Accuracy 0.5994\n",
            "Epoch 26 Batch 350 Loss 1.2320 Accuracy 0.5988\n",
            "Epoch 26 Batch 400 Loss 1.2356 Accuracy 0.5977\n",
            "Epoch 26 Batch 450 Loss 1.2370 Accuracy 0.5971\n",
            "Epoch 26 Batch 500 Loss 1.2376 Accuracy 0.5969\n",
            "Epoch 26 Batch 550 Loss 1.2382 Accuracy 0.5967\n",
            "Epoch 26 Batch 600 Loss 1.2389 Accuracy 0.5964\n",
            "Epoch 26 Batch 650 Loss 1.2391 Accuracy 0.5963\n",
            "Epoch 26 Loss 1.2390 Accuracy 0.5962\n",
            "Time taken for 1 epoch: 27.47 secs\n",
            "Epoch 27 Batch 0 Loss 1.2942 Accuracy 0.5754\n",
            "Epoch 27 Batch 50 Loss 1.2212 Accuracy 0.6023\n",
            "Epoch 27 Batch 100 Loss 1.2186 Accuracy 0.6029\n",
            "Epoch 27 Batch 150 Loss 1.2185 Accuracy 0.6027\n",
            "Epoch 27 Batch 200 Loss 1.2223 Accuracy 0.6011\n",
            "Epoch 27 Batch 250 Loss 1.2253 Accuracy 0.6002\n",
            "Epoch 27 Batch 300 Loss 1.2270 Accuracy 0.5997\n",
            "Epoch 27 Batch 350 Loss 1.2295 Accuracy 0.5990\n",
            "Epoch 27 Batch 400 Loss 1.2324 Accuracy 0.5982\n",
            "Epoch 27 Batch 450 Loss 1.2334 Accuracy 0.5978\n",
            "Epoch 27 Batch 500 Loss 1.2345 Accuracy 0.5975\n",
            "Epoch 27 Batch 550 Loss 1.2350 Accuracy 0.5973\n",
            "Epoch 27 Batch 600 Loss 1.2357 Accuracy 0.5970\n",
            "Epoch 27 Batch 650 Loss 1.2362 Accuracy 0.5968\n",
            "Epoch 27 Loss 1.2360 Accuracy 0.5968\n",
            "Time taken for 1 epoch: 27.77 secs\n",
            "Epoch 28 Batch 0 Loss 1.2902 Accuracy 0.5797\n",
            "Epoch 28 Batch 50 Loss 1.2243 Accuracy 0.6009\n",
            "Epoch 28 Batch 100 Loss 1.2174 Accuracy 0.6032\n",
            "Epoch 28 Batch 150 Loss 1.2170 Accuracy 0.6033\n",
            "Epoch 28 Batch 200 Loss 1.2205 Accuracy 0.6018\n",
            "Epoch 28 Batch 250 Loss 1.2230 Accuracy 0.6008\n",
            "Epoch 28 Batch 300 Loss 1.2238 Accuracy 0.6006\n",
            "Epoch 28 Batch 350 Loss 1.2265 Accuracy 0.5999\n",
            "Epoch 28 Batch 400 Loss 1.2291 Accuracy 0.5993\n",
            "Epoch 28 Batch 450 Loss 1.2307 Accuracy 0.5988\n",
            "Epoch 28 Batch 500 Loss 1.2317 Accuracy 0.5984\n",
            "Epoch 28 Batch 550 Loss 1.2325 Accuracy 0.5981\n",
            "Epoch 28 Batch 600 Loss 1.2332 Accuracy 0.5979\n",
            "Epoch 28 Batch 650 Loss 1.2334 Accuracy 0.5977\n",
            "Epoch 28 Loss 1.2336 Accuracy 0.5977\n",
            "Time taken for 1 epoch: 27.08 secs\n",
            "Epoch 29 Batch 0 Loss 1.2658 Accuracy 0.5781\n",
            "Epoch 29 Batch 50 Loss 1.2230 Accuracy 0.6012\n",
            "Epoch 29 Batch 100 Loss 1.2155 Accuracy 0.6039\n",
            "Epoch 29 Batch 150 Loss 1.2135 Accuracy 0.6043\n",
            "Epoch 29 Batch 200 Loss 1.2179 Accuracy 0.6028\n",
            "Epoch 29 Batch 250 Loss 1.2209 Accuracy 0.6018\n",
            "Epoch 29 Batch 300 Loss 1.2212 Accuracy 0.6018\n",
            "Epoch 29 Batch 350 Loss 1.2239 Accuracy 0.6010\n",
            "Epoch 29 Batch 400 Loss 1.2267 Accuracy 0.6001\n",
            "Epoch 29 Batch 450 Loss 1.2282 Accuracy 0.5996\n",
            "Epoch 29 Batch 500 Loss 1.2286 Accuracy 0.5993\n",
            "Epoch 29 Batch 550 Loss 1.2298 Accuracy 0.5989\n",
            "Epoch 29 Batch 600 Loss 1.2305 Accuracy 0.5986\n",
            "Epoch 29 Batch 650 Loss 1.2306 Accuracy 0.5986\n",
            "Epoch 29 Loss 1.2308 Accuracy 0.5984\n",
            "Time taken for 1 epoch: 27.23 secs\n",
            "Epoch 30 Batch 0 Loss 1.2233 Accuracy 0.5944\n",
            "Epoch 30 Batch 50 Loss 1.2224 Accuracy 0.6015\n",
            "Epoch 30 Batch 100 Loss 1.2140 Accuracy 0.6035\n",
            "Epoch 30 Batch 150 Loss 1.2125 Accuracy 0.6042\n",
            "Epoch 30 Batch 200 Loss 1.2160 Accuracy 0.6030\n",
            "Epoch 30 Batch 250 Loss 1.2189 Accuracy 0.6019\n",
            "Epoch 30 Batch 300 Loss 1.2193 Accuracy 0.6019\n",
            "Epoch 30 Batch 350 Loss 1.2218 Accuracy 0.6010\n",
            "Epoch 30 Batch 400 Loss 1.2244 Accuracy 0.6003\n",
            "Epoch 30 Batch 450 Loss 1.2255 Accuracy 0.6000\n",
            "Epoch 30 Batch 500 Loss 1.2265 Accuracy 0.5997\n",
            "Epoch 30 Batch 550 Loss 1.2278 Accuracy 0.5993\n",
            "Epoch 30 Batch 600 Loss 1.2282 Accuracy 0.5991\n",
            "Epoch 30 Batch 650 Loss 1.2282 Accuracy 0.5991\n",
            "Epoch 30 Loss 1.2283 Accuracy 0.5990\n",
            "Time taken for 1 epoch: 27.47 secs\n",
            "\n",
            "----- Text generated after epoch 30 -----\n",
            "\n",
            "ROMEO: with redression, you must sit at make him an in the sure of the sorrow. My father was with the humble paved up, and this way to the irrect of the stand, the ground, as well are the thought. What is the married hard man, how thou shalt die thee, and i\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    # Reset metrics at start of each epoch\n",
        "    train_loss.reset_state()\n",
        "    train_accuracy.reset_state()\n",
        "\n",
        "    for batch, inp in enumerate(dataset):\n",
        "        # The train_step now updates metrics internally\n",
        "        loss = train_step(inp)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(\n",
        "                f\"Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} \"\n",
        "                f\"Accuracy {train_accuracy.result():.4f}\"\n",
        "            )\n",
        "\n",
        "    # Print epoch results\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1} Loss {train_loss.result():.4f} \"\n",
        "        f\"Accuracy {train_accuracy.result():.4f}\"\n",
        "    )\n",
        "    print(f\"Time taken for 1 epoch: {time.time() - start:.2f} secs\")\n",
        "\n",
        "    # Generate text every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"\\n----- Text generated after epoch {epoch + 1} -----\\n\")\n",
        "        start_string = \"ROMEO: \"\n",
        "        generated_text = generate_text(\n",
        "            model, start_string, num_generate=250, temperature=0.8\n",
        "        )\n",
        "        print(generated_text)\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Save the model\n",
        "# model.save_weights(\"shakespeare_transformer/model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ogxK74r49g"
      },
      "source": [
        "### Generate text after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GUgRTpyHr49g",
        "outputId": "ab65894e-f370-4628-c010-14c4cb97d631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Text generated after training -----\n",
            "\n",
            "ROMEO: and then match we do you, sir: but become we may not be; the crimit it is not in your army, and be it much to make speed you from me to the course. The mayor come to the poor Bianca, let's see him place as the part, which I vow was well but passage against the prison? My wonder, thou art my true spring in thy face on my proclaim, and death I see thy grace to think in the first. And, O, to what have you been so much a robbed lord, or I am a contented merry subject. How now chaste this? What sport\n"
          ]
        }
      ],
      "source": [
        "# Generate final text\n",
        "print(\"\\n----- Text generated after training -----\\n\")\n",
        "start_string = \"ROMEO: \"\n",
        "generated_text = generate_text(model, start_string, num_generate=500, temperature=0.7)\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}