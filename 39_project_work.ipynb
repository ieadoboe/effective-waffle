{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d492e463",
   "metadata": {},
   "source": [
    "## AI-powered Code Autocompletion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f0ed0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d0e86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_generator(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dced5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"code_search_net/python/final/jsonl/train/python_train_0.jsonl\"\n",
    "test_path = \"code_search_net/python/final/jsonl/test/python_test_0.jsonl\"\n",
    "valid_path = \"code_search_net/python/final/jsonl/valid/python_valid_0.jsonl\"\n",
    "\n",
    "# Example usage\n",
    "for obj in jsonl_generator(train_path):\n",
    "    # process each JSON object\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6eb2d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_code_examples(filepath, num_examples=5):\n",
    "    count = 0\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                example = json.loads(line)\n",
    "                print(f\"\\n--- Example {count+1} ---\")\n",
    "                print(f\"Function name: {example.get('func_name', 'N/A')}\")\n",
    "                print(\n",
    "                    f\"Docstring: {example.get('docstring', 'N/A')[:100]}...\"\n",
    "                )  # Print first 100 chars\n",
    "                print(f\"Code:\\n{example.get('code', 'N/A')}\")\n",
    "                count += 1\n",
    "                if count >= num_examples:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba5f3a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EXAMPLES:\n",
      "\n",
      "--- Example 1 ---\n",
      "Function name: train\n",
      "Docstring: Trains a k-nearest neighbors classifier for face recognition.\n",
      "\n",
      "    :param train_dir: directory that ...\n",
      "Code:\n",
      "def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n",
      "    \"\"\"\n",
      "    Trains a k-nearest neighbors classifier for face recognition.\n",
      "\n",
      "    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n",
      "\n",
      "     (View in source code to see train_dir example tree structure)\n",
      "\n",
      "     Structure:\n",
      "        <train_dir>/\n",
      "        ├── <person1>/\n",
      "        │   ├── <somename1>.jpeg\n",
      "        │   ├── <somename2>.jpeg\n",
      "        │   ├── ...\n",
      "        ├── <person2>/\n",
      "        │   ├── <somename1>.jpeg\n",
      "        │   └── <somename2>.jpeg\n",
      "        └── ...\n",
      "\n",
      "    :param model_save_path: (optional) path to save model on disk\n",
      "    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n",
      "    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n",
      "    :param verbose: verbosity of training\n",
      "    :return: returns knn classifier that was trained on the given data.\n",
      "    \"\"\"\n",
      "    X = []\n",
      "    y = []\n",
      "\n",
      "    # Loop through each person in the training set\n",
      "    for class_dir in os.listdir(train_dir):\n",
      "        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n",
      "            continue\n",
      "\n",
      "        # Loop through each training image for the current person\n",
      "        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n",
      "            image = face_recognition.load_image_file(img_path)\n",
      "            face_bounding_boxes = face_recognition.face_locations(image)\n",
      "\n",
      "            if len(face_bounding_boxes) != 1:\n",
      "                # If there are no people (or too many people) in a training image, skip the image.\n",
      "                if verbose:\n",
      "                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n",
      "            else:\n",
      "                # Add face encoding for current image to the training set\n",
      "                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n",
      "                y.append(class_dir)\n",
      "\n",
      "    # Determine how many neighbors to use for weighting in the KNN classifier\n",
      "    if n_neighbors is None:\n",
      "        n_neighbors = int(round(math.sqrt(len(X))))\n",
      "        if verbose:\n",
      "            print(\"Chose n_neighbors automatically:\", n_neighbors)\n",
      "\n",
      "    # Create and train the KNN classifier\n",
      "    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n",
      "    knn_clf.fit(X, y)\n",
      "\n",
      "    # Save the trained KNN classifier\n",
      "    if model_save_path is not None:\n",
      "        with open(model_save_path, 'wb') as f:\n",
      "            pickle.dump(knn_clf, f)\n",
      "\n",
      "    return knn_clf\n",
      "\n",
      "--- Example 2 ---\n",
      "Function name: predict\n",
      "Docstring: Recognizes faces in given image using a trained KNN classifier\n",
      "\n",
      "    :param X_img_path: path to image...\n",
      "Code:\n",
      "def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\n",
      "    \"\"\"\n",
      "    Recognizes faces in given image using a trained KNN classifier\n",
      "\n",
      "    :param X_img_path: path to image to be recognized\n",
      "    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n",
      "    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n",
      "    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n",
      "           of mis-classifying an unknown person as a known one.\n",
      "    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n",
      "        For faces of unrecognized persons, the name 'unknown' will be returned.\n",
      "    \"\"\"\n",
      "    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\n",
      "        raise Exception(\"Invalid image path: {}\".format(X_img_path))\n",
      "\n",
      "    if knn_clf is None and model_path is None:\n",
      "        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\n",
      "\n",
      "    # Load a trained KNN model (if one was passed in)\n",
      "    if knn_clf is None:\n",
      "        with open(model_path, 'rb') as f:\n",
      "            knn_clf = pickle.load(f)\n",
      "\n",
      "    # Load image file and find face locations\n",
      "    X_img = face_recognition.load_image_file(X_img_path)\n",
      "    X_face_locations = face_recognition.face_locations(X_img)\n",
      "\n",
      "    # If no faces are found in the image, return an empty result.\n",
      "    if len(X_face_locations) == 0:\n",
      "        return []\n",
      "\n",
      "    # Find encodings for faces in the test iamge\n",
      "    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\n",
      "\n",
      "    # Use the KNN model to find the best matches for the test face\n",
      "    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\n",
      "    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\n",
      "\n",
      "    # Predict classes and remove classifications that aren't within the threshold\n",
      "    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]\n",
      "\n",
      "--- Example 3 ---\n",
      "Function name: show_prediction_labels_on_image\n",
      "Docstring: Shows the face recognition results visually.\n",
      "\n",
      "    :param img_path: path to image to be recognized\n",
      "  ...\n",
      "Code:\n",
      "def show_prediction_labels_on_image(img_path, predictions):\n",
      "    \"\"\"\n",
      "    Shows the face recognition results visually.\n",
      "\n",
      "    :param img_path: path to image to be recognized\n",
      "    :param predictions: results of the predict function\n",
      "    :return:\n",
      "    \"\"\"\n",
      "    pil_image = Image.open(img_path).convert(\"RGB\")\n",
      "    draw = ImageDraw.Draw(pil_image)\n",
      "\n",
      "    for name, (top, right, bottom, left) in predictions:\n",
      "        # Draw a box around the face using the Pillow module\n",
      "        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n",
      "\n",
      "        # There's a bug in Pillow where it blows up with non-UTF-8 text\n",
      "        # when using the default bitmap font\n",
      "        name = name.encode(\"UTF-8\")\n",
      "\n",
      "        # Draw a label with a name below the face\n",
      "        text_width, text_height = draw.textsize(name)\n",
      "        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n",
      "        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n",
      "\n",
      "    # Remove the drawing library from memory as per the Pillow docs\n",
      "    del draw\n",
      "\n",
      "    # Display the resulting image\n",
      "    pil_image.show()\n"
     ]
    }
   ],
   "source": [
    "# Print 3 examples from the training set\n",
    "print(\"TRAINING EXAMPLES:\")\n",
    "print_code_examples(train_path, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "218f63f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION EXAMPLES:\n",
      "\n",
      "--- Example 1 ---\n",
      "Function name: learn\n",
      "Docstring: Train a deepq model.\n",
      "\n",
      "    Parameters\n",
      "    -------\n",
      "    env: gym.Env\n",
      "        environment to train on\n",
      "  ...\n",
      "Code:\n",
      "def learn(env,\n",
      "          network,\n",
      "          seed=None,\n",
      "          lr=5e-4,\n",
      "          total_timesteps=100000,\n",
      "          buffer_size=50000,\n",
      "          exploration_fraction=0.1,\n",
      "          exploration_final_eps=0.02,\n",
      "          train_freq=1,\n",
      "          batch_size=32,\n",
      "          print_freq=100,\n",
      "          checkpoint_freq=10000,\n",
      "          checkpoint_path=None,\n",
      "          learning_starts=1000,\n",
      "          gamma=1.0,\n",
      "          target_network_update_freq=500,\n",
      "          prioritized_replay=False,\n",
      "          prioritized_replay_alpha=0.6,\n",
      "          prioritized_replay_beta0=0.4,\n",
      "          prioritized_replay_beta_iters=None,\n",
      "          prioritized_replay_eps=1e-6,\n",
      "          param_noise=False,\n",
      "          callback=None,\n",
      "          load_path=None,\n",
      "          **network_kwargs\n",
      "            ):\n",
      "    \"\"\"Train a deepq model.\n",
      "\n",
      "    Parameters\n",
      "    -------\n",
      "    env: gym.Env\n",
      "        environment to train on\n",
      "    network: string or a function\n",
      "        neural network to use as a q function approximator. If string, has to be one of the names of registered models in baselines.common.models\n",
      "        (mlp, cnn, conv_only). If a function, should take an observation tensor and return a latent variable tensor, which\n",
      "        will be mapped to the Q function heads (see build_q_func in baselines.deepq.models for details on that)\n",
      "    seed: int or None\n",
      "        prng seed. The runs with the same seed \"should\" give the same results. If None, no seeding is used.\n",
      "    lr: float\n",
      "        learning rate for adam optimizer\n",
      "    total_timesteps: int\n",
      "        number of env steps to optimizer for\n",
      "    buffer_size: int\n",
      "        size of the replay buffer\n",
      "    exploration_fraction: float\n",
      "        fraction of entire training period over which the exploration rate is annealed\n",
      "    exploration_final_eps: float\n",
      "        final value of random action probability\n",
      "    train_freq: int\n",
      "        update the model every `train_freq` steps.\n",
      "        set to None to disable printing\n",
      "    batch_size: int\n",
      "        size of a batched sampled from replay buffer for training\n",
      "    print_freq: int\n",
      "        how often to print out training progress\n",
      "        set to None to disable printing\n",
      "    checkpoint_freq: int\n",
      "        how often to save the model. This is so that the best version is restored\n",
      "        at the end of the training. If you do not wish to restore the best version at\n",
      "        the end of the training set this variable to None.\n",
      "    learning_starts: int\n",
      "        how many steps of the model to collect transitions for before learning starts\n",
      "    gamma: float\n",
      "        discount factor\n",
      "    target_network_update_freq: int\n",
      "        update the target network every `target_network_update_freq` steps.\n",
      "    prioritized_replay: True\n",
      "        if True prioritized replay buffer will be used.\n",
      "    prioritized_replay_alpha: float\n",
      "        alpha parameter for prioritized replay buffer\n",
      "    prioritized_replay_beta0: float\n",
      "        initial value of beta for prioritized replay buffer\n",
      "    prioritized_replay_beta_iters: int\n",
      "        number of iterations over which beta will be annealed from initial value\n",
      "        to 1.0. If set to None equals to total_timesteps.\n",
      "    prioritized_replay_eps: float\n",
      "        epsilon to add to the TD errors when updating priorities.\n",
      "    param_noise: bool\n",
      "        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n",
      "    callback: (locals, globals) -> None\n",
      "        function called at every steps with state of the algorithm.\n",
      "        If callback returns true training stops.\n",
      "    load_path: str\n",
      "        path to load the model from. (default: None)\n",
      "    **network_kwargs\n",
      "        additional keyword arguments to pass to the network builder.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    act: ActWrapper\n",
      "        Wrapper over act function. Adds ability to save it and load it.\n",
      "        See header of baselines/deepq/categorical.py for details on the act function.\n",
      "    \"\"\"\n",
      "    # Create all the functions necessary to train the model\n",
      "\n",
      "    sess = get_session()\n",
      "    set_global_seeds(seed)\n",
      "\n",
      "    q_func = build_q_func(network, **network_kwargs)\n",
      "\n",
      "    # capture the shape outside the closure so that the env object is not serialized\n",
      "    # by cloudpickle when serializing make_obs_ph\n",
      "\n",
      "    observation_space = env.observation_space\n",
      "    def make_obs_ph(name):\n",
      "        return ObservationInput(observation_space, name=name)\n",
      "\n",
      "    act, train, update_target, debug = deepq.build_train(\n",
      "        make_obs_ph=make_obs_ph,\n",
      "        q_func=q_func,\n",
      "        num_actions=env.action_space.n,\n",
      "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
      "        gamma=gamma,\n",
      "        grad_norm_clipping=10,\n",
      "        param_noise=param_noise\n",
      "    )\n",
      "\n",
      "    act_params = {\n",
      "        'make_obs_ph': make_obs_ph,\n",
      "        'q_func': q_func,\n",
      "        'num_actions': env.action_space.n,\n",
      "    }\n",
      "\n",
      "    act = ActWrapper(act, act_params)\n",
      "\n",
      "    # Create the replay buffer\n",
      "    if prioritized_replay:\n",
      "        replay_buffer = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
      "        if prioritized_replay_beta_iters is None:\n",
      "            prioritized_replay_beta_iters = total_timesteps\n",
      "        beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
      "                                       initial_p=prioritized_replay_beta0,\n",
      "                                       final_p=1.0)\n",
      "    else:\n",
      "        replay_buffer = ReplayBuffer(buffer_size)\n",
      "        beta_schedule = None\n",
      "    # Create the schedule for exploration starting from 1.\n",
      "    exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\n",
      "                                 initial_p=1.0,\n",
      "                                 final_p=exploration_final_eps)\n",
      "\n",
      "    # Initialize the parameters and copy them to the target network.\n",
      "    U.initialize()\n",
      "    update_target()\n",
      "\n",
      "    episode_rewards = [0.0]\n",
      "    saved_mean_reward = None\n",
      "    obs = env.reset()\n",
      "    reset = True\n",
      "\n",
      "    with tempfile.TemporaryDirectory() as td:\n",
      "        td = checkpoint_path or td\n",
      "\n",
      "        model_file = os.path.join(td, \"model\")\n",
      "        model_saved = False\n",
      "\n",
      "        if tf.train.latest_checkpoint(td) is not None:\n",
      "            load_variables(model_file)\n",
      "            logger.log('Loaded model from {}'.format(model_file))\n",
      "            model_saved = True\n",
      "        elif load_path is not None:\n",
      "            load_variables(load_path)\n",
      "            logger.log('Loaded model from {}'.format(load_path))\n",
      "\n",
      "\n",
      "        for t in range(total_timesteps):\n",
      "            if callback is not None:\n",
      "                if callback(locals(), globals()):\n",
      "                    break\n",
      "            # Take action and update exploration to the newest value\n",
      "            kwargs = {}\n",
      "            if not param_noise:\n",
      "                update_eps = exploration.value(t)\n",
      "                update_param_noise_threshold = 0.\n",
      "            else:\n",
      "                update_eps = 0.\n",
      "                # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n",
      "                # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n",
      "                # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n",
      "                # for detailed explanation.\n",
      "                update_param_noise_threshold = -np.log(1. - exploration.value(t) + exploration.value(t) / float(env.action_space.n))\n",
      "                kwargs['reset'] = reset\n",
      "                kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n",
      "                kwargs['update_param_noise_scale'] = True\n",
      "            action = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
      "            env_action = action\n",
      "            reset = False\n",
      "            new_obs, rew, done, _ = env.step(env_action)\n",
      "            # Store transition in the replay buffer.\n",
      "            replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
      "            obs = new_obs\n",
      "\n",
      "            episode_rewards[-1] += rew\n",
      "            if done:\n",
      "                obs = env.reset()\n",
      "                episode_rewards.append(0.0)\n",
      "                reset = True\n",
      "\n",
      "            if t > learning_starts and t % train_freq == 0:\n",
      "                # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
      "                if prioritized_replay:\n",
      "                    experience = replay_buffer.sample(batch_size, beta=beta_schedule.value(t))\n",
      "                    (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
      "                else:\n",
      "                    obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
      "                    weights, batch_idxes = np.ones_like(rewards), None\n",
      "                td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
      "                if prioritized_replay:\n",
      "                    new_priorities = np.abs(td_errors) + prioritized_replay_eps\n",
      "                    replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
      "\n",
      "            if t > learning_starts and t % target_network_update_freq == 0:\n",
      "                # Update target network periodically.\n",
      "                update_target()\n",
      "\n",
      "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
      "            num_episodes = len(episode_rewards)\n",
      "            if done and print_freq is not None and len(episode_rewards) % print_freq == 0:\n",
      "                logger.record_tabular(\"steps\", t)\n",
      "                logger.record_tabular(\"episodes\", num_episodes)\n",
      "                logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n",
      "                logger.record_tabular(\"% time spent exploring\", int(100 * exploration.value(t)))\n",
      "                logger.dump_tabular()\n",
      "\n",
      "            if (checkpoint_freq is not None and t > learning_starts and\n",
      "                    num_episodes > 100 and t % checkpoint_freq == 0):\n",
      "                if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward:\n",
      "                    if print_freq is not None:\n",
      "                        logger.log(\"Saving model due to mean reward increase: {} -> {}\".format(\n",
      "                                   saved_mean_reward, mean_100ep_reward))\n",
      "                    save_variables(model_file)\n",
      "                    model_saved = True\n",
      "                    saved_mean_reward = mean_100ep_reward\n",
      "        if model_saved:\n",
      "            if print_freq is not None:\n",
      "                logger.log(\"Restored model with mean reward: {}\".format(saved_mean_reward))\n",
      "            load_variables(model_file)\n",
      "\n",
      "    return act\n",
      "\n",
      "--- Example 2 ---\n",
      "Function name: ActWrapper.save_act\n",
      "Docstring: Save model to a pickle located at `path`...\n",
      "Code:\n",
      "def save_act(self, path=None):\n",
      "        \"\"\"Save model to a pickle located at `path`\"\"\"\n",
      "        if path is None:\n",
      "            path = os.path.join(logger.get_dir(), \"model.pkl\")\n",
      "\n",
      "        with tempfile.TemporaryDirectory() as td:\n",
      "            save_variables(os.path.join(td, \"model\"))\n",
      "            arc_name = os.path.join(td, \"packed.zip\")\n",
      "            with zipfile.ZipFile(arc_name, 'w') as zipf:\n",
      "                for root, dirs, files in os.walk(td):\n",
      "                    for fname in files:\n",
      "                        file_path = os.path.join(root, fname)\n",
      "                        if file_path != arc_name:\n",
      "                            zipf.write(file_path, os.path.relpath(file_path, td))\n",
      "            with open(arc_name, \"rb\") as f:\n",
      "                model_data = f.read()\n",
      "        with open(path, \"wb\") as f:\n",
      "            cloudpickle.dump((model_data, self._act_params), f)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVALIDATION EXAMPLES:\")\n",
    "print_code_examples(valid_path, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7ff56c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXAMPLES USING TARGET LIBRARIES:\n",
      "\n",
      "--- Example 1 using target libraries ---\n",
      "Function name: _trim_css_to_bounds\n",
      "Code:\n",
      "def _trim_css_to_bounds(css, image_shape):\n",
      "    \"\"\"\n",
      "    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n",
      "\n",
      "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    :param image_shape: numpy shape of the image array\n",
      "    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    \"\"\"\n",
      "    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n",
      "\n",
      "--- Example 2 using target libraries ---\n",
      "Function name: face_distance\n",
      "Code:\n",
      "def face_distance(face_encodings, face_to_compare):\n",
      "    \"\"\"\n",
      "    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n",
      "    for each comparison face. The distance tells you how similar the faces are.\n",
      "\n",
      "    :param faces: List of face encodings to compare\n",
      "    :param face_to_compare: A face encoding to compare against\n",
      "    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\n",
      "    \"\"\"\n",
      "    if len(face_encodings) == 0:\n",
      "        return np.empty((0))\n",
      "\n",
      "    return np.linalg.norm(face_encodings - face_to_compare, axis=1)\n",
      "\n",
      "--- Example 3 using target libraries ---\n",
      "Function name: load_image_file\n",
      "Code:\n",
      "def load_image_file(file, mode='RGB'):\n",
      "    \"\"\"\n",
      "    Loads an image file (.jpg, .png, etc) into a numpy array\n",
      "\n",
      "    :param file: image file name or file object to load\n",
      "    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n",
      "    :return: image contents as numpy array\n",
      "    \"\"\"\n",
      "    im = PIL.Image.open(file)\n",
      "    if mode:\n",
      "        im = im.convert(mode)\n",
      "    return np.array(im)\n"
     ]
    }
   ],
   "source": [
    "def print_filtered_examples(filepath, libraries=[\"numpy\"], num_examples=3):\n",
    "    count = 0\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                example = json.loads(line)\n",
    "                code = example.get(\"code\", \"\").lower()\n",
    "\n",
    "                # Check if any of the libraries are used in the code\n",
    "                if any(lib in code for lib in libraries):\n",
    "                    print(f\"\\n--- Example {count+1} using target libraries ---\")\n",
    "                    print(f\"Function name: {example.get('func_name', 'N/A')}\")\n",
    "                    print(f\"Code:\\n{example.get('code', 'N/A')}\")\n",
    "                    count += 1\n",
    "                    if count >= num_examples:\n",
    "                        break\n",
    "\n",
    "\n",
    "# Print examples that use our target libraries\n",
    "print(\"\\nEXAMPLES USING TARGET LIBRARIES:\")\n",
    "print_filtered_examples(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8161aef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 500 examples\n"
     ]
    }
   ],
   "source": [
    "def collect_training_data(\n",
    "    filepath, max_examples=1000, libraries=[\"tensorflow\", \"numpy\", \"transformers\"]\n",
    "):\n",
    "    examples = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                example = json.loads(line)\n",
    "                code = example.get(\"code\", \"\")\n",
    "\n",
    "                # Optionally filter for target libraries\n",
    "                if not libraries or any(lib in code.lower() for lib in libraries):\n",
    "                    examples.append(\n",
    "                        {\n",
    "                            \"function_name\": example.get(\"func_name\", \"\"),\n",
    "                            \"docstring\": example.get(\"docstring\", \"\"),\n",
    "                            \"code\": code,\n",
    "                            \"language\": example.get(\"language\", \"python\"),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                if len(examples) >= max_examples:\n",
    "                    break\n",
    "\n",
    "    print(f\"Collected {len(examples)} examples\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "# Collect training data\n",
    "training_data = collect_training_data(train_path, max_examples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbd3155b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'function_name': '_trim_css_to_bounds',\n",
       "  'docstring': 'Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\\n\\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\\n    :param image_shape: numpy shape of the image array\\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order',\n",
       "  'code': 'def _trim_css_to_bounds(css, image_shape):\\n    \"\"\"\\n    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\\n\\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\\n    :param image_shape: numpy shape of the image array\\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\\n    \"\"\"\\n    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'face_distance',\n",
       "  'docstring': \"Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\\n    for each comparison face. The distance tells you how similar the faces are.\\n\\n    :param faces: List of face encodings to compare\\n    :param face_to_compare: A face encoding to compare against\\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\",\n",
       "  'code': 'def face_distance(face_encodings, face_to_compare):\\n    \"\"\"\\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\\n    for each comparison face. The distance tells you how similar the faces are.\\n\\n    :param faces: List of face encodings to compare\\n    :param face_to_compare: A face encoding to compare against\\n    :return: A numpy ndarray with the distance for each face in the same order as the \\'faces\\' array\\n    \"\"\"\\n    if len(face_encodings) == 0:\\n        return np.empty((0))\\n\\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'load_image_file',\n",
       "  'docstring': \"Loads an image file (.jpg, .png, etc) into a numpy array\\n\\n    :param file: image file name or file object to load\\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\\n    :return: image contents as numpy array\",\n",
       "  'code': 'def load_image_file(file, mode=\\'RGB\\'):\\n    \"\"\"\\n    Loads an image file (.jpg, .png, etc) into a numpy array\\n\\n    :param file: image file name or file object to load\\n    :param mode: format to convert the image to. Only \\'RGB\\' (8-bit RGB, 3 channels) and \\'L\\' (black and white) are supported.\\n    :return: image contents as numpy array\\n    \"\"\"\\n    im = PIL.Image.open(file)\\n    if mode:\\n        im = im.convert(mode)\\n    return np.array(im)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_raw_face_locations',\n",
       "  'docstring': 'Returns an array of bounding boxes of human faces in a image\\n\\n    :param img: An image (as a numpy array)\\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\\n    :return: A list of dlib \\'rect\\' objects of found face locations',\n",
       "  'code': 'def _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\\n    \"\"\"\\n    Returns an array of bounding boxes of human faces in a image\\n\\n    :param img: An image (as a numpy array)\\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\\n    :return: A list of dlib \\'rect\\' objects of found face locations\\n    \"\"\"\\n    if model == \"cnn\":\\n        return cnn_face_detector(img, number_of_times_to_upsample)\\n    else:\\n        return face_detector(img, number_of_times_to_upsample)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'face_locations',\n",
       "  'docstring': 'Returns an array of bounding boxes of human faces in a image\\n\\n    :param img: An image (as a numpy array)\\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order',\n",
       "  'code': 'def face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\\n    \"\"\"\\n    Returns an array of bounding boxes of human faces in a image\\n\\n    :param img: An image (as a numpy array)\\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\\n    \"\"\"\\n    if model == \"cnn\":\\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\\n    else:\\n        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'batch_face_locations',\n",
       "  'docstring': \"Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\\n    If you are using a GPU, this can give you much faster results since the GPU\\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\\n\\n    :param img: A list of images (each as a numpy array)\\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\\n    :param batch_size: How many images to include in each GPU processing batch.\\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\",\n",
       "  'code': 'def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\\n    \"\"\"\\n    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\\n    If you are using a GPU, this can give you much faster results since the GPU\\n    can process batches of images at once. If you aren\\'t using a GPU, you don\\'t need this function.\\n\\n    :param img: A list of images (each as a numpy array)\\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\\n    :param batch_size: How many images to include in each GPU processing batch.\\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\\n    \"\"\"\\n    def convert_cnn_detections_to_css(detections):\\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\\n\\n    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\\n\\n    return list(map(convert_cnn_detections_to_css, raw_detections_batched))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DecisionTree.trainClassifier',\n",
       "  'docstring': 'Train a decision tree model for classification.\\n\\n        :param data:\\n          Training data: RDD of LabeledPoint. Labels should take values\\n          {0, 1, ..., numClasses-1}.\\n        :param numClasses:\\n          Number of classes for classification.\\n        :param categoricalFeaturesInfo:\\n          Map storing arity of categorical features. An entry (n -> k)\\n          indicates that feature n is categorical with k categories\\n          indexed from 0: {0, 1, ..., k-1}.\\n        :param impurity:\\n          Criterion used for information gain calculation.\\n          Supported values: \"gini\" or \"entropy\".\\n          (default: \"gini\")\\n        :param maxDepth:\\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\\n          means 1 internal node + 2 leaf nodes).\\n          (default: 5)\\n        :param maxBins:\\n          Number of bins used for finding splits at each node.\\n          (default: 32)\\n        :param minInstancesPerNode:\\n          Minimum number of instances required at child nodes to create\\n          the parent split.\\n          (default: 1)\\n        :param minInfoGain:\\n          Minimum info gain required to create a split.\\n          (default: 0.0)\\n        :return:\\n          DecisionTreeModel.\\n\\n        Example usage:\\n\\n        >>> from numpy import array\\n        >>> from pyspark.mllib.regression import LabeledPoint\\n        >>> from pyspark.mllib.tree import DecisionTree\\n        >>>\\n        >>> data = [\\n        ...     LabeledPoint(0.0, [0.0]),\\n        ...     LabeledPoint(1.0, [1.0]),\\n        ...     LabeledPoint(1.0, [2.0]),\\n        ...     LabeledPoint(1.0, [3.0])\\n        ... ]\\n        >>> model = DecisionTree.trainClassifier(sc.parallelize(data), 2, {})\\n        >>> print(model)\\n        DecisionTreeModel classifier of depth 1 with 3 nodes\\n\\n        >>> print(model.toDebugString())\\n        DecisionTreeModel classifier of depth 1 with 3 nodes\\n          If (feature 0 <= 0.5)\\n           Predict: 0.0\\n          Else (feature 0 > 0.5)\\n           Predict: 1.0\\n        <BLANKLINE>\\n        >>> model.predict(array([1.0]))\\n        1.0\\n        >>> model.predict(array([0.0]))\\n        0.0\\n        >>> rdd = sc.parallelize([[1.0], [0.0]])\\n        >>> model.predict(rdd).collect()\\n        [1.0, 0.0]',\n",
       "  'code': 'def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo,\\n                        impurity=\"gini\", maxDepth=5, maxBins=32, minInstancesPerNode=1,\\n                        minInfoGain=0.0):\\n        \"\"\"\\n        Train a decision tree model for classification.\\n\\n        :param data:\\n          Training data: RDD of LabeledPoint. Labels should take values\\n          {0, 1, ..., numClasses-1}.\\n        :param numClasses:\\n          Number of classes for classification.\\n        :param categoricalFeaturesInfo:\\n          Map storing arity of categorical features. An entry (n -> k)\\n          indicates that feature n is categorical with k categories\\n          indexed from 0: {0, 1, ..., k-1}.\\n        :param impurity:\\n          Criterion used for information gain calculation.\\n          Supported values: \"gini\" or \"entropy\".\\n          (default: \"gini\")\\n        :param maxDepth:\\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\\n          means 1 internal node + 2 leaf nodes).\\n          (default: 5)\\n        :param maxBins:\\n          Number of bins used for finding splits at each node.\\n          (default: 32)\\n        :param minInstancesPerNode:\\n          Minimum number of instances required at child nodes to create\\n          the parent split.\\n          (default: 1)\\n        :param minInfoGain:\\n          Minimum info gain required to create a split.\\n          (default: 0.0)\\n        :return:\\n          DecisionTreeModel.\\n\\n        Example usage:\\n\\n        >>> from numpy import array\\n        >>> from pyspark.mllib.regression import LabeledPoint\\n        >>> from pyspark.mllib.tree import DecisionTree\\n        >>>\\n        >>> data = [\\n        ...     LabeledPoint(0.0, [0.0]),\\n        ...     LabeledPoint(1.0, [1.0]),\\n        ...     LabeledPoint(1.0, [2.0]),\\n        ...     LabeledPoint(1.0, [3.0])\\n        ... ]\\n        >>> model = DecisionTree.trainClassifier(sc.parallelize(data), 2, {})\\n        >>> print(model)\\n        DecisionTreeModel classifier of depth 1 with 3 nodes\\n\\n        >>> print(model.toDebugString())\\n        DecisionTreeModel classifier of depth 1 with 3 nodes\\n          If (feature 0 <= 0.5)\\n           Predict: 0.0\\n          Else (feature 0 > 0.5)\\n           Predict: 1.0\\n        <BLANKLINE>\\n        >>> model.predict(array([1.0]))\\n        1.0\\n        >>> model.predict(array([0.0]))\\n        0.0\\n        >>> rdd = sc.parallelize([[1.0], [0.0]])\\n        >>> model.predict(rdd).collect()\\n        [1.0, 0.0]\\n        \"\"\"\\n        return cls._train(data, \"classification\", numClasses, categoricalFeaturesInfo,\\n                          impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_to_corrected_pandas_type',\n",
       "  'docstring': 'When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong.\\n    This method gets the corrected data type for Pandas if that type may be inferred uncorrectly.',\n",
       "  'code': 'def _to_corrected_pandas_type(dt):\\n    \"\"\"\\n    When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong.\\n    This method gets the corrected data type for Pandas if that type may be inferred uncorrectly.\\n    \"\"\"\\n    import numpy as np\\n    if type(dt) == ByteType:\\n        return np.int8\\n    elif type(dt) == ShortType:\\n        return np.int16\\n    elif type(dt) == IntegerType:\\n        return np.int32\\n    elif type(dt) == FloatType:\\n        return np.float32\\n    else:\\n        return None',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NaiveBayes.train',\n",
       "  'docstring': 'Train a Naive Bayes model given an RDD of (label, features)\\n        vectors.\\n\\n        This is the Multinomial NB (U{http://tinyurl.com/lsdw6p}) which\\n        can handle all kinds of discrete data.  For example, by\\n        converting documents into TF-IDF vectors, it can be used for\\n        document classification. By making every vector a 0-1 vector,\\n        it can also be used as Bernoulli NB (U{http://tinyurl.com/p7c96j6}).\\n        The input feature values must be nonnegative.\\n\\n        :param data:\\n          RDD of LabeledPoint.\\n        :param lambda_:\\n          The smoothing parameter.\\n          (default: 1.0)',\n",
       "  'code': 'def train(cls, data, lambda_=1.0):\\n        \"\"\"\\n        Train a Naive Bayes model given an RDD of (label, features)\\n        vectors.\\n\\n        This is the Multinomial NB (U{http://tinyurl.com/lsdw6p}) which\\n        can handle all kinds of discrete data.  For example, by\\n        converting documents into TF-IDF vectors, it can be used for\\n        document classification. By making every vector a 0-1 vector,\\n        it can also be used as Bernoulli NB (U{http://tinyurl.com/p7c96j6}).\\n        The input feature values must be nonnegative.\\n\\n        :param data:\\n          RDD of LabeledPoint.\\n        :param lambda_:\\n          The smoothing parameter.\\n          (default: 1.0)\\n        \"\"\"\\n        first = data.first()\\n        if not isinstance(first, LabeledPoint):\\n            raise ValueError(\"`data` should be an RDD of LabeledPoint\")\\n        labels, pi, theta = callMLlibFunc(\"trainNaiveBayesModel\", data, lambda_)\\n        return NaiveBayesModel(labels.toArray(), pi.toArray(), numpy.array(theta))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'pandas_udf',\n",
       "  'docstring': 'Creates a vectorized user defined function (UDF).\\n\\n    :param f: user-defined function. A python function if used as a standalone function\\n    :param returnType: the return type of the user-defined function. The value can be either a\\n        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\\n    :param functionType: an enum value in :class:`pyspark.sql.functions.PandasUDFType`.\\n                         Default: SCALAR.\\n\\n    .. note:: Experimental\\n\\n    The function type of the UDF can be one of the following:\\n\\n    1. SCALAR\\n\\n       A scalar UDF defines a transformation: One or more `pandas.Series` -> A `pandas.Series`.\\n       The length of the returned `pandas.Series` must be of the same as the input `pandas.Series`.\\n       If the return type is :class:`StructType`, the returned value should be a `pandas.DataFrame`.\\n\\n       :class:`MapType`, nested :class:`StructType` are currently not supported as output types.\\n\\n       Scalar UDFs are used with :meth:`pyspark.sql.DataFrame.withColumn` and\\n       :meth:`pyspark.sql.DataFrame.select`.\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> from pyspark.sql.types import IntegerType, StringType\\n       >>> slen = pandas_udf(lambda s: s.str.len(), IntegerType())  # doctest: +SKIP\\n       >>> @pandas_udf(StringType())  # doctest: +SKIP\\n       ... def to_upper(s):\\n       ...     return s.str.upper()\\n       ...\\n       >>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\\n       ... def add_one(x):\\n       ...     return x + 1\\n       ...\\n       >>> df = spark.createDataFrame([(1, \"John Doe\", 21)],\\n       ...                            (\"id\", \"name\", \"age\"))  # doctest: +SKIP\\n       >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")) \\\\\\\\\\n       ...     .show()  # doctest: +SKIP\\n       +----------+--------------+------------+\\n       |slen(name)|to_upper(name)|add_one(age)|\\n       +----------+--------------+------------+\\n       |         8|      JOHN DOE|          22|\\n       +----------+--------------+------------+\\n       >>> @pandas_udf(\"first string, last string\")  # doctest: +SKIP\\n       ... def split_expand(n):\\n       ...     return n.str.split(expand=True)\\n       >>> df.select(split_expand(\"name\")).show()  # doctest: +SKIP\\n       +------------------+\\n       |split_expand(name)|\\n       +------------------+\\n       |       [John, Doe]|\\n       +------------------+\\n\\n       .. note:: The length of `pandas.Series` within a scalar UDF is not that of the whole input\\n           column, but is the length of an internal batch used for each call to the function.\\n           Therefore, this can be used, for example, to ensure the length of each returned\\n           `pandas.Series`, and can not be used as the column length.\\n\\n    2. GROUPED_MAP\\n\\n       A grouped map UDF defines transformation: A `pandas.DataFrame` -> A `pandas.DataFrame`\\n       The returnType should be a :class:`StructType` describing the schema of the returned\\n       `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match\\n       the field names in the defined returnType schema if specified as strings, or match the\\n       field data types by position if not strings, e.g. integer indices.\\n       The length of the returned `pandas.DataFrame` can be arbitrary.\\n\\n       Grouped map UDFs are used with :meth:`pyspark.sql.GroupedData.apply`.\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))  # doctest: +SKIP\\n       >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\\n       ... def normalize(pdf):\\n       ...     v = pdf.v\\n       ...     return pdf.assign(v=(v - v.mean()) / v.std())\\n       >>> df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP\\n       +---+-------------------+\\n       | id|                  v|\\n       +---+-------------------+\\n       |  1|-0.7071067811865475|\\n       |  1| 0.7071067811865475|\\n       |  2|-0.8320502943378437|\\n       |  2|-0.2773500981126146|\\n       |  2| 1.1094003924504583|\\n       +---+-------------------+\\n\\n       Alternatively, the user can define a function that takes two arguments.\\n       In this case, the grouping key(s) will be passed as the first argument and the data will\\n       be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy\\n       data types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in\\n       as a `pandas.DataFrame` containing all columns from the original Spark DataFrame.\\n       This is useful when the user does not want to hardcode grouping key(s) in the function.\\n\\n       >>> import pandas as pd  # doctest: +SKIP\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))  # doctest: +SKIP\\n       >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\\n       ... def mean_udf(key, pdf):\\n       ...     # key is a tuple of one numpy.int64, which is the value\\n       ...     # of \\'id\\' for the current group\\n       ...     return pd.DataFrame([key + (pdf.v.mean(),)])\\n       >>> df.groupby(\\'id\\').apply(mean_udf).show()  # doctest: +SKIP\\n       +---+---+\\n       | id|  v|\\n       +---+---+\\n       |  1|1.5|\\n       |  2|6.0|\\n       +---+---+\\n       >>> @pandas_udf(\\n       ...    \"id long, `ceil(v / 2)` long, v double\",\\n       ...    PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\\n       >>> def sum_udf(key, pdf):\\n       ...     # key is a tuple of two numpy.int64s, which is the values\\n       ...     # of \\'id\\' and \\'ceil(df.v / 2)\\' for the current group\\n       ...     return pd.DataFrame([key + (pdf.v.sum(),)])\\n       >>> df.groupby(df.id, ceil(df.v / 2)).apply(sum_udf).show()  # doctest: +SKIP\\n       +---+-----------+----+\\n       | id|ceil(v / 2)|   v|\\n       +---+-----------+----+\\n       |  2|          5|10.0|\\n       |  1|          1| 3.0|\\n       |  2|          3| 5.0|\\n       |  2|          2| 3.0|\\n       +---+-----------+----+\\n\\n       .. note:: If returning a new `pandas.DataFrame` constructed with a dictionary, it is\\n           recommended to explicitly index the columns by name to ensure the positions are correct,\\n           or alternatively use an `OrderedDict`.\\n           For example, `pd.DataFrame({\\'id\\': ids, \\'a\\': data}, columns=[\\'id\\', \\'a\\'])` or\\n           `pd.DataFrame(OrderedDict([(\\'id\\', ids), (\\'a\\', data)]))`.\\n\\n       .. seealso:: :meth:`pyspark.sql.GroupedData.apply`\\n\\n    3. GROUPED_AGG\\n\\n       A grouped aggregate UDF defines a transformation: One or more `pandas.Series` -> A scalar\\n       The `returnType` should be a primitive data type, e.g., :class:`DoubleType`.\\n       The returned scalar can be either a python primitive type, e.g., `int` or `float`\\n       or a numpy data type, e.g., `numpy.int64` or `numpy.float64`.\\n\\n       :class:`MapType` and :class:`StructType` are currently not supported as output types.\\n\\n       Group aggregate UDFs are used with :meth:`pyspark.sql.GroupedData.agg` and\\n       :class:`pyspark.sql.Window`\\n\\n       This example shows using grouped aggregated UDFs with groupby:\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))\\n       >>> @pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\\n       ... def mean_udf(v):\\n       ...     return v.mean()\\n       >>> df.groupby(\"id\").agg(mean_udf(df[\\'v\\'])).show()  # doctest: +SKIP\\n       +---+-----------+\\n       | id|mean_udf(v)|\\n       +---+-----------+\\n       |  1|        1.5|\\n       |  2|        6.0|\\n       +---+-----------+\\n\\n       This example shows using grouped aggregated UDFs as window functions.\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> from pyspark.sql import Window\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))\\n       >>> @pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\\n       ... def mean_udf(v):\\n       ...     return v.mean()\\n       >>> w = (Window.partitionBy(\\'id\\')\\n       ...            .orderBy(\\'v\\')\\n       ...            .rowsBetween(-1, 0))\\n       >>> df.withColumn(\\'mean_v\\', mean_udf(df[\\'v\\']).over(w)).show()  # doctest: +SKIP\\n       +---+----+------+\\n       | id|   v|mean_v|\\n       +---+----+------+\\n       |  1| 1.0|   1.0|\\n       |  1| 2.0|   1.5|\\n       |  2| 3.0|   3.0|\\n       |  2| 5.0|   4.0|\\n       |  2|10.0|   7.5|\\n       +---+----+------+\\n\\n       .. note:: For performance reasons, the input series to window functions are not copied.\\n            Therefore, mutating the input series is not allowed and will cause incorrect results.\\n            For the same reason, users should also not rely on the index of the input series.\\n\\n       .. seealso:: :meth:`pyspark.sql.GroupedData.agg` and :class:`pyspark.sql.Window`\\n\\n    .. note:: The user-defined functions are considered deterministic by default. Due to\\n        optimization, duplicate invocations may be eliminated or the function may even be invoked\\n        more times than it is present in the query. If your function is not deterministic, call\\n        `asNondeterministic` on the user defined function. E.g.:\\n\\n    >>> @pandas_udf(\\'double\\', PandasUDFType.SCALAR)  # doctest: +SKIP\\n    ... def random(v):\\n    ...     import numpy as np\\n    ...     import pandas as pd\\n    ...     return pd.Series(np.random.randn(len(v))\\n    >>> random = random.asNondeterministic()  # doctest: +SKIP\\n\\n    .. note:: The user-defined functions do not support conditional expressions or short circuiting\\n        in boolean expressions and it ends up with being executed all internally. If the functions\\n        can fail on special rows, the workaround is to incorporate the condition into the functions.\\n\\n    .. note:: The user-defined functions do not take keyword arguments on the calling side.\\n\\n    .. note:: The data type of returned `pandas.Series` from the user-defined functions should be\\n        matched with defined returnType (see :meth:`types.to_arrow_type` and\\n        :meth:`types.from_arrow_type`). When there is mismatch between them, Spark might do\\n        conversion on returned data. The conversion is not guaranteed to be correct and results\\n        should be checked for accuracy by users.',\n",
       "  'code': 'def pandas_udf(f=None, returnType=None, functionType=None):\\n    \"\"\"\\n    Creates a vectorized user defined function (UDF).\\n\\n    :param f: user-defined function. A python function if used as a standalone function\\n    :param returnType: the return type of the user-defined function. The value can be either a\\n        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\\n    :param functionType: an enum value in :class:`pyspark.sql.functions.PandasUDFType`.\\n                         Default: SCALAR.\\n\\n    .. note:: Experimental\\n\\n    The function type of the UDF can be one of the following:\\n\\n    1. SCALAR\\n\\n       A scalar UDF defines a transformation: One or more `pandas.Series` -> A `pandas.Series`.\\n       The length of the returned `pandas.Series` must be of the same as the input `pandas.Series`.\\n       If the return type is :class:`StructType`, the returned value should be a `pandas.DataFrame`.\\n\\n       :class:`MapType`, nested :class:`StructType` are currently not supported as output types.\\n\\n       Scalar UDFs are used with :meth:`pyspark.sql.DataFrame.withColumn` and\\n       :meth:`pyspark.sql.DataFrame.select`.\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> from pyspark.sql.types import IntegerType, StringType\\n       >>> slen = pandas_udf(lambda s: s.str.len(), IntegerType())  # doctest: +SKIP\\n       >>> @pandas_udf(StringType())  # doctest: +SKIP\\n       ... def to_upper(s):\\n       ...     return s.str.upper()\\n       ...\\n       >>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\\n       ... def add_one(x):\\n       ...     return x + 1\\n       ...\\n       >>> df = spark.createDataFrame([(1, \"John Doe\", 21)],\\n       ...                            (\"id\", \"name\", \"age\"))  # doctest: +SKIP\\n       >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")) \\\\\\\\\\n       ...     .show()  # doctest: +SKIP\\n       +----------+--------------+------------+\\n       |slen(name)|to_upper(name)|add_one(age)|\\n       +----------+--------------+------------+\\n       |         8|      JOHN DOE|          22|\\n       +----------+--------------+------------+\\n       >>> @pandas_udf(\"first string, last string\")  # doctest: +SKIP\\n       ... def split_expand(n):\\n       ...     return n.str.split(expand=True)\\n       >>> df.select(split_expand(\"name\")).show()  # doctest: +SKIP\\n       +------------------+\\n       |split_expand(name)|\\n       +------------------+\\n       |       [John, Doe]|\\n       +------------------+\\n\\n       .. note:: The length of `pandas.Series` within a scalar UDF is not that of the whole input\\n           column, but is the length of an internal batch used for each call to the function.\\n           Therefore, this can be used, for example, to ensure the length of each returned\\n           `pandas.Series`, and can not be used as the column length.\\n\\n    2. GROUPED_MAP\\n\\n       A grouped map UDF defines transformation: A `pandas.DataFrame` -> A `pandas.DataFrame`\\n       The returnType should be a :class:`StructType` describing the schema of the returned\\n       `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match\\n       the field names in the defined returnType schema if specified as strings, or match the\\n       field data types by position if not strings, e.g. integer indices.\\n       The length of the returned `pandas.DataFrame` can be arbitrary.\\n\\n       Grouped map UDFs are used with :meth:`pyspark.sql.GroupedData.apply`.\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))  # doctest: +SKIP\\n       >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\\n       ... def normalize(pdf):\\n       ...     v = pdf.v\\n       ...     return pdf.assign(v=(v - v.mean()) / v.std())\\n       >>> df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP\\n       +---+-------------------+\\n       | id|                  v|\\n       +---+-------------------+\\n       |  1|-0.7071067811865475|\\n       |  1| 0.7071067811865475|\\n       |  2|-0.8320502943378437|\\n       |  2|-0.2773500981126146|\\n       |  2| 1.1094003924504583|\\n       +---+-------------------+\\n\\n       Alternatively, the user can define a function that takes two arguments.\\n       In this case, the grouping key(s) will be passed as the first argument and the data will\\n       be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy\\n       data types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in\\n       as a `pandas.DataFrame` containing all columns from the original Spark DataFrame.\\n       This is useful when the user does not want to hardcode grouping key(s) in the function.\\n\\n       >>> import pandas as pd  # doctest: +SKIP\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))  # doctest: +SKIP\\n       >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\\n       ... def mean_udf(key, pdf):\\n       ...     # key is a tuple of one numpy.int64, which is the value\\n       ...     # of \\'id\\' for the current group\\n       ...     return pd.DataFrame([key + (pdf.v.mean(),)])\\n       >>> df.groupby(\\'id\\').apply(mean_udf).show()  # doctest: +SKIP\\n       +---+---+\\n       | id|  v|\\n       +---+---+\\n       |  1|1.5|\\n       |  2|6.0|\\n       +---+---+\\n       >>> @pandas_udf(\\n       ...    \"id long, `ceil(v / 2)` long, v double\",\\n       ...    PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\\n       >>> def sum_udf(key, pdf):\\n       ...     # key is a tuple of two numpy.int64s, which is the values\\n       ...     # of \\'id\\' and \\'ceil(df.v / 2)\\' for the current group\\n       ...     return pd.DataFrame([key + (pdf.v.sum(),)])\\n       >>> df.groupby(df.id, ceil(df.v / 2)).apply(sum_udf).show()  # doctest: +SKIP\\n       +---+-----------+----+\\n       | id|ceil(v / 2)|   v|\\n       +---+-----------+----+\\n       |  2|          5|10.0|\\n       |  1|          1| 3.0|\\n       |  2|          3| 5.0|\\n       |  2|          2| 3.0|\\n       +---+-----------+----+\\n\\n       .. note:: If returning a new `pandas.DataFrame` constructed with a dictionary, it is\\n           recommended to explicitly index the columns by name to ensure the positions are correct,\\n           or alternatively use an `OrderedDict`.\\n           For example, `pd.DataFrame({\\'id\\': ids, \\'a\\': data}, columns=[\\'id\\', \\'a\\'])` or\\n           `pd.DataFrame(OrderedDict([(\\'id\\', ids), (\\'a\\', data)]))`.\\n\\n       .. seealso:: :meth:`pyspark.sql.GroupedData.apply`\\n\\n    3. GROUPED_AGG\\n\\n       A grouped aggregate UDF defines a transformation: One or more `pandas.Series` -> A scalar\\n       The `returnType` should be a primitive data type, e.g., :class:`DoubleType`.\\n       The returned scalar can be either a python primitive type, e.g., `int` or `float`\\n       or a numpy data type, e.g., `numpy.int64` or `numpy.float64`.\\n\\n       :class:`MapType` and :class:`StructType` are currently not supported as output types.\\n\\n       Group aggregate UDFs are used with :meth:`pyspark.sql.GroupedData.agg` and\\n       :class:`pyspark.sql.Window`\\n\\n       This example shows using grouped aggregated UDFs with groupby:\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))\\n       >>> @pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\\n       ... def mean_udf(v):\\n       ...     return v.mean()\\n       >>> df.groupby(\"id\").agg(mean_udf(df[\\'v\\'])).show()  # doctest: +SKIP\\n       +---+-----------+\\n       | id|mean_udf(v)|\\n       +---+-----------+\\n       |  1|        1.5|\\n       |  2|        6.0|\\n       +---+-----------+\\n\\n       This example shows using grouped aggregated UDFs as window functions.\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> from pyspark.sql import Window\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))\\n       >>> @pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\\n       ... def mean_udf(v):\\n       ...     return v.mean()\\n       >>> w = (Window.partitionBy(\\'id\\')\\n       ...            .orderBy(\\'v\\')\\n       ...            .rowsBetween(-1, 0))\\n       >>> df.withColumn(\\'mean_v\\', mean_udf(df[\\'v\\']).over(w)).show()  # doctest: +SKIP\\n       +---+----+------+\\n       | id|   v|mean_v|\\n       +---+----+------+\\n       |  1| 1.0|   1.0|\\n       |  1| 2.0|   1.5|\\n       |  2| 3.0|   3.0|\\n       |  2| 5.0|   4.0|\\n       |  2|10.0|   7.5|\\n       +---+----+------+\\n\\n       .. note:: For performance reasons, the input series to window functions are not copied.\\n            Therefore, mutating the input series is not allowed and will cause incorrect results.\\n            For the same reason, users should also not rely on the index of the input series.\\n\\n       .. seealso:: :meth:`pyspark.sql.GroupedData.agg` and :class:`pyspark.sql.Window`\\n\\n    .. note:: The user-defined functions are considered deterministic by default. Due to\\n        optimization, duplicate invocations may be eliminated or the function may even be invoked\\n        more times than it is present in the query. If your function is not deterministic, call\\n        `asNondeterministic` on the user defined function. E.g.:\\n\\n    >>> @pandas_udf(\\'double\\', PandasUDFType.SCALAR)  # doctest: +SKIP\\n    ... def random(v):\\n    ...     import numpy as np\\n    ...     import pandas as pd\\n    ...     return pd.Series(np.random.randn(len(v))\\n    >>> random = random.asNondeterministic()  # doctest: +SKIP\\n\\n    .. note:: The user-defined functions do not support conditional expressions or short circuiting\\n        in boolean expressions and it ends up with being executed all internally. If the functions\\n        can fail on special rows, the workaround is to incorporate the condition into the functions.\\n\\n    .. note:: The user-defined functions do not take keyword arguments on the calling side.\\n\\n    .. note:: The data type of returned `pandas.Series` from the user-defined functions should be\\n        matched with defined returnType (see :meth:`types.to_arrow_type` and\\n        :meth:`types.from_arrow_type`). When there is mismatch between them, Spark might do\\n        conversion on returned data. The conversion is not guaranteed to be correct and results\\n        should be checked for accuracy by users.\\n    \"\"\"\\n\\n    # The following table shows most of Pandas data and SQL type conversions in Pandas UDFs that\\n    # are not yet visible to the user. Some of behaviors are buggy and might be changed in the near\\n    # future. The table might have to be eventually documented externally.\\n    # Please see SPARK-25798\\'s PR to see the codes in order to generate the table below.\\n    #\\n    # +-----------------------------+----------------------+----------+-------+--------+--------------------+--------------------+--------+---------+---------+---------+------------+------------+------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+-------------+-----------------+------------------+-----------+--------------------------------+  # noqa\\n    # |SQL Type \\\\ Pandas Value(Type)|None(object(NoneType))|True(bool)|1(int8)|1(int16)|            1(int32)|            1(int64)|1(uint8)|1(uint16)|1(uint32)|1(uint64)|1.0(float16)|1.0(float32)|1.0(float64)|1970-01-01 00:00:00(datetime64[ns])|1970-01-01 00:00:00-05:00(datetime64[ns, US/Eastern])|a(object(string))|  1(object(Decimal))|[1 2 3](object(array[int32]))|1.0(float128)|(1+0j)(complex64)|(1+0j)(complex128)|A(category)|1 days 00:00:00(timedelta64[ns])|  # noqa\\n    # +-----------------------------+----------------------+----------+-------+--------+--------------------+--------------------+--------+---------+---------+---------+------------+------------+------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+-------------+-----------------+------------------+-----------+--------------------------------+  # noqa\\n    # |                      boolean|                  None|      True|   True|    True|                True|                True|    True|     True|     True|     True|       False|       False|       False|                              False|                                                False|                X|                   X|                            X|        False|            False|             False|          X|                           False|  # noqa\\n    # |                      tinyint|                  None|         1|      1|       1|                   1|                   1|       X|        X|        X|        X|           1|           1|           1|                                  X|                                                    X|                X|                   X|                            X|            X|                X|                 X|          0|                               X|  # noqa\\n    # |                     smallint|                  None|         1|      1|       1|                   1|                   1|       1|        X|        X|        X|           1|           1|           1|                                  X|                                                    X|                X|                   X|                            X|            X|                X|                 X|          X|                               X|  # noqa\\n    # |                          int|                  None|         1|      1|       1|                   1|                   1|       1|        1|        X|        X|           1|           1|           1|                                  X|                                                    X|                X|                   X|                            X|            X|                X|                 X|          X|                               X|  # noqa\\n    # |                       bigint|                  None|         1|      1|       1|                   1|                   1|       1|        1|        1|        X|           1|           1|           1|                                  0|                                       18000000000000|                X|                   X|                            X|            X|                X|                 X|          X|                               X|  # noqa\\n    # |                        float|                  None|       1.0|    1.0|     1.0|                 1.0|                 1.0|     1.0|      1.0|      1.0|      1.0|         1.0|         1.0|         1.0|                                  X|                                                    X|                X|1.401298464324817...|                            X|            X|                X|                 X|          X|                               X|  # noqa\\n    # |                       double|                  None|       1.0|    1.0|     1.0|                 1.0|                 1.0|     1.0|      1.0|      1.0|      1.0|         1.0|         1.0|         1.0|                                  X|                                                    X|                X|                   X|                            X|            X|                X|                 X|          X|                               X|  # noqa\\n    # |                         date|                  None|         X|      X|       X|datetime.date(197...|                   X|       X|        X|        X|        X|           X|           X|           X|               datetime.date(197...|                                                    X|                X|                   X|                            X|            X|                X|                 X|          X|                               X|  # noqa\\n    # |                    timestamp|                  None|         X|      X|       X|                   X|datetime.datetime...|       X|        X|        X|        X|           X|           X|           X|               datetime.datetime...|                                 datetime.datetime...|                X|                   X|                            X|            X|                X|                 X|          X|                               X|  # noqa\\n    # |                       string|                  None|       u\\'\\'|u\\'\\\\x01\\'| u\\'\\\\x01\\'|             u\\'\\\\x01\\'|             u\\'\\\\x01\\'| u\\'\\\\x01\\'|  u\\'\\\\x01\\'|  u\\'\\\\x01\\'|  u\\'\\\\x01\\'|         u\\'\\'|         u\\'\\'|         u\\'\\'|                                  X|                                                    X|             u\\'a\\'|                   X|                            X|          u\\'\\'|              u\\'\\'|               u\\'\\'|          X|                               X|  # noqa\\n    # |                decimal(10,0)|                  None|         X|      X|       X|                   X|                   X|       X|        X|        X|        X|           X|           X|           X|                                  X|                                                    X|                X|        Decimal(\\'1\\')|                            X|            X|                X|                 X|          X|                               X|  # noqa\\n    # |                   array<int>|                  None|         X|      X|       X|                   X|                   X|       X|        X|        X|        X|           X|           X|           X|                                  X|                                                    X|                X|                   X|                    [1, 2, 3]|            X|                X|                 X|          X|                               X|  # noqa\\n    # |              map<string,int>|                     X|         X|      X|       X|                   X|                   X|       X|        X|        X|        X|           X|           X|           X|                                  X|                                                    X|                X|                   X|                            X|            X|                X|                 X|          X|                               X|  # noqa\\n    # |               struct<_1:int>|                     X|         X|      X|       X|                   X|                   X|       X|        X|        X|        X|           X|           X|           X|                                  X|                                                    X|                X|                   X|                            X|            X|                X|                 X|          X|                               X|  # noqa\\n    # |                       binary|                     X|         X|      X|       X|                   X|                   X|       X|        X|        X|        X|           X|           X|           X|                                  X|                                                    X|                X|                   X|                            X|            X|                X|                 X|          X|                               X|  # noqa\\n    # +-----------------------------+----------------------+----------+-------+--------+--------------------+--------------------+--------+---------+---------+---------+------------+------------+------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+-------------+-----------------+------------------+-----------+--------------------------------+  # noqa\\n    #\\n    # Note: DDL formatted string is used for \\'SQL Type\\' for simplicity. This string can be\\n    #       used in `returnType`.\\n    # Note: The values inside of the table are generated by `repr`.\\n    # Note: Python 2 is used to generate this table since it is used to check the backward\\n    #       compatibility often in practice.\\n    # Note: Pandas 0.19.2 and PyArrow 0.9.0 are used.\\n    # Note: Timezone is Singapore timezone.\\n    # Note: \\'X\\' means it throws an exception during the conversion.\\n    # Note: \\'binary\\' type is only supported with PyArrow 0.10.0+ (SPARK-23555).\\n\\n    # decorator @pandas_udf(returnType, functionType)\\n    is_decorator = f is None or isinstance(f, (str, DataType))\\n\\n    if is_decorator:\\n        # If DataType has been passed as a positional argument\\n        # for decorator use it as a returnType\\n        return_type = f or returnType\\n\\n        if functionType is not None:\\n            # @pandas_udf(dataType, functionType=functionType)\\n            # @pandas_udf(returnType=dataType, functionType=functionType)\\n            eval_type = functionType\\n        elif returnType is not None and isinstance(returnType, int):\\n            # @pandas_udf(dataType, functionType)\\n            eval_type = returnType\\n        else:\\n            # @pandas_udf(dataType) or @pandas_udf(returnType=dataType)\\n            eval_type = PythonEvalType.SQL_SCALAR_PANDAS_UDF\\n    else:\\n        return_type = returnType\\n\\n        if functionType is not None:\\n            eval_type = functionType\\n        else:\\n            eval_type = PythonEvalType.SQL_SCALAR_PANDAS_UDF\\n\\n    if return_type is None:\\n        raise ValueError(\"Invalid returnType: returnType can not be None\")\\n\\n    if eval_type not in [PythonEvalType.SQL_SCALAR_PANDAS_UDF,\\n                         PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF,\\n                         PythonEvalType.SQL_GROUPED_AGG_PANDAS_UDF]:\\n        raise ValueError(\"Invalid functionType: \"\\n                         \"functionType must be one the values from PandasUDFType\")\\n\\n    if is_decorator:\\n        return functools.partial(_create_udf, returnType=return_type, evalType=eval_type)\\n    else:\\n        return _create_udf(f=f, returnType=return_type, evalType=eval_type)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DenseVector.dot',\n",
       "  'docstring': \"Compute the dot product of two Vectors. We support\\n        (Numpy array, list, SparseVector, or SciPy sparse)\\n        and a target NumPy array that is either 1- or 2-dimensional.\\n        Equivalent to calling numpy.dot of the two vectors.\\n\\n        >>> dense = DenseVector(array.array('d', [1., 2.]))\\n        >>> dense.dot(dense)\\n        5.0\\n        >>> dense.dot(SparseVector(2, [0, 1], [2., 1.]))\\n        4.0\\n        >>> dense.dot(range(1, 3))\\n        5.0\\n        >>> dense.dot(np.array(range(1, 3)))\\n        5.0\\n        >>> dense.dot([1.,])\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> dense.dot(np.reshape([1., 2., 3., 4.], (2, 2), order='F'))\\n        array([  5.,  11.])\\n        >>> dense.dot(np.reshape([1., 2., 3.], (3, 1), order='F'))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\",\n",
       "  'code': 'def dot(self, other):\\n        \"\"\"\\n        Compute the dot product of two Vectors. We support\\n        (Numpy array, list, SparseVector, or SciPy sparse)\\n        and a target NumPy array that is either 1- or 2-dimensional.\\n        Equivalent to calling numpy.dot of the two vectors.\\n\\n        >>> dense = DenseVector(array.array(\\'d\\', [1., 2.]))\\n        >>> dense.dot(dense)\\n        5.0\\n        >>> dense.dot(SparseVector(2, [0, 1], [2., 1.]))\\n        4.0\\n        >>> dense.dot(range(1, 3))\\n        5.0\\n        >>> dense.dot(np.array(range(1, 3)))\\n        5.0\\n        >>> dense.dot([1.,])\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> dense.dot(np.reshape([1., 2., 3., 4.], (2, 2), order=\\'F\\'))\\n        array([  5.,  11.])\\n        >>> dense.dot(np.reshape([1., 2., 3.], (3, 1), order=\\'F\\'))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        \"\"\"\\n        if type(other) == np.ndarray:\\n            if other.ndim > 1:\\n                assert len(self) == other.shape[0], \"dimension mismatch\"\\n            return np.dot(self.array, other)\\n        elif _have_scipy and scipy.sparse.issparse(other):\\n            assert len(self) == other.shape[0], \"dimension mismatch\"\\n            return other.transpose().dot(self.toArray())\\n        else:\\n            assert len(self) == _vector_size(other), \"dimension mismatch\"\\n            if isinstance(other, SparseVector):\\n                return other.dot(self)\\n            elif isinstance(other, Vector):\\n                return np.dot(self.toArray(), other.toArray())\\n            else:\\n                return np.dot(self.toArray(), other)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SparseVector.dot',\n",
       "  'docstring': \"Dot product with a SparseVector or 1- or 2-dimensional Numpy array.\\n\\n        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\\n        >>> a.dot(a)\\n        25.0\\n        >>> a.dot(array.array('d', [1., 2., 3., 4.]))\\n        22.0\\n        >>> b = SparseVector(4, [2], [1.0])\\n        >>> a.dot(b)\\n        0.0\\n        >>> a.dot(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]))\\n        array([ 22.,  22.])\\n        >>> a.dot([1., 2., 3.])\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> a.dot(np.array([1., 2.]))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> a.dot(DenseVector([1., 2.]))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> a.dot(np.zeros((3, 2)))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\",\n",
       "  'code': 'def dot(self, other):\\n        \"\"\"\\n        Dot product with a SparseVector or 1- or 2-dimensional Numpy array.\\n\\n        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\\n        >>> a.dot(a)\\n        25.0\\n        >>> a.dot(array.array(\\'d\\', [1., 2., 3., 4.]))\\n        22.0\\n        >>> b = SparseVector(4, [2], [1.0])\\n        >>> a.dot(b)\\n        0.0\\n        >>> a.dot(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]))\\n        array([ 22.,  22.])\\n        >>> a.dot([1., 2., 3.])\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> a.dot(np.array([1., 2.]))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> a.dot(DenseVector([1., 2.]))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> a.dot(np.zeros((3, 2)))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        \"\"\"\\n\\n        if isinstance(other, np.ndarray):\\n            if other.ndim not in [2, 1]:\\n                raise ValueError(\"Cannot call dot with %d-dimensional array\" % other.ndim)\\n            assert len(self) == other.shape[0], \"dimension mismatch\"\\n            return np.dot(self.values, other[self.indices])\\n\\n        assert len(self) == _vector_size(other), \"dimension mismatch\"\\n\\n        if isinstance(other, DenseVector):\\n            return np.dot(other.array[self.indices], self.values)\\n\\n        elif isinstance(other, SparseVector):\\n            # Find out common indices.\\n            self_cmind = np.in1d(self.indices, other.indices, assume_unique=True)\\n            self_values = self.values[self_cmind]\\n            if self_values.size == 0:\\n                return 0.0\\n            else:\\n                other_cmind = np.in1d(other.indices, self.indices, assume_unique=True)\\n                return np.dot(self_values, other.values[other_cmind])\\n\\n        else:\\n            return self.dot(_convert_to_vector(other))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SparseVector.squared_distance',\n",
       "  'docstring': \"Squared distance from a SparseVector or 1-dimensional NumPy array.\\n\\n        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\\n        >>> a.squared_distance(a)\\n        0.0\\n        >>> a.squared_distance(array.array('d', [1., 2., 3., 4.]))\\n        11.0\\n        >>> a.squared_distance(np.array([1., 2., 3., 4.]))\\n        11.0\\n        >>> b = SparseVector(4, [2], [1.0])\\n        >>> a.squared_distance(b)\\n        26.0\\n        >>> b.squared_distance(a)\\n        26.0\\n        >>> b.squared_distance([1., 2.])\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> b.squared_distance(SparseVector(3, [1,], [1.0,]))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\",\n",
       "  'code': 'def squared_distance(self, other):\\n        \"\"\"\\n        Squared distance from a SparseVector or 1-dimensional NumPy array.\\n\\n        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\\n        >>> a.squared_distance(a)\\n        0.0\\n        >>> a.squared_distance(array.array(\\'d\\', [1., 2., 3., 4.]))\\n        11.0\\n        >>> a.squared_distance(np.array([1., 2., 3., 4.]))\\n        11.0\\n        >>> b = SparseVector(4, [2], [1.0])\\n        >>> a.squared_distance(b)\\n        26.0\\n        >>> b.squared_distance(a)\\n        26.0\\n        >>> b.squared_distance([1., 2.])\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> b.squared_distance(SparseVector(3, [1,], [1.0,]))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        \"\"\"\\n        assert len(self) == _vector_size(other), \"dimension mismatch\"\\n\\n        if isinstance(other, np.ndarray) or isinstance(other, DenseVector):\\n            if isinstance(other, np.ndarray) and other.ndim != 1:\\n                raise Exception(\"Cannot call squared_distance with %d-dimensional array\" %\\n                                other.ndim)\\n            if isinstance(other, DenseVector):\\n                other = other.array\\n            sparse_ind = np.zeros(other.size, dtype=bool)\\n            sparse_ind[self.indices] = True\\n            dist = other[sparse_ind] - self.values\\n            result = np.dot(dist, dist)\\n\\n            other_ind = other[~sparse_ind]\\n            result += np.dot(other_ind, other_ind)\\n            return result\\n\\n        elif isinstance(other, SparseVector):\\n            result = 0.0\\n            i, j = 0, 0\\n            while i < len(self.indices) and j < len(other.indices):\\n                if self.indices[i] == other.indices[j]:\\n                    diff = self.values[i] - other.values[j]\\n                    result += diff * diff\\n                    i += 1\\n                    j += 1\\n                elif self.indices[i] < other.indices[j]:\\n                    result += self.values[i] * self.values[i]\\n                    i += 1\\n                else:\\n                    result += other.values[j] * other.values[j]\\n                    j += 1\\n            while i < len(self.indices):\\n                result += self.values[i] * self.values[i]\\n                i += 1\\n            while j < len(other.indices):\\n                result += other.values[j] * other.values[j]\\n                j += 1\\n            return result\\n        else:\\n            return self.squared_distance(_convert_to_vector(other))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SparseVector.toArray',\n",
       "  'docstring': 'Returns a copy of this SparseVector as a 1-dimensional NumPy array.',\n",
       "  'code': 'def toArray(self):\\n        \"\"\"\\n        Returns a copy of this SparseVector as a 1-dimensional NumPy array.\\n        \"\"\"\\n        arr = np.zeros((self.size,), dtype=np.float64)\\n        arr[self.indices] = self.values\\n        return arr',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Vectors.dense',\n",
       "  'docstring': 'Create a dense vector of 64-bit floats from a Python list or numbers.\\n\\n        >>> Vectors.dense([1, 2, 3])\\n        DenseVector([1.0, 2.0, 3.0])\\n        >>> Vectors.dense(1.0, 2.0)\\n        DenseVector([1.0, 2.0])',\n",
       "  'code': 'def dense(*elements):\\n        \"\"\"\\n        Create a dense vector of 64-bit floats from a Python list or numbers.\\n\\n        >>> Vectors.dense([1, 2, 3])\\n        DenseVector([1.0, 2.0, 3.0])\\n        >>> Vectors.dense(1.0, 2.0)\\n        DenseVector([1.0, 2.0])\\n        \"\"\"\\n        if len(elements) == 1 and not isinstance(elements[0], (float, int, long)):\\n            # it\\'s list, numpy.array or other iterable object.\\n            elements = elements[0]\\n        return DenseVector(elements)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DenseMatrix.toArray',\n",
       "  'docstring': 'Return an numpy.ndarray\\n\\n        >>> m = DenseMatrix(2, 2, range(4))\\n        >>> m.toArray()\\n        array([[ 0.,  2.],\\n               [ 1.,  3.]])',\n",
       "  'code': 'def toArray(self):\\n        \"\"\"\\n        Return an numpy.ndarray\\n\\n        >>> m = DenseMatrix(2, 2, range(4))\\n        >>> m.toArray()\\n        array([[ 0.,  2.],\\n               [ 1.,  3.]])\\n        \"\"\"\\n        if self.isTransposed:\\n            return np.asfortranarray(\\n                self.values.reshape((self.numRows, self.numCols)))\\n        else:\\n            return self.values.reshape((self.numRows, self.numCols), order=\\'F\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SparseMatrix.toArray',\n",
       "  'docstring': 'Return an numpy.ndarray',\n",
       "  'code': 'def toArray(self):\\n        \"\"\"\\n        Return an numpy.ndarray\\n        \"\"\"\\n        A = np.zeros((self.numRows, self.numCols), dtype=np.float64, order=\\'F\\')\\n        for k in xrange(self.colPtrs.size - 1):\\n            startptr = self.colPtrs[k]\\n            endptr = self.colPtrs[k + 1]\\n            if self.isTransposed:\\n                A[k, self.rowIndices[startptr:endptr]] = self.values[startptr:endptr]\\n            else:\\n                A[self.rowIndices[startptr:endptr], k] = self.values[startptr:endptr]\\n        return A',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_ImageSchema.toNDArray',\n",
       "  'docstring': 'Converts an image to an array with metadata.\\n\\n        :param `Row` image: A row that contains the image to be converted. It should\\n            have the attributes specified in `ImageSchema.imageSchema`.\\n        :return: a `numpy.ndarray` that is an image.\\n\\n        .. versionadded:: 2.3.0',\n",
       "  'code': 'def toNDArray(self, image):\\n        \"\"\"\\n        Converts an image to an array with metadata.\\n\\n        :param `Row` image: A row that contains the image to be converted. It should\\n            have the attributes specified in `ImageSchema.imageSchema`.\\n        :return: a `numpy.ndarray` that is an image.\\n\\n        .. versionadded:: 2.3.0\\n        \"\"\"\\n\\n        if not isinstance(image, Row):\\n            raise TypeError(\\n                \"image argument should be pyspark.sql.types.Row; however, \"\\n                \"it got [%s].\" % type(image))\\n\\n        if any(not hasattr(image, f) for f in self.imageFields):\\n            raise ValueError(\\n                \"image argument should have attributes specified in \"\\n                \"ImageSchema.imageSchema [%s].\" % \", \".join(self.imageFields))\\n\\n        height = image.height\\n        width = image.width\\n        nChannels = image.nChannels\\n        return np.ndarray(\\n            shape=(height, width, nChannels),\\n            dtype=np.uint8,\\n            buffer=image.data,\\n            strides=(width * nChannels, nChannels, 1))',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_ImageSchema.toImage',\n",
       "  'docstring': 'Converts an array with metadata to a two-dimensional image.\\n\\n        :param `numpy.ndarray` array: The array to convert to image.\\n        :param str origin: Path to the image, optional.\\n        :return: a :class:`Row` that is a two dimensional image.\\n\\n        .. versionadded:: 2.3.0',\n",
       "  'code': 'def toImage(self, array, origin=\"\"):\\n        \"\"\"\\n        Converts an array with metadata to a two-dimensional image.\\n\\n        :param `numpy.ndarray` array: The array to convert to image.\\n        :param str origin: Path to the image, optional.\\n        :return: a :class:`Row` that is a two dimensional image.\\n\\n        .. versionadded:: 2.3.0\\n        \"\"\"\\n\\n        if not isinstance(array, np.ndarray):\\n            raise TypeError(\\n                \"array argument should be numpy.ndarray; however, it got [%s].\" % type(array))\\n\\n        if array.ndim != 3:\\n            raise ValueError(\"Invalid array shape\")\\n\\n        height, width, nChannels = array.shape\\n        ocvTypes = ImageSchema.ocvTypes\\n        if nChannels == 1:\\n            mode = ocvTypes[\"CV_8UC1\"]\\n        elif nChannels == 3:\\n            mode = ocvTypes[\"CV_8UC3\"]\\n        elif nChannels == 4:\\n            mode = ocvTypes[\"CV_8UC4\"]\\n        else:\\n            raise ValueError(\"Invalid number of channels\")\\n\\n        # Running `bytearray(numpy.array([1]))` fails in specific Python versions\\n        # with a specific Numpy version, for example in Python 3.6.0 and NumPy 1.13.3.\\n        # Here, it avoids it by converting it to bytes.\\n        if LooseVersion(np.__version__) >= LooseVersion(\\'1.9\\'):\\n            data = bytearray(array.astype(dtype=np.uint8).ravel().tobytes())\\n        else:\\n            # Numpy prior to 1.9 don\\'t have `tobytes` method.\\n            data = bytearray(array.astype(dtype=np.uint8).ravel())\\n\\n        # Creating new Row with _create_row(), because Row(name = value, ... )\\n        # orders fields by name, which conflicts with expected schema order\\n        # when the new DataFrame is created by UDF\\n        return _create_row(self.imageFields,\\n                           [origin, height, width, nChannels, mode, data])',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'RandomRDDs.uniformVectorRDD',\n",
       "  'docstring': 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the uniform distribution U(0.0, 1.0).\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD.\\n        :param seed: Seed for the RNG that generates the seed for the generator in each partition.\\n        :return: RDD of Vector with vectors containing i.i.d samples ~ `U(0.0, 1.0)`.\\n\\n        >>> import numpy as np\\n        >>> mat = np.matrix(RandomRDDs.uniformVectorRDD(sc, 10, 10).collect())\\n        >>> mat.shape\\n        (10, 10)\\n        >>> mat.max() <= 1.0 and mat.min() >= 0.0\\n        True\\n        >>> RandomRDDs.uniformVectorRDD(sc, 10, 10, 4).getNumPartitions()\\n        4',\n",
       "  'code': 'def uniformVectorRDD(sc, numRows, numCols, numPartitions=None, seed=None):\\n        \"\"\"\\n        Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the uniform distribution U(0.0, 1.0).\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD.\\n        :param seed: Seed for the RNG that generates the seed for the generator in each partition.\\n        :return: RDD of Vector with vectors containing i.i.d samples ~ `U(0.0, 1.0)`.\\n\\n        >>> import numpy as np\\n        >>> mat = np.matrix(RandomRDDs.uniformVectorRDD(sc, 10, 10).collect())\\n        >>> mat.shape\\n        (10, 10)\\n        >>> mat.max() <= 1.0 and mat.min() >= 0.0\\n        True\\n        >>> RandomRDDs.uniformVectorRDD(sc, 10, 10, 4).getNumPartitions()\\n        4\\n        \"\"\"\\n        return callMLlibFunc(\"uniformVectorRDD\", sc._jsc, numRows, numCols, numPartitions, seed)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'RandomRDDs.normalVectorRDD',\n",
       "  'docstring': 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the standard normal distribution.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ `N(0.0, 1.0)`.\\n\\n        >>> import numpy as np\\n        >>> mat = np.matrix(RandomRDDs.normalVectorRDD(sc, 100, 100, seed=1).collect())\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - 0.0) < 0.1\\n        True\\n        >>> abs(mat.std() - 1.0) < 0.1\\n        True',\n",
       "  'code': 'def normalVectorRDD(sc, numRows, numCols, numPartitions=None, seed=None):\\n        \"\"\"\\n        Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the standard normal distribution.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ `N(0.0, 1.0)`.\\n\\n        >>> import numpy as np\\n        >>> mat = np.matrix(RandomRDDs.normalVectorRDD(sc, 100, 100, seed=1).collect())\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - 0.0) < 0.1\\n        True\\n        >>> abs(mat.std() - 1.0) < 0.1\\n        True\\n        \"\"\"\\n        return callMLlibFunc(\"normalVectorRDD\", sc._jsc, numRows, numCols, numPartitions, seed)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'RandomRDDs.logNormalVectorRDD',\n",
       "  'docstring': 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the log normal distribution.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param mean: Mean of the log normal distribution\\n        :param std: Standard Deviation of the log normal distribution\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ log `N(mean, std)`.\\n\\n        >>> import numpy as np\\n        >>> from math import sqrt, exp\\n        >>> mean = 0.0\\n        >>> std = 1.0\\n        >>> expMean = exp(mean + 0.5 * std * std)\\n        >>> expStd = sqrt((exp(std * std) - 1.0) * exp(2.0 * mean + std * std))\\n        >>> m = RandomRDDs.logNormalVectorRDD(sc, mean, std, 100, 100, seed=1).collect()\\n        >>> mat = np.matrix(m)\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - expMean) < 0.1\\n        True\\n        >>> abs(mat.std() - expStd) < 0.1\\n        True',\n",
       "  'code': 'def logNormalVectorRDD(sc, mean, std, numRows, numCols, numPartitions=None, seed=None):\\n        \"\"\"\\n        Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the log normal distribution.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param mean: Mean of the log normal distribution\\n        :param std: Standard Deviation of the log normal distribution\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ log `N(mean, std)`.\\n\\n        >>> import numpy as np\\n        >>> from math import sqrt, exp\\n        >>> mean = 0.0\\n        >>> std = 1.0\\n        >>> expMean = exp(mean + 0.5 * std * std)\\n        >>> expStd = sqrt((exp(std * std) - 1.0) * exp(2.0 * mean + std * std))\\n        >>> m = RandomRDDs.logNormalVectorRDD(sc, mean, std, 100, 100, seed=1).collect()\\n        >>> mat = np.matrix(m)\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - expMean) < 0.1\\n        True\\n        >>> abs(mat.std() - expStd) < 0.1\\n        True\\n        \"\"\"\\n        return callMLlibFunc(\"logNormalVectorRDD\", sc._jsc, float(mean), float(std),\\n                             numRows, numCols, numPartitions, seed)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'RandomRDDs.poissonVectorRDD',\n",
       "  'docstring': 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the Poisson distribution with the input mean.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param mean: Mean, or lambda, for the Poisson distribution.\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`)\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ Pois(mean).\\n\\n        >>> import numpy as np\\n        >>> mean = 100.0\\n        >>> rdd = RandomRDDs.poissonVectorRDD(sc, mean, 100, 100, seed=1)\\n        >>> mat = np.mat(rdd.collect())\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - mean) < 0.5\\n        True\\n        >>> from math import sqrt\\n        >>> abs(mat.std() - sqrt(mean)) < 0.5\\n        True',\n",
       "  'code': 'def poissonVectorRDD(sc, mean, numRows, numCols, numPartitions=None, seed=None):\\n        \"\"\"\\n        Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the Poisson distribution with the input mean.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param mean: Mean, or lambda, for the Poisson distribution.\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`)\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ Pois(mean).\\n\\n        >>> import numpy as np\\n        >>> mean = 100.0\\n        >>> rdd = RandomRDDs.poissonVectorRDD(sc, mean, 100, 100, seed=1)\\n        >>> mat = np.mat(rdd.collect())\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - mean) < 0.5\\n        True\\n        >>> from math import sqrt\\n        >>> abs(mat.std() - sqrt(mean)) < 0.5\\n        True\\n        \"\"\"\\n        return callMLlibFunc(\"poissonVectorRDD\", sc._jsc, float(mean), numRows, numCols,\\n                             numPartitions, seed)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'RandomRDDs.gammaVectorRDD',\n",
       "  'docstring': 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the Gamma distribution.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param shape: Shape (> 0) of the Gamma distribution\\n        :param scale: Scale (> 0) of the Gamma distribution\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ Gamma(shape, scale).\\n\\n        >>> import numpy as np\\n        >>> from math import sqrt\\n        >>> shape = 1.0\\n        >>> scale = 2.0\\n        >>> expMean = shape * scale\\n        >>> expStd = sqrt(shape * scale * scale)\\n        >>> mat = np.matrix(RandomRDDs.gammaVectorRDD(sc, shape, scale, 100, 100, seed=1).collect())\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - expMean) < 0.1\\n        True\\n        >>> abs(mat.std() - expStd) < 0.1\\n        True',\n",
       "  'code': 'def gammaVectorRDD(sc, shape, scale, numRows, numCols, numPartitions=None, seed=None):\\n        \"\"\"\\n        Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the Gamma distribution.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param shape: Shape (> 0) of the Gamma distribution\\n        :param scale: Scale (> 0) of the Gamma distribution\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ Gamma(shape, scale).\\n\\n        >>> import numpy as np\\n        >>> from math import sqrt\\n        >>> shape = 1.0\\n        >>> scale = 2.0\\n        >>> expMean = shape * scale\\n        >>> expStd = sqrt(shape * scale * scale)\\n        >>> mat = np.matrix(RandomRDDs.gammaVectorRDD(sc, shape, scale, 100, 100, seed=1).collect())\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - expMean) < 0.1\\n        True\\n        >>> abs(mat.std() - expStd) < 0.1\\n        True\\n        \"\"\"\\n        return callMLlibFunc(\"gammaVectorRDD\", sc._jsc, float(shape), float(scale),\\n                             numRows, numCols, numPartitions, seed)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SparkSession._get_numpy_record_dtype',\n",
       "  'docstring': 'Used when converting a pandas.DataFrame to Spark using to_records(), this will correct\\n        the dtypes of fields in a record so they can be properly loaded into Spark.\\n        :param rec: a numpy record to check field dtypes\\n        :return corrected dtype for a numpy.record or None if no correction needed',\n",
       "  'code': 'def _get_numpy_record_dtype(self, rec):\\n        \"\"\"\\n        Used when converting a pandas.DataFrame to Spark using to_records(), this will correct\\n        the dtypes of fields in a record so they can be properly loaded into Spark.\\n        :param rec: a numpy record to check field dtypes\\n        :return corrected dtype for a numpy.record or None if no correction needed\\n        \"\"\"\\n        import numpy as np\\n        cur_dtypes = rec.dtype\\n        col_names = cur_dtypes.names\\n        record_type_list = []\\n        has_rec_fix = False\\n        for i in xrange(len(cur_dtypes)):\\n            curr_type = cur_dtypes[i]\\n            # If type is a datetime64 timestamp, convert to microseconds\\n            # NOTE: if dtype is datetime[ns] then np.record.tolist() will output values as longs,\\n            # conversion from [us] or lower will lead to py datetime objects, see SPARK-22417\\n            if curr_type == np.dtype(\\'datetime64[ns]\\'):\\n                curr_type = \\'datetime64[us]\\'\\n                has_rec_fix = True\\n            record_type_list.append((str(col_names[i]), curr_type))\\n        return np.dtype(record_type_list) if has_rec_fix else None',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SparkSession._convert_from_pandas',\n",
       "  'docstring': 'Convert a pandas.DataFrame to list of records that can be used to make a DataFrame\\n         :return list of records',\n",
       "  'code': 'def _convert_from_pandas(self, pdf, schema, timezone):\\n        \"\"\"\\n         Convert a pandas.DataFrame to list of records that can be used to make a DataFrame\\n         :return list of records\\n        \"\"\"\\n        if timezone is not None:\\n            from pyspark.sql.types import _check_series_convert_timestamps_tz_local\\n            copied = False\\n            if isinstance(schema, StructType):\\n                for field in schema:\\n                    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\\n                    if isinstance(field.dataType, TimestampType):\\n                        s = _check_series_convert_timestamps_tz_local(pdf[field.name], timezone)\\n                        if s is not pdf[field.name]:\\n                            if not copied:\\n                                # Copy once if the series is modified to prevent the original\\n                                # Pandas DataFrame from being updated\\n                                pdf = pdf.copy()\\n                                copied = True\\n                            pdf[field.name] = s\\n            else:\\n                for column, series in pdf.iteritems():\\n                    s = _check_series_convert_timestamps_tz_local(series, timezone)\\n                    if s is not series:\\n                        if not copied:\\n                            # Copy once if the series is modified to prevent the original\\n                            # Pandas DataFrame from being updated\\n                            pdf = pdf.copy()\\n                            copied = True\\n                        pdf[column] = s\\n\\n        # Convert pandas.DataFrame to list of numpy records\\n        np_records = pdf.to_records(index=False)\\n\\n        # Check if any columns need to be fixed for Spark to infer properly\\n        if len(np_records) > 0:\\n            record_dtype = self._get_numpy_record_dtype(np_records[0])\\n            if record_dtype is not None:\\n                return [r.astype(record_dtype).tolist() for r in np_records]\\n\\n        # Convert list of numpy records to python lists\\n        return [r.tolist() for r in np_records]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'StreamingContext.getOrCreate',\n",
       "  'docstring': 'Either recreate a StreamingContext from checkpoint data or create a new StreamingContext.\\n        If checkpoint data exists in the provided `checkpointPath`, then StreamingContext will be\\n        recreated from the checkpoint data. If the data does not exist, then the provided setupFunc\\n        will be used to create a new context.\\n\\n        @param checkpointPath: Checkpoint directory used in an earlier streaming program\\n        @param setupFunc:      Function to create a new context and setup DStreams',\n",
       "  'code': 'def getOrCreate(cls, checkpointPath, setupFunc):\\n        \"\"\"\\n        Either recreate a StreamingContext from checkpoint data or create a new StreamingContext.\\n        If checkpoint data exists in the provided `checkpointPath`, then StreamingContext will be\\n        recreated from the checkpoint data. If the data does not exist, then the provided setupFunc\\n        will be used to create a new context.\\n\\n        @param checkpointPath: Checkpoint directory used in an earlier streaming program\\n        @param setupFunc:      Function to create a new context and setup DStreams\\n        \"\"\"\\n        cls._ensure_initialized()\\n        gw = SparkContext._gateway\\n\\n        # Check whether valid checkpoint information exists in the given path\\n        ssc_option = gw.jvm.StreamingContextPythonHelper().tryRecoverFromCheckpoint(checkpointPath)\\n        if ssc_option.isEmpty():\\n            ssc = setupFunc()\\n            ssc.checkpoint(checkpointPath)\\n            return ssc\\n\\n        jssc = gw.jvm.JavaStreamingContext(ssc_option.get())\\n\\n        # If there is already an active instance of Python SparkContext use it, or create a new one\\n        if not SparkContext._active_spark_context:\\n            jsc = jssc.sparkContext()\\n            conf = SparkConf(_jconf=jsc.getConf())\\n            SparkContext(conf=conf, gateway=gw, jsc=jsc)\\n\\n        sc = SparkContext._active_spark_context\\n\\n        # update ctx in serializer\\n        cls._transformerSerializer.ctx = sc\\n        return StreamingContext(sc, None, jssc)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'load_tf_weights_in_gpt2',\n",
       "  'docstring': 'Load tf checkpoints in a pytorch model',\n",
       "  'code': 'def load_tf_weights_in_gpt2(model, gpt2_checkpoint_path):\\n    \"\"\" Load tf checkpoints in a pytorch model\\n    \"\"\"\\n    try:\\n        import re\\n        import numpy as np\\n        import tensorflow as tf\\n    except ImportError:\\n        print(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\\n            \"https://www.tensorflow.org/install/ for installation instructions.\")\\n        raise\\n    tf_path = os.path.abspath(gpt2_checkpoint_path)\\n    print(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\\n    # Load weights from TF model\\n    init_vars = tf.train.list_variables(tf_path)\\n    names = []\\n    arrays = []\\n    for name, shape in init_vars:\\n        print(\"Loading TF weight {} with shape {}\".format(name, shape))\\n        array = tf.train.load_variable(tf_path, name)\\n        names.append(name)\\n        arrays.append(array.squeeze())\\n\\n    for name, array in zip(names, arrays):\\n        name = name[6:]  # skip \"model/\"\\n        name = name.split(\\'/\\')\\n        pointer = model\\n        for m_name in name:\\n            if re.fullmatch(r\\'[A-Za-z]+\\\\d+\\', m_name):\\n                l = re.split(r\\'(\\\\d+)\\', m_name)\\n            else:\\n                l = [m_name]\\n            if l[0] == \\'w\\' or l[0] == \\'g\\':\\n                pointer = getattr(pointer, \\'weight\\')\\n            elif l[0] == \\'b\\':\\n                pointer = getattr(pointer, \\'bias\\')\\n            elif l[0] == \\'wpe\\' or l[0] == \\'wte\\':\\n                pointer = getattr(pointer, l[0])\\n                pointer = getattr(pointer, \\'weight\\')\\n            else:\\n                pointer = getattr(pointer, l[0])\\n            if len(l) >= 2:\\n                num = int(l[1])\\n                pointer = pointer[num]\\n        try:\\n            assert pointer.shape == array.shape\\n        except AssertionError as e:\\n            e.args += (pointer.shape, array.shape)\\n            raise\\n        print(\"Initialize PyTorch weight {}\".format(name))\\n        pointer.data = torch.from_numpy(array)\\n    return model',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'GPT2PreTrainedModel.from_pretrained',\n",
       "  'docstring': 'Instantiate a GPT2PreTrainedModel from a pre-trained model file or a pytorch state dict.\\n        Download and cache the pre-trained model file if needed.\\n\\n        Params:\\n            pretrained_model_name_or_path: either:\\n                - a str with the name of a pre-trained model to load selected in the list of:\\n                    . `gpt2`\\n                - a path or url to a pretrained model archive containing:\\n                    . `gpt2_config.json` a configuration file for the model\\n                    . `pytorch_model.bin` a PyTorch dump of a GPT2Model instance\\n                - a path or url to a pretrained model archive containing:\\n                    . `gpt2_config.json` a configuration file for the model\\n                    . a TensorFlow checkpoint with trained weights\\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\\n            state_dict: an optional state dictionary (collections.OrderedDict object) to use instead of pre-trained models\\n            *inputs, **kwargs: additional input for the specific GPT class',\n",
       "  'code': 'def from_pretrained(\\n        cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None, from_tf=False, *inputs, **kwargs\\n    ):\\n        \"\"\"\\n        Instantiate a GPT2PreTrainedModel from a pre-trained model file or a pytorch state dict.\\n        Download and cache the pre-trained model file if needed.\\n\\n        Params:\\n            pretrained_model_name_or_path: either:\\n                - a str with the name of a pre-trained model to load selected in the list of:\\n                    . `gpt2`\\n                - a path or url to a pretrained model archive containing:\\n                    . `gpt2_config.json` a configuration file for the model\\n                    . `pytorch_model.bin` a PyTorch dump of a GPT2Model instance\\n                - a path or url to a pretrained model archive containing:\\n                    . `gpt2_config.json` a configuration file for the model\\n                    . a TensorFlow checkpoint with trained weights\\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\\n            state_dict: an optional state dictionary (collections.OrderedDict object) to use instead of pre-trained models\\n            *inputs, **kwargs: additional input for the specific GPT class\\n        \"\"\"\\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\\n            config_file = PRETRAINED_CONFIG_ARCHIVE_MAP[pretrained_model_name_or_path]\\n        else:\\n            archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\\n        # redirect to the cache, if necessary\\n        try:\\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\\n        except EnvironmentError:\\n            logger.error(\\n                \"Model name \\'{}\\' was not found in model name list ({}). \"\\n                \"We assumed \\'{}\\' was a path or url but couldn\\'t find files {} and {} \"\\n                \"at this path or url.\".format(\\n                    pretrained_model_name_or_path, \", \".join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()), pretrained_model_name_or_path,\\n                    archive_file, config_file\\n                )\\n            )\\n            return None\\n        if resolved_archive_file == archive_file and resolved_config_file == config_file:\\n            logger.info(\"loading weights file {}\".format(archive_file))\\n            logger.info(\"loading configuration file {}\".format(config_file))\\n        else:\\n            logger.info(\"loading weights file {} from cache at {}\".format(\\n                archive_file, resolved_archive_file))\\n            logger.info(\"loading configuration file {} from cache at {}\".format(\\n                config_file, resolved_config_file))\\n        # Load config\\n        config = GPT2Config.from_json_file(resolved_config_file)\\n        logger.info(\"Model config {}\".format(config))\\n        # Instantiate model.\\n        model = cls(config, *inputs, **kwargs)\\n        if state_dict is None and not from_tf:\\n            state_dict = torch.load(resolved_archive_file, map_location=\\'cpu\\')\\n        if from_tf:\\n            # Directly load from a TensorFlow checkpoint (stored as NumPy array)\\n            return load_tf_weights_in_gpt2(model, resolved_archive_file)\\n\\n        old_keys = []\\n        new_keys = []\\n        for key in state_dict.keys():\\n            new_key = None\\n            if key.endswith(\".g\"):\\n                new_key = key[:-2] + \".weight\"\\n            elif key.endswith(\".b\"):\\n                new_key = key[:-2] + \".bias\"\\n            elif key.endswith(\".w\"):\\n                new_key = key[:-2] + \".weight\"\\n            if new_key:\\n                old_keys.append(key)\\n                new_keys.append(new_key)\\n        for old_key, new_key in zip(old_keys, new_keys):\\n            state_dict[new_key] = state_dict.pop(old_key)\\n\\n        missing_keys = []\\n        unexpected_keys = []\\n        error_msgs = []\\n        # copy state_dict so _load_from_state_dict can modify it\\n        metadata = getattr(state_dict, \"_metadata\", None)\\n        state_dict = state_dict.copy()\\n        if metadata is not None:\\n            state_dict._metadata = metadata\\n\\n        def load(module, prefix=\"\"):\\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\\n            module._load_from_state_dict(\\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\\n            )\\n            for name, child in module._modules.items():\\n                if child is not None:\\n                    load(child, prefix + name + \".\")\\n\\n        start_model = model\\n        if hasattr(model, \"transformer\") and all(not s.startswith(\\'transformer.\\') for s in state_dict.keys()):\\n            start_model = model.transformer\\n        load(start_model, prefix=\"\")\\n\\n        if len(missing_keys) > 0:\\n            logger.info(\\n                \"Weights of {} not initialized from pretrained model: {}\".format(model.__class__.__name__, missing_keys)\\n            )\\n        if len(unexpected_keys) > 0:\\n            logger.info(\\n                \"Weights from pretrained model not used in {}: {}\".format(model.__class__.__name__, unexpected_keys)\\n            )\\n        if len(error_msgs) > 0:\\n            raise RuntimeError(\\n                \"Error(s) in loading state_dict for {}:\\\\n\\\\t{}\".format(model.__class__.__name__, \"\\\\n\\\\t\".join(error_msgs))\\n            )\\n\\n        # Make sure we are still sharing the output and input embeddings after loading weights\\n        model.set_tied()\\n        return model',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'load_tf_weights_in_bert',\n",
       "  'docstring': 'Load tf checkpoints in a pytorch model',\n",
       "  'code': 'def load_tf_weights_in_bert(model, tf_checkpoint_path):\\n    \"\"\" Load tf checkpoints in a pytorch model\\n    \"\"\"\\n    try:\\n        import re\\n        import numpy as np\\n        import tensorflow as tf\\n    except ImportError:\\n        print(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\\n            \"https://www.tensorflow.org/install/ for installation instructions.\")\\n        raise\\n    tf_path = os.path.abspath(tf_checkpoint_path)\\n    print(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\\n    # Load weights from TF model\\n    init_vars = tf.train.list_variables(tf_path)\\n    names = []\\n    arrays = []\\n    for name, shape in init_vars:\\n        print(\"Loading TF weight {} with shape {}\".format(name, shape))\\n        array = tf.train.load_variable(tf_path, name)\\n        names.append(name)\\n        arrays.append(array)\\n\\n    for name, array in zip(names, arrays):\\n        name = name.split(\\'/\\')\\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\\n        # which are not required for using pretrained model\\n        if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\\n            print(\"Skipping {}\".format(\"/\".join(name)))\\n            continue\\n        pointer = model\\n        for m_name in name:\\n            if re.fullmatch(r\\'[A-Za-z]+_\\\\d+\\', m_name):\\n                l = re.split(r\\'_(\\\\d+)\\', m_name)\\n            else:\\n                l = [m_name]\\n            if l[0] == \\'kernel\\' or l[0] == \\'gamma\\':\\n                pointer = getattr(pointer, \\'weight\\')\\n            elif l[0] == \\'output_bias\\' or l[0] == \\'beta\\':\\n                pointer = getattr(pointer, \\'bias\\')\\n            elif l[0] == \\'output_weights\\':\\n                pointer = getattr(pointer, \\'weight\\')\\n            elif l[0] == \\'squad\\':\\n                pointer = getattr(pointer, \\'classifier\\')\\n            else:\\n                try:\\n                    pointer = getattr(pointer, l[0])\\n                except AttributeError:\\n                    print(\"Skipping {}\".format(\"/\".join(name)))\\n                    continue\\n            if len(l) >= 2:\\n                num = int(l[1])\\n                pointer = pointer[num]\\n        if m_name[-11:] == \\'_embeddings\\':\\n            pointer = getattr(pointer, \\'weight\\')\\n        elif m_name == \\'kernel\\':\\n            array = np.transpose(array)\\n        try:\\n            assert pointer.shape == array.shape\\n        except AssertionError as e:\\n            e.args += (pointer.shape, array.shape)\\n            raise\\n        print(\"Initialize PyTorch weight {}\".format(name))\\n        pointer.data = torch.from_numpy(array)\\n    return model',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'BertPreTrainedModel.from_pretrained',\n",
       "  'docstring': 'Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\\n        Download and cache the pre-trained model file if needed.\\n\\n        Params:\\n            pretrained_model_name_or_path: either:\\n                - a str with the name of a pre-trained model to load selected in the list of:\\n                    . `bert-base-uncased`\\n                    . `bert-large-uncased`\\n                    . `bert-base-cased`\\n                    . `bert-large-cased`\\n                    . `bert-base-multilingual-uncased`\\n                    . `bert-base-multilingual-cased`\\n                    . `bert-base-chinese`\\n                - a path or url to a pretrained model archive containing:\\n                    . `bert_config.json` a configuration file for the model\\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\\n                - a path or url to a pretrained model archive containing:\\n                    . `bert_config.json` a configuration file for the model\\n                    . `model.chkpt` a TensorFlow checkpoint\\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\\n            *inputs, **kwargs: additional input for the specific Bert class\\n                (ex: num_labels for BertForSequenceClassification)',\n",
       "  'code': 'def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\\n        \"\"\"\\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\\n        Download and cache the pre-trained model file if needed.\\n\\n        Params:\\n            pretrained_model_name_or_path: either:\\n                - a str with the name of a pre-trained model to load selected in the list of:\\n                    . `bert-base-uncased`\\n                    . `bert-large-uncased`\\n                    . `bert-base-cased`\\n                    . `bert-large-cased`\\n                    . `bert-base-multilingual-uncased`\\n                    . `bert-base-multilingual-cased`\\n                    . `bert-base-chinese`\\n                - a path or url to a pretrained model archive containing:\\n                    . `bert_config.json` a configuration file for the model\\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\\n                - a path or url to a pretrained model archive containing:\\n                    . `bert_config.json` a configuration file for the model\\n                    . `model.chkpt` a TensorFlow checkpoint\\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\\n            *inputs, **kwargs: additional input for the specific Bert class\\n                (ex: num_labels for BertForSequenceClassification)\\n        \"\"\"\\n        state_dict = kwargs.get(\\'state_dict\\', None)\\n        kwargs.pop(\\'state_dict\\', None)\\n        cache_dir = kwargs.get(\\'cache_dir\\', None)\\n        kwargs.pop(\\'cache_dir\\', None)\\n        from_tf = kwargs.get(\\'from_tf\\', False)\\n        kwargs.pop(\\'from_tf\\', None)\\n\\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\\n        else:\\n            archive_file = pretrained_model_name_or_path\\n        # redirect to the cache, if necessary\\n        try:\\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\\n        except EnvironmentError:\\n            logger.error(\\n                \"Model name \\'{}\\' was not found in model name list ({}). \"\\n                \"We assumed \\'{}\\' was a path or url but couldn\\'t find any file \"\\n                \"associated to this path or url.\".format(\\n                    pretrained_model_name_or_path,\\n                    \\', \\'.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\\n                    archive_file))\\n            return None\\n        if resolved_archive_file == archive_file:\\n            logger.info(\"loading archive file {}\".format(archive_file))\\n        else:\\n            logger.info(\"loading archive file {} from cache at {}\".format(\\n                archive_file, resolved_archive_file))\\n        tempdir = None\\n        if os.path.isdir(resolved_archive_file) or from_tf:\\n            serialization_dir = resolved_archive_file\\n        else:\\n            # Extract archive to temp dir\\n            tempdir = tempfile.mkdtemp()\\n            logger.info(\"extracting archive file {} to temp dir {}\".format(\\n                resolved_archive_file, tempdir))\\n            with tarfile.open(resolved_archive_file, \\'r:gz\\') as archive:\\n                archive.extractall(tempdir)\\n            serialization_dir = tempdir\\n        # Load config\\n        config_file = os.path.join(serialization_dir, CONFIG_NAME)\\n        if not os.path.exists(config_file):\\n            # Backward compatibility with old naming format\\n            config_file = os.path.join(serialization_dir, BERT_CONFIG_NAME)\\n        config = BertConfig.from_json_file(config_file)\\n        logger.info(\"Model config {}\".format(config))\\n        # Instantiate model.\\n        model = cls(config, *inputs, **kwargs)\\n        if state_dict is None and not from_tf:\\n            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\\n            state_dict = torch.load(weights_path, map_location=\\'cpu\\')\\n        if tempdir:\\n            # Clean up temp dir\\n            shutil.rmtree(tempdir)\\n        if from_tf:\\n            # Directly load from a TensorFlow checkpoint\\n            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\\n            return load_tf_weights_in_bert(model, weights_path)\\n        # Load from a PyTorch state_dict\\n        old_keys = []\\n        new_keys = []\\n        for key in state_dict.keys():\\n            new_key = None\\n            if \\'gamma\\' in key:\\n                new_key = key.replace(\\'gamma\\', \\'weight\\')\\n            if \\'beta\\' in key:\\n                new_key = key.replace(\\'beta\\', \\'bias\\')\\n            if new_key:\\n                old_keys.append(key)\\n                new_keys.append(new_key)\\n        for old_key, new_key in zip(old_keys, new_keys):\\n            state_dict[new_key] = state_dict.pop(old_key)\\n\\n        missing_keys = []\\n        unexpected_keys = []\\n        error_msgs = []\\n        # copy state_dict so _load_from_state_dict can modify it\\n        metadata = getattr(state_dict, \\'_metadata\\', None)\\n        state_dict = state_dict.copy()\\n        if metadata is not None:\\n            state_dict._metadata = metadata\\n\\n        def load(module, prefix=\\'\\'):\\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\\n            module._load_from_state_dict(\\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\\n            for name, child in module._modules.items():\\n                if child is not None:\\n                    load(child, prefix + name + \\'.\\')\\n        start_prefix = \\'\\'\\n        if not hasattr(model, \\'bert\\') and any(s.startswith(\\'bert.\\') for s in state_dict.keys()):\\n            start_prefix = \\'bert.\\'\\n        load(model, prefix=start_prefix)\\n        if len(missing_keys) > 0:\\n            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\\n                model.__class__.__name__, missing_keys))\\n        if len(unexpected_keys) > 0:\\n            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\\n                model.__class__.__name__, unexpected_keys))\\n        if len(error_msgs) > 0:\\n            raise RuntimeError(\\'Error(s) in loading state_dict for {}:\\\\n\\\\t{}\\'.format(\\n                               model.__class__.__name__, \"\\\\n\\\\t\".join(error_msgs)))\\n        return model',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'load_tf_weights_in_openai_gpt',\n",
       "  'docstring': 'Load tf pre-trained weights in a pytorch model (from NumPy arrays here)',\n",
       "  'code': 'def load_tf_weights_in_openai_gpt(model, openai_checkpoint_folder_path):\\n    \"\"\" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)\\n    \"\"\"\\n    import re\\n    import numpy as np\\n    print(\"Loading weights...\")\\n    names = json.load(open(openai_checkpoint_folder_path + \\'/parameters_names.json\\', \"r\", encoding=\\'utf-8\\'))\\n    shapes = json.load(open(openai_checkpoint_folder_path + \\'/params_shapes.json\\', \"r\", encoding=\\'utf-8\\'))\\n    offsets = np.cumsum([np.prod(shape) for shape in shapes])\\n    init_params = [np.load(openai_checkpoint_folder_path + \\'/params_{}.npy\\'.format(n)) for n in range(10)]\\n    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\\n    init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\\n\\n    # This was used when we had a single embedding matrix for positions and tokens\\n    # init_params[0] = np.concatenate([init_params[1], init_params[0]], 0)\\n    # del init_params[1]\\n    init_params = [arr.squeeze() for arr in init_params]\\n\\n    try:\\n        assert model.tokens_embed.weight.shape == init_params[1].shape\\n        assert model.positions_embed.weight.shape == init_params[0].shape\\n    except AssertionError as e:\\n        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)\\n        e.args += (model.positions_embed.weight.shape, init_params[0].shape)\\n        raise\\n\\n    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])\\n    model.positions_embed.weight.data = torch.from_numpy(init_params[0])\\n    names.pop(0)\\n    # Pop position and token embedding arrays\\n    init_params.pop(0)\\n    init_params.pop(0)\\n\\n    for name, array in zip(names, init_params): # names[1:n_transfer], init_params[1:n_transfer]):\\n        name = name[6:]  # skip \"model/\"\\n        assert name[-2:] == \":0\"\\n        name = name[:-2]\\n        name = name.split(\\'/\\')\\n        pointer = model\\n        for m_name in name:\\n            if re.fullmatch(r\\'[A-Za-z]+\\\\d+\\', m_name):\\n                l = re.split(r\\'(\\\\d+)\\', m_name)\\n            else:\\n                l = [m_name]\\n            if l[0] == \\'g\\':\\n                pointer = getattr(pointer, \\'weight\\')\\n            elif l[0] == \\'b\\':\\n                pointer = getattr(pointer, \\'bias\\')\\n            elif l[0] == \\'w\\':\\n                pointer = getattr(pointer, \\'weight\\')\\n            else:\\n                pointer = getattr(pointer, l[0])\\n            if len(l) >= 2:\\n                num = int(l[1])\\n                pointer = pointer[num]\\n        try:\\n            assert pointer.shape == array.shape\\n        except AssertionError as e:\\n            e.args += (pointer.shape, array.shape)\\n            raise\\n        try:\\n            assert pointer.shape == array.shape\\n        except AssertionError as e:\\n            e.args += (pointer.shape, array.shape)\\n            raise\\n        print(\"Initialize PyTorch weight {}\".format(name))\\n        pointer.data = torch.from_numpy(array)\\n    return model',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'load_tf_weights_in_transfo_xl',\n",
       "  'docstring': 'Load tf checkpoints in a pytorch model',\n",
       "  'code': 'def load_tf_weights_in_transfo_xl(model, config, tf_path):\\n    \"\"\" Load tf checkpoints in a pytorch model\\n    \"\"\"\\n    try:\\n        import numpy as np\\n        import tensorflow as tf\\n    except ImportError:\\n        print(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\\n            \"https://www.tensorflow.org/install/ for installation instructions.\")\\n        raise\\n    # Build TF to PyTorch weights loading map\\n    tf_to_pt_map = build_tf_to_pytorch_map(model, config)\\n\\n    # Load weights from TF model\\n    init_vars = tf.train.list_variables(tf_path)\\n    tf_weights = {}\\n    for name, shape in init_vars:\\n        print(\"Loading TF weight {} with shape {}\".format(name, shape))\\n        array = tf.train.load_variable(tf_path, name)\\n        tf_weights[name] = array\\n\\n    for name, pointer in tf_to_pt_map.items():\\n        assert name in tf_weights\\n        array = tf_weights[name]\\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\\n        # which are not required for using pretrained model\\n        if \\'kernel\\' in name or \\'proj\\' in name:\\n            array = np.transpose(array)\\n        if (\\'r_r_bias\\' in name or \\'r_w_bias\\' in name) and len(pointer) > 1:\\n            # Here we will split the TF weigths\\n            assert len(pointer) == array.shape[0]\\n            for i, p_i in enumerate(pointer):\\n                arr_i = array[i, ...]\\n                try:\\n                    assert p_i.shape == arr_i.shape\\n                except AssertionError as e:\\n                    e.args += (p_i.shape, arr_i.shape)\\n                    raise\\n                print(\"Initialize PyTorch weight {} for layer {}\".format(name, i))\\n                p_i.data = torch.from_numpy(arr_i)\\n        else:\\n            try:\\n                assert pointer.shape == array.shape\\n            except AssertionError as e:\\n                e.args += (pointer.shape, array.shape)\\n                raise\\n            print(\"Initialize PyTorch weight {}\".format(name))\\n            pointer.data = torch.from_numpy(array)\\n        tf_weights.pop(name, None)\\n        tf_weights.pop(name + \\'/Adam\\', None)\\n        tf_weights.pop(name + \\'/Adam_1\\', None)\\n\\n    print(\"Weights not copied to PyTorch model: {}\".format(\\', \\'.join(tf_weights.keys())))\\n    return model',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'TransfoXLPreTrainedModel.from_pretrained',\n",
       "  'docstring': 'Instantiate a TransfoXLPreTrainedModel from a pre-trained model file or a pytorch state dict.\\n        Download and cache the pre-trained model file if needed.\\n\\n        Params:\\n            pretrained_model_name_or_path: either:\\n                - a str with the name of a pre-trained model to load selected in the list of:\\n                    . `transfo-xl`\\n                - a path or url to a pretrained model archive containing:\\n                    . `transfo_xl_config.json` a configuration file for the model\\n                    . `pytorch_model.bin` a PyTorch dump of a TransfoXLModel instance\\n                - a path or url to a pretrained model archive containing:\\n                    . `bert_config.json` a configuration file for the model\\n                    . `model.chkpt` a TensorFlow checkpoint\\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of pre-trained models\\n            *inputs, **kwargs: additional input for the specific Bert class\\n                (ex: num_labels for BertForSequenceClassification)',\n",
       "  'code': 'def from_pretrained(cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None,\\n                        from_tf=False, *inputs, **kwargs):\\n        \"\"\"\\n        Instantiate a TransfoXLPreTrainedModel from a pre-trained model file or a pytorch state dict.\\n        Download and cache the pre-trained model file if needed.\\n\\n        Params:\\n            pretrained_model_name_or_path: either:\\n                - a str with the name of a pre-trained model to load selected in the list of:\\n                    . `transfo-xl`\\n                - a path or url to a pretrained model archive containing:\\n                    . `transfo_xl_config.json` a configuration file for the model\\n                    . `pytorch_model.bin` a PyTorch dump of a TransfoXLModel instance\\n                - a path or url to a pretrained model archive containing:\\n                    . `bert_config.json` a configuration file for the model\\n                    . `model.chkpt` a TensorFlow checkpoint\\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of pre-trained models\\n            *inputs, **kwargs: additional input for the specific Bert class\\n                (ex: num_labels for BertForSequenceClassification)\\n        \"\"\"\\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\\n            config_file = PRETRAINED_CONFIG_ARCHIVE_MAP[pretrained_model_name_or_path]\\n        else:\\n            archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\\n        # redirect to the cache, if necessary\\n        try:\\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\\n        except EnvironmentError:\\n            logger.error(\\n                \"Model name \\'{}\\' was not found in model name list ({}). \"\\n                \"We assumed \\'{}\\' was a path or url but couldn\\'t find files {} and {} \"\\n                \"at this path or url.\".format(\\n                    pretrained_model_name_or_path,\\n                    \\', \\'.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\\n                    pretrained_model_name_or_path,\\n                    archive_file, config_file))\\n            return None\\n        if resolved_archive_file == archive_file and resolved_config_file == config_file:\\n            logger.info(\"loading weights file {}\".format(archive_file))\\n            logger.info(\"loading configuration file {}\".format(config_file))\\n        else:\\n            logger.info(\"loading weights file {} from cache at {}\".format(\\n                archive_file, resolved_archive_file))\\n            logger.info(\"loading configuration file {} from cache at {}\".format(\\n                config_file, resolved_config_file))\\n        # Load config\\n        config = TransfoXLConfig.from_json_file(resolved_config_file)\\n        logger.info(\"Model config {}\".format(config))\\n        # Instantiate model.\\n        model = cls(config, *inputs, **kwargs)\\n        if state_dict is None and not from_tf:\\n            state_dict = torch.load(resolved_archive_file, map_location=\\'cpu\\')\\n        if from_tf:\\n            # Directly load from a TensorFlow checkpoint\\n            return load_tf_weights_in_transfo_xl(model, config, pretrained_model_name_or_path)\\n\\n        missing_keys = []\\n        unexpected_keys = []\\n        error_msgs = []\\n        # copy state_dict so _load_from_state_dict can modify it\\n        metadata = getattr(state_dict, \\'_metadata\\', None)\\n        state_dict = state_dict.copy()\\n        if metadata is not None:\\n            state_dict._metadata = metadata\\n\\n        def load(module, prefix=\\'\\'):\\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\\n            module._load_from_state_dict(\\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\\n            for name, child in module._modules.items():\\n                if child is not None:\\n                    load(child, prefix + name + \\'.\\')\\n\\n        start_prefix = \\'\\'\\n        if not hasattr(model, \\'transformer\\') and any(s.startswith(\\'transformer.\\') for s in state_dict.keys()):\\n            start_prefix = \\'transformer.\\'\\n        load(model, prefix=start_prefix)\\n\\n        if len(missing_keys) > 0:\\n            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\\n                model.__class__.__name__, missing_keys))\\n        if len(unexpected_keys) > 0:\\n            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\\n                model.__class__.__name__, unexpected_keys))\\n        if len(error_msgs) > 0:\\n            raise RuntimeError(\\'Error(s) in loading state_dict for {}:\\\\n\\\\t{}\\'.format(\\n                               model.__class__.__name__, \"\\\\n\\\\t\".join(error_msgs)))\\n        # Make sure we are still sharing the input and output embeddings\\n        if hasattr(model, \\'tie_weights\\'):\\n            model.tie_weights()\\n        return model',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Index._convert_listlike_indexer',\n",
       "  'docstring': 'Parameters\\n        ----------\\n        keyarr : list-like\\n            Indexer to convert.\\n\\n        Returns\\n        -------\\n        indexer : numpy.ndarray or None\\n            Return an ndarray or None if cannot convert.\\n        keyarr : numpy.ndarray\\n            Return tuple-safe keys.',\n",
       "  'code': 'def _convert_listlike_indexer(self, keyarr, kind=None):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        keyarr : list-like\\n            Indexer to convert.\\n\\n        Returns\\n        -------\\n        indexer : numpy.ndarray or None\\n            Return an ndarray or None if cannot convert.\\n        keyarr : numpy.ndarray\\n            Return tuple-safe keys.\\n        \"\"\"\\n        if isinstance(keyarr, Index):\\n            keyarr = self._convert_index_indexer(keyarr)\\n        else:\\n            keyarr = self._convert_arr_indexer(keyarr)\\n\\n        indexer = self._convert_list_indexer(keyarr, kind=kind)\\n        return indexer, keyarr',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Index.putmask',\n",
       "  'docstring': 'Return a new Index of the values set with the mask.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.putmask',\n",
       "  'code': 'def putmask(self, mask, value):\\n        \"\"\"\\n        Return a new Index of the values set with the mask.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.putmask\\n        \"\"\"\\n        values = self.values.copy()\\n        try:\\n            np.putmask(values, mask, self._convert_for_op(value))\\n            return self._shallow_copy(values)\\n        except (ValueError, TypeError) as err:\\n            if is_object_dtype(self):\\n                raise err\\n\\n            # coerces to object\\n            return self.astype(object).putmask(mask, value)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Index.asof_locs',\n",
       "  'docstring': 'Find the locations (indices) of the labels from the index for\\n        every entry in the `where` argument.\\n\\n        As in the `asof` function, if the label (a particular entry in\\n        `where`) is not in the index, the latest index label upto the\\n        passed label is chosen and its index returned.\\n\\n        If all of the labels in the index are later than a label in `where`,\\n        -1 is returned.\\n\\n        `mask` is used to ignore NA values in the index during calculation.\\n\\n        Parameters\\n        ----------\\n        where : Index\\n            An Index consisting of an array of timestamps.\\n        mask : array-like\\n            Array of booleans denoting where values in the original\\n            data are not NA.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n            An array of locations (indices) of the labels from the Index\\n            which correspond to the return values of the `asof` function\\n            for every element in `where`.',\n",
       "  'code': 'def asof_locs(self, where, mask):\\n        \"\"\"\\n        Find the locations (indices) of the labels from the index for\\n        every entry in the `where` argument.\\n\\n        As in the `asof` function, if the label (a particular entry in\\n        `where`) is not in the index, the latest index label upto the\\n        passed label is chosen and its index returned.\\n\\n        If all of the labels in the index are later than a label in `where`,\\n        -1 is returned.\\n\\n        `mask` is used to ignore NA values in the index during calculation.\\n\\n        Parameters\\n        ----------\\n        where : Index\\n            An Index consisting of an array of timestamps.\\n        mask : array-like\\n            Array of booleans denoting where values in the original\\n            data are not NA.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n            An array of locations (indices) of the labels from the Index\\n            which correspond to the return values of the `asof` function\\n            for every element in `where`.\\n        \"\"\"\\n        locs = self.values[mask].searchsorted(where.values, side=\\'right\\')\\n        locs = np.where(locs > 0, locs - 1, 0)\\n\\n        result = np.arange(len(self))[mask].take(locs)\\n\\n        first = mask.argmax()\\n        result[(locs == 0) & (where.values < self.values[first])] = -1\\n\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Index.sort_values',\n",
       "  'docstring': \"Return a sorted copy of the index.\\n\\n        Return a sorted copy of the index, and optionally return the indices\\n        that sorted the index itself.\\n\\n        Parameters\\n        ----------\\n        return_indexer : bool, default False\\n            Should the indices that would sort the index be returned.\\n        ascending : bool, default True\\n            Should the index values be sorted in an ascending order.\\n\\n        Returns\\n        -------\\n        sorted_index : pandas.Index\\n            Sorted copy of the index.\\n        indexer : numpy.ndarray, optional\\n            The indices that the index itself was sorted by.\\n\\n        See Also\\n        --------\\n        Series.sort_values : Sort values of a Series.\\n        DataFrame.sort_values : Sort values in a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.Index([10, 100, 1, 1000])\\n        >>> idx\\n        Int64Index([10, 100, 1, 1000], dtype='int64')\\n\\n        Sort values in ascending order (default behavior).\\n\\n        >>> idx.sort_values()\\n        Int64Index([1, 10, 100, 1000], dtype='int64')\\n\\n        Sort values in descending order, and also get the indices `idx` was\\n        sorted by.\\n\\n        >>> idx.sort_values(ascending=False, return_indexer=True)\\n        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\",\n",
       "  'code': 'def sort_values(self, return_indexer=False, ascending=True):\\n        \"\"\"\\n        Return a sorted copy of the index.\\n\\n        Return a sorted copy of the index, and optionally return the indices\\n        that sorted the index itself.\\n\\n        Parameters\\n        ----------\\n        return_indexer : bool, default False\\n            Should the indices that would sort the index be returned.\\n        ascending : bool, default True\\n            Should the index values be sorted in an ascending order.\\n\\n        Returns\\n        -------\\n        sorted_index : pandas.Index\\n            Sorted copy of the index.\\n        indexer : numpy.ndarray, optional\\n            The indices that the index itself was sorted by.\\n\\n        See Also\\n        --------\\n        Series.sort_values : Sort values of a Series.\\n        DataFrame.sort_values : Sort values in a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.Index([10, 100, 1, 1000])\\n        >>> idx\\n        Int64Index([10, 100, 1, 1000], dtype=\\'int64\\')\\n\\n        Sort values in ascending order (default behavior).\\n\\n        >>> idx.sort_values()\\n        Int64Index([1, 10, 100, 1000], dtype=\\'int64\\')\\n\\n        Sort values in descending order, and also get the indices `idx` was\\n        sorted by.\\n\\n        >>> idx.sort_values(ascending=False, return_indexer=True)\\n        (Int64Index([1000, 100, 10, 1], dtype=\\'int64\\'), array([3, 1, 0, 2]))\\n        \"\"\"\\n        _as = self.argsort()\\n        if not ascending:\\n            _as = _as[::-1]\\n\\n        sorted_index = self.take(_as)\\n\\n        if return_indexer:\\n            return sorted_index, _as\\n        else:\\n            return sorted_index',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Index.argsort',\n",
       "  'docstring': \"Return the integer indices that would sort the index.\\n\\n        Parameters\\n        ----------\\n        *args\\n            Passed to `numpy.ndarray.argsort`.\\n        **kwargs\\n            Passed to `numpy.ndarray.argsort`.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n            Integer indices that would sort the index if used as\\n            an indexer.\\n\\n        See Also\\n        --------\\n        numpy.argsort : Similar method for NumPy arrays.\\n        Index.sort_values : Return sorted copy of Index.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\\n        >>> idx\\n        Index(['b', 'a', 'd', 'c'], dtype='object')\\n\\n        >>> order = idx.argsort()\\n        >>> order\\n        array([1, 0, 3, 2])\\n\\n        >>> idx[order]\\n        Index(['a', 'b', 'c', 'd'], dtype='object')\",\n",
       "  'code': 'def argsort(self, *args, **kwargs):\\n        \"\"\"\\n        Return the integer indices that would sort the index.\\n\\n        Parameters\\n        ----------\\n        *args\\n            Passed to `numpy.ndarray.argsort`.\\n        **kwargs\\n            Passed to `numpy.ndarray.argsort`.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n            Integer indices that would sort the index if used as\\n            an indexer.\\n\\n        See Also\\n        --------\\n        numpy.argsort : Similar method for NumPy arrays.\\n        Index.sort_values : Return sorted copy of Index.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.Index([\\'b\\', \\'a\\', \\'d\\', \\'c\\'])\\n        >>> idx\\n        Index([\\'b\\', \\'a\\', \\'d\\', \\'c\\'], dtype=\\'object\\')\\n\\n        >>> order = idx.argsort()\\n        >>> order\\n        array([1, 0, 3, 2])\\n\\n        >>> idx[order]\\n        Index([\\'a\\', \\'b\\', \\'c\\', \\'d\\'], dtype=\\'object\\')\\n        \"\"\"\\n        result = self.asi8\\n        if result is None:\\n            result = np.array(self)\\n        return result.argsort(*args, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Index.isin',\n",
       "  'docstring': \"Return a boolean array where the index values are in `values`.\\n\\n        Compute boolean array of whether each index value is found in the\\n        passed set of values. The length of the returned boolean array matches\\n        the length of the index.\\n\\n        Parameters\\n        ----------\\n        values : set or list-like\\n            Sought values.\\n\\n            .. versionadded:: 0.18.1\\n\\n               Support for values as a set.\\n\\n        level : str or int, optional\\n            Name or position of the index level to use (if the index is a\\n            `MultiIndex`).\\n\\n        Returns\\n        -------\\n        is_contained : ndarray\\n            NumPy array of boolean values.\\n\\n        See Also\\n        --------\\n        Series.isin : Same for Series.\\n        DataFrame.isin : Same method for DataFrames.\\n\\n        Notes\\n        -----\\n        In the case of `MultiIndex` you must either specify `values` as a\\n        list-like object containing tuples that are the same length as the\\n        number of levels, or specify `level`. Otherwise it will raise a\\n        ``ValueError``.\\n\\n        If `level` is specified:\\n\\n        - if it is the name of one *and only one* index level, use that level;\\n        - otherwise it should be a number indicating level position.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.Index([1,2,3])\\n        >>> idx\\n        Int64Index([1, 2, 3], dtype='int64')\\n\\n        Check whether each index value in a list of values.\\n        >>> idx.isin([1, 4])\\n        array([ True, False, False])\\n\\n        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\\n        ...                                  ['red', 'blue', 'green']],\\n        ...                                  names=('number', 'color'))\\n        >>> midx\\n        MultiIndex(levels=[[1, 2, 3], ['blue', 'green', 'red']],\\n                   codes=[[0, 1, 2], [2, 0, 1]],\\n                   names=['number', 'color'])\\n\\n        Check whether the strings in the 'color' level of the MultiIndex\\n        are in a list of colors.\\n\\n        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\\n        array([ True, False, False])\\n\\n        To check across the levels of a MultiIndex, pass a list of tuples:\\n\\n        >>> midx.isin([(1, 'red'), (3, 'red')])\\n        array([ True, False, False])\\n\\n        For a DatetimeIndex, string values in `values` are converted to\\n        Timestamps.\\n\\n        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\\n        >>> dti = pd.to_datetime(dates)\\n        >>> dti\\n        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\\n        dtype='datetime64[ns]', freq=None)\\n\\n        >>> dti.isin(['2000-03-11'])\\n        array([ True, False, False])\",\n",
       "  'code': 'def isin(self, values, level=None):\\n        \"\"\"\\n        Return a boolean array where the index values are in `values`.\\n\\n        Compute boolean array of whether each index value is found in the\\n        passed set of values. The length of the returned boolean array matches\\n        the length of the index.\\n\\n        Parameters\\n        ----------\\n        values : set or list-like\\n            Sought values.\\n\\n            .. versionadded:: 0.18.1\\n\\n               Support for values as a set.\\n\\n        level : str or int, optional\\n            Name or position of the index level to use (if the index is a\\n            `MultiIndex`).\\n\\n        Returns\\n        -------\\n        is_contained : ndarray\\n            NumPy array of boolean values.\\n\\n        See Also\\n        --------\\n        Series.isin : Same for Series.\\n        DataFrame.isin : Same method for DataFrames.\\n\\n        Notes\\n        -----\\n        In the case of `MultiIndex` you must either specify `values` as a\\n        list-like object containing tuples that are the same length as the\\n        number of levels, or specify `level`. Otherwise it will raise a\\n        ``ValueError``.\\n\\n        If `level` is specified:\\n\\n        - if it is the name of one *and only one* index level, use that level;\\n        - otherwise it should be a number indicating level position.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.Index([1,2,3])\\n        >>> idx\\n        Int64Index([1, 2, 3], dtype=\\'int64\\')\\n\\n        Check whether each index value in a list of values.\\n        >>> idx.isin([1, 4])\\n        array([ True, False, False])\\n\\n        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\\n        ...                                  [\\'red\\', \\'blue\\', \\'green\\']],\\n        ...                                  names=(\\'number\\', \\'color\\'))\\n        >>> midx\\n        MultiIndex(levels=[[1, 2, 3], [\\'blue\\', \\'green\\', \\'red\\']],\\n                   codes=[[0, 1, 2], [2, 0, 1]],\\n                   names=[\\'number\\', \\'color\\'])\\n\\n        Check whether the strings in the \\'color\\' level of the MultiIndex\\n        are in a list of colors.\\n\\n        >>> midx.isin([\\'red\\', \\'orange\\', \\'yellow\\'], level=\\'color\\')\\n        array([ True, False, False])\\n\\n        To check across the levels of a MultiIndex, pass a list of tuples:\\n\\n        >>> midx.isin([(1, \\'red\\'), (3, \\'red\\')])\\n        array([ True, False, False])\\n\\n        For a DatetimeIndex, string values in `values` are converted to\\n        Timestamps.\\n\\n        >>> dates = [\\'2000-03-11\\', \\'2000-03-12\\', \\'2000-03-13\\']\\n        >>> dti = pd.to_datetime(dates)\\n        >>> dti\\n        DatetimeIndex([\\'2000-03-11\\', \\'2000-03-12\\', \\'2000-03-13\\'],\\n        dtype=\\'datetime64[ns]\\', freq=None)\\n\\n        >>> dti.isin([\\'2000-03-11\\'])\\n        array([ True, False, False])\\n        \"\"\"\\n        if level is not None:\\n            self._validate_index_level(level)\\n        return algos.isin(self, values)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Index._add_logical_methods',\n",
       "  'docstring': 'Add in logical methods.',\n",
       "  'code': 'def _add_logical_methods(cls):\\n        \"\"\"\\n        Add in logical methods.\\n        \"\"\"\\n        _doc = \"\"\"\\n        %(desc)s\\n\\n        Parameters\\n        ----------\\n        *args\\n            These parameters will be passed to numpy.%(outname)s.\\n        **kwargs\\n            These parameters will be passed to numpy.%(outname)s.\\n\\n        Returns\\n        -------\\n        %(outname)s : bool or array_like (if axis is specified)\\n            A single element array_like may be converted to bool.\"\"\"\\n\\n        _index_shared_docs[\\'index_all\\'] = dedent(\"\"\"\\n\\n        See Also\\n        --------\\n        Index.any : Return whether any element in an Index is True.\\n        Series.any : Return whether any element in a Series is True.\\n        Series.all : Return whether all elements in a Series are True.\\n\\n        Notes\\n        -----\\n        Not a Number (NaN), positive infinity and negative infinity\\n        evaluate to True because these are not equal to zero.\\n\\n        Examples\\n        --------\\n        **all**\\n\\n        True, because nonzero integers are considered True.\\n\\n        >>> pd.Index([1, 2, 3]).all()\\n        True\\n\\n        False, because ``0`` is considered False.\\n\\n        >>> pd.Index([0, 1, 2]).all()\\n        False\\n\\n        **any**\\n\\n        True, because ``1`` is considered True.\\n\\n        >>> pd.Index([0, 0, 1]).any()\\n        True\\n\\n        False, because ``0`` is considered False.\\n\\n        >>> pd.Index([0, 0, 0]).any()\\n        False\\n        \"\"\")\\n\\n        _index_shared_docs[\\'index_any\\'] = dedent(\"\"\"\\n\\n        See Also\\n        --------\\n        Index.all : Return whether all elements are True.\\n        Series.all : Return whether all elements are True.\\n\\n        Notes\\n        -----\\n        Not a Number (NaN), positive infinity and negative infinity\\n        evaluate to True because these are not equal to zero.\\n\\n        Examples\\n        --------\\n        >>> index = pd.Index([0, 1, 2])\\n        >>> index.any()\\n        True\\n\\n        >>> index = pd.Index([0, 0, 0])\\n        >>> index.any()\\n        False\\n        \"\"\")\\n\\n        def _make_logical_function(name, desc, f):\\n            @Substitution(outname=name, desc=desc)\\n            @Appender(_index_shared_docs[\\'index_\\' + name])\\n            @Appender(_doc)\\n            def logical_func(self, *args, **kwargs):\\n                result = f(self.values)\\n                if (isinstance(result, (np.ndarray, ABCSeries, Index)) and\\n                        result.ndim == 0):\\n                    # return NumPy type\\n                    return result.dtype.type(result.item())\\n                else:  # pragma: no cover\\n                    return result\\n\\n            logical_func.__name__ = name\\n            return logical_func\\n\\n        cls.all = _make_logical_function(\\'all\\', \\'Return whether all elements \\'\\n                                                \\'are True.\\',\\n                                         np.all)\\n        cls.any = _make_logical_function(\\'any\\',\\n                                         \\'Return whether any element is True.\\',\\n                                         np.any)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'mask_missing',\n",
       "  'docstring': 'Return a masking array of same size/shape as arr\\n    with entries equaling any member of values_to_mask set to True',\n",
       "  'code': 'def mask_missing(arr, values_to_mask):\\n    \"\"\"\\n    Return a masking array of same size/shape as arr\\n    with entries equaling any member of values_to_mask set to True\\n    \"\"\"\\n    dtype, values_to_mask = infer_dtype_from_array(values_to_mask)\\n\\n    try:\\n        values_to_mask = np.array(values_to_mask, dtype=dtype)\\n\\n    except Exception:\\n        values_to_mask = np.array(values_to_mask, dtype=object)\\n\\n    na_mask = isna(values_to_mask)\\n    nonna = values_to_mask[~na_mask]\\n\\n    mask = None\\n    for x in nonna:\\n        if mask is None:\\n\\n            # numpy elementwise comparison warning\\n            if is_numeric_v_string_like(arr, x):\\n                mask = False\\n            else:\\n                mask = arr == x\\n\\n            # if x is a string and arr is not, then we get False and we must\\n            # expand the mask to size arr.shape\\n            if is_scalar(mask):\\n                mask = np.zeros(arr.shape, dtype=bool)\\n        else:\\n\\n            # numpy elementwise comparison warning\\n            if is_numeric_v_string_like(arr, x):\\n                mask |= False\\n            else:\\n                mask |= arr == x\\n\\n    if na_mask.any():\\n        if mask is None:\\n            mask = isna(arr)\\n        else:\\n            mask |= isna(arr)\\n\\n    # GH 21977\\n    if mask is None:\\n        mask = np.zeros(arr.shape, dtype=bool)\\n\\n    return mask',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'mask_zero_div_zero',\n",
       "  'docstring': 'Set results of 0 / 0 or 0 // 0 to np.nan, regardless of the dtypes\\n    of the numerator or the denominator.\\n\\n    Parameters\\n    ----------\\n    x : ndarray\\n    y : ndarray\\n    result : ndarray\\n    copy : bool (default False)\\n        Whether to always create a new array or try to fill in the existing\\n        array if possible.\\n\\n    Returns\\n    -------\\n    filled_result : ndarray\\n\\n    Examples\\n    --------\\n    >>> x = np.array([1, 0, -1], dtype=np.int64)\\n    >>> y = 0       # int 0; numpy behavior is different with float\\n    >>> result = x / y\\n    >>> result      # raw numpy result does not fill division by zero\\n    array([0, 0, 0])\\n    >>> mask_zero_div_zero(x, y, result)\\n    array([ inf,  nan, -inf])',\n",
       "  'code': 'def mask_zero_div_zero(x, y, result, copy=False):\\n    \"\"\"\\n    Set results of 0 / 0 or 0 // 0 to np.nan, regardless of the dtypes\\n    of the numerator or the denominator.\\n\\n    Parameters\\n    ----------\\n    x : ndarray\\n    y : ndarray\\n    result : ndarray\\n    copy : bool (default False)\\n        Whether to always create a new array or try to fill in the existing\\n        array if possible.\\n\\n    Returns\\n    -------\\n    filled_result : ndarray\\n\\n    Examples\\n    --------\\n    >>> x = np.array([1, 0, -1], dtype=np.int64)\\n    >>> y = 0       # int 0; numpy behavior is different with float\\n    >>> result = x / y\\n    >>> result      # raw numpy result does not fill division by zero\\n    array([0, 0, 0])\\n    >>> mask_zero_div_zero(x, y, result)\\n    array([ inf,  nan, -inf])\\n    \"\"\"\\n    if is_scalar(y):\\n        y = np.array(y)\\n\\n    zmask = y == 0\\n    if zmask.any():\\n        shape = result.shape\\n\\n        nan_mask = (zmask & (x == 0)).ravel()\\n        neginf_mask = (zmask & (x < 0)).ravel()\\n        posinf_mask = (zmask & (x > 0)).ravel()\\n\\n        if nan_mask.any() or neginf_mask.any() or posinf_mask.any():\\n            # Fill negative/0 with -inf, positive/0 with +inf, 0/0 with NaN\\n            result = result.astype(\\'float64\\', copy=copy).ravel()\\n\\n            np.putmask(result, nan_mask, np.nan)\\n            np.putmask(result, posinf_mask, np.inf)\\n            np.putmask(result, neginf_mask, -np.inf)\\n\\n            result = result.reshape(shape)\\n\\n    return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'GroupBy._add_numeric_operations',\n",
       "  'docstring': 'Add numeric operations to the GroupBy generically.',\n",
       "  'code': 'def _add_numeric_operations(cls):\\n        \"\"\"\\n        Add numeric operations to the GroupBy generically.\\n        \"\"\"\\n\\n        def groupby_function(name, alias, npfunc,\\n                             numeric_only=True, _convert=False,\\n                             min_count=-1):\\n\\n            _local_template = \"Compute %(f)s of group values\"\\n\\n            @Substitution(name=\\'groupby\\', f=name)\\n            @Appender(_common_see_also)\\n            @Appender(_local_template)\\n            def f(self, **kwargs):\\n                if \\'numeric_only\\' not in kwargs:\\n                    kwargs[\\'numeric_only\\'] = numeric_only\\n                if \\'min_count\\' not in kwargs:\\n                    kwargs[\\'min_count\\'] = min_count\\n\\n                self._set_group_selection()\\n                try:\\n                    return self._cython_agg_general(\\n                        alias, alt=npfunc, **kwargs)\\n                except AssertionError as e:\\n                    raise SpecificationError(str(e))\\n                except Exception:\\n                    result = self.aggregate(\\n                        lambda x: npfunc(x, axis=self.axis))\\n                    if _convert:\\n                        result = result._convert(datetime=True)\\n                    return result\\n\\n            set_function_name(f, name, cls)\\n\\n            return f\\n\\n        def first_compat(x, axis=0):\\n\\n            def first(x):\\n                x = x.to_numpy()\\n\\n                x = x[notna(x)]\\n                if len(x) == 0:\\n                    return np.nan\\n                return x[0]\\n\\n            if isinstance(x, DataFrame):\\n                return x.apply(first, axis=axis)\\n            else:\\n                return first(x)\\n\\n        def last_compat(x, axis=0):\\n\\n            def last(x):\\n                x = x.to_numpy()\\n                x = x[notna(x)]\\n                if len(x) == 0:\\n                    return np.nan\\n                return x[-1]\\n\\n            if isinstance(x, DataFrame):\\n                return x.apply(last, axis=axis)\\n            else:\\n                return last(x)\\n\\n        cls.sum = groupby_function(\\'sum\\', \\'add\\', np.sum, min_count=0)\\n        cls.prod = groupby_function(\\'prod\\', \\'prod\\', np.prod, min_count=0)\\n        cls.min = groupby_function(\\'min\\', \\'min\\', np.min, numeric_only=False)\\n        cls.max = groupby_function(\\'max\\', \\'max\\', np.max, numeric_only=False)\\n        cls.first = groupby_function(\\'first\\', \\'first\\', first_compat,\\n                                     numeric_only=False)\\n        cls.last = groupby_function(\\'last\\', \\'last\\', last_compat,\\n                                    numeric_only=False)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'GroupBy.quantile',\n",
       "  'docstring': \"Return group values at the given quantile, a la numpy.percentile.\\n\\n        Parameters\\n        ----------\\n        q : float or array-like, default 0.5 (50% quantile)\\n            Value(s) between 0 and 1 providing the quantile(s) to compute.\\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\\n            Method to use when the desired quantile falls between two points.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Return type determined by caller of GroupBy object.\\n\\n        See Also\\n        --------\\n        Series.quantile : Similar method for Series.\\n        DataFrame.quantile : Similar method for DataFrame.\\n        numpy.percentile : NumPy method to compute qth percentile.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([\\n        ...     ['a', 1], ['a', 2], ['a', 3],\\n        ...     ['b', 1], ['b', 3], ['b', 5]\\n        ... ], columns=['key', 'val'])\\n        >>> df.groupby('key').quantile()\\n            val\\n        key\\n        a    2.0\\n        b    3.0\",\n",
       "  'code': 'def quantile(self, q=0.5, interpolation=\\'linear\\'):\\n        \"\"\"\\n        Return group values at the given quantile, a la numpy.percentile.\\n\\n        Parameters\\n        ----------\\n        q : float or array-like, default 0.5 (50% quantile)\\n            Value(s) between 0 and 1 providing the quantile(s) to compute.\\n        interpolation : {\\'linear\\', \\'lower\\', \\'higher\\', \\'midpoint\\', \\'nearest\\'}\\n            Method to use when the desired quantile falls between two points.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Return type determined by caller of GroupBy object.\\n\\n        See Also\\n        --------\\n        Series.quantile : Similar method for Series.\\n        DataFrame.quantile : Similar method for DataFrame.\\n        numpy.percentile : NumPy method to compute qth percentile.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([\\n        ...     [\\'a\\', 1], [\\'a\\', 2], [\\'a\\', 3],\\n        ...     [\\'b\\', 1], [\\'b\\', 3], [\\'b\\', 5]\\n        ... ], columns=[\\'key\\', \\'val\\'])\\n        >>> df.groupby(\\'key\\').quantile()\\n            val\\n        key\\n        a    2.0\\n        b    3.0\\n        \"\"\"\\n\\n        def pre_processor(\\n                vals: np.ndarray\\n        ) -> Tuple[np.ndarray, Optional[Type]]:\\n            if is_object_dtype(vals):\\n                raise TypeError(\"\\'quantile\\' cannot be performed against \"\\n                                \"\\'object\\' dtypes!\")\\n\\n            inference = None\\n            if is_integer_dtype(vals):\\n                inference = np.int64\\n            elif is_datetime64_dtype(vals):\\n                inference = \\'datetime64[ns]\\'\\n                vals = vals.astype(np.float)\\n\\n            return vals, inference\\n\\n        def post_processor(\\n                vals: np.ndarray,\\n                inference: Optional[Type]\\n        ) -> np.ndarray:\\n            if inference:\\n                # Check for edge case\\n                if not (is_integer_dtype(inference) and\\n                        interpolation in {\\'linear\\', \\'midpoint\\'}):\\n                    vals = vals.astype(inference)\\n\\n            return vals\\n\\n        return self._get_cythonized_result(\\'group_quantile\\', self.grouper,\\n                                           aggregate=True,\\n                                           needs_values=True,\\n                                           needs_mask=True,\\n                                           cython_dtype=np.float64,\\n                                           pre_processing=pre_processor,\\n                                           post_processing=post_processor,\\n                                           q=q, interpolation=interpolation)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'array',\n",
       "  'docstring': 'Create an array.\\n\\n    .. versionadded:: 0.24.0\\n\\n    Parameters\\n    ----------\\n    data : Sequence of objects\\n        The scalars inside `data` should be instances of the\\n        scalar type for `dtype`. It\\'s expected that `data`\\n        represents a 1-dimensional array of data.\\n\\n        When `data` is an Index or Series, the underlying array\\n        will be extracted from `data`.\\n\\n    dtype : str, np.dtype, or ExtensionDtype, optional\\n        The dtype to use for the array. This may be a NumPy\\n        dtype or an extension type registered with pandas using\\n        :meth:`pandas.api.extensions.register_extension_dtype`.\\n\\n        If not specified, there are two possibilities:\\n\\n        1. When `data` is a :class:`Series`, :class:`Index`, or\\n           :class:`ExtensionArray`, the `dtype` will be taken\\n           from the data.\\n        2. Otherwise, pandas will attempt to infer the `dtype`\\n           from the data.\\n\\n        Note that when `data` is a NumPy array, ``data.dtype`` is\\n        *not* used for inferring the array type. This is because\\n        NumPy cannot represent all the types of data that can be\\n        held in extension arrays.\\n\\n        Currently, pandas will infer an extension dtype for sequences of\\n\\n        ============================== =====================================\\n        Scalar Type                    Array Type\\n        ============================== =====================================\\n        :class:`pandas.Interval`       :class:`pandas.arrays.IntervalArray`\\n        :class:`pandas.Period`         :class:`pandas.arrays.PeriodArray`\\n        :class:`datetime.datetime`     :class:`pandas.arrays.DatetimeArray`\\n        :class:`datetime.timedelta`    :class:`pandas.arrays.TimedeltaArray`\\n        ============================== =====================================\\n\\n        For all other cases, NumPy\\'s usual inference rules will be used.\\n\\n    copy : bool, default True\\n        Whether to copy the data, even if not necessary. Depending\\n        on the type of `data`, creating the new array may require\\n        copying data, even if ``copy=False``.\\n\\n    Returns\\n    -------\\n    ExtensionArray\\n        The newly created array.\\n\\n    Raises\\n    ------\\n    ValueError\\n        When `data` is not 1-dimensional.\\n\\n    See Also\\n    --------\\n    numpy.array : Construct a NumPy array.\\n    Series : Construct a pandas Series.\\n    Index : Construct a pandas Index.\\n    arrays.PandasArray : ExtensionArray wrapping a NumPy array.\\n    Series.array : Extract the array stored within a Series.\\n\\n    Notes\\n    -----\\n    Omitting the `dtype` argument means pandas will attempt to infer the\\n    best array type from the values in the data. As new array types are\\n    added by pandas and 3rd party libraries, the \"best\" array type may\\n    change. We recommend specifying `dtype` to ensure that\\n\\n    1. the correct array type for the data is returned\\n    2. the returned array type doesn\\'t change as new extension types\\n       are added by pandas and third-party libraries\\n\\n    Additionally, if the underlying memory representation of the returned\\n    array matters, we recommend specifying the `dtype` as a concrete object\\n    rather than a string alias or allowing it to be inferred. For example,\\n    a future version of pandas or a 3rd-party library may include a\\n    dedicated ExtensionArray for string data. In this event, the following\\n    would no longer return a :class:`arrays.PandasArray` backed by a NumPy\\n    array.\\n\\n    >>> pd.array([\\'a\\', \\'b\\'], dtype=str)\\n    <PandasArray>\\n    [\\'a\\', \\'b\\']\\n    Length: 2, dtype: str32\\n\\n    This would instead return the new ExtensionArray dedicated for string\\n    data. If you really need the new array to be backed by a  NumPy array,\\n    specify that in the dtype.\\n\\n    >>> pd.array([\\'a\\', \\'b\\'], dtype=np.dtype(\"<U1\"))\\n    <PandasArray>\\n    [\\'a\\', \\'b\\']\\n    Length: 2, dtype: str32\\n\\n    Or use the dedicated constructor for the array you\\'re expecting, and\\n    wrap that in a PandasArray\\n\\n    >>> pd.array(np.array([\\'a\\', \\'b\\'], dtype=\\'<U1\\'))\\n    <PandasArray>\\n    [\\'a\\', \\'b\\']\\n    Length: 2, dtype: str32\\n\\n    Finally, Pandas has arrays that mostly overlap with NumPy\\n\\n      * :class:`arrays.DatetimeArray`\\n      * :class:`arrays.TimedeltaArray`\\n\\n    When data with a ``datetime64[ns]`` or ``timedelta64[ns]`` dtype is\\n    passed, pandas will always return a ``DatetimeArray`` or ``TimedeltaArray``\\n    rather than a ``PandasArray``. This is for symmetry with the case of\\n    timezone-aware data, which NumPy does not natively support.\\n\\n    >>> pd.array([\\'2015\\', \\'2016\\'], dtype=\\'datetime64[ns]\\')\\n    <DatetimeArray>\\n    [\\'2015-01-01 00:00:00\\', \\'2016-01-01 00:00:00\\']\\n    Length: 2, dtype: datetime64[ns]\\n\\n    >>> pd.array([\"1H\", \"2H\"], dtype=\\'timedelta64[ns]\\')\\n    <TimedeltaArray>\\n    [\\'01:00:00\\', \\'02:00:00\\']\\n    Length: 2, dtype: timedelta64[ns]\\n\\n    Examples\\n    --------\\n    If a dtype is not specified, `data` is passed through to\\n    :meth:`numpy.array`, and a :class:`arrays.PandasArray` is returned.\\n\\n    >>> pd.array([1, 2])\\n    <PandasArray>\\n    [1, 2]\\n    Length: 2, dtype: int64\\n\\n    Or the NumPy dtype can be specified\\n\\n    >>> pd.array([1, 2], dtype=np.dtype(\"int32\"))\\n    <PandasArray>\\n    [1, 2]\\n    Length: 2, dtype: int32\\n\\n    You can use the string alias for `dtype`\\n\\n    >>> pd.array([\\'a\\', \\'b\\', \\'a\\'], dtype=\\'category\\')\\n    [a, b, a]\\n    Categories (2, object): [a, b]\\n\\n    Or specify the actual dtype\\n\\n    >>> pd.array([\\'a\\', \\'b\\', \\'a\\'],\\n    ...          dtype=pd.CategoricalDtype([\\'a\\', \\'b\\', \\'c\\'], ordered=True))\\n    [a, b, a]\\n    Categories (3, object): [a < b < c]\\n\\n    Because omitting the `dtype` passes the data through to NumPy,\\n    a mixture of valid integers and NA will return a floating-point\\n    NumPy array.\\n\\n    >>> pd.array([1, 2, np.nan])\\n    <PandasArray>\\n    [1.0,  2.0, nan]\\n    Length: 3, dtype: float64\\n\\n    To use pandas\\' nullable :class:`pandas.arrays.IntegerArray`, specify\\n    the dtype:\\n\\n    >>> pd.array([1, 2, np.nan], dtype=\\'Int64\\')\\n    <IntegerArray>\\n    [1, 2, NaN]\\n    Length: 3, dtype: Int64\\n\\n    Pandas will infer an ExtensionArray for some types of data:\\n\\n    >>> pd.array([pd.Period(\\'2000\\', freq=\"D\"), pd.Period(\"2000\", freq=\"D\")])\\n    <PeriodArray>\\n    [\\'2000-01-01\\', \\'2000-01-01\\']\\n    Length: 2, dtype: period[D]\\n\\n    `data` must be 1-dimensional. A ValueError is raised when the input\\n    has the wrong dimensionality.\\n\\n    >>> pd.array(1)\\n    Traceback (most recent call last):\\n      ...\\n    ValueError: Cannot pass scalar \\'1\\' to \\'pandas.array\\'.',\n",
       "  'code': 'def array(data: Sequence[object],\\n          dtype: Optional[Union[str, np.dtype, ExtensionDtype]] = None,\\n          copy: bool = True,\\n          ) -> ABCExtensionArray:\\n    \"\"\"\\n    Create an array.\\n\\n    .. versionadded:: 0.24.0\\n\\n    Parameters\\n    ----------\\n    data : Sequence of objects\\n        The scalars inside `data` should be instances of the\\n        scalar type for `dtype`. It\\'s expected that `data`\\n        represents a 1-dimensional array of data.\\n\\n        When `data` is an Index or Series, the underlying array\\n        will be extracted from `data`.\\n\\n    dtype : str, np.dtype, or ExtensionDtype, optional\\n        The dtype to use for the array. This may be a NumPy\\n        dtype or an extension type registered with pandas using\\n        :meth:`pandas.api.extensions.register_extension_dtype`.\\n\\n        If not specified, there are two possibilities:\\n\\n        1. When `data` is a :class:`Series`, :class:`Index`, or\\n           :class:`ExtensionArray`, the `dtype` will be taken\\n           from the data.\\n        2. Otherwise, pandas will attempt to infer the `dtype`\\n           from the data.\\n\\n        Note that when `data` is a NumPy array, ``data.dtype`` is\\n        *not* used for inferring the array type. This is because\\n        NumPy cannot represent all the types of data that can be\\n        held in extension arrays.\\n\\n        Currently, pandas will infer an extension dtype for sequences of\\n\\n        ============================== =====================================\\n        Scalar Type                    Array Type\\n        ============================== =====================================\\n        :class:`pandas.Interval`       :class:`pandas.arrays.IntervalArray`\\n        :class:`pandas.Period`         :class:`pandas.arrays.PeriodArray`\\n        :class:`datetime.datetime`     :class:`pandas.arrays.DatetimeArray`\\n        :class:`datetime.timedelta`    :class:`pandas.arrays.TimedeltaArray`\\n        ============================== =====================================\\n\\n        For all other cases, NumPy\\'s usual inference rules will be used.\\n\\n    copy : bool, default True\\n        Whether to copy the data, even if not necessary. Depending\\n        on the type of `data`, creating the new array may require\\n        copying data, even if ``copy=False``.\\n\\n    Returns\\n    -------\\n    ExtensionArray\\n        The newly created array.\\n\\n    Raises\\n    ------\\n    ValueError\\n        When `data` is not 1-dimensional.\\n\\n    See Also\\n    --------\\n    numpy.array : Construct a NumPy array.\\n    Series : Construct a pandas Series.\\n    Index : Construct a pandas Index.\\n    arrays.PandasArray : ExtensionArray wrapping a NumPy array.\\n    Series.array : Extract the array stored within a Series.\\n\\n    Notes\\n    -----\\n    Omitting the `dtype` argument means pandas will attempt to infer the\\n    best array type from the values in the data. As new array types are\\n    added by pandas and 3rd party libraries, the \"best\" array type may\\n    change. We recommend specifying `dtype` to ensure that\\n\\n    1. the correct array type for the data is returned\\n    2. the returned array type doesn\\'t change as new extension types\\n       are added by pandas and third-party libraries\\n\\n    Additionally, if the underlying memory representation of the returned\\n    array matters, we recommend specifying the `dtype` as a concrete object\\n    rather than a string alias or allowing it to be inferred. For example,\\n    a future version of pandas or a 3rd-party library may include a\\n    dedicated ExtensionArray for string data. In this event, the following\\n    would no longer return a :class:`arrays.PandasArray` backed by a NumPy\\n    array.\\n\\n    >>> pd.array([\\'a\\', \\'b\\'], dtype=str)\\n    <PandasArray>\\n    [\\'a\\', \\'b\\']\\n    Length: 2, dtype: str32\\n\\n    This would instead return the new ExtensionArray dedicated for string\\n    data. If you really need the new array to be backed by a  NumPy array,\\n    specify that in the dtype.\\n\\n    >>> pd.array([\\'a\\', \\'b\\'], dtype=np.dtype(\"<U1\"))\\n    <PandasArray>\\n    [\\'a\\', \\'b\\']\\n    Length: 2, dtype: str32\\n\\n    Or use the dedicated constructor for the array you\\'re expecting, and\\n    wrap that in a PandasArray\\n\\n    >>> pd.array(np.array([\\'a\\', \\'b\\'], dtype=\\'<U1\\'))\\n    <PandasArray>\\n    [\\'a\\', \\'b\\']\\n    Length: 2, dtype: str32\\n\\n    Finally, Pandas has arrays that mostly overlap with NumPy\\n\\n      * :class:`arrays.DatetimeArray`\\n      * :class:`arrays.TimedeltaArray`\\n\\n    When data with a ``datetime64[ns]`` or ``timedelta64[ns]`` dtype is\\n    passed, pandas will always return a ``DatetimeArray`` or ``TimedeltaArray``\\n    rather than a ``PandasArray``. This is for symmetry with the case of\\n    timezone-aware data, which NumPy does not natively support.\\n\\n    >>> pd.array([\\'2015\\', \\'2016\\'], dtype=\\'datetime64[ns]\\')\\n    <DatetimeArray>\\n    [\\'2015-01-01 00:00:00\\', \\'2016-01-01 00:00:00\\']\\n    Length: 2, dtype: datetime64[ns]\\n\\n    >>> pd.array([\"1H\", \"2H\"], dtype=\\'timedelta64[ns]\\')\\n    <TimedeltaArray>\\n    [\\'01:00:00\\', \\'02:00:00\\']\\n    Length: 2, dtype: timedelta64[ns]\\n\\n    Examples\\n    --------\\n    If a dtype is not specified, `data` is passed through to\\n    :meth:`numpy.array`, and a :class:`arrays.PandasArray` is returned.\\n\\n    >>> pd.array([1, 2])\\n    <PandasArray>\\n    [1, 2]\\n    Length: 2, dtype: int64\\n\\n    Or the NumPy dtype can be specified\\n\\n    >>> pd.array([1, 2], dtype=np.dtype(\"int32\"))\\n    <PandasArray>\\n    [1, 2]\\n    Length: 2, dtype: int32\\n\\n    You can use the string alias for `dtype`\\n\\n    >>> pd.array([\\'a\\', \\'b\\', \\'a\\'], dtype=\\'category\\')\\n    [a, b, a]\\n    Categories (2, object): [a, b]\\n\\n    Or specify the actual dtype\\n\\n    >>> pd.array([\\'a\\', \\'b\\', \\'a\\'],\\n    ...          dtype=pd.CategoricalDtype([\\'a\\', \\'b\\', \\'c\\'], ordered=True))\\n    [a, b, a]\\n    Categories (3, object): [a < b < c]\\n\\n    Because omitting the `dtype` passes the data through to NumPy,\\n    a mixture of valid integers and NA will return a floating-point\\n    NumPy array.\\n\\n    >>> pd.array([1, 2, np.nan])\\n    <PandasArray>\\n    [1.0,  2.0, nan]\\n    Length: 3, dtype: float64\\n\\n    To use pandas\\' nullable :class:`pandas.arrays.IntegerArray`, specify\\n    the dtype:\\n\\n    >>> pd.array([1, 2, np.nan], dtype=\\'Int64\\')\\n    <IntegerArray>\\n    [1, 2, NaN]\\n    Length: 3, dtype: Int64\\n\\n    Pandas will infer an ExtensionArray for some types of data:\\n\\n    >>> pd.array([pd.Period(\\'2000\\', freq=\"D\"), pd.Period(\"2000\", freq=\"D\")])\\n    <PeriodArray>\\n    [\\'2000-01-01\\', \\'2000-01-01\\']\\n    Length: 2, dtype: period[D]\\n\\n    `data` must be 1-dimensional. A ValueError is raised when the input\\n    has the wrong dimensionality.\\n\\n    >>> pd.array(1)\\n    Traceback (most recent call last):\\n      ...\\n    ValueError: Cannot pass scalar \\'1\\' to \\'pandas.array\\'.\\n    \"\"\"\\n    from pandas.core.arrays import (\\n        period_array, ExtensionArray, IntervalArray, PandasArray,\\n        DatetimeArray,\\n        TimedeltaArray,\\n    )\\n    from pandas.core.internals.arrays import extract_array\\n\\n    if lib.is_scalar(data):\\n        msg = (\\n            \"Cannot pass scalar \\'{}\\' to \\'pandas.array\\'.\"\\n        )\\n        raise ValueError(msg.format(data))\\n\\n    data = extract_array(data, extract_numpy=True)\\n\\n    if dtype is None and isinstance(data, ExtensionArray):\\n        dtype = data.dtype\\n\\n    # this returns None for not-found dtypes.\\n    if isinstance(dtype, str):\\n        dtype = registry.find(dtype) or dtype\\n\\n    if is_extension_array_dtype(dtype):\\n        cls = dtype.construct_array_type()\\n        return cls._from_sequence(data, dtype=dtype, copy=copy)\\n\\n    if dtype is None:\\n        inferred_dtype = lib.infer_dtype(data, skipna=False)\\n        if inferred_dtype == \\'period\\':\\n            try:\\n                return period_array(data, copy=copy)\\n            except tslibs.IncompatibleFrequency:\\n                # We may have a mixture of frequencies.\\n                # We choose to return an ndarray, rather than raising.\\n                pass\\n        elif inferred_dtype == \\'interval\\':\\n            try:\\n                return IntervalArray(data, copy=copy)\\n            except ValueError:\\n                # We may have a mixture of `closed` here.\\n                # We choose to return an ndarray, rather than raising.\\n                pass\\n\\n        elif inferred_dtype.startswith(\\'datetime\\'):\\n            # datetime, datetime64\\n            try:\\n                return DatetimeArray._from_sequence(data, copy=copy)\\n            except ValueError:\\n                # Mixture of timezones, fall back to PandasArray\\n                pass\\n\\n        elif inferred_dtype.startswith(\\'timedelta\\'):\\n            # timedelta, timedelta64\\n            return TimedeltaArray._from_sequence(data, copy=copy)\\n\\n        # TODO(BooleanArray): handle this type\\n\\n    # Pandas overrides NumPy for\\n    #   1. datetime64[ns]\\n    #   2. timedelta64[ns]\\n    # so that a DatetimeArray is returned.\\n    if is_datetime64_ns_dtype(dtype):\\n        return DatetimeArray._from_sequence(data, dtype=dtype, copy=copy)\\n    elif is_timedelta64_ns_dtype(dtype):\\n        return TimedeltaArray._from_sequence(data, dtype=dtype, copy=copy)\\n\\n    result = PandasArray._from_sequence(data, dtype=dtype, copy=copy)\\n    return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'is_list_like',\n",
       "  'docstring': 'Check if the object is list-like.\\n\\n    Objects that are considered list-like are for example Python\\n    lists, tuples, sets, NumPy arrays, and Pandas Series.\\n\\n    Strings and datetime objects, however, are not considered list-like.\\n\\n    Parameters\\n    ----------\\n    obj : The object to check\\n    allow_sets : boolean, default True\\n        If this parameter is False, sets will not be considered list-like\\n\\n        .. versionadded:: 0.24.0\\n\\n    Returns\\n    -------\\n    is_list_like : bool\\n        Whether `obj` has list-like properties.\\n\\n    Examples\\n    --------\\n    >>> is_list_like([1, 2, 3])\\n    True\\n    >>> is_list_like({1, 2, 3})\\n    True\\n    >>> is_list_like(datetime(2017, 1, 1))\\n    False\\n    >>> is_list_like(\"foo\")\\n    False\\n    >>> is_list_like(1)\\n    False\\n    >>> is_list_like(np.array([2]))\\n    True\\n    >>> is_list_like(np.array(2)))\\n    False',\n",
       "  'code': 'def is_list_like(obj, allow_sets=True):\\n    \"\"\"\\n    Check if the object is list-like.\\n\\n    Objects that are considered list-like are for example Python\\n    lists, tuples, sets, NumPy arrays, and Pandas Series.\\n\\n    Strings and datetime objects, however, are not considered list-like.\\n\\n    Parameters\\n    ----------\\n    obj : The object to check\\n    allow_sets : boolean, default True\\n        If this parameter is False, sets will not be considered list-like\\n\\n        .. versionadded:: 0.24.0\\n\\n    Returns\\n    -------\\n    is_list_like : bool\\n        Whether `obj` has list-like properties.\\n\\n    Examples\\n    --------\\n    >>> is_list_like([1, 2, 3])\\n    True\\n    >>> is_list_like({1, 2, 3})\\n    True\\n    >>> is_list_like(datetime(2017, 1, 1))\\n    False\\n    >>> is_list_like(\"foo\")\\n    False\\n    >>> is_list_like(1)\\n    False\\n    >>> is_list_like(np.array([2]))\\n    True\\n    >>> is_list_like(np.array(2)))\\n    False\\n    \"\"\"\\n\\n    return (isinstance(obj, abc.Iterable) and\\n            # we do not count strings/unicode/bytes as list-like\\n            not isinstance(obj, (str, bytes)) and\\n\\n            # exclude zero-dimensional numpy arrays, effectively scalars\\n            not (isinstance(obj, np.ndarray) and obj.ndim == 0) and\\n\\n            # exclude sets if allow_sets is False\\n            not (allow_sets is False and isinstance(obj, abc.Set)))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'bdate_range',\n",
       "  'docstring': \"Return a fixed frequency DatetimeIndex, with business day as the default\\n    frequency\\n\\n    Parameters\\n    ----------\\n    start : string or datetime-like, default None\\n        Left bound for generating dates.\\n    end : string or datetime-like, default None\\n        Right bound for generating dates.\\n    periods : integer, default None\\n        Number of periods to generate.\\n    freq : string or DateOffset, default 'B' (business daily)\\n        Frequency strings can have multiples, e.g. '5H'.\\n    tz : string or None\\n        Time zone name for returning localized DatetimeIndex, for example\\n        Asia/Beijing.\\n    normalize : bool, default False\\n        Normalize start/end dates to midnight before generating date range.\\n    name : string, default None\\n        Name of the resulting DatetimeIndex.\\n    weekmask : string or None, default None\\n        Weekmask of valid business days, passed to ``numpy.busdaycalendar``,\\n        only used when custom frequency strings are passed.  The default\\n        value None is equivalent to 'Mon Tue Wed Thu Fri'.\\n\\n        .. versionadded:: 0.21.0\\n\\n    holidays : list-like or None, default None\\n        Dates to exclude from the set of valid business days, passed to\\n        ``numpy.busdaycalendar``, only used when custom frequency strings\\n        are passed.\\n\\n        .. versionadded:: 0.21.0\\n\\n    closed : string, default None\\n        Make the interval closed with respect to the given frequency to\\n        the 'left', 'right', or both sides (None).\\n    **kwargs\\n        For compatibility. Has no effect on the result.\\n\\n    Returns\\n    -------\\n    DatetimeIndex\\n\\n    Notes\\n    -----\\n    Of the four parameters: ``start``, ``end``, ``periods``, and ``freq``,\\n    exactly three must be specified.  Specifying ``freq`` is a requirement\\n    for ``bdate_range``.  Use ``date_range`` if specifying ``freq`` is not\\n    desired.\\n\\n    To learn more about the frequency strings, please see `this link\\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\\n\\n    Examples\\n    --------\\n    Note how the two weekend days are skipped in the result.\\n\\n    >>> pd.bdate_range(start='1/1/2018', end='1/08/2018')\\n    DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\\n               '2018-01-05', '2018-01-08'],\\n              dtype='datetime64[ns]', freq='B')\",\n",
       "  'code': 'def bdate_range(start=None, end=None, periods=None, freq=\\'B\\', tz=None,\\n                normalize=True, name=None, weekmask=None, holidays=None,\\n                closed=None, **kwargs):\\n    \"\"\"\\n    Return a fixed frequency DatetimeIndex, with business day as the default\\n    frequency\\n\\n    Parameters\\n    ----------\\n    start : string or datetime-like, default None\\n        Left bound for generating dates.\\n    end : string or datetime-like, default None\\n        Right bound for generating dates.\\n    periods : integer, default None\\n        Number of periods to generate.\\n    freq : string or DateOffset, default \\'B\\' (business daily)\\n        Frequency strings can have multiples, e.g. \\'5H\\'.\\n    tz : string or None\\n        Time zone name for returning localized DatetimeIndex, for example\\n        Asia/Beijing.\\n    normalize : bool, default False\\n        Normalize start/end dates to midnight before generating date range.\\n    name : string, default None\\n        Name of the resulting DatetimeIndex.\\n    weekmask : string or None, default None\\n        Weekmask of valid business days, passed to ``numpy.busdaycalendar``,\\n        only used when custom frequency strings are passed.  The default\\n        value None is equivalent to \\'Mon Tue Wed Thu Fri\\'.\\n\\n        .. versionadded:: 0.21.0\\n\\n    holidays : list-like or None, default None\\n        Dates to exclude from the set of valid business days, passed to\\n        ``numpy.busdaycalendar``, only used when custom frequency strings\\n        are passed.\\n\\n        .. versionadded:: 0.21.0\\n\\n    closed : string, default None\\n        Make the interval closed with respect to the given frequency to\\n        the \\'left\\', \\'right\\', or both sides (None).\\n    **kwargs\\n        For compatibility. Has no effect on the result.\\n\\n    Returns\\n    -------\\n    DatetimeIndex\\n\\n    Notes\\n    -----\\n    Of the four parameters: ``start``, ``end``, ``periods``, and ``freq``,\\n    exactly three must be specified.  Specifying ``freq`` is a requirement\\n    for ``bdate_range``.  Use ``date_range`` if specifying ``freq`` is not\\n    desired.\\n\\n    To learn more about the frequency strings, please see `this link\\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\\n\\n    Examples\\n    --------\\n    Note how the two weekend days are skipped in the result.\\n\\n    >>> pd.bdate_range(start=\\'1/1/2018\\', end=\\'1/08/2018\\')\\n    DatetimeIndex([\\'2018-01-01\\', \\'2018-01-02\\', \\'2018-01-03\\', \\'2018-01-04\\',\\n               \\'2018-01-05\\', \\'2018-01-08\\'],\\n              dtype=\\'datetime64[ns]\\', freq=\\'B\\')\\n    \"\"\"\\n    if freq is None:\\n        msg = \\'freq must be specified for bdate_range; use date_range instead\\'\\n        raise TypeError(msg)\\n\\n    if is_string_like(freq) and freq.startswith(\\'C\\'):\\n        try:\\n            weekmask = weekmask or \\'Mon Tue Wed Thu Fri\\'\\n            freq = prefix_mapping[freq](holidays=holidays, weekmask=weekmask)\\n        except (KeyError, TypeError):\\n            msg = \\'invalid custom frequency string: {freq}\\'.format(freq=freq)\\n            raise ValueError(msg)\\n    elif holidays or weekmask:\\n        msg = (\\'a custom frequency string is required when holidays or \\'\\n               \\'weekmask are passed, got frequency {freq}\\').format(freq=freq)\\n        raise ValueError(msg)\\n\\n    return date_range(start=start, end=end, periods=periods,\\n                      freq=freq, tz=tz, normalize=normalize, name=name,\\n                      closed=closed, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'cdate_range',\n",
       "  'docstring': \"Return a fixed frequency DatetimeIndex, with CustomBusinessDay as the\\n    default frequency\\n\\n    .. deprecated:: 0.21.0\\n\\n    Parameters\\n    ----------\\n    start : string or datetime-like, default None\\n        Left bound for generating dates\\n    end : string or datetime-like, default None\\n        Right bound for generating dates\\n    periods : integer, default None\\n        Number of periods to generate\\n    freq : string or DateOffset, default 'C' (CustomBusinessDay)\\n        Frequency strings can have multiples, e.g. '5H'\\n    tz : string, default None\\n        Time zone name for returning localized DatetimeIndex, for example\\n        Asia/Beijing\\n    normalize : bool, default False\\n        Normalize start/end dates to midnight before generating date range\\n    name : string, default None\\n        Name of the resulting DatetimeIndex\\n    weekmask : string, Default 'Mon Tue Wed Thu Fri'\\n        weekmask of valid business days, passed to ``numpy.busdaycalendar``\\n    holidays : list\\n        list/array of dates to exclude from the set of valid business days,\\n        passed to ``numpy.busdaycalendar``\\n    closed : string, default None\\n        Make the interval closed with respect to the given frequency to\\n        the 'left', 'right', or both sides (None)\\n\\n    Notes\\n    -----\\n    Of the three parameters: ``start``, ``end``, and ``periods``, exactly two\\n    must be specified.\\n\\n    To learn more about the frequency strings, please see `this link\\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\\n\\n    Returns\\n    -------\\n    rng : DatetimeIndex\",\n",
       "  'code': 'def cdate_range(start=None, end=None, periods=None, freq=\\'C\\', tz=None,\\n                normalize=True, name=None, closed=None, **kwargs):\\n    \"\"\"\\n    Return a fixed frequency DatetimeIndex, with CustomBusinessDay as the\\n    default frequency\\n\\n    .. deprecated:: 0.21.0\\n\\n    Parameters\\n    ----------\\n    start : string or datetime-like, default None\\n        Left bound for generating dates\\n    end : string or datetime-like, default None\\n        Right bound for generating dates\\n    periods : integer, default None\\n        Number of periods to generate\\n    freq : string or DateOffset, default \\'C\\' (CustomBusinessDay)\\n        Frequency strings can have multiples, e.g. \\'5H\\'\\n    tz : string, default None\\n        Time zone name for returning localized DatetimeIndex, for example\\n        Asia/Beijing\\n    normalize : bool, default False\\n        Normalize start/end dates to midnight before generating date range\\n    name : string, default None\\n        Name of the resulting DatetimeIndex\\n    weekmask : string, Default \\'Mon Tue Wed Thu Fri\\'\\n        weekmask of valid business days, passed to ``numpy.busdaycalendar``\\n    holidays : list\\n        list/array of dates to exclude from the set of valid business days,\\n        passed to ``numpy.busdaycalendar``\\n    closed : string, default None\\n        Make the interval closed with respect to the given frequency to\\n        the \\'left\\', \\'right\\', or both sides (None)\\n\\n    Notes\\n    -----\\n    Of the three parameters: ``start``, ``end``, and ``periods``, exactly two\\n    must be specified.\\n\\n    To learn more about the frequency strings, please see `this link\\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\\n\\n    Returns\\n    -------\\n    rng : DatetimeIndex\\n    \"\"\"\\n    warnings.warn(\"cdate_range is deprecated and will be removed in a future \"\\n                  \"version, instead use pd.bdate_range(..., freq=\\'{freq}\\')\"\\n                  .format(freq=freq), FutureWarning, stacklevel=2)\\n\\n    if freq == \\'C\\':\\n        holidays = kwargs.pop(\\'holidays\\', [])\\n        weekmask = kwargs.pop(\\'weekmask\\', \\'Mon Tue Wed Thu Fri\\')\\n        freq = CDay(holidays=holidays, weekmask=weekmask)\\n\\n    return date_range(start=start, end=end, periods=periods, freq=freq,\\n                      tz=tz, normalize=normalize, name=name,\\n                      closed=closed, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Panel.round',\n",
       "  'docstring': 'Round each value in Panel to a specified number of decimal places.\\n\\n        .. versionadded:: 0.18.0\\n\\n        Parameters\\n        ----------\\n        decimals : int\\n            Number of decimal places to round to (default: 0).\\n            If decimals is negative, it specifies the number of\\n            positions to the left of the decimal point.\\n\\n        Returns\\n        -------\\n        Panel object\\n\\n        See Also\\n        --------\\n        numpy.around',\n",
       "  'code': 'def round(self, decimals=0, *args, **kwargs):\\n        \"\"\"\\n        Round each value in Panel to a specified number of decimal places.\\n\\n        .. versionadded:: 0.18.0\\n\\n        Parameters\\n        ----------\\n        decimals : int\\n            Number of decimal places to round to (default: 0).\\n            If decimals is negative, it specifies the number of\\n            positions to the left of the decimal point.\\n\\n        Returns\\n        -------\\n        Panel object\\n\\n        See Also\\n        --------\\n        numpy.around\\n        \"\"\"\\n        nv.validate_round(args, kwargs)\\n\\n        if is_integer(decimals):\\n            result = np.apply_along_axis(np.round, 0, self.values)\\n            return self._wrap_result(result, axis=0)\\n        raise TypeError(\"decimals must be an integer\")',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'get_block_type',\n",
       "  'docstring': 'Find the appropriate Block subclass to use for the given values and dtype.\\n\\n    Parameters\\n    ----------\\n    values : ndarray-like\\n    dtype : numpy or pandas dtype\\n\\n    Returns\\n    -------\\n    cls : class, subclass of Block',\n",
       "  'code': 'def get_block_type(values, dtype=None):\\n    \"\"\"\\n    Find the appropriate Block subclass to use for the given values and dtype.\\n\\n    Parameters\\n    ----------\\n    values : ndarray-like\\n    dtype : numpy or pandas dtype\\n\\n    Returns\\n    -------\\n    cls : class, subclass of Block\\n    \"\"\"\\n    dtype = dtype or values.dtype\\n    vtype = dtype.type\\n\\n    if is_sparse(dtype):\\n        # Need this first(ish) so that Sparse[datetime] is sparse\\n        cls = ExtensionBlock\\n    elif is_categorical(values):\\n        cls = CategoricalBlock\\n    elif issubclass(vtype, np.datetime64):\\n        assert not is_datetime64tz_dtype(values)\\n        cls = DatetimeBlock\\n    elif is_datetime64tz_dtype(values):\\n        cls = DatetimeTZBlock\\n    elif is_interval_dtype(dtype) or is_period_dtype(dtype):\\n        cls = ObjectValuesExtensionBlock\\n    elif is_extension_array_dtype(values):\\n        cls = ExtensionBlock\\n    elif issubclass(vtype, np.floating):\\n        cls = FloatBlock\\n    elif issubclass(vtype, np.timedelta64):\\n        assert issubclass(vtype, np.integer)\\n        cls = TimeDeltaBlock\\n    elif issubclass(vtype, np.complexfloating):\\n        cls = ComplexBlock\\n    elif issubclass(vtype, np.integer):\\n        cls = IntBlock\\n    elif dtype == np.bool_:\\n        cls = BoolBlock\\n    else:\\n        cls = ObjectBlock\\n    return cls',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_putmask_smart',\n",
       "  'docstring': 'Return a new ndarray, try to preserve dtype if possible.\\n\\n    Parameters\\n    ----------\\n    v : `values`, updated in-place (array like)\\n    m : `mask`, applies to both sides (array like)\\n    n : `new values` either scalar or an array like aligned with `values`\\n\\n    Returns\\n    -------\\n    values : ndarray with updated values\\n        this *may* be a copy of the original\\n\\n    See Also\\n    --------\\n    ndarray.putmask',\n",
       "  'code': 'def _putmask_smart(v, m, n):\\n    \"\"\"\\n    Return a new ndarray, try to preserve dtype if possible.\\n\\n    Parameters\\n    ----------\\n    v : `values`, updated in-place (array like)\\n    m : `mask`, applies to both sides (array like)\\n    n : `new values` either scalar or an array like aligned with `values`\\n\\n    Returns\\n    -------\\n    values : ndarray with updated values\\n        this *may* be a copy of the original\\n\\n    See Also\\n    --------\\n    ndarray.putmask\\n    \"\"\"\\n\\n    # we cannot use np.asarray() here as we cannot have conversions\\n    # that numpy does when numeric are mixed with strings\\n\\n    # n should be the length of the mask or a scalar here\\n    if not is_list_like(n):\\n        n = np.repeat(n, len(m))\\n    elif isinstance(n, np.ndarray) and n.ndim == 0:  # numpy scalar\\n        n = np.repeat(np.array(n, ndmin=1), len(m))\\n\\n    # see if we are only masking values that if putted\\n    # will work in the current dtype\\n    try:\\n        nn = n[m]\\n\\n        # make sure that we have a nullable type\\n        # if we have nulls\\n        if not _isna_compat(v, nn[0]):\\n            raise ValueError\\n\\n        # we ignore ComplexWarning here\\n        with warnings.catch_warnings(record=True):\\n            warnings.simplefilter(\"ignore\", np.ComplexWarning)\\n            nn_at = nn.astype(v.dtype)\\n\\n        # avoid invalid dtype comparisons\\n        # between numbers & strings\\n\\n        # only compare integers/floats\\n        # don\\'t compare integers to datetimelikes\\n        if (not is_numeric_v_string_like(nn, nn_at) and\\n            (is_float_dtype(nn.dtype) or\\n             is_integer_dtype(nn.dtype) and\\n             is_float_dtype(nn_at.dtype) or\\n             is_integer_dtype(nn_at.dtype))):\\n\\n            comp = (nn == nn_at)\\n            if is_list_like(comp) and comp.all():\\n                nv = v.copy()\\n                nv[m] = nn_at\\n                return nv\\n    except (ValueError, IndexError, TypeError, OverflowError):\\n        pass\\n\\n    n = np.asarray(n)\\n\\n    def _putmask_preserve(nv, n):\\n        try:\\n            nv[m] = n[m]\\n        except (IndexError, ValueError):\\n            nv[m] = n\\n        return nv\\n\\n    # preserves dtype if possible\\n    if v.dtype.kind == n.dtype.kind:\\n        return _putmask_preserve(v, n)\\n\\n    # change the dtype if needed\\n    dtype, _ = maybe_promote(n.dtype)\\n\\n    if is_extension_type(v.dtype) and is_object_dtype(dtype):\\n        v = v.get_values(dtype)\\n    else:\\n        v = v.astype(dtype)\\n\\n    return _putmask_preserve(v, n)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatetimeTZBlock.get_values',\n",
       "  'docstring': 'Returns an ndarray of values.\\n\\n        Parameters\\n        ----------\\n        dtype : np.dtype\\n            Only `object`-like dtypes are respected here (not sure\\n            why).\\n\\n        Returns\\n        -------\\n        values : ndarray\\n            When ``dtype=object``, then and object-dtype ndarray of\\n            boxed values is returned. Otherwise, an M8[ns] ndarray\\n            is returned.\\n\\n            DatetimeArray is always 1-d. ``get_values`` will reshape\\n            the return value to be the same dimensionality as the\\n            block.',\n",
       "  'code': 'def get_values(self, dtype=None):\\n        \"\"\"\\n        Returns an ndarray of values.\\n\\n        Parameters\\n        ----------\\n        dtype : np.dtype\\n            Only `object`-like dtypes are respected here (not sure\\n            why).\\n\\n        Returns\\n        -------\\n        values : ndarray\\n            When ``dtype=object``, then and object-dtype ndarray of\\n            boxed values is returned. Otherwise, an M8[ns] ndarray\\n            is returned.\\n\\n            DatetimeArray is always 1-d. ``get_values`` will reshape\\n            the return value to be the same dimensionality as the\\n            block.\\n        \"\"\"\\n        values = self.values\\n        if is_object_dtype(dtype):\\n            values = values._box_values(values._data)\\n\\n        values = np.asarray(values)\\n\\n        if self.ndim == 2:\\n            # Ensure that our shape is correct for DataFrame.\\n            # ExtensionArrays are always 1-D, even in a DataFrame when\\n            # the analogous NumPy-backed column would be a 2-D ndarray.\\n            values = values.reshape(1, -1)\\n        return values',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_unstack_extension_series',\n",
       "  'docstring': 'Unstack an ExtensionArray-backed Series.\\n\\n    The ExtensionDtype is preserved.\\n\\n    Parameters\\n    ----------\\n    series : Series\\n        A Series with an ExtensionArray for values\\n    level : Any\\n        The level name or number.\\n    fill_value : Any\\n        The user-level (not physical storage) fill value to use for\\n        missing values introduced by the reshape. Passed to\\n        ``series.values.take``.\\n\\n    Returns\\n    -------\\n    DataFrame\\n        Each column of the DataFrame will have the same dtype as\\n        the input Series.',\n",
       "  'code': 'def _unstack_extension_series(series, level, fill_value):\\n    \"\"\"\\n    Unstack an ExtensionArray-backed Series.\\n\\n    The ExtensionDtype is preserved.\\n\\n    Parameters\\n    ----------\\n    series : Series\\n        A Series with an ExtensionArray for values\\n    level : Any\\n        The level name or number.\\n    fill_value : Any\\n        The user-level (not physical storage) fill value to use for\\n        missing values introduced by the reshape. Passed to\\n        ``series.values.take``.\\n\\n    Returns\\n    -------\\n    DataFrame\\n        Each column of the DataFrame will have the same dtype as\\n        the input Series.\\n    \"\"\"\\n    # Implementation note: the basic idea is to\\n    # 1. Do a regular unstack on a dummy array of integers\\n    # 2. Followup with a columnwise take.\\n    # We use the dummy take to discover newly-created missing values\\n    # introduced by the reshape.\\n    from pandas.core.reshape.concat import concat\\n\\n    dummy_arr = np.arange(len(series))\\n    # fill_value=-1, since we will do a series.values.take later\\n    result = _Unstacker(dummy_arr, series.index,\\n                        level=level, fill_value=-1).get_result()\\n\\n    out = []\\n    values = extract_array(series, extract_numpy=False)\\n\\n    for col, indices in result.iteritems():\\n        out.append(Series(values.take(indices.values,\\n                                      allow_fill=True,\\n                                      fill_value=fill_value),\\n                          name=col, index=result.index))\\n    return concat(out, axis=\\'columns\\', copy=False, keys=result.columns)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'get_dummies',\n",
       "  'docstring': \"Convert categorical variable into dummy/indicator variables.\\n\\n    Parameters\\n    ----------\\n    data : array-like, Series, or DataFrame\\n        Data of which to get dummy indicators.\\n    prefix : str, list of str, or dict of str, default None\\n        String to append DataFrame column names.\\n        Pass a list with length equal to the number of columns\\n        when calling get_dummies on a DataFrame. Alternatively, `prefix`\\n        can be a dictionary mapping column names to prefixes.\\n    prefix_sep : str, default '_'\\n        If appending prefix, separator/delimiter to use. Or pass a\\n        list or dictionary as with `prefix`.\\n    dummy_na : bool, default False\\n        Add a column to indicate NaNs, if False NaNs are ignored.\\n    columns : list-like, default None\\n        Column names in the DataFrame to be encoded.\\n        If `columns` is None then all the columns with\\n        `object` or `category` dtype will be converted.\\n    sparse : bool, default False\\n        Whether the dummy-encoded columns should be backed by\\n        a :class:`SparseArray` (True) or a regular NumPy array (False).\\n    drop_first : bool, default False\\n        Whether to get k-1 dummies out of k categorical levels by removing the\\n        first level.\\n\\n        .. versionadded:: 0.18.0\\n\\n    dtype : dtype, default np.uint8\\n        Data type for new columns. Only a single dtype is allowed.\\n\\n        .. versionadded:: 0.23.0\\n\\n    Returns\\n    -------\\n    DataFrame\\n        Dummy-coded data.\\n\\n    See Also\\n    --------\\n    Series.str.get_dummies : Convert Series to dummy codes.\\n\\n    Examples\\n    --------\\n    >>> s = pd.Series(list('abca'))\\n\\n    >>> pd.get_dummies(s)\\n       a  b  c\\n    0  1  0  0\\n    1  0  1  0\\n    2  0  0  1\\n    3  1  0  0\\n\\n    >>> s1 = ['a', 'b', np.nan]\\n\\n    >>> pd.get_dummies(s1)\\n       a  b\\n    0  1  0\\n    1  0  1\\n    2  0  0\\n\\n    >>> pd.get_dummies(s1, dummy_na=True)\\n       a  b  NaN\\n    0  1  0    0\\n    1  0  1    0\\n    2  0  0    1\\n\\n    >>> df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],\\n    ...                    'C': [1, 2, 3]})\\n\\n    >>> pd.get_dummies(df, prefix=['col1', 'col2'])\\n       C  col1_a  col1_b  col2_a  col2_b  col2_c\\n    0  1       1       0       0       1       0\\n    1  2       0       1       1       0       0\\n    2  3       1       0       0       0       1\\n\\n    >>> pd.get_dummies(pd.Series(list('abcaa')))\\n       a  b  c\\n    0  1  0  0\\n    1  0  1  0\\n    2  0  0  1\\n    3  1  0  0\\n    4  1  0  0\\n\\n    >>> pd.get_dummies(pd.Series(list('abcaa')), drop_first=True)\\n       b  c\\n    0  0  0\\n    1  1  0\\n    2  0  1\\n    3  0  0\\n    4  0  0\\n\\n    >>> pd.get_dummies(pd.Series(list('abc')), dtype=float)\\n         a    b    c\\n    0  1.0  0.0  0.0\\n    1  0.0  1.0  0.0\\n    2  0.0  0.0  1.0\",\n",
       "  'code': 'def get_dummies(data, prefix=None, prefix_sep=\\'_\\', dummy_na=False,\\n                columns=None, sparse=False, drop_first=False, dtype=None):\\n    \"\"\"\\n    Convert categorical variable into dummy/indicator variables.\\n\\n    Parameters\\n    ----------\\n    data : array-like, Series, or DataFrame\\n        Data of which to get dummy indicators.\\n    prefix : str, list of str, or dict of str, default None\\n        String to append DataFrame column names.\\n        Pass a list with length equal to the number of columns\\n        when calling get_dummies on a DataFrame. Alternatively, `prefix`\\n        can be a dictionary mapping column names to prefixes.\\n    prefix_sep : str, default \\'_\\'\\n        If appending prefix, separator/delimiter to use. Or pass a\\n        list or dictionary as with `prefix`.\\n    dummy_na : bool, default False\\n        Add a column to indicate NaNs, if False NaNs are ignored.\\n    columns : list-like, default None\\n        Column names in the DataFrame to be encoded.\\n        If `columns` is None then all the columns with\\n        `object` or `category` dtype will be converted.\\n    sparse : bool, default False\\n        Whether the dummy-encoded columns should be backed by\\n        a :class:`SparseArray` (True) or a regular NumPy array (False).\\n    drop_first : bool, default False\\n        Whether to get k-1 dummies out of k categorical levels by removing the\\n        first level.\\n\\n        .. versionadded:: 0.18.0\\n\\n    dtype : dtype, default np.uint8\\n        Data type for new columns. Only a single dtype is allowed.\\n\\n        .. versionadded:: 0.23.0\\n\\n    Returns\\n    -------\\n    DataFrame\\n        Dummy-coded data.\\n\\n    See Also\\n    --------\\n    Series.str.get_dummies : Convert Series to dummy codes.\\n\\n    Examples\\n    --------\\n    >>> s = pd.Series(list(\\'abca\\'))\\n\\n    >>> pd.get_dummies(s)\\n       a  b  c\\n    0  1  0  0\\n    1  0  1  0\\n    2  0  0  1\\n    3  1  0  0\\n\\n    >>> s1 = [\\'a\\', \\'b\\', np.nan]\\n\\n    >>> pd.get_dummies(s1)\\n       a  b\\n    0  1  0\\n    1  0  1\\n    2  0  0\\n\\n    >>> pd.get_dummies(s1, dummy_na=True)\\n       a  b  NaN\\n    0  1  0    0\\n    1  0  1    0\\n    2  0  0    1\\n\\n    >>> df = pd.DataFrame({\\'A\\': [\\'a\\', \\'b\\', \\'a\\'], \\'B\\': [\\'b\\', \\'a\\', \\'c\\'],\\n    ...                    \\'C\\': [1, 2, 3]})\\n\\n    >>> pd.get_dummies(df, prefix=[\\'col1\\', \\'col2\\'])\\n       C  col1_a  col1_b  col2_a  col2_b  col2_c\\n    0  1       1       0       0       1       0\\n    1  2       0       1       1       0       0\\n    2  3       1       0       0       0       1\\n\\n    >>> pd.get_dummies(pd.Series(list(\\'abcaa\\')))\\n       a  b  c\\n    0  1  0  0\\n    1  0  1  0\\n    2  0  0  1\\n    3  1  0  0\\n    4  1  0  0\\n\\n    >>> pd.get_dummies(pd.Series(list(\\'abcaa\\')), drop_first=True)\\n       b  c\\n    0  0  0\\n    1  1  0\\n    2  0  1\\n    3  0  0\\n    4  0  0\\n\\n    >>> pd.get_dummies(pd.Series(list(\\'abc\\')), dtype=float)\\n         a    b    c\\n    0  1.0  0.0  0.0\\n    1  0.0  1.0  0.0\\n    2  0.0  0.0  1.0\\n    \"\"\"\\n    from pandas.core.reshape.concat import concat\\n    from itertools import cycle\\n\\n    dtypes_to_encode = [\\'object\\', \\'category\\']\\n\\n    if isinstance(data, DataFrame):\\n        # determine columns being encoded\\n        if columns is None:\\n            data_to_encode = data.select_dtypes(\\n                include=dtypes_to_encode)\\n        else:\\n            data_to_encode = data[columns]\\n\\n        # validate prefixes and separator to avoid silently dropping cols\\n        def check_len(item, name):\\n            len_msg = (\"Length of \\'{name}\\' ({len_item}) did not match the \"\\n                       \"length of the columns being encoded ({len_enc}).\")\\n\\n            if is_list_like(item):\\n                if not len(item) == data_to_encode.shape[1]:\\n                    len_msg = len_msg.format(name=name, len_item=len(item),\\n                                             len_enc=data_to_encode.shape[1])\\n                    raise ValueError(len_msg)\\n\\n        check_len(prefix, \\'prefix\\')\\n        check_len(prefix_sep, \\'prefix_sep\\')\\n\\n        if isinstance(prefix, str):\\n            prefix = cycle([prefix])\\n        if isinstance(prefix, dict):\\n            prefix = [prefix[col] for col in data_to_encode.columns]\\n\\n        if prefix is None:\\n            prefix = data_to_encode.columns\\n\\n        # validate separators\\n        if isinstance(prefix_sep, str):\\n            prefix_sep = cycle([prefix_sep])\\n        elif isinstance(prefix_sep, dict):\\n            prefix_sep = [prefix_sep[col] for col in data_to_encode.columns]\\n\\n        if data_to_encode.shape == data.shape:\\n            # Encoding the entire df, do not prepend any dropped columns\\n            with_dummies = []\\n        elif columns is not None:\\n            # Encoding only cols specified in columns. Get all cols not in\\n            # columns to prepend to result.\\n            with_dummies = [data.drop(columns, axis=1)]\\n        else:\\n            # Encoding only object and category dtype columns. Get remaining\\n            # columns to prepend to result.\\n            with_dummies = [data.select_dtypes(exclude=dtypes_to_encode)]\\n\\n        for (col, pre, sep) in zip(data_to_encode.iteritems(), prefix,\\n                                   prefix_sep):\\n            # col is (column_name, column), use just column data here\\n            dummy = _get_dummies_1d(col[1], prefix=pre, prefix_sep=sep,\\n                                    dummy_na=dummy_na, sparse=sparse,\\n                                    drop_first=drop_first, dtype=dtype)\\n            with_dummies.append(dummy)\\n        result = concat(with_dummies, axis=1)\\n    else:\\n        result = _get_dummies_1d(data, prefix, prefix_sep, dummy_na,\\n                                 sparse=sparse,\\n                                 drop_first=drop_first,\\n                                 dtype=dtype)\\n    return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_compare_or_regex_search',\n",
       "  'docstring': 'Compare two array_like inputs of the same shape or two scalar values\\n\\n    Calls operator.eq or re.search, depending on regex argument. If regex is\\n    True, perform an element-wise regex matching.\\n\\n    Parameters\\n    ----------\\n    a : array_like or scalar\\n    b : array_like or scalar\\n    regex : bool, default False\\n\\n    Returns\\n    -------\\n    mask : array_like of bool',\n",
       "  'code': 'def _compare_or_regex_search(a, b, regex=False):\\n    \"\"\"\\n    Compare two array_like inputs of the same shape or two scalar values\\n\\n    Calls operator.eq or re.search, depending on regex argument. If regex is\\n    True, perform an element-wise regex matching.\\n\\n    Parameters\\n    ----------\\n    a : array_like or scalar\\n    b : array_like or scalar\\n    regex : bool, default False\\n\\n    Returns\\n    -------\\n    mask : array_like of bool\\n    \"\"\"\\n    if not regex:\\n        op = lambda x: operator.eq(x, b)\\n    else:\\n        op = np.vectorize(lambda x: bool(re.search(b, x)) if isinstance(x, str)\\n                          else False)\\n\\n    is_a_array = isinstance(a, np.ndarray)\\n    is_b_array = isinstance(b, np.ndarray)\\n\\n    # numpy deprecation warning to have i8 vs integer comparisons\\n    if is_datetimelike_v_numeric(a, b):\\n        result = False\\n\\n    # numpy deprecation warning if comparing numeric vs string-like\\n    elif is_numeric_v_string_like(a, b):\\n        result = False\\n    else:\\n        result = op(a)\\n\\n    if is_scalar(result) and (is_a_array or is_b_array):\\n        type_names = [type(a).__name__, type(b).__name__]\\n\\n        if is_a_array:\\n            type_names[0] = \\'ndarray(dtype={dtype})\\'.format(dtype=a.dtype)\\n\\n        if is_b_array:\\n            type_names[1] = \\'ndarray(dtype={dtype})\\'.format(dtype=b.dtype)\\n\\n        raise TypeError(\\n            \"Cannot compare types {a!r} and {b!r}\".format(a=type_names[0],\\n                                                          b=type_names[1]))\\n    return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'BlockManager.as_array',\n",
       "  'docstring': 'Convert the blockmanager data into an numpy array.\\n\\n        Parameters\\n        ----------\\n        transpose : boolean, default False\\n            If True, transpose the return array\\n        items : list of strings or None\\n            Names of block items that will be included in the returned\\n            array. ``None`` means that all block items will be used\\n\\n        Returns\\n        -------\\n        arr : ndarray',\n",
       "  'code': 'def as_array(self, transpose=False, items=None):\\n        \"\"\"Convert the blockmanager data into an numpy array.\\n\\n        Parameters\\n        ----------\\n        transpose : boolean, default False\\n            If True, transpose the return array\\n        items : list of strings or None\\n            Names of block items that will be included in the returned\\n            array. ``None`` means that all block items will be used\\n\\n        Returns\\n        -------\\n        arr : ndarray\\n        \"\"\"\\n        if len(self.blocks) == 0:\\n            arr = np.empty(self.shape, dtype=float)\\n            return arr.transpose() if transpose else arr\\n\\n        if items is not None:\\n            mgr = self.reindex_axis(items, axis=0)\\n        else:\\n            mgr = self\\n\\n        if self._is_single_block and mgr.blocks[0].is_datetimetz:\\n            # TODO(Block.get_values): Make DatetimeTZBlock.get_values\\n            # always be object dtype. Some callers seem to want the\\n            # DatetimeArray (previously DTI)\\n            arr = mgr.blocks[0].get_values(dtype=object)\\n        elif self._is_single_block or not self.is_mixed_type:\\n            arr = np.asarray(mgr.blocks[0].get_values())\\n        else:\\n            arr = mgr._interleave()\\n\\n        return arr.transpose() if transpose else arr',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_maybe_cache',\n",
       "  'docstring': 'Create a cache of unique dates from an array of dates\\n\\n    Parameters\\n    ----------\\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\\n    format : string\\n        Strftime format to parse time\\n    cache : boolean\\n        True attempts to create a cache of converted values\\n    convert_listlike : function\\n        Conversion function to apply on dates\\n\\n    Returns\\n    -------\\n    cache_array : Series\\n        Cache of converted, unique dates. Can be empty',\n",
       "  'code': 'def _maybe_cache(arg, format, cache, convert_listlike):\\n    \"\"\"\\n    Create a cache of unique dates from an array of dates\\n\\n    Parameters\\n    ----------\\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\\n    format : string\\n        Strftime format to parse time\\n    cache : boolean\\n        True attempts to create a cache of converted values\\n    convert_listlike : function\\n        Conversion function to apply on dates\\n\\n    Returns\\n    -------\\n    cache_array : Series\\n        Cache of converted, unique dates. Can be empty\\n    \"\"\"\\n    from pandas import Series\\n    cache_array = Series()\\n    if cache:\\n        # Perform a quicker unique check\\n        from pandas import Index\\n        unique_dates = Index(arg).unique()\\n        if len(unique_dates) < len(arg):\\n            cache_dates = convert_listlike(unique_dates.to_numpy(),\\n                                           True, format)\\n            cache_array = Series(cache_dates, index=unique_dates)\\n    return cache_array',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_convert_listlike_datetimes',\n",
       "  'docstring': \"Helper function for to_datetime. Performs the conversions of 1D listlike\\n    of dates\\n\\n    Parameters\\n    ----------\\n    arg : list, tuple, ndarray, Series, Index\\n        date to be parced\\n    box : boolean\\n        True boxes result as an Index-like, False returns an ndarray\\n    name : object\\n        None or string for the Index name\\n    tz : object\\n        None or 'utc'\\n    unit : string\\n        None or string of the frequency of the passed data\\n    errors : string\\n        error handing behaviors from to_datetime, 'raise', 'coerce', 'ignore'\\n    infer_datetime_format : boolean\\n        inferring format behavior from to_datetime\\n    dayfirst : boolean\\n        dayfirst parsing behavior from to_datetime\\n    yearfirst : boolean\\n        yearfirst parsing behavior from to_datetime\\n    exact : boolean\\n        exact format matching behavior from to_datetime\\n\\n    Returns\\n    -------\\n    ndarray of parsed dates\\n        Returns:\\n\\n        - Index-like if box=True\\n        - ndarray of Timestamps if box=False\",\n",
       "  'code': 'def _convert_listlike_datetimes(arg, box, format, name=None, tz=None,\\n                                unit=None, errors=None,\\n                                infer_datetime_format=None, dayfirst=None,\\n                                yearfirst=None, exact=None):\\n    \"\"\"\\n    Helper function for to_datetime. Performs the conversions of 1D listlike\\n    of dates\\n\\n    Parameters\\n    ----------\\n    arg : list, tuple, ndarray, Series, Index\\n        date to be parced\\n    box : boolean\\n        True boxes result as an Index-like, False returns an ndarray\\n    name : object\\n        None or string for the Index name\\n    tz : object\\n        None or \\'utc\\'\\n    unit : string\\n        None or string of the frequency of the passed data\\n    errors : string\\n        error handing behaviors from to_datetime, \\'raise\\', \\'coerce\\', \\'ignore\\'\\n    infer_datetime_format : boolean\\n        inferring format behavior from to_datetime\\n    dayfirst : boolean\\n        dayfirst parsing behavior from to_datetime\\n    yearfirst : boolean\\n        yearfirst parsing behavior from to_datetime\\n    exact : boolean\\n        exact format matching behavior from to_datetime\\n\\n    Returns\\n    -------\\n    ndarray of parsed dates\\n        Returns:\\n\\n        - Index-like if box=True\\n        - ndarray of Timestamps if box=False\\n    \"\"\"\\n    from pandas import DatetimeIndex\\n    from pandas.core.arrays import DatetimeArray\\n    from pandas.core.arrays.datetimes import (\\n        maybe_convert_dtype, objects_to_datetime64ns)\\n\\n    if isinstance(arg, (list, tuple)):\\n        arg = np.array(arg, dtype=\\'O\\')\\n\\n    # these are shortcutable\\n    if is_datetime64tz_dtype(arg):\\n        if not isinstance(arg, (DatetimeArray, DatetimeIndex)):\\n            return DatetimeIndex(arg, tz=tz, name=name)\\n        if tz == \\'utc\\':\\n            arg = arg.tz_convert(None).tz_localize(tz)\\n        return arg\\n\\n    elif is_datetime64_ns_dtype(arg):\\n        if box and not isinstance(arg, (DatetimeArray, DatetimeIndex)):\\n            try:\\n                return DatetimeIndex(arg, tz=tz, name=name)\\n            except ValueError:\\n                pass\\n\\n        return arg\\n\\n    elif unit is not None:\\n        if format is not None:\\n            raise ValueError(\"cannot specify both format and unit\")\\n        arg = getattr(arg, \\'values\\', arg)\\n        result = tslib.array_with_unit_to_datetime(arg, unit,\\n                                                   errors=errors)\\n        if box:\\n            if errors == \\'ignore\\':\\n                from pandas import Index\\n                result = Index(result, name=name)\\n                # GH 23758: We may still need to localize the result with tz\\n                try:\\n                    return result.tz_localize(tz)\\n                except AttributeError:\\n                    return result\\n\\n            return DatetimeIndex(result, tz=tz, name=name)\\n        return result\\n    elif getattr(arg, \\'ndim\\', 1) > 1:\\n        raise TypeError(\\'arg must be a string, datetime, list, tuple, \\'\\n                        \\'1-d array, or Series\\')\\n\\n    # warn if passing timedelta64, raise for PeriodDtype\\n    # NB: this must come after unit transformation\\n    orig_arg = arg\\n    arg, _ = maybe_convert_dtype(arg, copy=False)\\n\\n    arg = ensure_object(arg)\\n    require_iso8601 = False\\n\\n    if infer_datetime_format and format is None:\\n        format = _guess_datetime_format_for_array(arg, dayfirst=dayfirst)\\n\\n    if format is not None:\\n        # There is a special fast-path for iso8601 formatted\\n        # datetime strings, so in those cases don\\'t use the inferred\\n        # format because this path makes process slower in this\\n        # special case\\n        format_is_iso8601 = _format_is_iso(format)\\n        if format_is_iso8601:\\n            require_iso8601 = not infer_datetime_format\\n            format = None\\n\\n    tz_parsed = None\\n    result = None\\n\\n    if format is not None:\\n        try:\\n            # shortcut formatting here\\n            if format == \\'%Y%m%d\\':\\n                try:\\n                    # pass orig_arg as float-dtype may have been converted to\\n                    # datetime64[ns]\\n                    orig_arg = ensure_object(orig_arg)\\n                    result = _attempt_YYYYMMDD(orig_arg, errors=errors)\\n                except (ValueError, TypeError, tslibs.OutOfBoundsDatetime):\\n                    raise ValueError(\"cannot convert the input to \"\\n                                     \"\\'%Y%m%d\\' date format\")\\n\\n            # fallback\\n            if result is None:\\n                try:\\n                    result, timezones = array_strptime(\\n                        arg, format, exact=exact, errors=errors)\\n                    if \\'%Z\\' in format or \\'%z\\' in format:\\n                        return _return_parsed_timezone_results(\\n                            result, timezones, box, tz, name)\\n                except tslibs.OutOfBoundsDatetime:\\n                    if errors == \\'raise\\':\\n                        raise\\n                    elif errors == \\'coerce\\':\\n                        result = np.empty(arg.shape, dtype=\\'M8[ns]\\')\\n                        iresult = result.view(\\'i8\\')\\n                        iresult.fill(tslibs.iNaT)\\n                    else:\\n                        result = arg\\n                except ValueError:\\n                    # if format was inferred, try falling back\\n                    # to array_to_datetime - terminate here\\n                    # for specified formats\\n                    if not infer_datetime_format:\\n                        if errors == \\'raise\\':\\n                            raise\\n                        elif errors == \\'coerce\\':\\n                            result = np.empty(arg.shape, dtype=\\'M8[ns]\\')\\n                            iresult = result.view(\\'i8\\')\\n                            iresult.fill(tslibs.iNaT)\\n                        else:\\n                            result = arg\\n        except ValueError as e:\\n            # Fallback to try to convert datetime objects if timezone-aware\\n            #  datetime objects are found without passing `utc=True`\\n            try:\\n                values, tz = conversion.datetime_to_datetime64(arg)\\n                return DatetimeIndex._simple_new(values, name=name, tz=tz)\\n            except (ValueError, TypeError):\\n                raise e\\n\\n    if result is None:\\n        assert format is None or infer_datetime_format\\n        utc = tz == \\'utc\\'\\n        result, tz_parsed = objects_to_datetime64ns(\\n            arg, dayfirst=dayfirst, yearfirst=yearfirst,\\n            utc=utc, errors=errors, require_iso8601=require_iso8601,\\n            allow_object=True)\\n\\n    if tz_parsed is not None:\\n        if box:\\n            # We can take a shortcut since the datetime64 numpy array\\n            # is in UTC\\n            return DatetimeIndex._simple_new(result, name=name,\\n                                             tz=tz_parsed)\\n        else:\\n            # Convert the datetime64 numpy array to an numpy array\\n            # of datetime objects\\n            result = [Timestamp(ts, tz=tz_parsed).to_pydatetime()\\n                      for ts in result]\\n            return np.array(result, dtype=object)\\n\\n    if box:\\n        # Ensure we return an Index in all cases where box=True\\n        if is_datetime64_dtype(result):\\n            return DatetimeIndex(result, tz=tz, name=name)\\n        elif is_object_dtype(result):\\n            # e.g. an Index of datetime objects\\n            from pandas import Index\\n            return Index(result, name=name)\\n    return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'to_datetime',\n",
       "  'docstring': 'Convert argument to datetime.\\n\\n    Parameters\\n    ----------\\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\\n\\n        .. versionadded:: 0.18.1\\n\\n           or DataFrame/dict-like\\n\\n    errors : {\\'ignore\\', \\'raise\\', \\'coerce\\'}, default \\'raise\\'\\n\\n        - If \\'raise\\', then invalid parsing will raise an exception\\n        - If \\'coerce\\', then invalid parsing will be set as NaT\\n        - If \\'ignore\\', then invalid parsing will return the input\\n    dayfirst : boolean, default False\\n        Specify a date parse order if `arg` is str or its list-likes.\\n        If True, parses dates with the day first, eg 10/11/12 is parsed as\\n        2012-11-10.\\n        Warning: dayfirst=True is not strict, but will prefer to parse\\n        with day first (this is a known bug, based on dateutil behavior).\\n    yearfirst : boolean, default False\\n        Specify a date parse order if `arg` is str or its list-likes.\\n\\n        - If True parses dates with the year first, eg 10/11/12 is parsed as\\n          2010-11-12.\\n        - If both dayfirst and yearfirst are True, yearfirst is preceded (same\\n          as dateutil).\\n\\n        Warning: yearfirst=True is not strict, but will prefer to parse\\n        with year first (this is a known bug, based on dateutil behavior).\\n\\n        .. versionadded:: 0.16.1\\n\\n    utc : boolean, default None\\n        Return UTC DatetimeIndex if True (converting any tz-aware\\n        datetime.datetime objects as well).\\n    box : boolean, default True\\n\\n        - If True returns a DatetimeIndex or Index-like object\\n        - If False returns ndarray of values.\\n\\n        .. deprecated:: 0.25.0\\n            Use :meth:`.to_numpy` or :meth:`Timestamp.to_datetime64`\\n            instead to get an ndarray of values or numpy.datetime64,\\n            respectively.\\n\\n    format : string, default None\\n        strftime to parse time, eg \"%d/%m/%Y\", note that \"%f\" will parse\\n        all the way up to nanoseconds.\\n        See strftime documentation for more information on choices:\\n        https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\\n    exact : boolean, True by default\\n\\n        - If True, require an exact format match.\\n        - If False, allow the format to match anywhere in the target string.\\n\\n    unit : string, default \\'ns\\'\\n        unit of the arg (D,s,ms,us,ns) denote the unit, which is an\\n        integer or float number. This will be based off the origin.\\n        Example, with unit=\\'ms\\' and origin=\\'unix\\' (the default), this\\n        would calculate the number of milliseconds to the unix epoch start.\\n    infer_datetime_format : boolean, default False\\n        If True and no `format` is given, attempt to infer the format of the\\n        datetime strings, and if it can be inferred, switch to a faster\\n        method of parsing them. In some cases this can increase the parsing\\n        speed by ~5-10x.\\n    origin : scalar, default is \\'unix\\'\\n        Define the reference date. The numeric values would be parsed as number\\n        of units (defined by `unit`) since this reference date.\\n\\n        - If \\'unix\\' (or POSIX) time; origin is set to 1970-01-01.\\n        - If \\'julian\\', unit must be \\'D\\', and origin is set to beginning of\\n          Julian Calendar. Julian day number 0 is assigned to the day starting\\n          at noon on January 1, 4713 BC.\\n        - If Timestamp convertible, origin is set to Timestamp identified by\\n          origin.\\n\\n        .. versionadded:: 0.20.0\\n    cache : boolean, default False\\n        If True, use a cache of unique, converted dates to apply the datetime\\n        conversion. May produce significant speed-up when parsing duplicate\\n        date strings, especially ones with timezone offsets.\\n\\n        .. versionadded:: 0.23.0\\n\\n    Returns\\n    -------\\n    ret : datetime if parsing succeeded.\\n        Return type depends on input:\\n\\n        - list-like: DatetimeIndex\\n        - Series: Series of datetime64 dtype\\n        - scalar: Timestamp\\n\\n        In case when it is not possible to return designated types (e.g. when\\n        any element of input is before Timestamp.min or after Timestamp.max)\\n        return will have datetime.datetime type (or corresponding\\n        array/Series).\\n\\n    See Also\\n    --------\\n    DataFrame.astype : Cast argument to a specified dtype.\\n    to_timedelta : Convert argument to timedelta.\\n\\n    Examples\\n    --------\\n    Assembling a datetime from multiple columns of a DataFrame. The keys can be\\n    common abbreviations like [\\'year\\', \\'month\\', \\'day\\', \\'minute\\', \\'second\\',\\n    \\'ms\\', \\'us\\', \\'ns\\']) or plurals of the same\\n\\n    >>> df = pd.DataFrame({\\'year\\': [2015, 2016],\\n                           \\'month\\': [2, 3],\\n                           \\'day\\': [4, 5]})\\n    >>> pd.to_datetime(df)\\n    0   2015-02-04\\n    1   2016-03-05\\n    dtype: datetime64[ns]\\n\\n    If a date does not meet the `timestamp limitations\\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html\\n    #timeseries-timestamp-limits>`_, passing errors=\\'ignore\\'\\n    will return the original input instead of raising any exception.\\n\\n    Passing errors=\\'coerce\\' will force an out-of-bounds date to NaT,\\n    in addition to forcing non-dates (or non-parseable dates) to NaT.\\n\\n    >>> pd.to_datetime(\\'13000101\\', format=\\'%Y%m%d\\', errors=\\'ignore\\')\\n    datetime.datetime(1300, 1, 1, 0, 0)\\n    >>> pd.to_datetime(\\'13000101\\', format=\\'%Y%m%d\\', errors=\\'coerce\\')\\n    NaT\\n\\n    Passing infer_datetime_format=True can often-times speedup a parsing\\n    if its not an ISO8601 format exactly, but in a regular format.\\n\\n    >>> s = pd.Series([\\'3/11/2000\\', \\'3/12/2000\\', \\'3/13/2000\\']*1000)\\n\\n    >>> s.head()\\n    0    3/11/2000\\n    1    3/12/2000\\n    2    3/13/2000\\n    3    3/11/2000\\n    4    3/12/2000\\n    dtype: object\\n\\n    >>> %timeit pd.to_datetime(s,infer_datetime_format=True)\\n    100 loops, best of 3: 10.4 ms per loop\\n\\n    >>> %timeit pd.to_datetime(s,infer_datetime_format=False)\\n    1 loop, best of 3: 471 ms per loop\\n\\n    Using a unix epoch time\\n\\n    >>> pd.to_datetime(1490195805, unit=\\'s\\')\\n    Timestamp(\\'2017-03-22 15:16:45\\')\\n    >>> pd.to_datetime(1490195805433502912, unit=\\'ns\\')\\n    Timestamp(\\'2017-03-22 15:16:45.433502912\\')\\n\\n    .. warning:: For float arg, precision rounding might happen. To prevent\\n        unexpected behavior use a fixed-width exact type.\\n\\n    Using a non-unix epoch origin\\n\\n    >>> pd.to_datetime([1, 2, 3], unit=\\'D\\',\\n                       origin=pd.Timestamp(\\'1960-01-01\\'))\\n    0    1960-01-02\\n    1    1960-01-03\\n    2    1960-01-04',\n",
       "  'code': 'def to_datetime(arg, errors=\\'raise\\', dayfirst=False, yearfirst=False,\\n                utc=None, box=True, format=None, exact=True,\\n                unit=None, infer_datetime_format=False, origin=\\'unix\\',\\n                cache=False):\\n    \"\"\"\\n    Convert argument to datetime.\\n\\n    Parameters\\n    ----------\\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\\n\\n        .. versionadded:: 0.18.1\\n\\n           or DataFrame/dict-like\\n\\n    errors : {\\'ignore\\', \\'raise\\', \\'coerce\\'}, default \\'raise\\'\\n\\n        - If \\'raise\\', then invalid parsing will raise an exception\\n        - If \\'coerce\\', then invalid parsing will be set as NaT\\n        - If \\'ignore\\', then invalid parsing will return the input\\n    dayfirst : boolean, default False\\n        Specify a date parse order if `arg` is str or its list-likes.\\n        If True, parses dates with the day first, eg 10/11/12 is parsed as\\n        2012-11-10.\\n        Warning: dayfirst=True is not strict, but will prefer to parse\\n        with day first (this is a known bug, based on dateutil behavior).\\n    yearfirst : boolean, default False\\n        Specify a date parse order if `arg` is str or its list-likes.\\n\\n        - If True parses dates with the year first, eg 10/11/12 is parsed as\\n          2010-11-12.\\n        - If both dayfirst and yearfirst are True, yearfirst is preceded (same\\n          as dateutil).\\n\\n        Warning: yearfirst=True is not strict, but will prefer to parse\\n        with year first (this is a known bug, based on dateutil behavior).\\n\\n        .. versionadded:: 0.16.1\\n\\n    utc : boolean, default None\\n        Return UTC DatetimeIndex if True (converting any tz-aware\\n        datetime.datetime objects as well).\\n    box : boolean, default True\\n\\n        - If True returns a DatetimeIndex or Index-like object\\n        - If False returns ndarray of values.\\n\\n        .. deprecated:: 0.25.0\\n            Use :meth:`.to_numpy` or :meth:`Timestamp.to_datetime64`\\n            instead to get an ndarray of values or numpy.datetime64,\\n            respectively.\\n\\n    format : string, default None\\n        strftime to parse time, eg \"%d/%m/%Y\", note that \"%f\" will parse\\n        all the way up to nanoseconds.\\n        See strftime documentation for more information on choices:\\n        https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\\n    exact : boolean, True by default\\n\\n        - If True, require an exact format match.\\n        - If False, allow the format to match anywhere in the target string.\\n\\n    unit : string, default \\'ns\\'\\n        unit of the arg (D,s,ms,us,ns) denote the unit, which is an\\n        integer or float number. This will be based off the origin.\\n        Example, with unit=\\'ms\\' and origin=\\'unix\\' (the default), this\\n        would calculate the number of milliseconds to the unix epoch start.\\n    infer_datetime_format : boolean, default False\\n        If True and no `format` is given, attempt to infer the format of the\\n        datetime strings, and if it can be inferred, switch to a faster\\n        method of parsing them. In some cases this can increase the parsing\\n        speed by ~5-10x.\\n    origin : scalar, default is \\'unix\\'\\n        Define the reference date. The numeric values would be parsed as number\\n        of units (defined by `unit`) since this reference date.\\n\\n        - If \\'unix\\' (or POSIX) time; origin is set to 1970-01-01.\\n        - If \\'julian\\', unit must be \\'D\\', and origin is set to beginning of\\n          Julian Calendar. Julian day number 0 is assigned to the day starting\\n          at noon on January 1, 4713 BC.\\n        - If Timestamp convertible, origin is set to Timestamp identified by\\n          origin.\\n\\n        .. versionadded:: 0.20.0\\n    cache : boolean, default False\\n        If True, use a cache of unique, converted dates to apply the datetime\\n        conversion. May produce significant speed-up when parsing duplicate\\n        date strings, especially ones with timezone offsets.\\n\\n        .. versionadded:: 0.23.0\\n\\n    Returns\\n    -------\\n    ret : datetime if parsing succeeded.\\n        Return type depends on input:\\n\\n        - list-like: DatetimeIndex\\n        - Series: Series of datetime64 dtype\\n        - scalar: Timestamp\\n\\n        In case when it is not possible to return designated types (e.g. when\\n        any element of input is before Timestamp.min or after Timestamp.max)\\n        return will have datetime.datetime type (or corresponding\\n        array/Series).\\n\\n    See Also\\n    --------\\n    DataFrame.astype : Cast argument to a specified dtype.\\n    to_timedelta : Convert argument to timedelta.\\n\\n    Examples\\n    --------\\n    Assembling a datetime from multiple columns of a DataFrame. The keys can be\\n    common abbreviations like [\\'year\\', \\'month\\', \\'day\\', \\'minute\\', \\'second\\',\\n    \\'ms\\', \\'us\\', \\'ns\\']) or plurals of the same\\n\\n    >>> df = pd.DataFrame({\\'year\\': [2015, 2016],\\n                           \\'month\\': [2, 3],\\n                           \\'day\\': [4, 5]})\\n    >>> pd.to_datetime(df)\\n    0   2015-02-04\\n    1   2016-03-05\\n    dtype: datetime64[ns]\\n\\n    If a date does not meet the `timestamp limitations\\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html\\n    #timeseries-timestamp-limits>`_, passing errors=\\'ignore\\'\\n    will return the original input instead of raising any exception.\\n\\n    Passing errors=\\'coerce\\' will force an out-of-bounds date to NaT,\\n    in addition to forcing non-dates (or non-parseable dates) to NaT.\\n\\n    >>> pd.to_datetime(\\'13000101\\', format=\\'%Y%m%d\\', errors=\\'ignore\\')\\n    datetime.datetime(1300, 1, 1, 0, 0)\\n    >>> pd.to_datetime(\\'13000101\\', format=\\'%Y%m%d\\', errors=\\'coerce\\')\\n    NaT\\n\\n    Passing infer_datetime_format=True can often-times speedup a parsing\\n    if its not an ISO8601 format exactly, but in a regular format.\\n\\n    >>> s = pd.Series([\\'3/11/2000\\', \\'3/12/2000\\', \\'3/13/2000\\']*1000)\\n\\n    >>> s.head()\\n    0    3/11/2000\\n    1    3/12/2000\\n    2    3/13/2000\\n    3    3/11/2000\\n    4    3/12/2000\\n    dtype: object\\n\\n    >>> %timeit pd.to_datetime(s,infer_datetime_format=True)\\n    100 loops, best of 3: 10.4 ms per loop\\n\\n    >>> %timeit pd.to_datetime(s,infer_datetime_format=False)\\n    1 loop, best of 3: 471 ms per loop\\n\\n    Using a unix epoch time\\n\\n    >>> pd.to_datetime(1490195805, unit=\\'s\\')\\n    Timestamp(\\'2017-03-22 15:16:45\\')\\n    >>> pd.to_datetime(1490195805433502912, unit=\\'ns\\')\\n    Timestamp(\\'2017-03-22 15:16:45.433502912\\')\\n\\n    .. warning:: For float arg, precision rounding might happen. To prevent\\n        unexpected behavior use a fixed-width exact type.\\n\\n    Using a non-unix epoch origin\\n\\n    >>> pd.to_datetime([1, 2, 3], unit=\\'D\\',\\n                       origin=pd.Timestamp(\\'1960-01-01\\'))\\n    0    1960-01-02\\n    1    1960-01-03\\n    2    1960-01-04\\n    \"\"\"\\n    if arg is None:\\n        return None\\n\\n    if origin != \\'unix\\':\\n        arg = _adjust_to_origin(arg, origin, unit)\\n\\n    tz = \\'utc\\' if utc else None\\n    convert_listlike = partial(_convert_listlike_datetimes, tz=tz, unit=unit,\\n                               dayfirst=dayfirst, yearfirst=yearfirst,\\n                               errors=errors, exact=exact,\\n                               infer_datetime_format=infer_datetime_format)\\n\\n    if isinstance(arg, Timestamp):\\n        result = arg\\n        if tz is not None:\\n            if arg.tz is not None:\\n                result = result.tz_convert(tz)\\n            else:\\n                result = result.tz_localize(tz)\\n    elif isinstance(arg, ABCSeries):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = arg.map(cache_array)\\n        else:\\n            values = convert_listlike(arg._values, True, format)\\n            result = arg._constructor(values, index=arg.index, name=arg.name)\\n    elif isinstance(arg, (ABCDataFrame, abc.MutableMapping)):\\n        result = _assemble_from_unit_mappings(arg, errors, box, tz)\\n    elif isinstance(arg, ABCIndexClass):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = _convert_and_box_cache(arg, cache_array, box, errors,\\n                                            name=arg.name)\\n        else:\\n            convert_listlike = partial(convert_listlike, name=arg.name)\\n            result = convert_listlike(arg, box, format)\\n    elif is_list_like(arg):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = _convert_and_box_cache(arg, cache_array, box, errors)\\n        else:\\n            result = convert_listlike(arg, box, format)\\n    else:\\n        result = convert_listlike(np.array([arg]), box, format)[0]\\n\\n    return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'RangeIndex.argsort',\n",
       "  'docstring': 'Returns the indices that would sort the index and its\\n        underlying data.\\n\\n        Returns\\n        -------\\n        argsorted : numpy array\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argsort',\n",
       "  'code': 'def argsort(self, *args, **kwargs):\\n        \"\"\"\\n        Returns the indices that would sort the index and its\\n        underlying data.\\n\\n        Returns\\n        -------\\n        argsorted : numpy array\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argsort\\n        \"\"\"\\n        nv.validate_argsort(args, kwargs)\\n\\n        if self._step > 0:\\n            return np.arange(len(self))\\n        else:\\n            return np.arange(len(self) - 1, -1, -1)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'RangeIndex._add_numeric_methods_binary',\n",
       "  'docstring': 'add in numeric methods, specialized to RangeIndex',\n",
       "  'code': 'def _add_numeric_methods_binary(cls):\\n        \"\"\" add in numeric methods, specialized to RangeIndex \"\"\"\\n\\n        def _make_evaluate_binop(op, step=False):\\n            \"\"\"\\n            Parameters\\n            ----------\\n            op : callable that accepts 2 parms\\n                perform the binary op\\n            step : callable, optional, default to False\\n                op to apply to the step parm if not None\\n                if False, use the existing step\\n            \"\"\"\\n\\n            def _evaluate_numeric_binop(self, other):\\n                if isinstance(other, (ABCSeries, ABCDataFrame)):\\n                    return NotImplemented\\n                elif isinstance(other, ABCTimedeltaIndex):\\n                    # Defer to TimedeltaIndex implementation\\n                    return NotImplemented\\n                elif isinstance(other, (timedelta, np.timedelta64)):\\n                    # GH#19333 is_integer evaluated True on timedelta64,\\n                    # so we need to catch these explicitly\\n                    return op(self._int64index, other)\\n                elif is_timedelta64_dtype(other):\\n                    # Must be an np.ndarray; GH#22390\\n                    return op(self._int64index, other)\\n\\n                other = self._validate_for_numeric_binop(other, op)\\n                attrs = self._get_attributes_dict()\\n                attrs = self._maybe_update_attributes(attrs)\\n\\n                left, right = self, other\\n\\n                try:\\n                    # apply if we have an override\\n                    if step:\\n                        with np.errstate(all=\\'ignore\\'):\\n                            rstep = step(left._step, right)\\n\\n                        # we don\\'t have a representable op\\n                        # so return a base index\\n                        if not is_integer(rstep) or not rstep:\\n                            raise ValueError\\n\\n                    else:\\n                        rstep = left._step\\n\\n                    with np.errstate(all=\\'ignore\\'):\\n                        rstart = op(left._start, right)\\n                        rstop = op(left._stop, right)\\n\\n                    result = RangeIndex(rstart,\\n                                        rstop,\\n                                        rstep,\\n                                        **attrs)\\n\\n                    # for compat with numpy / Int64Index\\n                    # even if we can represent as a RangeIndex, return\\n                    # as a Float64Index if we have float-like descriptors\\n                    if not all(is_integer(x) for x in\\n                               [rstart, rstop, rstep]):\\n                        result = result.astype(\\'float64\\')\\n\\n                    return result\\n\\n                except (ValueError, TypeError, ZeroDivisionError):\\n                    # Defer to Int64Index implementation\\n                    return op(self._int64index, other)\\n                    # TODO: Do attrs get handled reliably?\\n\\n            name = \\'__{name}__\\'.format(name=op.__name__)\\n            return compat.set_function_name(_evaluate_numeric_binop, name, cls)\\n\\n        cls.__add__ = _make_evaluate_binop(operator.add)\\n        cls.__radd__ = _make_evaluate_binop(ops.radd)\\n        cls.__sub__ = _make_evaluate_binop(operator.sub)\\n        cls.__rsub__ = _make_evaluate_binop(ops.rsub)\\n        cls.__mul__ = _make_evaluate_binop(operator.mul, step=operator.mul)\\n        cls.__rmul__ = _make_evaluate_binop(ops.rmul, step=ops.rmul)\\n        cls.__truediv__ = _make_evaluate_binop(operator.truediv,\\n                                               step=operator.truediv)\\n        cls.__rtruediv__ = _make_evaluate_binop(ops.rtruediv,\\n                                                step=ops.rtruediv)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'PandasArray.to_numpy',\n",
       "  'docstring': 'Convert the PandasArray to a :class:`numpy.ndarray`.\\n\\n        By default, this requires no coercion or copying of data.\\n\\n        Parameters\\n        ----------\\n        dtype : numpy.dtype\\n            The NumPy dtype to pass to :func:`numpy.asarray`.\\n        copy : bool, default False\\n            Whether to copy the underlying data.\\n\\n        Returns\\n        -------\\n        ndarray',\n",
       "  'code': 'def to_numpy(self, dtype=None, copy=False):\\n        \"\"\"\\n        Convert the PandasArray to a :class:`numpy.ndarray`.\\n\\n        By default, this requires no coercion or copying of data.\\n\\n        Parameters\\n        ----------\\n        dtype : numpy.dtype\\n            The NumPy dtype to pass to :func:`numpy.asarray`.\\n        copy : bool, default False\\n            Whether to copy the underlying data.\\n\\n        Returns\\n        -------\\n        ndarray\\n        \"\"\"\\n        result = np.asarray(self._ndarray, dtype=dtype)\\n        if copy and result is self._ndarray:\\n            result = result.copy()\\n\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'andrews_curves',\n",
       "  'docstring': 'Generate a matplotlib plot of Andrews curves, for visualising clusters of\\n    multivariate data.\\n\\n    Andrews curves have the functional form:\\n\\n    f(t) = x_1/sqrt(2) + x_2 sin(t) + x_3 cos(t) +\\n           x_4 sin(2t) + x_5 cos(2t) + ...\\n\\n    Where x coefficients correspond to the values of each dimension and t is\\n    linearly spaced between -pi and +pi. Each row of frame then corresponds to\\n    a single curve.\\n\\n    Parameters\\n    ----------\\n    frame : DataFrame\\n        Data to be plotted, preferably normalized to (0.0, 1.0)\\n    class_column : Name of the column containing class names\\n    ax : matplotlib axes object, default None\\n    samples : Number of points to plot in each curve\\n    color : list or tuple, optional\\n        Colors to use for the different classes\\n    colormap : str or matplotlib colormap object, default None\\n        Colormap to select colors from. If string, load colormap with that name\\n        from matplotlib.\\n    kwds : keywords\\n        Options to pass to matplotlib plotting method\\n\\n    Returns\\n    -------\\n    class:`matplotlip.axis.Axes`',\n",
       "  'code': 'def andrews_curves(frame, class_column, ax=None, samples=200, color=None,\\n                   colormap=None, **kwds):\\n    \"\"\"\\n    Generate a matplotlib plot of Andrews curves, for visualising clusters of\\n    multivariate data.\\n\\n    Andrews curves have the functional form:\\n\\n    f(t) = x_1/sqrt(2) + x_2 sin(t) + x_3 cos(t) +\\n           x_4 sin(2t) + x_5 cos(2t) + ...\\n\\n    Where x coefficients correspond to the values of each dimension and t is\\n    linearly spaced between -pi and +pi. Each row of frame then corresponds to\\n    a single curve.\\n\\n    Parameters\\n    ----------\\n    frame : DataFrame\\n        Data to be plotted, preferably normalized to (0.0, 1.0)\\n    class_column : Name of the column containing class names\\n    ax : matplotlib axes object, default None\\n    samples : Number of points to plot in each curve\\n    color : list or tuple, optional\\n        Colors to use for the different classes\\n    colormap : str or matplotlib colormap object, default None\\n        Colormap to select colors from. If string, load colormap with that name\\n        from matplotlib.\\n    kwds : keywords\\n        Options to pass to matplotlib plotting method\\n\\n    Returns\\n    -------\\n    class:`matplotlip.axis.Axes`\\n    \"\"\"\\n    from math import sqrt, pi\\n    import matplotlib.pyplot as plt\\n\\n    def function(amplitudes):\\n        def f(t):\\n            x1 = amplitudes[0]\\n            result = x1 / sqrt(2.0)\\n\\n            # Take the rest of the coefficients and resize them\\n            # appropriately. Take a copy of amplitudes as otherwise numpy\\n            # deletes the element from amplitudes itself.\\n            coeffs = np.delete(np.copy(amplitudes), 0)\\n            coeffs.resize(int((coeffs.size + 1) / 2), 2)\\n\\n            # Generate the harmonics and arguments for the sin and cos\\n            # functions.\\n            harmonics = np.arange(0, coeffs.shape[0]) + 1\\n            trig_args = np.outer(harmonics, t)\\n\\n            result += np.sum(coeffs[:, 0, np.newaxis] * np.sin(trig_args) +\\n                             coeffs[:, 1, np.newaxis] * np.cos(trig_args),\\n                             axis=0)\\n            return result\\n        return f\\n\\n    n = len(frame)\\n    class_col = frame[class_column]\\n    classes = frame[class_column].drop_duplicates()\\n    df = frame.drop(class_column, axis=1)\\n    t = np.linspace(-pi, pi, samples)\\n    used_legends = set()\\n\\n    color_values = _get_standard_colors(num_colors=len(classes),\\n                                        colormap=colormap, color_type=\\'random\\',\\n                                        color=color)\\n    colors = dict(zip(classes, color_values))\\n    if ax is None:\\n        ax = plt.gca(xlim=(-pi, pi))\\n    for i in range(n):\\n        row = df.iloc[i].values\\n        f = function(row)\\n        y = f(t)\\n        kls = class_col.iat[i]\\n        label = pprint_thing(kls)\\n        if label not in used_legends:\\n            used_legends.add(label)\\n            ax.plot(t, y, color=colors[kls], label=label, **kwds)\\n        else:\\n            ax.plot(t, y, color=colors[kls], **kwds)\\n\\n    ax.legend(loc=\\'upper right\\')\\n    ax.grid()\\n    return ax',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame.dot',\n",
       "  'docstring': 'Compute the matrix mutiplication between the DataFrame and other.\\n\\n        This method computes the matrix product between the DataFrame and the\\n        values of an other Series, DataFrame or a numpy array.\\n\\n        It can also be called using ``self @ other`` in Python >= 3.5.\\n\\n        Parameters\\n        ----------\\n        other : Series, DataFrame or array-like\\n            The other object to compute the matrix product with.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            If other is a Series, return the matrix product between self and\\n            other as a Serie. If other is a DataFrame or a numpy.array, return\\n            the matrix product of self and other in a DataFrame of a np.array.\\n\\n        See Also\\n        --------\\n        Series.dot: Similar method for Series.\\n\\n        Notes\\n        -----\\n        The dimensions of DataFrame and other must be compatible in order to\\n        compute the matrix multiplication.\\n\\n        The dot method for Series computes the inner product, instead of the\\n        matrix product here.\\n\\n        Examples\\n        --------\\n        Here we multiply a DataFrame with a Series.\\n\\n        >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\\n        >>> s = pd.Series([1, 1, 2, 1])\\n        >>> df.dot(s)\\n        0    -4\\n        1     5\\n        dtype: int64\\n\\n        Here we multiply a DataFrame with another DataFrame.\\n\\n        >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\\n        >>> df.dot(other)\\n            0   1\\n        0   1   4\\n        1   2   2\\n\\n        Note that the dot method give the same result as @\\n\\n        >>> df @ other\\n            0   1\\n        0   1   4\\n        1   2   2\\n\\n        The dot method works also if other is an np.array.\\n\\n        >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\\n        >>> df.dot(arr)\\n            0   1\\n        0   1   4\\n        1   2   2',\n",
       "  'code': 'def dot(self, other):\\n        \"\"\"\\n        Compute the matrix mutiplication between the DataFrame and other.\\n\\n        This method computes the matrix product between the DataFrame and the\\n        values of an other Series, DataFrame or a numpy array.\\n\\n        It can also be called using ``self @ other`` in Python >= 3.5.\\n\\n        Parameters\\n        ----------\\n        other : Series, DataFrame or array-like\\n            The other object to compute the matrix product with.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            If other is a Series, return the matrix product between self and\\n            other as a Serie. If other is a DataFrame or a numpy.array, return\\n            the matrix product of self and other in a DataFrame of a np.array.\\n\\n        See Also\\n        --------\\n        Series.dot: Similar method for Series.\\n\\n        Notes\\n        -----\\n        The dimensions of DataFrame and other must be compatible in order to\\n        compute the matrix multiplication.\\n\\n        The dot method for Series computes the inner product, instead of the\\n        matrix product here.\\n\\n        Examples\\n        --------\\n        Here we multiply a DataFrame with a Series.\\n\\n        >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\\n        >>> s = pd.Series([1, 1, 2, 1])\\n        >>> df.dot(s)\\n        0    -4\\n        1     5\\n        dtype: int64\\n\\n        Here we multiply a DataFrame with another DataFrame.\\n\\n        >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\\n        >>> df.dot(other)\\n            0   1\\n        0   1   4\\n        1   2   2\\n\\n        Note that the dot method give the same result as @\\n\\n        >>> df @ other\\n            0   1\\n        0   1   4\\n        1   2   2\\n\\n        The dot method works also if other is an np.array.\\n\\n        >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\\n        >>> df.dot(arr)\\n            0   1\\n        0   1   4\\n        1   2   2\\n        \"\"\"\\n        if isinstance(other, (Series, DataFrame)):\\n            common = self.columns.union(other.index)\\n            if (len(common) > len(self.columns) or\\n                    len(common) > len(other.index)):\\n                raise ValueError(\\'matrices are not aligned\\')\\n\\n            left = self.reindex(columns=common, copy=False)\\n            right = other.reindex(index=common, copy=False)\\n            lvals = left.values\\n            rvals = right.values\\n        else:\\n            left = self\\n            lvals = self.values\\n            rvals = np.asarray(other)\\n            if lvals.shape[1] != rvals.shape[0]:\\n                raise ValueError(\\'Dot product shape mismatch, \\'\\n                                 \\'{s} vs {r}\\'.format(s=lvals.shape,\\n                                                     r=rvals.shape))\\n\\n        if isinstance(other, DataFrame):\\n            return self._constructor(np.dot(lvals, rvals), index=left.index,\\n                                     columns=other.columns)\\n        elif isinstance(other, Series):\\n            return Series(np.dot(lvals, rvals), index=left.index)\\n        elif isinstance(rvals, (np.ndarray, Index)):\\n            result = np.dot(lvals, rvals)\\n            if result.ndim == 2:\\n                return self._constructor(result, index=left.index)\\n            else:\\n                return Series(result, index=left.index)\\n        else:  # pragma: no cover\\n            raise TypeError(\\'unsupported type: {oth}\\'.format(oth=type(other)))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame.to_numpy',\n",
       "  'docstring': 'Convert the DataFrame to a NumPy array.\\n\\n        .. versionadded:: 0.24.0\\n\\n        By default, the dtype of the returned array will be the common NumPy\\n        dtype of all types in the DataFrame. For example, if the dtypes are\\n        ``float16`` and ``float32``, the results dtype will be ``float32``.\\n        This may require copying data and coercing values, which may be\\n        expensive.\\n\\n        Parameters\\n        ----------\\n        dtype : str or numpy.dtype, optional\\n            The dtype to pass to :meth:`numpy.asarray`\\n        copy : bool, default False\\n            Whether to ensure that the returned value is a not a view on\\n            another array. Note that ``copy=False`` does not *ensure* that\\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\\n            a copy is made, even if not strictly necessary.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n\\n        See Also\\n        --------\\n        Series.to_numpy : Similar method for Series.\\n\\n        Examples\\n        --------\\n        >>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\\n        array([[1, 3],\\n               [2, 4]])\\n\\n        With heterogenous data, the lowest common type will have to\\n        be used.\\n\\n        >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\\n        >>> df.to_numpy()\\n        array([[1. , 3. ],\\n               [2. , 4.5]])\\n\\n        For a mix of numeric and non-numeric types, the output array will\\n        have object dtype.\\n\\n        >>> df[\\'C\\'] = pd.date_range(\\'2000\\', periods=2)\\n        >>> df.to_numpy()\\n        array([[1, 3.0, Timestamp(\\'2000-01-01 00:00:00\\')],\\n               [2, 4.5, Timestamp(\\'2000-01-02 00:00:00\\')]], dtype=object)',\n",
       "  'code': 'def to_numpy(self, dtype=None, copy=False):\\n        \"\"\"\\n        Convert the DataFrame to a NumPy array.\\n\\n        .. versionadded:: 0.24.0\\n\\n        By default, the dtype of the returned array will be the common NumPy\\n        dtype of all types in the DataFrame. For example, if the dtypes are\\n        ``float16`` and ``float32``, the results dtype will be ``float32``.\\n        This may require copying data and coercing values, which may be\\n        expensive.\\n\\n        Parameters\\n        ----------\\n        dtype : str or numpy.dtype, optional\\n            The dtype to pass to :meth:`numpy.asarray`\\n        copy : bool, default False\\n            Whether to ensure that the returned value is a not a view on\\n            another array. Note that ``copy=False`` does not *ensure* that\\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\\n            a copy is made, even if not strictly necessary.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n\\n        See Also\\n        --------\\n        Series.to_numpy : Similar method for Series.\\n\\n        Examples\\n        --------\\n        >>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\\n        array([[1, 3],\\n               [2, 4]])\\n\\n        With heterogenous data, the lowest common type will have to\\n        be used.\\n\\n        >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\\n        >>> df.to_numpy()\\n        array([[1. , 3. ],\\n               [2. , 4.5]])\\n\\n        For a mix of numeric and non-numeric types, the output array will\\n        have object dtype.\\n\\n        >>> df[\\'C\\'] = pd.date_range(\\'2000\\', periods=2)\\n        >>> df.to_numpy()\\n        array([[1, 3.0, Timestamp(\\'2000-01-01 00:00:00\\')],\\n               [2, 4.5, Timestamp(\\'2000-01-02 00:00:00\\')]], dtype=object)\\n        \"\"\"\\n        result = np.array(self.values, dtype=dtype, copy=copy)\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame.to_records',\n",
       "  'docstring': 'Convert DataFrame to a NumPy record array.\\n\\n        Index will be included as the first field of the record array if\\n        requested.\\n\\n        Parameters\\n        ----------\\n        index : bool, default True\\n            Include index in resulting record array, stored in \\'index\\'\\n            field or using the index label, if set.\\n        convert_datetime64 : bool, default None\\n            .. deprecated:: 0.23.0\\n\\n            Whether to convert the index to datetime.datetime if it is a\\n            DatetimeIndex.\\n        column_dtypes : str, type, dict, default None\\n            .. versionadded:: 0.24.0\\n\\n            If a string or type, the data type to store all columns. If\\n            a dictionary, a mapping of column names and indices (zero-indexed)\\n            to specific data types.\\n        index_dtypes : str, type, dict, default None\\n            .. versionadded:: 0.24.0\\n\\n            If a string or type, the data type to store all index levels. If\\n            a dictionary, a mapping of index level names and indices\\n            (zero-indexed) to specific data types.\\n\\n            This mapping is applied only if `index=True`.\\n\\n        Returns\\n        -------\\n        numpy.recarray\\n            NumPy ndarray with the DataFrame labels as fields and each row\\n            of the DataFrame as entries.\\n\\n        See Also\\n        --------\\n        DataFrame.from_records: Convert structured or record ndarray\\n            to DataFrame.\\n        numpy.recarray: An ndarray that allows field access using\\n            attributes, analogous to typed columns in a\\n            spreadsheet.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'A\\': [1, 2], \\'B\\': [0.5, 0.75]},\\n        ...                   index=[\\'a\\', \\'b\\'])\\n        >>> df\\n           A     B\\n        a  1  0.50\\n        b  2  0.75\\n        >>> df.to_records()\\n        rec.array([(\\'a\\', 1, 0.5 ), (\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'index\\', \\'O\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        If the DataFrame index has no label then the recarray field name\\n        is set to \\'index\\'. If the index has a label then this is used as the\\n        field name:\\n\\n        >>> df.index = df.index.rename(\"I\")\\n        >>> df.to_records()\\n        rec.array([(\\'a\\', 1, 0.5 ), (\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'O\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        The index can be excluded from the record array:\\n\\n        >>> df.to_records(index=False)\\n        rec.array([(1, 0.5 ), (2, 0.75)],\\n                  dtype=[(\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        Data types can be specified for the columns:\\n\\n        >>> df.to_records(column_dtypes={\"A\": \"int32\"})\\n        rec.array([(\\'a\\', 1, 0.5 ), (\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'O\\'), (\\'A\\', \\'<i4\\'), (\\'B\\', \\'<f8\\')])\\n\\n        As well as for the index:\\n\\n        >>> df.to_records(index_dtypes=\"<S2\")\\n        rec.array([(b\\'a\\', 1, 0.5 ), (b\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'S2\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        >>> index_dtypes = \"<S{}\".format(df.index.str.len().max())\\n        >>> df.to_records(index_dtypes=index_dtypes)\\n        rec.array([(b\\'a\\', 1, 0.5 ), (b\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'S1\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])',\n",
       "  'code': 'def to_records(self, index=True, convert_datetime64=None,\\n                   column_dtypes=None, index_dtypes=None):\\n        \"\"\"\\n        Convert DataFrame to a NumPy record array.\\n\\n        Index will be included as the first field of the record array if\\n        requested.\\n\\n        Parameters\\n        ----------\\n        index : bool, default True\\n            Include index in resulting record array, stored in \\'index\\'\\n            field or using the index label, if set.\\n        convert_datetime64 : bool, default None\\n            .. deprecated:: 0.23.0\\n\\n            Whether to convert the index to datetime.datetime if it is a\\n            DatetimeIndex.\\n        column_dtypes : str, type, dict, default None\\n            .. versionadded:: 0.24.0\\n\\n            If a string or type, the data type to store all columns. If\\n            a dictionary, a mapping of column names and indices (zero-indexed)\\n            to specific data types.\\n        index_dtypes : str, type, dict, default None\\n            .. versionadded:: 0.24.0\\n\\n            If a string or type, the data type to store all index levels. If\\n            a dictionary, a mapping of index level names and indices\\n            (zero-indexed) to specific data types.\\n\\n            This mapping is applied only if `index=True`.\\n\\n        Returns\\n        -------\\n        numpy.recarray\\n            NumPy ndarray with the DataFrame labels as fields and each row\\n            of the DataFrame as entries.\\n\\n        See Also\\n        --------\\n        DataFrame.from_records: Convert structured or record ndarray\\n            to DataFrame.\\n        numpy.recarray: An ndarray that allows field access using\\n            attributes, analogous to typed columns in a\\n            spreadsheet.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'A\\': [1, 2], \\'B\\': [0.5, 0.75]},\\n        ...                   index=[\\'a\\', \\'b\\'])\\n        >>> df\\n           A     B\\n        a  1  0.50\\n        b  2  0.75\\n        >>> df.to_records()\\n        rec.array([(\\'a\\', 1, 0.5 ), (\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'index\\', \\'O\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        If the DataFrame index has no label then the recarray field name\\n        is set to \\'index\\'. If the index has a label then this is used as the\\n        field name:\\n\\n        >>> df.index = df.index.rename(\"I\")\\n        >>> df.to_records()\\n        rec.array([(\\'a\\', 1, 0.5 ), (\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'O\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        The index can be excluded from the record array:\\n\\n        >>> df.to_records(index=False)\\n        rec.array([(1, 0.5 ), (2, 0.75)],\\n                  dtype=[(\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        Data types can be specified for the columns:\\n\\n        >>> df.to_records(column_dtypes={\"A\": \"int32\"})\\n        rec.array([(\\'a\\', 1, 0.5 ), (\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'O\\'), (\\'A\\', \\'<i4\\'), (\\'B\\', \\'<f8\\')])\\n\\n        As well as for the index:\\n\\n        >>> df.to_records(index_dtypes=\"<S2\")\\n        rec.array([(b\\'a\\', 1, 0.5 ), (b\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'S2\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        >>> index_dtypes = \"<S{}\".format(df.index.str.len().max())\\n        >>> df.to_records(index_dtypes=index_dtypes)\\n        rec.array([(b\\'a\\', 1, 0.5 ), (b\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'S1\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n        \"\"\"\\n\\n        if convert_datetime64 is not None:\\n            warnings.warn(\"The \\'convert_datetime64\\' parameter is \"\\n                          \"deprecated and will be removed in a future \"\\n                          \"version\",\\n                          FutureWarning, stacklevel=2)\\n\\n        if index:\\n            if is_datetime64_any_dtype(self.index) and convert_datetime64:\\n                ix_vals = [self.index.to_pydatetime()]\\n            else:\\n                if isinstance(self.index, MultiIndex):\\n                    # array of tuples to numpy cols. copy copy copy\\n                    ix_vals = lmap(np.array, zip(*self.index.values))\\n                else:\\n                    ix_vals = [self.index.values]\\n\\n            arrays = ix_vals + [self[c].get_values() for c in self.columns]\\n\\n            count = 0\\n            index_names = list(self.index.names)\\n\\n            if isinstance(self.index, MultiIndex):\\n                for i, n in enumerate(index_names):\\n                    if n is None:\\n                        index_names[i] = \\'level_%d\\' % count\\n                        count += 1\\n            elif index_names[0] is None:\\n                index_names = [\\'index\\']\\n\\n            names = lmap(str, index_names) + lmap(str, self.columns)\\n        else:\\n            arrays = [self[c].get_values() for c in self.columns]\\n            names = lmap(str, self.columns)\\n            index_names = []\\n\\n        index_len = len(index_names)\\n        formats = []\\n\\n        for i, v in enumerate(arrays):\\n            index = i\\n\\n            # When the names and arrays are collected, we\\n            # first collect those in the DataFrame\\'s index,\\n            # followed by those in its columns.\\n            #\\n            # Thus, the total length of the array is:\\n            # len(index_names) + len(DataFrame.columns).\\n            #\\n            # This check allows us to see whether we are\\n            # handling a name / array in the index or column.\\n            if index < index_len:\\n                dtype_mapping = index_dtypes\\n                name = index_names[index]\\n            else:\\n                index -= index_len\\n                dtype_mapping = column_dtypes\\n                name = self.columns[index]\\n\\n            # We have a dictionary, so we get the data type\\n            # associated with the index or column (which can\\n            # be denoted by its name in the DataFrame or its\\n            # position in DataFrame\\'s array of indices or\\n            # columns, whichever is applicable.\\n            if is_dict_like(dtype_mapping):\\n                if name in dtype_mapping:\\n                    dtype_mapping = dtype_mapping[name]\\n                elif index in dtype_mapping:\\n                    dtype_mapping = dtype_mapping[index]\\n                else:\\n                    dtype_mapping = None\\n\\n            # If no mapping can be found, use the array\\'s\\n            # dtype attribute for formatting.\\n            #\\n            # A valid dtype must either be a type or\\n            # string naming a type.\\n            if dtype_mapping is None:\\n                formats.append(v.dtype)\\n            elif isinstance(dtype_mapping, (type, np.dtype, str)):\\n                formats.append(dtype_mapping)\\n            else:\\n                element = \"row\" if i < index_len else \"column\"\\n                msg = (\"Invalid dtype {dtype} specified for \"\\n                       \"{element} {name}\").format(dtype=dtype_mapping,\\n                                                  element=element, name=name)\\n                raise ValueError(msg)\\n\\n        return np.rec.fromarrays(\\n            arrays,\\n            dtype={\\'names\\': names, \\'formats\\': formats}\\n        )',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame.memory_usage',\n",
       "  'docstring': \"Return the memory usage of each column in bytes.\\n\\n        The memory usage can optionally include the contribution of\\n        the index and elements of `object` dtype.\\n\\n        This value is displayed in `DataFrame.info` by default. This can be\\n        suppressed by setting ``pandas.options.display.memory_usage`` to False.\\n\\n        Parameters\\n        ----------\\n        index : bool, default True\\n            Specifies whether to include the memory usage of the DataFrame's\\n            index in returned Series. If ``index=True``, the memory usage of\\n            the index is the first item in the output.\\n        deep : bool, default False\\n            If True, introspect the data deeply by interrogating\\n            `object` dtypes for system-level memory consumption, and include\\n            it in the returned values.\\n\\n        Returns\\n        -------\\n        Series\\n            A Series whose index is the original column names and whose values\\n            is the memory usage of each column in bytes.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of an\\n            ndarray.\\n        Series.memory_usage : Bytes consumed by a Series.\\n        Categorical : Memory-efficient array for string values with\\n            many repeated values.\\n        DataFrame.info : Concise summary of a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']\\n        >>> data = dict([(t, np.ones(shape=5000).astype(t))\\n        ...              for t in dtypes])\\n        >>> df = pd.DataFrame(data)\\n        >>> df.head()\\n           int64  float64  complex128  object  bool\\n        0      1      1.0    1.0+0.0j       1  True\\n        1      1      1.0    1.0+0.0j       1  True\\n        2      1      1.0    1.0+0.0j       1  True\\n        3      1      1.0    1.0+0.0j       1  True\\n        4      1      1.0    1.0+0.0j       1  True\\n\\n        >>> df.memory_usage()\\n        Index            80\\n        int64         40000\\n        float64       40000\\n        complex128    80000\\n        object        40000\\n        bool           5000\\n        dtype: int64\\n\\n        >>> df.memory_usage(index=False)\\n        int64         40000\\n        float64       40000\\n        complex128    80000\\n        object        40000\\n        bool           5000\\n        dtype: int64\\n\\n        The memory footprint of `object` dtype columns is ignored by default:\\n\\n        >>> df.memory_usage(deep=True)\\n        Index             80\\n        int64          40000\\n        float64        40000\\n        complex128     80000\\n        object        160000\\n        bool            5000\\n        dtype: int64\\n\\n        Use a Categorical for efficient storage of an object-dtype column with\\n        many repeated values.\\n\\n        >>> df['object'].astype('category').memory_usage(deep=True)\\n        5168\",\n",
       "  'code': 'def memory_usage(self, index=True, deep=False):\\n        \"\"\"\\n        Return the memory usage of each column in bytes.\\n\\n        The memory usage can optionally include the contribution of\\n        the index and elements of `object` dtype.\\n\\n        This value is displayed in `DataFrame.info` by default. This can be\\n        suppressed by setting ``pandas.options.display.memory_usage`` to False.\\n\\n        Parameters\\n        ----------\\n        index : bool, default True\\n            Specifies whether to include the memory usage of the DataFrame\\'s\\n            index in returned Series. If ``index=True``, the memory usage of\\n            the index is the first item in the output.\\n        deep : bool, default False\\n            If True, introspect the data deeply by interrogating\\n            `object` dtypes for system-level memory consumption, and include\\n            it in the returned values.\\n\\n        Returns\\n        -------\\n        Series\\n            A Series whose index is the original column names and whose values\\n            is the memory usage of each column in bytes.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of an\\n            ndarray.\\n        Series.memory_usage : Bytes consumed by a Series.\\n        Categorical : Memory-efficient array for string values with\\n            many repeated values.\\n        DataFrame.info : Concise summary of a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> dtypes = [\\'int64\\', \\'float64\\', \\'complex128\\', \\'object\\', \\'bool\\']\\n        >>> data = dict([(t, np.ones(shape=5000).astype(t))\\n        ...              for t in dtypes])\\n        >>> df = pd.DataFrame(data)\\n        >>> df.head()\\n           int64  float64  complex128  object  bool\\n        0      1      1.0    1.0+0.0j       1  True\\n        1      1      1.0    1.0+0.0j       1  True\\n        2      1      1.0    1.0+0.0j       1  True\\n        3      1      1.0    1.0+0.0j       1  True\\n        4      1      1.0    1.0+0.0j       1  True\\n\\n        >>> df.memory_usage()\\n        Index            80\\n        int64         40000\\n        float64       40000\\n        complex128    80000\\n        object        40000\\n        bool           5000\\n        dtype: int64\\n\\n        >>> df.memory_usage(index=False)\\n        int64         40000\\n        float64       40000\\n        complex128    80000\\n        object        40000\\n        bool           5000\\n        dtype: int64\\n\\n        The memory footprint of `object` dtype columns is ignored by default:\\n\\n        >>> df.memory_usage(deep=True)\\n        Index             80\\n        int64          40000\\n        float64        40000\\n        complex128     80000\\n        object        160000\\n        bool            5000\\n        dtype: int64\\n\\n        Use a Categorical for efficient storage of an object-dtype column with\\n        many repeated values.\\n\\n        >>> df[\\'object\\'].astype(\\'category\\').memory_usage(deep=True)\\n        5168\\n        \"\"\"\\n        result = Series([c.memory_usage(index=False, deep=deep)\\n                         for col, c in self.iteritems()], index=self.columns)\\n        if index:\\n            result = Series(self.index.memory_usage(deep=deep),\\n                            index=[\\'Index\\']).append(result)\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame.transpose',\n",
       "  'docstring': \"Transpose index and columns.\\n\\n        Reflect the DataFrame over its main diagonal by writing rows as columns\\n        and vice-versa. The property :attr:`.T` is an accessor to the method\\n        :meth:`transpose`.\\n\\n        Parameters\\n        ----------\\n        copy : bool, default False\\n            If True, the underlying data is copied. Otherwise (default), no\\n            copy is made if possible.\\n        *args, **kwargs\\n            Additional keywords have no effect but might be accepted for\\n            compatibility with numpy.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            The transposed DataFrame.\\n\\n        See Also\\n        --------\\n        numpy.transpose : Permute the dimensions of a given array.\\n\\n        Notes\\n        -----\\n        Transposing a DataFrame with mixed dtypes will result in a homogeneous\\n        DataFrame with the `object` dtype. In such a case, a copy of the data\\n        is always made.\\n\\n        Examples\\n        --------\\n        **Square DataFrame with homogeneous dtype**\\n\\n        >>> d1 = {'col1': [1, 2], 'col2': [3, 4]}\\n        >>> df1 = pd.DataFrame(data=d1)\\n        >>> df1\\n           col1  col2\\n        0     1     3\\n        1     2     4\\n\\n        >>> df1_transposed = df1.T # or df1.transpose()\\n        >>> df1_transposed\\n              0  1\\n        col1  1  2\\n        col2  3  4\\n\\n        When the dtype is homogeneous in the original DataFrame, we get a\\n        transposed DataFrame with the same dtype:\\n\\n        >>> df1.dtypes\\n        col1    int64\\n        col2    int64\\n        dtype: object\\n        >>> df1_transposed.dtypes\\n        0    int64\\n        1    int64\\n        dtype: object\\n\\n        **Non-square DataFrame with mixed dtypes**\\n\\n        >>> d2 = {'name': ['Alice', 'Bob'],\\n        ...       'score': [9.5, 8],\\n        ...       'employed': [False, True],\\n        ...       'kids': [0, 0]}\\n        >>> df2 = pd.DataFrame(data=d2)\\n        >>> df2\\n            name  score  employed  kids\\n        0  Alice    9.5     False     0\\n        1    Bob    8.0      True     0\\n\\n        >>> df2_transposed = df2.T # or df2.transpose()\\n        >>> df2_transposed\\n                      0     1\\n        name      Alice   Bob\\n        score       9.5     8\\n        employed  False  True\\n        kids          0     0\\n\\n        When the DataFrame has mixed dtypes, we get a transposed DataFrame with\\n        the `object` dtype:\\n\\n        >>> df2.dtypes\\n        name         object\\n        score       float64\\n        employed       bool\\n        kids          int64\\n        dtype: object\\n        >>> df2_transposed.dtypes\\n        0    object\\n        1    object\\n        dtype: object\",\n",
       "  'code': 'def transpose(self, *args, **kwargs):\\n        \"\"\"\\n        Transpose index and columns.\\n\\n        Reflect the DataFrame over its main diagonal by writing rows as columns\\n        and vice-versa. The property :attr:`.T` is an accessor to the method\\n        :meth:`transpose`.\\n\\n        Parameters\\n        ----------\\n        copy : bool, default False\\n            If True, the underlying data is copied. Otherwise (default), no\\n            copy is made if possible.\\n        *args, **kwargs\\n            Additional keywords have no effect but might be accepted for\\n            compatibility with numpy.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            The transposed DataFrame.\\n\\n        See Also\\n        --------\\n        numpy.transpose : Permute the dimensions of a given array.\\n\\n        Notes\\n        -----\\n        Transposing a DataFrame with mixed dtypes will result in a homogeneous\\n        DataFrame with the `object` dtype. In such a case, a copy of the data\\n        is always made.\\n\\n        Examples\\n        --------\\n        **Square DataFrame with homogeneous dtype**\\n\\n        >>> d1 = {\\'col1\\': [1, 2], \\'col2\\': [3, 4]}\\n        >>> df1 = pd.DataFrame(data=d1)\\n        >>> df1\\n           col1  col2\\n        0     1     3\\n        1     2     4\\n\\n        >>> df1_transposed = df1.T # or df1.transpose()\\n        >>> df1_transposed\\n              0  1\\n        col1  1  2\\n        col2  3  4\\n\\n        When the dtype is homogeneous in the original DataFrame, we get a\\n        transposed DataFrame with the same dtype:\\n\\n        >>> df1.dtypes\\n        col1    int64\\n        col2    int64\\n        dtype: object\\n        >>> df1_transposed.dtypes\\n        0    int64\\n        1    int64\\n        dtype: object\\n\\n        **Non-square DataFrame with mixed dtypes**\\n\\n        >>> d2 = {\\'name\\': [\\'Alice\\', \\'Bob\\'],\\n        ...       \\'score\\': [9.5, 8],\\n        ...       \\'employed\\': [False, True],\\n        ...       \\'kids\\': [0, 0]}\\n        >>> df2 = pd.DataFrame(data=d2)\\n        >>> df2\\n            name  score  employed  kids\\n        0  Alice    9.5     False     0\\n        1    Bob    8.0      True     0\\n\\n        >>> df2_transposed = df2.T # or df2.transpose()\\n        >>> df2_transposed\\n                      0     1\\n        name      Alice   Bob\\n        score       9.5     8\\n        employed  False  True\\n        kids          0     0\\n\\n        When the DataFrame has mixed dtypes, we get a transposed DataFrame with\\n        the `object` dtype:\\n\\n        >>> df2.dtypes\\n        name         object\\n        score       float64\\n        employed       bool\\n        kids          int64\\n        dtype: object\\n        >>> df2_transposed.dtypes\\n        0    object\\n        1    object\\n        dtype: object\\n        \"\"\"\\n        nv.validate_transpose(args, dict())\\n        return super().transpose(1, 0, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame._ixs',\n",
       "  'docstring': 'Parameters\\n        ----------\\n        i : int, slice, or sequence of integers\\n        axis : int\\n\\n        Notes\\n        -----\\n        If slice passed, the resulting data will be a view.',\n",
       "  'code': 'def _ixs(self, i, axis=0):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        i : int, slice, or sequence of integers\\n        axis : int\\n\\n        Notes\\n        -----\\n        If slice passed, the resulting data will be a view.\\n        \"\"\"\\n        # irow\\n        if axis == 0:\\n            if isinstance(i, slice):\\n                return self[i]\\n            else:\\n                label = self.index[i]\\n                if isinstance(label, Index):\\n                    # a location index by definition\\n                    result = self.take(i, axis=axis)\\n                    copy = True\\n                else:\\n                    new_values = self._data.fast_xs(i)\\n                    if is_scalar(new_values):\\n                        return new_values\\n\\n                    # if we are a copy, mark as such\\n                    copy = (isinstance(new_values, np.ndarray) and\\n                            new_values.base is None)\\n                    result = self._constructor_sliced(new_values,\\n                                                      index=self.columns,\\n                                                      name=self.index[i],\\n                                                      dtype=new_values.dtype)\\n                result._set_is_copy(self, copy=copy)\\n                return result\\n\\n        # icol\\n        else:\\n            label = self.columns[i]\\n            if isinstance(i, slice):\\n                # need to return view\\n                lab_slice = slice(label[0], label[-1])\\n                return self.loc[:, lab_slice]\\n            else:\\n                if isinstance(label, Index):\\n                    return self._take(i, axis=1)\\n\\n                index_len = len(self.index)\\n\\n                # if the values returned are not the same length\\n                # as the index (iow a not found value), iget returns\\n                # a 0-len ndarray. This is effectively catching\\n                # a numpy error (as numpy should really raise)\\n                values = self._data.iget(i)\\n\\n                if index_len and not len(values):\\n                    values = np.array([np.nan] * index_len, dtype=object)\\n                result = self._box_col_values(values, label)\\n\\n                # this is a cached value, mark it so\\n                result._set_as_cached(label, self)\\n\\n                return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame.select_dtypes',\n",
       "  'docstring': \"Return a subset of the DataFrame's columns based on the column dtypes.\\n\\n        Parameters\\n        ----------\\n        include, exclude : scalar or list-like\\n            A selection of dtypes or strings to be included/excluded. At least\\n            one of these parameters must be supplied.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            The subset of the frame including the dtypes in ``include`` and\\n            excluding the dtypes in ``exclude``.\\n\\n        Raises\\n        ------\\n        ValueError\\n            * If both of ``include`` and ``exclude`` are empty\\n            * If ``include`` and ``exclude`` have overlapping elements\\n            * If any kind of string dtype is passed in.\\n\\n        Notes\\n        -----\\n        * To select all *numeric* types, use ``np.number`` or ``'number'``\\n        * To select strings you must use the ``object`` dtype, but note that\\n          this will return *all* object dtype columns\\n        * See the `numpy dtype hierarchy\\n          <http://docs.scipy.org/doc/numpy/reference/arrays.scalars.html>`__\\n        * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\\n          ``'datetime64'``\\n        * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\\n          ``'timedelta64'``\\n        * To select Pandas categorical dtypes, use ``'category'``\\n        * To select Pandas datetimetz dtypes, use ``'datetimetz'`` (new in\\n          0.20.0) or ``'datetime64[ns, tz]'``\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({'a': [1, 2] * 3,\\n        ...                    'b': [True, False] * 3,\\n        ...                    'c': [1.0, 2.0] * 3})\\n        >>> df\\n                a      b  c\\n        0       1   True  1.0\\n        1       2  False  2.0\\n        2       1   True  1.0\\n        3       2  False  2.0\\n        4       1   True  1.0\\n        5       2  False  2.0\\n\\n        >>> df.select_dtypes(include='bool')\\n           b\\n        0  True\\n        1  False\\n        2  True\\n        3  False\\n        4  True\\n        5  False\\n\\n        >>> df.select_dtypes(include=['float64'])\\n           c\\n        0  1.0\\n        1  2.0\\n        2  1.0\\n        3  2.0\\n        4  1.0\\n        5  2.0\\n\\n        >>> df.select_dtypes(exclude=['int'])\\n               b    c\\n        0   True  1.0\\n        1  False  2.0\\n        2   True  1.0\\n        3  False  2.0\\n        4   True  1.0\\n        5  False  2.0\",\n",
       "  'code': 'def select_dtypes(self, include=None, exclude=None):\\n        \"\"\"\\n        Return a subset of the DataFrame\\'s columns based on the column dtypes.\\n\\n        Parameters\\n        ----------\\n        include, exclude : scalar or list-like\\n            A selection of dtypes or strings to be included/excluded. At least\\n            one of these parameters must be supplied.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            The subset of the frame including the dtypes in ``include`` and\\n            excluding the dtypes in ``exclude``.\\n\\n        Raises\\n        ------\\n        ValueError\\n            * If both of ``include`` and ``exclude`` are empty\\n            * If ``include`` and ``exclude`` have overlapping elements\\n            * If any kind of string dtype is passed in.\\n\\n        Notes\\n        -----\\n        * To select all *numeric* types, use ``np.number`` or ``\\'number\\'``\\n        * To select strings you must use the ``object`` dtype, but note that\\n          this will return *all* object dtype columns\\n        * See the `numpy dtype hierarchy\\n          <http://docs.scipy.org/doc/numpy/reference/arrays.scalars.html>`__\\n        * To select datetimes, use ``np.datetime64``, ``\\'datetime\\'`` or\\n          ``\\'datetime64\\'``\\n        * To select timedeltas, use ``np.timedelta64``, ``\\'timedelta\\'`` or\\n          ``\\'timedelta64\\'``\\n        * To select Pandas categorical dtypes, use ``\\'category\\'``\\n        * To select Pandas datetimetz dtypes, use ``\\'datetimetz\\'`` (new in\\n          0.20.0) or ``\\'datetime64[ns, tz]\\'``\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'a\\': [1, 2] * 3,\\n        ...                    \\'b\\': [True, False] * 3,\\n        ...                    \\'c\\': [1.0, 2.0] * 3})\\n        >>> df\\n                a      b  c\\n        0       1   True  1.0\\n        1       2  False  2.0\\n        2       1   True  1.0\\n        3       2  False  2.0\\n        4       1   True  1.0\\n        5       2  False  2.0\\n\\n        >>> df.select_dtypes(include=\\'bool\\')\\n           b\\n        0  True\\n        1  False\\n        2  True\\n        3  False\\n        4  True\\n        5  False\\n\\n        >>> df.select_dtypes(include=[\\'float64\\'])\\n           c\\n        0  1.0\\n        1  2.0\\n        2  1.0\\n        3  2.0\\n        4  1.0\\n        5  2.0\\n\\n        >>> df.select_dtypes(exclude=[\\'int\\'])\\n               b    c\\n        0   True  1.0\\n        1  False  2.0\\n        2   True  1.0\\n        3  False  2.0\\n        4   True  1.0\\n        5  False  2.0\\n        \"\"\"\\n        def _get_info_slice(obj, indexer):\\n            \"\"\"Slice the info axis of `obj` with `indexer`.\"\"\"\\n            if not hasattr(obj, \\'_info_axis_number\\'):\\n                msg = \\'object of type {typ!r} has no info axis\\'\\n                raise TypeError(msg.format(typ=type(obj).__name__))\\n            slices = [slice(None)] * obj.ndim\\n            slices[obj._info_axis_number] = indexer\\n            return tuple(slices)\\n\\n        if not is_list_like(include):\\n            include = (include,) if include is not None else ()\\n        if not is_list_like(exclude):\\n            exclude = (exclude,) if exclude is not None else ()\\n\\n        selection = tuple(map(frozenset, (include, exclude)))\\n\\n        if not any(selection):\\n            raise ValueError(\\'at least one of include or exclude must be \\'\\n                             \\'nonempty\\')\\n\\n        # convert the myriad valid dtypes object to a single representation\\n        include, exclude = map(\\n            lambda x: frozenset(map(infer_dtype_from_object, x)), selection)\\n        for dtypes in (include, exclude):\\n            invalidate_string_dtypes(dtypes)\\n\\n        # can\\'t both include AND exclude!\\n        if not include.isdisjoint(exclude):\\n            raise ValueError(\\'include and exclude overlap on {inc_ex}\\'.format(\\n                inc_ex=(include & exclude)))\\n\\n        # empty include/exclude -> defaults to True\\n        # three cases (we\\'ve already raised if both are empty)\\n        # case 1: empty include, nonempty exclude\\n        # we have True, True, ... True for include, same for exclude\\n        # in the loop below we get the excluded\\n        # and when we call \\'&\\' below we get only the excluded\\n        # case 2: nonempty include, empty exclude\\n        # same as case 1, but with include\\n        # case 3: both nonempty\\n        # the \"union\" of the logic of case 1 and case 2:\\n        # we get the included and excluded, and return their logical and\\n        include_these = Series(not bool(include), index=self.columns)\\n        exclude_these = Series(not bool(exclude), index=self.columns)\\n\\n        def is_dtype_instance_mapper(idx, dtype):\\n            return idx, functools.partial(issubclass, dtype.type)\\n\\n        for idx, f in itertools.starmap(is_dtype_instance_mapper,\\n                                        enumerate(self.dtypes)):\\n            if include:  # checks for the case of empty include or exclude\\n                include_these.iloc[idx] = any(map(f, include))\\n            if exclude:\\n                exclude_these.iloc[idx] = not any(map(f, exclude))\\n\\n        dtype_indexer = include_these & exclude_these\\n        return self.loc[_get_info_slice(self, dtype_indexer)]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame._set_item',\n",
       "  'docstring': 'Add series to DataFrame in specified column.\\n\\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\\n        same length as the DataFrames index or an error will be thrown.\\n\\n        Series/TimeSeries will be conformed to the DataFrames index to\\n        ensure homogeneity.',\n",
       "  'code': 'def _set_item(self, key, value):\\n        \"\"\"\\n        Add series to DataFrame in specified column.\\n\\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\\n        same length as the DataFrames index or an error will be thrown.\\n\\n        Series/TimeSeries will be conformed to the DataFrames index to\\n        ensure homogeneity.\\n        \"\"\"\\n\\n        self._ensure_valid_index(value)\\n        value = self._sanitize_column(key, value)\\n        NDFrame._set_item(self, key, value)\\n\\n        # check if we are modifying a copy\\n        # try to set first as we want an invalid\\n        # value exception to occur first\\n        if len(self):\\n            self._check_setitem_copy()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame._sanitize_column',\n",
       "  'docstring': 'Ensures new columns (which go into the BlockManager as new blocks) are\\n        always copied and converted into an array.\\n\\n        Parameters\\n        ----------\\n        key : object\\n        value : scalar, Series, or array-like\\n        broadcast : bool, default True\\n            If ``key`` matches multiple duplicate column names in the\\n            DataFrame, this parameter indicates whether ``value`` should be\\n            tiled so that the returned array contains a (duplicated) column for\\n            each occurrence of the key. If False, ``value`` will not be tiled.\\n\\n        Returns\\n        -------\\n        numpy.ndarray',\n",
       "  'code': 'def _sanitize_column(self, key, value, broadcast=True):\\n        \"\"\"\\n        Ensures new columns (which go into the BlockManager as new blocks) are\\n        always copied and converted into an array.\\n\\n        Parameters\\n        ----------\\n        key : object\\n        value : scalar, Series, or array-like\\n        broadcast : bool, default True\\n            If ``key`` matches multiple duplicate column names in the\\n            DataFrame, this parameter indicates whether ``value`` should be\\n            tiled so that the returned array contains a (duplicated) column for\\n            each occurrence of the key. If False, ``value`` will not be tiled.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n        \"\"\"\\n\\n        def reindexer(value):\\n            # reindex if necessary\\n\\n            if value.index.equals(self.index) or not len(self.index):\\n                value = value._values.copy()\\n            else:\\n\\n                # GH 4107\\n                try:\\n                    value = value.reindex(self.index)._values\\n                except Exception as e:\\n\\n                    # duplicate axis\\n                    if not value.index.is_unique:\\n                        raise e\\n\\n                    # other\\n                    raise TypeError(\\'incompatible index of inserted column \\'\\n                                    \\'with frame index\\')\\n            return value\\n\\n        if isinstance(value, Series):\\n            value = reindexer(value)\\n\\n        elif isinstance(value, DataFrame):\\n            # align right-hand-side columns if self.columns\\n            # is multi-index and self[key] is a sub-frame\\n            if isinstance(self.columns, MultiIndex) and key in self.columns:\\n                loc = self.columns.get_loc(key)\\n                if isinstance(loc, (slice, Series, np.ndarray, Index)):\\n                    cols = maybe_droplevels(self.columns[loc], key)\\n                    if len(cols) and not cols.equals(value.columns):\\n                        value = value.reindex(cols, axis=1)\\n            # now align rows\\n            value = reindexer(value).T\\n\\n        elif isinstance(value, ExtensionArray):\\n            # Explicitly copy here, instead of in sanitize_index,\\n            # as sanitize_index won\\'t copy an EA, even with copy=True\\n            value = value.copy()\\n            value = sanitize_index(value, self.index, copy=False)\\n\\n        elif isinstance(value, Index) or is_sequence(value):\\n\\n            # turn me into an ndarray\\n            value = sanitize_index(value, self.index, copy=False)\\n            if not isinstance(value, (np.ndarray, Index)):\\n                if isinstance(value, list) and len(value) > 0:\\n                    value = maybe_convert_platform(value)\\n                else:\\n                    value = com.asarray_tuplesafe(value)\\n            elif value.ndim == 2:\\n                value = value.copy().T\\n            elif isinstance(value, Index):\\n                value = value.copy(deep=True)\\n            else:\\n                value = value.copy()\\n\\n            # possibly infer to datetimelike\\n            if is_object_dtype(value.dtype):\\n                value = maybe_infer_to_datetimelike(value)\\n\\n        else:\\n            # cast ignores pandas dtypes. so save the dtype first\\n            infer_dtype, _ = infer_dtype_from_scalar(\\n                value, pandas_dtype=True)\\n\\n            # upcast\\n            value = cast_scalar_to_array(len(self.index), value)\\n            value = maybe_cast_to_datetime(value, infer_dtype)\\n\\n        # return internal types directly\\n        if is_extension_type(value) or is_extension_array_dtype(value):\\n            return value\\n\\n        # broadcast across multiple columns if necessary\\n        if broadcast and key in self.columns and value.ndim == 1:\\n            if (not self.columns.is_unique or\\n                    isinstance(self.columns, MultiIndex)):\\n                existing_piece = self[key]\\n                if isinstance(existing_piece, DataFrame):\\n                    value = np.tile(value, (len(existing_piece.columns), 1))\\n\\n        return np.atleast_2d(np.asarray(value))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame.apply',\n",
       "  'docstring': \"Apply a function along an axis of the DataFrame.\\n\\n        Objects passed to the function are Series objects whose index is\\n        either the DataFrame's index (``axis=0``) or the DataFrame's columns\\n        (``axis=1``). By default (``result_type=None``), the final return type\\n        is inferred from the return type of the applied function. Otherwise,\\n        it depends on the `result_type` argument.\\n\\n        Parameters\\n        ----------\\n        func : function\\n            Function to apply to each column or row.\\n        axis : {0 or 'index', 1 or 'columns'}, default 0\\n            Axis along which the function is applied:\\n\\n            * 0 or 'index': apply function to each column.\\n            * 1 or 'columns': apply function to each row.\\n        broadcast : bool, optional\\n            Only relevant for aggregation functions:\\n\\n            * ``False`` or ``None`` : returns a Series whose length is the\\n              length of the index or the number of columns (based on the\\n              `axis` parameter)\\n            * ``True`` : results will be broadcast to the original shape\\n              of the frame, the original index and columns will be retained.\\n\\n            .. deprecated:: 0.23.0\\n               This argument will be removed in a future version, replaced\\n               by result_type='broadcast'.\\n\\n        raw : bool, default False\\n            * ``False`` : passes each row or column as a Series to the\\n              function.\\n            * ``True`` : the passed function will receive ndarray objects\\n              instead.\\n              If you are just applying a NumPy reduction function this will\\n              achieve much better performance.\\n        reduce : bool or None, default None\\n            Try to apply reduction procedures. If the DataFrame is empty,\\n            `apply` will use `reduce` to determine whether the result\\n            should be a Series or a DataFrame. If ``reduce=None`` (the\\n            default), `apply`'s return value will be guessed by calling\\n            `func` on an empty Series\\n            (note: while guessing, exceptions raised by `func` will be\\n            ignored).\\n            If ``reduce=True`` a Series will always be returned, and if\\n            ``reduce=False`` a DataFrame will always be returned.\\n\\n            .. deprecated:: 0.23.0\\n               This argument will be removed in a future version, replaced\\n               by ``result_type='reduce'``.\\n\\n        result_type : {'expand', 'reduce', 'broadcast', None}, default None\\n            These only act when ``axis=1`` (columns):\\n\\n            * 'expand' : list-like results will be turned into columns.\\n            * 'reduce' : returns a Series if possible rather than expanding\\n              list-like results. This is the opposite of 'expand'.\\n            * 'broadcast' : results will be broadcast to the original shape\\n              of the DataFrame, the original index and columns will be\\n              retained.\\n\\n            The default behaviour (None) depends on the return value of the\\n            applied function: list-like results will be returned as a Series\\n            of those. However if the apply function returns a Series these\\n            are expanded to columns.\\n\\n            .. versionadded:: 0.23.0\\n\\n        args : tuple\\n            Positional arguments to pass to `func` in addition to the\\n            array/series.\\n        **kwds\\n            Additional keyword arguments to pass as keywords arguments to\\n            `func`.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Result of applying ``func`` along the given axis of the\\n            DataFrame.\\n\\n        See Also\\n        --------\\n        DataFrame.applymap: For elementwise operations.\\n        DataFrame.aggregate: Only perform aggregating type operations.\\n        DataFrame.transform: Only perform transforming type operations.\\n\\n        Notes\\n        -----\\n        In the current implementation apply calls `func` twice on the\\n        first column/row to decide whether it can take a fast or slow\\n        code path. This can lead to unexpected behavior if `func` has\\n        side-effects, as they will take effect twice for the first\\n        column/row.\\n\\n        Examples\\n        --------\\n\\n        >>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\\n        >>> df\\n           A  B\\n        0  4  9\\n        1  4  9\\n        2  4  9\\n\\n        Using a numpy universal function (in this case the same as\\n        ``np.sqrt(df)``):\\n\\n        >>> df.apply(np.sqrt)\\n             A    B\\n        0  2.0  3.0\\n        1  2.0  3.0\\n        2  2.0  3.0\\n\\n        Using a reducing function on either axis\\n\\n        >>> df.apply(np.sum, axis=0)\\n        A    12\\n        B    27\\n        dtype: int64\\n\\n        >>> df.apply(np.sum, axis=1)\\n        0    13\\n        1    13\\n        2    13\\n        dtype: int64\\n\\n        Retuning a list-like will result in a Series\\n\\n        >>> df.apply(lambda x: [1, 2], axis=1)\\n        0    [1, 2]\\n        1    [1, 2]\\n        2    [1, 2]\\n        dtype: object\\n\\n        Passing result_type='expand' will expand list-like results\\n        to columns of a Dataframe\\n\\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='expand')\\n           0  1\\n        0  1  2\\n        1  1  2\\n        2  1  2\\n\\n        Returning a Series inside the function is similar to passing\\n        ``result_type='expand'``. The resulting column names\\n        will be the Series index.\\n\\n        >>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)\\n           foo  bar\\n        0    1    2\\n        1    1    2\\n        2    1    2\\n\\n        Passing ``result_type='broadcast'`` will ensure the same shape\\n        result, whether list-like or scalar is returned by the function,\\n        and broadcast it along the axis. The resulting column names will\\n        be the originals.\\n\\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')\\n           A  B\\n        0  1  2\\n        1  1  2\\n        2  1  2\",\n",
       "  'code': 'def apply(self, func, axis=0, broadcast=None, raw=False, reduce=None,\\n              result_type=None, args=(), **kwds):\\n        \"\"\"\\n        Apply a function along an axis of the DataFrame.\\n\\n        Objects passed to the function are Series objects whose index is\\n        either the DataFrame\\'s index (``axis=0``) or the DataFrame\\'s columns\\n        (``axis=1``). By default (``result_type=None``), the final return type\\n        is inferred from the return type of the applied function. Otherwise,\\n        it depends on the `result_type` argument.\\n\\n        Parameters\\n        ----------\\n        func : function\\n            Function to apply to each column or row.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            Axis along which the function is applied:\\n\\n            * 0 or \\'index\\': apply function to each column.\\n            * 1 or \\'columns\\': apply function to each row.\\n        broadcast : bool, optional\\n            Only relevant for aggregation functions:\\n\\n            * ``False`` or ``None`` : returns a Series whose length is the\\n              length of the index or the number of columns (based on the\\n              `axis` parameter)\\n            * ``True`` : results will be broadcast to the original shape\\n              of the frame, the original index and columns will be retained.\\n\\n            .. deprecated:: 0.23.0\\n               This argument will be removed in a future version, replaced\\n               by result_type=\\'broadcast\\'.\\n\\n        raw : bool, default False\\n            * ``False`` : passes each row or column as a Series to the\\n              function.\\n            * ``True`` : the passed function will receive ndarray objects\\n              instead.\\n              If you are just applying a NumPy reduction function this will\\n              achieve much better performance.\\n        reduce : bool or None, default None\\n            Try to apply reduction procedures. If the DataFrame is empty,\\n            `apply` will use `reduce` to determine whether the result\\n            should be a Series or a DataFrame. If ``reduce=None`` (the\\n            default), `apply`\\'s return value will be guessed by calling\\n            `func` on an empty Series\\n            (note: while guessing, exceptions raised by `func` will be\\n            ignored).\\n            If ``reduce=True`` a Series will always be returned, and if\\n            ``reduce=False`` a DataFrame will always be returned.\\n\\n            .. deprecated:: 0.23.0\\n               This argument will be removed in a future version, replaced\\n               by ``result_type=\\'reduce\\'``.\\n\\n        result_type : {\\'expand\\', \\'reduce\\', \\'broadcast\\', None}, default None\\n            These only act when ``axis=1`` (columns):\\n\\n            * \\'expand\\' : list-like results will be turned into columns.\\n            * \\'reduce\\' : returns a Series if possible rather than expanding\\n              list-like results. This is the opposite of \\'expand\\'.\\n            * \\'broadcast\\' : results will be broadcast to the original shape\\n              of the DataFrame, the original index and columns will be\\n              retained.\\n\\n            The default behaviour (None) depends on the return value of the\\n            applied function: list-like results will be returned as a Series\\n            of those. However if the apply function returns a Series these\\n            are expanded to columns.\\n\\n            .. versionadded:: 0.23.0\\n\\n        args : tuple\\n            Positional arguments to pass to `func` in addition to the\\n            array/series.\\n        **kwds\\n            Additional keyword arguments to pass as keywords arguments to\\n            `func`.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Result of applying ``func`` along the given axis of the\\n            DataFrame.\\n\\n        See Also\\n        --------\\n        DataFrame.applymap: For elementwise operations.\\n        DataFrame.aggregate: Only perform aggregating type operations.\\n        DataFrame.transform: Only perform transforming type operations.\\n\\n        Notes\\n        -----\\n        In the current implementation apply calls `func` twice on the\\n        first column/row to decide whether it can take a fast or slow\\n        code path. This can lead to unexpected behavior if `func` has\\n        side-effects, as they will take effect twice for the first\\n        column/row.\\n\\n        Examples\\n        --------\\n\\n        >>> df = pd.DataFrame([[4, 9]] * 3, columns=[\\'A\\', \\'B\\'])\\n        >>> df\\n           A  B\\n        0  4  9\\n        1  4  9\\n        2  4  9\\n\\n        Using a numpy universal function (in this case the same as\\n        ``np.sqrt(df)``):\\n\\n        >>> df.apply(np.sqrt)\\n             A    B\\n        0  2.0  3.0\\n        1  2.0  3.0\\n        2  2.0  3.0\\n\\n        Using a reducing function on either axis\\n\\n        >>> df.apply(np.sum, axis=0)\\n        A    12\\n        B    27\\n        dtype: int64\\n\\n        >>> df.apply(np.sum, axis=1)\\n        0    13\\n        1    13\\n        2    13\\n        dtype: int64\\n\\n        Retuning a list-like will result in a Series\\n\\n        >>> df.apply(lambda x: [1, 2], axis=1)\\n        0    [1, 2]\\n        1    [1, 2]\\n        2    [1, 2]\\n        dtype: object\\n\\n        Passing result_type=\\'expand\\' will expand list-like results\\n        to columns of a Dataframe\\n\\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type=\\'expand\\')\\n           0  1\\n        0  1  2\\n        1  1  2\\n        2  1  2\\n\\n        Returning a Series inside the function is similar to passing\\n        ``result_type=\\'expand\\'``. The resulting column names\\n        will be the Series index.\\n\\n        >>> df.apply(lambda x: pd.Series([1, 2], index=[\\'foo\\', \\'bar\\']), axis=1)\\n           foo  bar\\n        0    1    2\\n        1    1    2\\n        2    1    2\\n\\n        Passing ``result_type=\\'broadcast\\'`` will ensure the same shape\\n        result, whether list-like or scalar is returned by the function,\\n        and broadcast it along the axis. The resulting column names will\\n        be the originals.\\n\\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type=\\'broadcast\\')\\n           A  B\\n        0  1  2\\n        1  1  2\\n        2  1  2\\n        \"\"\"\\n        from pandas.core.apply import frame_apply\\n        op = frame_apply(self,\\n                         func=func,\\n                         axis=axis,\\n                         broadcast=broadcast,\\n                         raw=raw,\\n                         reduce=reduce,\\n                         result_type=result_type,\\n                         args=args,\\n                         kwds=kwds)\\n        return op.get_result()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame.round',\n",
       "  'docstring': \"Round a DataFrame to a variable number of decimal places.\\n\\n        Parameters\\n        ----------\\n        decimals : int, dict, Series\\n            Number of decimal places to round each column to. If an int is\\n            given, round each column to the same number of places.\\n            Otherwise dict and Series round to variable numbers of places.\\n            Column names should be in the keys if `decimals` is a\\n            dict-like, or in the index if `decimals` is a Series. Any\\n            columns not included in `decimals` will be left as is. Elements\\n            of `decimals` which are not columns of the input will be\\n            ignored.\\n        *args\\n            Additional keywords have no effect but might be accepted for\\n            compatibility with numpy.\\n        **kwargs\\n            Additional keywords have no effect but might be accepted for\\n            compatibility with numpy.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            A DataFrame with the affected columns rounded to the specified\\n            number of decimal places.\\n\\n        See Also\\n        --------\\n        numpy.around : Round a numpy array to the given number of decimals.\\n        Series.round : Round a Series to the given number of decimals.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],\\n        ...                   columns=['dogs', 'cats'])\\n        >>> df\\n            dogs  cats\\n        0  0.21  0.32\\n        1  0.01  0.67\\n        2  0.66  0.03\\n        3  0.21  0.18\\n\\n        By providing an integer each column is rounded to the same number\\n        of decimal places\\n\\n        >>> df.round(1)\\n            dogs  cats\\n        0   0.2   0.3\\n        1   0.0   0.7\\n        2   0.7   0.0\\n        3   0.2   0.2\\n\\n        With a dict, the number of places for specific columns can be\\n        specfified with the column names as key and the number of decimal\\n        places as value\\n\\n        >>> df.round({'dogs': 1, 'cats': 0})\\n            dogs  cats\\n        0   0.2   0.0\\n        1   0.0   1.0\\n        2   0.7   0.0\\n        3   0.2   0.0\\n\\n        Using a Series, the number of places for specific columns can be\\n        specfified with the column names as index and the number of\\n        decimal places as value\\n\\n        >>> decimals = pd.Series([0, 1], index=['cats', 'dogs'])\\n        >>> df.round(decimals)\\n            dogs  cats\\n        0   0.2   0.0\\n        1   0.0   1.0\\n        2   0.7   0.0\\n        3   0.2   0.0\",\n",
       "  'code': 'def round(self, decimals=0, *args, **kwargs):\\n        \"\"\"\\n        Round a DataFrame to a variable number of decimal places.\\n\\n        Parameters\\n        ----------\\n        decimals : int, dict, Series\\n            Number of decimal places to round each column to. If an int is\\n            given, round each column to the same number of places.\\n            Otherwise dict and Series round to variable numbers of places.\\n            Column names should be in the keys if `decimals` is a\\n            dict-like, or in the index if `decimals` is a Series. Any\\n            columns not included in `decimals` will be left as is. Elements\\n            of `decimals` which are not columns of the input will be\\n            ignored.\\n        *args\\n            Additional keywords have no effect but might be accepted for\\n            compatibility with numpy.\\n        **kwargs\\n            Additional keywords have no effect but might be accepted for\\n            compatibility with numpy.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            A DataFrame with the affected columns rounded to the specified\\n            number of decimal places.\\n\\n        See Also\\n        --------\\n        numpy.around : Round a numpy array to the given number of decimals.\\n        Series.round : Round a Series to the given number of decimals.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],\\n        ...                   columns=[\\'dogs\\', \\'cats\\'])\\n        >>> df\\n            dogs  cats\\n        0  0.21  0.32\\n        1  0.01  0.67\\n        2  0.66  0.03\\n        3  0.21  0.18\\n\\n        By providing an integer each column is rounded to the same number\\n        of decimal places\\n\\n        >>> df.round(1)\\n            dogs  cats\\n        0   0.2   0.3\\n        1   0.0   0.7\\n        2   0.7   0.0\\n        3   0.2   0.2\\n\\n        With a dict, the number of places for specific columns can be\\n        specfified with the column names as key and the number of decimal\\n        places as value\\n\\n        >>> df.round({\\'dogs\\': 1, \\'cats\\': 0})\\n            dogs  cats\\n        0   0.2   0.0\\n        1   0.0   1.0\\n        2   0.7   0.0\\n        3   0.2   0.0\\n\\n        Using a Series, the number of places for specific columns can be\\n        specfified with the column names as index and the number of\\n        decimal places as value\\n\\n        >>> decimals = pd.Series([0, 1], index=[\\'cats\\', \\'dogs\\'])\\n        >>> df.round(decimals)\\n            dogs  cats\\n        0   0.2   0.0\\n        1   0.0   1.0\\n        2   0.7   0.0\\n        3   0.2   0.0\\n        \"\"\"\\n        from pandas.core.reshape.concat import concat\\n\\n        def _dict_round(df, decimals):\\n            for col, vals in df.iteritems():\\n                try:\\n                    yield _series_round(vals, decimals[col])\\n                except KeyError:\\n                    yield vals\\n\\n        def _series_round(s, decimals):\\n            if is_integer_dtype(s) or is_float_dtype(s):\\n                return s.round(decimals)\\n            return s\\n\\n        nv.validate_round(args, kwargs)\\n\\n        if isinstance(decimals, (dict, Series)):\\n            if isinstance(decimals, Series):\\n                if not decimals.index.is_unique:\\n                    raise ValueError(\"Index of decimals must be unique\")\\n            new_cols = [col for col in _dict_round(self, decimals)]\\n        elif is_integer(decimals):\\n            # Dispatch to Series.round\\n            new_cols = [_series_round(v, decimals)\\n                        for _, v in self.iteritems()]\\n        else:\\n            raise TypeError(\"decimals must be an integer, a dict-like or a \"\\n                            \"Series\")\\n\\n        if len(new_cols) > 0:\\n            return self._constructor(concat(new_cols, axis=1),\\n                                     index=self.index,\\n                                     columns=self.columns)\\n        else:\\n            return self',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame.count',\n",
       "  'docstring': 'Count non-NA cells for each column or row.\\n\\n        The values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending\\n        on `pandas.options.mode.use_inf_as_na`) are considered NA.\\n\\n        Parameters\\n        ----------\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            If 0 or \\'index\\' counts are generated for each column.\\n            If 1 or \\'columns\\' counts are generated for each **row**.\\n        level : int or str, optional\\n            If the axis is a `MultiIndex` (hierarchical), count along a\\n            particular `level`, collapsing into a `DataFrame`.\\n            A `str` specifies the level name.\\n        numeric_only : bool, default False\\n            Include only `float`, `int` or `boolean` data.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            For each column/row the number of non-NA/null entries.\\n            If `level` is specified returns a `DataFrame`.\\n\\n        See Also\\n        --------\\n        Series.count: Number of non-NA elements in a Series.\\n        DataFrame.shape: Number of DataFrame rows and columns (including NA\\n            elements).\\n        DataFrame.isna: Boolean same-sized DataFrame showing places of NA\\n            elements.\\n\\n        Examples\\n        --------\\n        Constructing DataFrame from a dictionary:\\n\\n        >>> df = pd.DataFrame({\"Person\":\\n        ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\\n        ...                    \"Age\": [24., np.nan, 21., 33, 26],\\n        ...                    \"Single\": [False, True, True, True, False]})\\n        >>> df\\n           Person   Age  Single\\n        0    John  24.0   False\\n        1    Myla   NaN    True\\n        2   Lewis  21.0    True\\n        3    John  33.0    True\\n        4    Myla  26.0   False\\n\\n        Notice the uncounted NA values:\\n\\n        >>> df.count()\\n        Person    5\\n        Age       4\\n        Single    5\\n        dtype: int64\\n\\n        Counts for each **row**:\\n\\n        >>> df.count(axis=\\'columns\\')\\n        0    3\\n        1    2\\n        2    3\\n        3    3\\n        4    3\\n        dtype: int64\\n\\n        Counts for one level of a `MultiIndex`:\\n\\n        >>> df.set_index([\"Person\", \"Single\"]).count(level=\"Person\")\\n                Age\\n        Person\\n        John      2\\n        Lewis     1\\n        Myla      1',\n",
       "  'code': 'def count(self, axis=0, level=None, numeric_only=False):\\n        \"\"\"\\n        Count non-NA cells for each column or row.\\n\\n        The values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending\\n        on `pandas.options.mode.use_inf_as_na`) are considered NA.\\n\\n        Parameters\\n        ----------\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            If 0 or \\'index\\' counts are generated for each column.\\n            If 1 or \\'columns\\' counts are generated for each **row**.\\n        level : int or str, optional\\n            If the axis is a `MultiIndex` (hierarchical), count along a\\n            particular `level`, collapsing into a `DataFrame`.\\n            A `str` specifies the level name.\\n        numeric_only : bool, default False\\n            Include only `float`, `int` or `boolean` data.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            For each column/row the number of non-NA/null entries.\\n            If `level` is specified returns a `DataFrame`.\\n\\n        See Also\\n        --------\\n        Series.count: Number of non-NA elements in a Series.\\n        DataFrame.shape: Number of DataFrame rows and columns (including NA\\n            elements).\\n        DataFrame.isna: Boolean same-sized DataFrame showing places of NA\\n            elements.\\n\\n        Examples\\n        --------\\n        Constructing DataFrame from a dictionary:\\n\\n        >>> df = pd.DataFrame({\"Person\":\\n        ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\\n        ...                    \"Age\": [24., np.nan, 21., 33, 26],\\n        ...                    \"Single\": [False, True, True, True, False]})\\n        >>> df\\n           Person   Age  Single\\n        0    John  24.0   False\\n        1    Myla   NaN    True\\n        2   Lewis  21.0    True\\n        3    John  33.0    True\\n        4    Myla  26.0   False\\n\\n        Notice the uncounted NA values:\\n\\n        >>> df.count()\\n        Person    5\\n        Age       4\\n        Single    5\\n        dtype: int64\\n\\n        Counts for each **row**:\\n\\n        >>> df.count(axis=\\'columns\\')\\n        0    3\\n        1    2\\n        2    3\\n        3    3\\n        4    3\\n        dtype: int64\\n\\n        Counts for one level of a `MultiIndex`:\\n\\n        >>> df.set_index([\"Person\", \"Single\"]).count(level=\"Person\")\\n                Age\\n        Person\\n        John      2\\n        Lewis     1\\n        Myla      1\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n        if level is not None:\\n            return self._count_level(level, axis=axis,\\n                                     numeric_only=numeric_only)\\n\\n        if numeric_only:\\n            frame = self._get_numeric_data()\\n        else:\\n            frame = self\\n\\n        # GH #423\\n        if len(frame._get_axis(axis)) == 0:\\n            result = Series(0, index=frame._get_agg_axis(axis))\\n        else:\\n            if frame._is_mixed_type or frame._data.any_extension_types:\\n                # the or any_extension_types is really only hit for single-\\n                # column frames with an extension array\\n                result = notna(frame).sum(axis=axis)\\n            else:\\n                # GH13407\\n                series_counts = notna(frame).sum(axis=axis)\\n                counts = series_counts.values\\n                result = Series(counts, index=frame._get_agg_axis(axis))\\n\\n        return result.astype(\\'int64\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DataFrame.quantile',\n",
       "  'docstring': \"Return values at the given quantile over requested axis.\\n\\n        Parameters\\n        ----------\\n        q : float or array-like, default 0.5 (50% quantile)\\n            Value between 0 <= q <= 1, the quantile(s) to compute.\\n        axis : {0, 1, 'index', 'columns'} (default 0)\\n            Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\\n        numeric_only : bool, default True\\n            If False, the quantile of datetime and timedelta data will be\\n            computed as well.\\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\\n            This optional parameter specifies the interpolation method to use,\\n            when the desired quantile lies between two data points `i` and `j`:\\n\\n            * linear: `i + (j - i) * fraction`, where `fraction` is the\\n              fractional part of the index surrounded by `i` and `j`.\\n            * lower: `i`.\\n            * higher: `j`.\\n            * nearest: `i` or `j` whichever is nearest.\\n            * midpoint: (`i` + `j`) / 2.\\n\\n            .. versionadded:: 0.18.0\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n\\n            If ``q`` is an array, a DataFrame will be returned where the\\n              index is ``q``, the columns are the columns of self, and the\\n              values are the quantiles.\\n            If ``q`` is a float, a Series will be returned where the\\n              index is the columns of self and the values are the quantiles.\\n\\n        See Also\\n        --------\\n        core.window.Rolling.quantile: Rolling quantile.\\n        numpy.percentile: Numpy function to compute the percentile.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),\\n        ...                   columns=['a', 'b'])\\n        >>> df.quantile(.1)\\n        a    1.3\\n        b    3.7\\n        Name: 0.1, dtype: float64\\n        >>> df.quantile([.1, .5])\\n               a     b\\n        0.1  1.3   3.7\\n        0.5  2.5  55.0\\n\\n        Specifying `numeric_only=False` will also compute the quantile of\\n        datetime and timedelta data.\\n\\n        >>> df = pd.DataFrame({'A': [1, 2],\\n        ...                    'B': [pd.Timestamp('2010'),\\n        ...                          pd.Timestamp('2011')],\\n        ...                    'C': [pd.Timedelta('1 days'),\\n        ...                          pd.Timedelta('2 days')]})\\n        >>> df.quantile(0.5, numeric_only=False)\\n        A                    1.5\\n        B    2010-07-02 12:00:00\\n        C        1 days 12:00:00\\n        Name: 0.5, dtype: object\",\n",
       "  'code': 'def quantile(self, q=0.5, axis=0, numeric_only=True,\\n                 interpolation=\\'linear\\'):\\n        \"\"\"\\n        Return values at the given quantile over requested axis.\\n\\n        Parameters\\n        ----------\\n        q : float or array-like, default 0.5 (50% quantile)\\n            Value between 0 <= q <= 1, the quantile(s) to compute.\\n        axis : {0, 1, \\'index\\', \\'columns\\'} (default 0)\\n            Equals 0 or \\'index\\' for row-wise, 1 or \\'columns\\' for column-wise.\\n        numeric_only : bool, default True\\n            If False, the quantile of datetime and timedelta data will be\\n            computed as well.\\n        interpolation : {\\'linear\\', \\'lower\\', \\'higher\\', \\'midpoint\\', \\'nearest\\'}\\n            This optional parameter specifies the interpolation method to use,\\n            when the desired quantile lies between two data points `i` and `j`:\\n\\n            * linear: `i + (j - i) * fraction`, where `fraction` is the\\n              fractional part of the index surrounded by `i` and `j`.\\n            * lower: `i`.\\n            * higher: `j`.\\n            * nearest: `i` or `j` whichever is nearest.\\n            * midpoint: (`i` + `j`) / 2.\\n\\n            .. versionadded:: 0.18.0\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n\\n            If ``q`` is an array, a DataFrame will be returned where the\\n              index is ``q``, the columns are the columns of self, and the\\n              values are the quantiles.\\n            If ``q`` is a float, a Series will be returned where the\\n              index is the columns of self and the values are the quantiles.\\n\\n        See Also\\n        --------\\n        core.window.Rolling.quantile: Rolling quantile.\\n        numpy.percentile: Numpy function to compute the percentile.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),\\n        ...                   columns=[\\'a\\', \\'b\\'])\\n        >>> df.quantile(.1)\\n        a    1.3\\n        b    3.7\\n        Name: 0.1, dtype: float64\\n        >>> df.quantile([.1, .5])\\n               a     b\\n        0.1  1.3   3.7\\n        0.5  2.5  55.0\\n\\n        Specifying `numeric_only=False` will also compute the quantile of\\n        datetime and timedelta data.\\n\\n        >>> df = pd.DataFrame({\\'A\\': [1, 2],\\n        ...                    \\'B\\': [pd.Timestamp(\\'2010\\'),\\n        ...                          pd.Timestamp(\\'2011\\')],\\n        ...                    \\'C\\': [pd.Timedelta(\\'1 days\\'),\\n        ...                          pd.Timedelta(\\'2 days\\')]})\\n        >>> df.quantile(0.5, numeric_only=False)\\n        A                    1.5\\n        B    2010-07-02 12:00:00\\n        C        1 days 12:00:00\\n        Name: 0.5, dtype: object\\n        \"\"\"\\n        self._check_percentile(q)\\n\\n        data = self._get_numeric_data() if numeric_only else self\\n        axis = self._get_axis_number(axis)\\n        is_transposed = axis == 1\\n\\n        if is_transposed:\\n            data = data.T\\n\\n        result = data._data.quantile(qs=q,\\n                                     axis=1,\\n                                     interpolation=interpolation,\\n                                     transposed=is_transposed)\\n\\n        if result.ndim == 2:\\n            result = self._constructor(result)\\n        else:\\n            result = self._constructor_sliced(result, name=q)\\n\\n        if is_transposed:\\n            result = result.T\\n\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'coerce_to_array',\n",
       "  'docstring': 'Coerce the input values array to numpy arrays with a mask\\n\\n    Parameters\\n    ----------\\n    values : 1D list-like\\n    dtype : integer dtype\\n    mask : boolean 1D array, optional\\n    copy : boolean, default False\\n        if True, copy the input\\n\\n    Returns\\n    -------\\n    tuple of (values, mask)',\n",
       "  'code': 'def coerce_to_array(values, dtype, mask=None, copy=False):\\n    \"\"\"\\n    Coerce the input values array to numpy arrays with a mask\\n\\n    Parameters\\n    ----------\\n    values : 1D list-like\\n    dtype : integer dtype\\n    mask : boolean 1D array, optional\\n    copy : boolean, default False\\n        if True, copy the input\\n\\n    Returns\\n    -------\\n    tuple of (values, mask)\\n    \"\"\"\\n    # if values is integer numpy array, preserve it\\'s dtype\\n    if dtype is None and hasattr(values, \\'dtype\\'):\\n        if is_integer_dtype(values.dtype):\\n            dtype = values.dtype\\n\\n    if dtype is not None:\\n        if (isinstance(dtype, str) and\\n                (dtype.startswith(\"Int\") or dtype.startswith(\"UInt\"))):\\n            # Avoid DeprecationWarning from NumPy about np.dtype(\"Int64\")\\n            # https://github.com/numpy/numpy/pull/7476\\n            dtype = dtype.lower()\\n\\n        if not issubclass(type(dtype), _IntegerDtype):\\n            try:\\n                dtype = _dtypes[str(np.dtype(dtype))]\\n            except KeyError:\\n                raise ValueError(\"invalid dtype specified {}\".format(dtype))\\n\\n    if isinstance(values, IntegerArray):\\n        values, mask = values._data, values._mask\\n        if dtype is not None:\\n            values = values.astype(dtype.numpy_dtype, copy=False)\\n\\n        if copy:\\n            values = values.copy()\\n            mask = mask.copy()\\n        return values, mask\\n\\n    values = np.array(values, copy=copy)\\n    if is_object_dtype(values):\\n        inferred_type = lib.infer_dtype(values, skipna=True)\\n        if inferred_type == \\'empty\\':\\n            values = np.empty(len(values))\\n            values.fill(np.nan)\\n        elif inferred_type not in [\\'floating\\', \\'integer\\',\\n                                   \\'mixed-integer\\', \\'mixed-integer-float\\']:\\n            raise TypeError(\"{} cannot be converted to an IntegerDtype\".format(\\n                values.dtype))\\n\\n    elif not (is_integer_dtype(values) or is_float_dtype(values)):\\n        raise TypeError(\"{} cannot be converted to an IntegerDtype\".format(\\n            values.dtype))\\n\\n    if mask is None:\\n        mask = isna(values)\\n    else:\\n        assert len(mask) == len(values)\\n\\n    if not values.ndim == 1:\\n        raise TypeError(\"values must be a 1D list-like\")\\n    if not mask.ndim == 1:\\n        raise TypeError(\"mask must be a 1D list-like\")\\n\\n    # infer dtype if needed\\n    if dtype is None:\\n        dtype = np.dtype(\\'int64\\')\\n    else:\\n        dtype = dtype.type\\n\\n    # if we are float, let\\'s make sure that we can\\n    # safely cast\\n\\n    # we copy as need to coerce here\\n    if mask.any():\\n        values = values.copy()\\n        values[mask] = 1\\n        values = safe_cast(values, dtype, copy=False)\\n    else:\\n        values = safe_cast(values, dtype, copy=False)\\n\\n    return values, mask',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'IntegerArray.astype',\n",
       "  'docstring': \"Cast to a NumPy array or IntegerArray with 'dtype'.\\n\\n        Parameters\\n        ----------\\n        dtype : str or dtype\\n            Typecode or data-type to which the array is cast.\\n        copy : bool, default True\\n            Whether to copy the data, even if not necessary. If False,\\n            a copy is made only if the old dtype does not match the\\n            new dtype.\\n\\n        Returns\\n        -------\\n        array : ndarray or IntegerArray\\n            NumPy ndarray or IntergerArray with 'dtype' for its dtype.\\n\\n        Raises\\n        ------\\n        TypeError\\n            if incompatible type with an IntegerDtype, equivalent of same_kind\\n            casting\",\n",
       "  'code': 'def astype(self, dtype, copy=True):\\n        \"\"\"\\n        Cast to a NumPy array or IntegerArray with \\'dtype\\'.\\n\\n        Parameters\\n        ----------\\n        dtype : str or dtype\\n            Typecode or data-type to which the array is cast.\\n        copy : bool, default True\\n            Whether to copy the data, even if not necessary. If False,\\n            a copy is made only if the old dtype does not match the\\n            new dtype.\\n\\n        Returns\\n        -------\\n        array : ndarray or IntegerArray\\n            NumPy ndarray or IntergerArray with \\'dtype\\' for its dtype.\\n\\n        Raises\\n        ------\\n        TypeError\\n            if incompatible type with an IntegerDtype, equivalent of same_kind\\n            casting\\n        \"\"\"\\n\\n        # if we are astyping to an existing IntegerDtype we can fastpath\\n        if isinstance(dtype, _IntegerDtype):\\n            result = self._data.astype(dtype.numpy_dtype, copy=False)\\n            return type(self)(result, mask=self._mask, copy=False)\\n\\n        # coerce\\n        data = self._coerce_to_ndarray()\\n        return astype_nansafe(data, dtype, copy=None)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_IXIndexer._convert_for_reindex',\n",
       "  'docstring': 'Transform a list of keys into a new array ready to be used as axis of\\n        the object we return (e.g. including NaNs).\\n\\n        Parameters\\n        ----------\\n        key : list-like\\n            Target labels\\n        axis: int\\n            Where the indexing is being made\\n\\n        Returns\\n        -------\\n        list-like of labels',\n",
       "  'code': 'def _convert_for_reindex(self, key, axis=None):\\n        \"\"\"\\n        Transform a list of keys into a new array ready to be used as axis of\\n        the object we return (e.g. including NaNs).\\n\\n        Parameters\\n        ----------\\n        key : list-like\\n            Target labels\\n        axis: int\\n            Where the indexing is being made\\n\\n        Returns\\n        -------\\n        list-like of labels\\n        \"\"\"\\n\\n        if axis is None:\\n            axis = self.axis or 0\\n        labels = self.obj._get_axis(axis)\\n\\n        if com.is_bool_indexer(key):\\n            key = check_bool_indexer(labels, key)\\n            return labels[key]\\n\\n        if isinstance(key, Index):\\n            keyarr = labels._convert_index_indexer(key)\\n        else:\\n            # asarray can be unsafe, NumPy strings are weird\\n            keyarr = com.asarray_tuplesafe(key)\\n\\n        if is_integer_dtype(keyarr):\\n            # Cast the indexer to uint64 if possible so\\n            # that the values returned from indexing are\\n            # also uint64.\\n            keyarr = labels._convert_arr_indexer(keyarr)\\n\\n            if not labels.is_integer():\\n                keyarr = ensure_platform_int(keyarr)\\n                return labels.take(keyarr)\\n\\n        return keyarr',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SparseDataFrame.to_coo',\n",
       "  'docstring': 'Return the contents of the frame as a sparse SciPy COO matrix.\\n\\n        .. versionadded:: 0.20.0\\n\\n        Returns\\n        -------\\n        coo_matrix : scipy.sparse.spmatrix\\n            If the caller is heterogeneous and contains booleans or objects,\\n            the result will be of dtype=object. See Notes.\\n\\n        Notes\\n        -----\\n        The dtype will be the lowest-common-denominator type (implicit\\n        upcasting); that is to say if the dtypes (even of numeric types)\\n        are mixed, the one that accommodates all will be chosen.\\n\\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\\n        float32. By numpy.find_common_type convention, mixing int64 and\\n        and uint64 will result in a float64 dtype.',\n",
       "  'code': 'def to_coo(self):\\n        \"\"\"\\n        Return the contents of the frame as a sparse SciPy COO matrix.\\n\\n        .. versionadded:: 0.20.0\\n\\n        Returns\\n        -------\\n        coo_matrix : scipy.sparse.spmatrix\\n            If the caller is heterogeneous and contains booleans or objects,\\n            the result will be of dtype=object. See Notes.\\n\\n        Notes\\n        -----\\n        The dtype will be the lowest-common-denominator type (implicit\\n        upcasting); that is to say if the dtypes (even of numeric types)\\n        are mixed, the one that accommodates all will be chosen.\\n\\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\\n        float32. By numpy.find_common_type convention, mixing int64 and\\n        and uint64 will result in a float64 dtype.\\n        \"\"\"\\n        try:\\n            from scipy.sparse import coo_matrix\\n        except ImportError:\\n            raise ImportError(\\'Scipy is not installed\\')\\n\\n        dtype = find_common_type(self.dtypes)\\n        if isinstance(dtype, SparseDtype):\\n            dtype = dtype.subtype\\n\\n        cols, rows, datas = [], [], []\\n        for col, name in enumerate(self):\\n            s = self[name]\\n            row = s.sp_index.to_int_index().indices\\n            cols.append(np.repeat(col, len(row)))\\n            rows.append(row)\\n            datas.append(s.sp_values.astype(dtype, copy=False))\\n\\n        cols = np.concatenate(cols)\\n        rows = np.concatenate(rows)\\n        datas = np.concatenate(datas)\\n        return coo_matrix((datas, (rows, cols)), shape=self.shape)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'infer_dtype_from_scalar',\n",
       "  'docstring': 'interpret the dtype from a scalar\\n\\n    Parameters\\n    ----------\\n    pandas_dtype : bool, default False\\n        whether to infer dtype including pandas extension types.\\n        If False, scalar belongs to pandas extension types is inferred as\\n        object',\n",
       "  'code': 'def infer_dtype_from_scalar(val, pandas_dtype=False):\\n    \"\"\"\\n    interpret the dtype from a scalar\\n\\n    Parameters\\n    ----------\\n    pandas_dtype : bool, default False\\n        whether to infer dtype including pandas extension types.\\n        If False, scalar belongs to pandas extension types is inferred as\\n        object\\n    \"\"\"\\n\\n    dtype = np.object_\\n\\n    # a 1-element ndarray\\n    if isinstance(val, np.ndarray):\\n        msg = \"invalid ndarray passed to infer_dtype_from_scalar\"\\n        if val.ndim != 0:\\n            raise ValueError(msg)\\n\\n        dtype = val.dtype\\n        val = val.item()\\n\\n    elif isinstance(val, str):\\n\\n        # If we create an empty array using a string to infer\\n        # the dtype, NumPy will only allocate one character per entry\\n        # so this is kind of bad. Alternately we could use np.repeat\\n        # instead of np.empty (but then you still don\\'t want things\\n        # coming out as np.str_!\\n\\n        dtype = np.object_\\n\\n    elif isinstance(val, (np.datetime64, datetime)):\\n        val = tslibs.Timestamp(val)\\n        if val is tslibs.NaT or val.tz is None:\\n            dtype = np.dtype(\\'M8[ns]\\')\\n        else:\\n            if pandas_dtype:\\n                dtype = DatetimeTZDtype(unit=\\'ns\\', tz=val.tz)\\n            else:\\n                # return datetimetz as object\\n                return np.object_, val\\n        val = val.value\\n\\n    elif isinstance(val, (np.timedelta64, timedelta)):\\n        val = tslibs.Timedelta(val).value\\n        dtype = np.dtype(\\'m8[ns]\\')\\n\\n    elif is_bool(val):\\n        dtype = np.bool_\\n\\n    elif is_integer(val):\\n        if isinstance(val, np.integer):\\n            dtype = type(val)\\n        else:\\n            dtype = np.int64\\n\\n    elif is_float(val):\\n        if isinstance(val, np.floating):\\n            dtype = type(val)\\n        else:\\n            dtype = np.float64\\n\\n    elif is_complex(val):\\n        dtype = np.complex_\\n\\n    elif pandas_dtype:\\n        if lib.is_period(val):\\n            dtype = PeriodDtype(freq=val.freq)\\n            val = val.ordinal\\n\\n    return dtype, val',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'infer_dtype_from_array',\n",
       "  'docstring': \"infer the dtype from a scalar or array\\n\\n    Parameters\\n    ----------\\n    arr : scalar or array\\n    pandas_dtype : bool, default False\\n        whether to infer dtype including pandas extension types.\\n        If False, array belongs to pandas extension types\\n        is inferred as object\\n\\n    Returns\\n    -------\\n    tuple (numpy-compat/pandas-compat dtype, array)\\n\\n    Notes\\n    -----\\n    if pandas_dtype=False. these infer to numpy dtypes\\n    exactly with the exception that mixed / object dtypes\\n    are not coerced by stringifying or conversion\\n\\n    if pandas_dtype=True. datetime64tz-aware/categorical\\n    types will retain there character.\\n\\n    Examples\\n    --------\\n    >>> np.asarray([1, '1'])\\n    array(['1', '1'], dtype='<U21')\\n\\n    >>> infer_dtype_from_array([1, '1'])\\n    (numpy.object_, [1, '1'])\",\n",
       "  'code': 'def infer_dtype_from_array(arr, pandas_dtype=False):\\n    \"\"\"\\n    infer the dtype from a scalar or array\\n\\n    Parameters\\n    ----------\\n    arr : scalar or array\\n    pandas_dtype : bool, default False\\n        whether to infer dtype including pandas extension types.\\n        If False, array belongs to pandas extension types\\n        is inferred as object\\n\\n    Returns\\n    -------\\n    tuple (numpy-compat/pandas-compat dtype, array)\\n\\n    Notes\\n    -----\\n    if pandas_dtype=False. these infer to numpy dtypes\\n    exactly with the exception that mixed / object dtypes\\n    are not coerced by stringifying or conversion\\n\\n    if pandas_dtype=True. datetime64tz-aware/categorical\\n    types will retain there character.\\n\\n    Examples\\n    --------\\n    >>> np.asarray([1, \\'1\\'])\\n    array([\\'1\\', \\'1\\'], dtype=\\'<U21\\')\\n\\n    >>> infer_dtype_from_array([1, \\'1\\'])\\n    (numpy.object_, [1, \\'1\\'])\\n\\n    \"\"\"\\n\\n    if isinstance(arr, np.ndarray):\\n        return arr.dtype, arr\\n\\n    if not is_list_like(arr):\\n        arr = [arr]\\n\\n    if pandas_dtype and is_extension_type(arr):\\n        return arr.dtype, arr\\n\\n    elif isinstance(arr, ABCSeries):\\n        return arr.dtype, np.asarray(arr)\\n\\n    # don\\'t force numpy coerce with nan\\'s\\n    inferred = lib.infer_dtype(arr, skipna=False)\\n    if inferred in [\\'string\\', \\'bytes\\', \\'unicode\\',\\n                    \\'mixed\\', \\'mixed-integer\\']:\\n        return (np.object_, arr)\\n\\n    arr = np.asarray(arr)\\n    return arr.dtype, arr',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'maybe_infer_dtype_type',\n",
       "  'docstring': 'Try to infer an object\\'s dtype, for use in arithmetic ops\\n\\n    Uses `element.dtype` if that\\'s available.\\n    Objects implementing the iterator protocol are cast to a NumPy array,\\n    and from there the array\\'s type is used.\\n\\n    Parameters\\n    ----------\\n    element : object\\n        Possibly has a `.dtype` attribute, and possibly the iterator\\n        protocol.\\n\\n    Returns\\n    -------\\n    tipo : type\\n\\n    Examples\\n    --------\\n    >>> from collections import namedtuple\\n    >>> Foo = namedtuple(\"Foo\", \"dtype\")\\n    >>> maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))\\n    numpy.int64',\n",
       "  'code': 'def maybe_infer_dtype_type(element):\\n    \"\"\"Try to infer an object\\'s dtype, for use in arithmetic ops\\n\\n    Uses `element.dtype` if that\\'s available.\\n    Objects implementing the iterator protocol are cast to a NumPy array,\\n    and from there the array\\'s type is used.\\n\\n    Parameters\\n    ----------\\n    element : object\\n        Possibly has a `.dtype` attribute, and possibly the iterator\\n        protocol.\\n\\n    Returns\\n    -------\\n    tipo : type\\n\\n    Examples\\n    --------\\n    >>> from collections import namedtuple\\n    >>> Foo = namedtuple(\"Foo\", \"dtype\")\\n    >>> maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))\\n    numpy.int64\\n    \"\"\"\\n    tipo = None\\n    if hasattr(element, \\'dtype\\'):\\n        tipo = element.dtype\\n    elif is_list_like(element):\\n        element = np.asarray(element)\\n        tipo = element.dtype\\n    return tipo',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'astype_nansafe',\n",
       "  'docstring': \"Cast the elements of an array to a given dtype a nan-safe manner.\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n    dtype : np.dtype\\n    copy : bool, default True\\n        If False, a view will be attempted but may fail, if\\n        e.g. the item sizes don't align.\\n    skipna: bool, default False\\n        Whether or not we should skip NaN when casting as a string-type.\\n\\n    Raises\\n    ------\\n    ValueError\\n        The dtype was a datetime64/timedelta64 dtype, but it had no unit.\",\n",
       "  'code': 'def astype_nansafe(arr, dtype, copy=True, skipna=False):\\n    \"\"\"\\n    Cast the elements of an array to a given dtype a nan-safe manner.\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n    dtype : np.dtype\\n    copy : bool, default True\\n        If False, a view will be attempted but may fail, if\\n        e.g. the item sizes don\\'t align.\\n    skipna: bool, default False\\n        Whether or not we should skip NaN when casting as a string-type.\\n\\n    Raises\\n    ------\\n    ValueError\\n        The dtype was a datetime64/timedelta64 dtype, but it had no unit.\\n    \"\"\"\\n\\n    # dispatch on extension dtype if needed\\n    if is_extension_array_dtype(dtype):\\n        return dtype.construct_array_type()._from_sequence(\\n            arr, dtype=dtype, copy=copy)\\n\\n    if not isinstance(dtype, np.dtype):\\n        dtype = pandas_dtype(dtype)\\n\\n    if issubclass(dtype.type, str):\\n        return lib.astype_str(arr.ravel(),\\n                              skipna=skipna).reshape(arr.shape)\\n\\n    elif is_datetime64_dtype(arr):\\n        if is_object_dtype(dtype):\\n            return tslib.ints_to_pydatetime(arr.view(np.int64))\\n        elif dtype == np.int64:\\n            return arr.view(dtype)\\n\\n        # allow frequency conversions\\n        if dtype.kind == \\'M\\':\\n            return arr.astype(dtype)\\n\\n        raise TypeError(\"cannot astype a datetimelike from [{from_dtype}] \"\\n                        \"to [{to_dtype}]\".format(from_dtype=arr.dtype,\\n                                                 to_dtype=dtype))\\n\\n    elif is_timedelta64_dtype(arr):\\n        if is_object_dtype(dtype):\\n            return tslibs.ints_to_pytimedelta(arr.view(np.int64))\\n        elif dtype == np.int64:\\n            return arr.view(dtype)\\n\\n        if dtype not in [_INT64_DTYPE, _TD_DTYPE]:\\n\\n            # allow frequency conversions\\n            # we return a float here!\\n            if dtype.kind == \\'m\\':\\n                mask = isna(arr)\\n                result = arr.astype(dtype).astype(np.float64)\\n                result[mask] = np.nan\\n                return result\\n        elif dtype == _TD_DTYPE:\\n            return arr.astype(_TD_DTYPE, copy=copy)\\n\\n        raise TypeError(\"cannot astype a timedelta from [{from_dtype}] \"\\n                        \"to [{to_dtype}]\".format(from_dtype=arr.dtype,\\n                                                 to_dtype=dtype))\\n\\n    elif (np.issubdtype(arr.dtype, np.floating) and\\n          np.issubdtype(dtype, np.integer)):\\n\\n        if not np.isfinite(arr).all():\\n            raise ValueError(\\'Cannot convert non-finite values (NA or inf) to \\'\\n                             \\'integer\\')\\n\\n    elif is_object_dtype(arr):\\n\\n        # work around NumPy brokenness, #1987\\n        if np.issubdtype(dtype.type, np.integer):\\n            return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)\\n\\n        # if we have a datetime/timedelta array of objects\\n        # then coerce to a proper dtype and recall astype_nansafe\\n\\n        elif is_datetime64_dtype(dtype):\\n            from pandas import to_datetime\\n            return astype_nansafe(to_datetime(arr).values, dtype, copy=copy)\\n        elif is_timedelta64_dtype(dtype):\\n            from pandas import to_timedelta\\n            return astype_nansafe(to_timedelta(arr).values, dtype, copy=copy)\\n\\n    if dtype.name in (\"datetime64\", \"timedelta64\"):\\n        msg = (\"The \\'{dtype}\\' dtype has no unit. \"\\n               \"Please pass in \\'{dtype}[ns]\\' instead.\")\\n        raise ValueError(msg.format(dtype=dtype.name))\\n\\n    if copy or is_object_dtype(arr) or is_object_dtype(dtype):\\n        # Explicit copy, or required since NumPy can\\'t view from / to object.\\n        return arr.astype(dtype, copy=True)\\n\\n    return arr.view(dtype)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'soft_convert_objects',\n",
       "  'docstring': 'if we have an object dtype, try to coerce dates and/or numbers',\n",
       "  'code': 'def soft_convert_objects(values, datetime=True, numeric=True, timedelta=True,\\n                         coerce=False, copy=True):\\n    \"\"\" if we have an object dtype, try to coerce dates and/or numbers \"\"\"\\n\\n    conversion_count = sum((datetime, numeric, timedelta))\\n    if conversion_count == 0:\\n        raise ValueError(\\'At least one of datetime, numeric or timedelta must \\'\\n                         \\'be True.\\')\\n    elif conversion_count > 1 and coerce:\\n        raise ValueError(\"Only one of \\'datetime\\', \\'numeric\\' or \"\\n                         \"\\'timedelta\\' can be True when when coerce=True.\")\\n\\n    if isinstance(values, (list, tuple)):\\n        # List or scalar\\n        values = np.array(values, dtype=np.object_)\\n    elif not hasattr(values, \\'dtype\\'):\\n        values = np.array([values], dtype=np.object_)\\n    elif not is_object_dtype(values.dtype):\\n        # If not object, do not attempt conversion\\n        values = values.copy() if copy else values\\n        return values\\n\\n    # If 1 flag is coerce, ensure 2 others are False\\n    if coerce:\\n        # Immediate return if coerce\\n        if datetime:\\n            from pandas import to_datetime\\n            return to_datetime(values, errors=\\'coerce\\').to_numpy()\\n        elif timedelta:\\n            from pandas import to_timedelta\\n            return to_timedelta(values, errors=\\'coerce\\').to_numpy()\\n        elif numeric:\\n            from pandas import to_numeric\\n            return to_numeric(values, errors=\\'coerce\\')\\n\\n    # Soft conversions\\n    if datetime:\\n        # GH 20380, when datetime is beyond year 2262, hence outside\\n        # bound of nanosecond-resolution 64-bit integers.\\n        try:\\n            values = lib.maybe_convert_objects(values,\\n                                               convert_datetime=datetime)\\n        except OutOfBoundsDatetime:\\n            pass\\n\\n    if timedelta and is_object_dtype(values.dtype):\\n        # Object check to ensure only run if previous did not convert\\n        values = lib.maybe_convert_objects(values, convert_timedelta=timedelta)\\n\\n    if numeric and is_object_dtype(values.dtype):\\n        try:\\n            converted = lib.maybe_convert_numeric(values, set(),\\n                                                  coerce_numeric=True)\\n            # If all NaNs, then do not-alter\\n            values = converted if not isna(converted).all() else values\\n            values = values.copy() if copy else values\\n        except Exception:\\n            pass\\n\\n    return values',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'find_common_type',\n",
       "  'docstring': 'Find a common data type among the given dtypes.\\n\\n    Parameters\\n    ----------\\n    types : list of dtypes\\n\\n    Returns\\n    -------\\n    pandas extension or numpy dtype\\n\\n    See Also\\n    --------\\n    numpy.find_common_type',\n",
       "  'code': 'def find_common_type(types):\\n    \"\"\"\\n    Find a common data type among the given dtypes.\\n\\n    Parameters\\n    ----------\\n    types : list of dtypes\\n\\n    Returns\\n    -------\\n    pandas extension or numpy dtype\\n\\n    See Also\\n    --------\\n    numpy.find_common_type\\n\\n    \"\"\"\\n\\n    if len(types) == 0:\\n        raise ValueError(\\'no types given\\')\\n\\n    first = types[0]\\n\\n    # workaround for find_common_type([np.dtype(\\'datetime64[ns]\\')] * 2)\\n    # => object\\n    if all(is_dtype_equal(first, t) for t in types[1:]):\\n        return first\\n\\n    if any(isinstance(t, (PandasExtensionDtype, ExtensionDtype))\\n           for t in types):\\n        return np.object\\n\\n    # take lowest unit\\n    if all(is_datetime64_dtype(t) for t in types):\\n        return np.dtype(\\'datetime64[ns]\\')\\n    if all(is_timedelta64_dtype(t) for t in types):\\n        return np.dtype(\\'timedelta64[ns]\\')\\n\\n    # don\\'t mix bool / int or float or complex\\n    # this is different from numpy, which casts bool with float/int as int\\n    has_bools = any(is_bool_dtype(t) for t in types)\\n    if has_bools:\\n        for t in types:\\n            if is_integer_dtype(t) or is_float_dtype(t) or is_complex_dtype(t):\\n                return np.object\\n\\n    return np.find_common_type(types, [])',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'construct_1d_arraylike_from_scalar',\n",
       "  'docstring': 'create a np.ndarray / pandas type of specified shape and dtype\\n    filled with values\\n\\n    Parameters\\n    ----------\\n    value : scalar value\\n    length : int\\n    dtype : pandas_dtype / np.dtype\\n\\n    Returns\\n    -------\\n    np.ndarray / pandas type of length, filled with value',\n",
       "  'code': 'def construct_1d_arraylike_from_scalar(value, length, dtype):\\n    \"\"\"\\n    create a np.ndarray / pandas type of specified shape and dtype\\n    filled with values\\n\\n    Parameters\\n    ----------\\n    value : scalar value\\n    length : int\\n    dtype : pandas_dtype / np.dtype\\n\\n    Returns\\n    -------\\n    np.ndarray / pandas type of length, filled with value\\n\\n    \"\"\"\\n    if is_datetime64tz_dtype(dtype):\\n        from pandas import DatetimeIndex\\n        subarr = DatetimeIndex([value] * length, dtype=dtype)\\n    elif is_categorical_dtype(dtype):\\n        from pandas import Categorical\\n        subarr = Categorical([value] * length, dtype=dtype)\\n    else:\\n        if not isinstance(dtype, (np.dtype, type(np.dtype))):\\n            dtype = dtype.dtype\\n\\n        if length and is_integer_dtype(dtype) and isna(value):\\n            # coerce if we have nan for an integer dtype\\n            dtype = np.dtype(\\'float64\\')\\n        elif isinstance(dtype, np.dtype) and dtype.kind in (\"U\", \"S\"):\\n            # we need to coerce to object dtype to avoid\\n            # to allow numpy to take our string as a scalar value\\n            dtype = object\\n            if not isna(value):\\n                value = to_str(value)\\n\\n        subarr = np.empty(length, dtype=dtype)\\n        subarr.fill(value)\\n\\n    return subarr',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'construct_1d_object_array_from_listlike',\n",
       "  'docstring': 'Transform any list-like object in a 1-dimensional numpy array of object\\n    dtype.\\n\\n    Parameters\\n    ----------\\n    values : any iterable which has a len()\\n\\n    Raises\\n    ------\\n    TypeError\\n        * If `values` does not have a len()\\n\\n    Returns\\n    -------\\n    1-dimensional numpy array of dtype object',\n",
       "  'code': 'def construct_1d_object_array_from_listlike(values):\\n    \"\"\"\\n    Transform any list-like object in a 1-dimensional numpy array of object\\n    dtype.\\n\\n    Parameters\\n    ----------\\n    values : any iterable which has a len()\\n\\n    Raises\\n    ------\\n    TypeError\\n        * If `values` does not have a len()\\n\\n    Returns\\n    -------\\n    1-dimensional numpy array of dtype object\\n    \"\"\"\\n    # numpy will try to interpret nested lists as further dimensions, hence\\n    # making a 1D array that contains list-likes is a bit tricky:\\n    result = np.empty(len(values), dtype=\\'object\\')\\n    result[:] = values\\n    return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'construct_1d_ndarray_preserving_na',\n",
       "  'docstring': \"Construct a new ndarray, coercing `values` to `dtype`, preserving NA.\\n\\n    Parameters\\n    ----------\\n    values : Sequence\\n    dtype : numpy.dtype, optional\\n    copy : bool, default False\\n        Note that copies may still be made with ``copy=False`` if casting\\n        is required.\\n\\n    Returns\\n    -------\\n    arr : ndarray[dtype]\\n\\n    Examples\\n    --------\\n    >>> np.array([1.0, 2.0, None], dtype='str')\\n    array(['1.0', '2.0', 'None'], dtype='<U4')\\n\\n    >>> construct_1d_ndarray_preserving_na([1.0, 2.0, None], dtype='str')\",\n",
       "  'code': 'def construct_1d_ndarray_preserving_na(values, dtype=None, copy=False):\\n    \"\"\"\\n    Construct a new ndarray, coercing `values` to `dtype`, preserving NA.\\n\\n    Parameters\\n    ----------\\n    values : Sequence\\n    dtype : numpy.dtype, optional\\n    copy : bool, default False\\n        Note that copies may still be made with ``copy=False`` if casting\\n        is required.\\n\\n    Returns\\n    -------\\n    arr : ndarray[dtype]\\n\\n    Examples\\n    --------\\n    >>> np.array([1.0, 2.0, None], dtype=\\'str\\')\\n    array([\\'1.0\\', \\'2.0\\', \\'None\\'], dtype=\\'<U4\\')\\n\\n    >>> construct_1d_ndarray_preserving_na([1.0, 2.0, None], dtype=\\'str\\')\\n\\n\\n    \"\"\"\\n    subarr = np.array(values, dtype=dtype, copy=copy)\\n\\n    if dtype is not None and dtype.kind in (\"U\", \"S\"):\\n        # GH-21083\\n        # We can\\'t just return np.array(subarr, dtype=\\'str\\') since\\n        # NumPy will convert the non-string objects into strings\\n        # Including NA values. Se we have to go\\n        # string -> object -> update NA, which requires an\\n        # additional pass over the data.\\n        na_values = isna(values)\\n        subarr2 = subarr.astype(object)\\n        subarr2[na_values] = np.asarray(values, dtype=object)[na_values]\\n        subarr = subarr2\\n\\n    return subarr',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'maybe_cast_to_integer_array',\n",
       "  'docstring': 'Takes any dtype and returns the casted version, raising for when data is\\n    incompatible with integer/unsigned integer dtypes.\\n\\n    .. versionadded:: 0.24.0\\n\\n    Parameters\\n    ----------\\n    arr : array-like\\n        The array to cast.\\n    dtype : str, np.dtype\\n        The integer dtype to cast the array to.\\n    copy: boolean, default False\\n        Whether to make a copy of the array before returning.\\n\\n    Returns\\n    -------\\n    int_arr : ndarray\\n        An array of integer or unsigned integer dtype\\n\\n    Raises\\n    ------\\n    OverflowError : the dtype is incompatible with the data\\n    ValueError : loss of precision has occurred during casting\\n\\n    Examples\\n    --------\\n    If you try to coerce negative values to unsigned integers, it raises:\\n\\n    >>> Series([-1], dtype=\"uint64\")\\n    Traceback (most recent call last):\\n        ...\\n    OverflowError: Trying to coerce negative values to unsigned integers\\n\\n    Also, if you try to coerce float values to integers, it raises:\\n\\n    >>> Series([1, 2, 3.5], dtype=\"int64\")\\n    Traceback (most recent call last):\\n        ...\\n    ValueError: Trying to coerce float values to integers',\n",
       "  'code': 'def maybe_cast_to_integer_array(arr, dtype, copy=False):\\n    \"\"\"\\n    Takes any dtype and returns the casted version, raising for when data is\\n    incompatible with integer/unsigned integer dtypes.\\n\\n    .. versionadded:: 0.24.0\\n\\n    Parameters\\n    ----------\\n    arr : array-like\\n        The array to cast.\\n    dtype : str, np.dtype\\n        The integer dtype to cast the array to.\\n    copy: boolean, default False\\n        Whether to make a copy of the array before returning.\\n\\n    Returns\\n    -------\\n    int_arr : ndarray\\n        An array of integer or unsigned integer dtype\\n\\n    Raises\\n    ------\\n    OverflowError : the dtype is incompatible with the data\\n    ValueError : loss of precision has occurred during casting\\n\\n    Examples\\n    --------\\n    If you try to coerce negative values to unsigned integers, it raises:\\n\\n    >>> Series([-1], dtype=\"uint64\")\\n    Traceback (most recent call last):\\n        ...\\n    OverflowError: Trying to coerce negative values to unsigned integers\\n\\n    Also, if you try to coerce float values to integers, it raises:\\n\\n    >>> Series([1, 2, 3.5], dtype=\"int64\")\\n    Traceback (most recent call last):\\n        ...\\n    ValueError: Trying to coerce float values to integers\\n    \"\"\"\\n\\n    try:\\n        if not hasattr(arr, \"astype\"):\\n            casted = np.array(arr, dtype=dtype, copy=copy)\\n        else:\\n            casted = arr.astype(dtype, copy=copy)\\n    except OverflowError:\\n        raise OverflowError(\"The elements provided in the data cannot all be \"\\n                            \"casted to the dtype {dtype}\".format(dtype=dtype))\\n\\n    if np.array_equal(arr, casted):\\n        return casted\\n\\n    # We do this casting to allow for proper\\n    # data and dtype checking.\\n    #\\n    # We didn\\'t do this earlier because NumPy\\n    # doesn\\'t handle `uint64` correctly.\\n    arr = np.asarray(arr)\\n\\n    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\\n        raise OverflowError(\"Trying to coerce negative values \"\\n                            \"to unsigned integers\")\\n\\n    if is_integer_dtype(dtype) and (is_float_dtype(arr) or\\n                                    is_object_dtype(arr)):\\n        raise ValueError(\"Trying to coerce float values to integers\")',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'hist_frame',\n",
       "  'docstring': \"Make a histogram of the DataFrame's.\\n\\n    A `histogram`_ is a representation of the distribution of data.\\n    This function calls :meth:`matplotlib.pyplot.hist`, on each series in\\n    the DataFrame, resulting in one histogram per column.\\n\\n    .. _histogram: https://en.wikipedia.org/wiki/Histogram\\n\\n    Parameters\\n    ----------\\n    data : DataFrame\\n        The pandas object holding the data.\\n    column : string or sequence\\n        If passed, will be used to limit data to a subset of columns.\\n    by : object, optional\\n        If passed, then used to form histograms for separate groups.\\n    grid : bool, default True\\n        Whether to show axis grid lines.\\n    xlabelsize : int, default None\\n        If specified changes the x-axis label size.\\n    xrot : float, default None\\n        Rotation of x axis labels. For example, a value of 90 displays the\\n        x labels rotated 90 degrees clockwise.\\n    ylabelsize : int, default None\\n        If specified changes the y-axis label size.\\n    yrot : float, default None\\n        Rotation of y axis labels. For example, a value of 90 displays the\\n        y labels rotated 90 degrees clockwise.\\n    ax : Matplotlib axes object, default None\\n        The axes to plot the histogram on.\\n    sharex : bool, default True if ax is None else False\\n        In case subplots=True, share x axis and set some x axis labels to\\n        invisible; defaults to True if ax is None otherwise False if an ax\\n        is passed in.\\n        Note that passing in both an ax and sharex=True will alter all x axis\\n        labels for all subplots in a figure.\\n    sharey : bool, default False\\n        In case subplots=True, share y axis and set some y axis labels to\\n        invisible.\\n    figsize : tuple\\n        The size in inches of the figure to create. Uses the value in\\n        `matplotlib.rcParams` by default.\\n    layout : tuple, optional\\n        Tuple of (rows, columns) for the layout of the histograms.\\n    bins : integer or sequence, default 10\\n        Number of histogram bins to be used. If an integer is given, bins + 1\\n        bin edges are calculated and returned. If bins is a sequence, gives\\n        bin edges, including left edge of first bin and right edge of last\\n        bin. In this case, bins is returned unmodified.\\n    **kwds\\n        All other plotting keyword arguments to be passed to\\n        :meth:`matplotlib.pyplot.hist`.\\n\\n    Returns\\n    -------\\n    matplotlib.AxesSubplot or numpy.ndarray of them\\n\\n    See Also\\n    --------\\n    matplotlib.pyplot.hist : Plot a histogram using matplotlib.\\n\\n    Examples\\n    --------\\n\\n    .. plot::\\n        :context: close-figs\\n\\n        This example draws a histogram based on the length and width of\\n        some animals, displayed in three bins\\n\\n        >>> df = pd.DataFrame({\\n        ...     'length': [1.5, 0.5, 1.2, 0.9, 3],\\n        ...     'width': [0.7, 0.2, 0.15, 0.2, 1.1]\\n        ...     }, index= ['pig', 'rabbit', 'duck', 'chicken', 'horse'])\\n        >>> hist = df.hist(bins=3)\",\n",
       "  'code': 'def hist_frame(data, column=None, by=None, grid=True, xlabelsize=None,\\n               xrot=None, ylabelsize=None, yrot=None, ax=None, sharex=False,\\n               sharey=False, figsize=None, layout=None, bins=10, **kwds):\\n    \"\"\"\\n    Make a histogram of the DataFrame\\'s.\\n\\n    A `histogram`_ is a representation of the distribution of data.\\n    This function calls :meth:`matplotlib.pyplot.hist`, on each series in\\n    the DataFrame, resulting in one histogram per column.\\n\\n    .. _histogram: https://en.wikipedia.org/wiki/Histogram\\n\\n    Parameters\\n    ----------\\n    data : DataFrame\\n        The pandas object holding the data.\\n    column : string or sequence\\n        If passed, will be used to limit data to a subset of columns.\\n    by : object, optional\\n        If passed, then used to form histograms for separate groups.\\n    grid : bool, default True\\n        Whether to show axis grid lines.\\n    xlabelsize : int, default None\\n        If specified changes the x-axis label size.\\n    xrot : float, default None\\n        Rotation of x axis labels. For example, a value of 90 displays the\\n        x labels rotated 90 degrees clockwise.\\n    ylabelsize : int, default None\\n        If specified changes the y-axis label size.\\n    yrot : float, default None\\n        Rotation of y axis labels. For example, a value of 90 displays the\\n        y labels rotated 90 degrees clockwise.\\n    ax : Matplotlib axes object, default None\\n        The axes to plot the histogram on.\\n    sharex : bool, default True if ax is None else False\\n        In case subplots=True, share x axis and set some x axis labels to\\n        invisible; defaults to True if ax is None otherwise False if an ax\\n        is passed in.\\n        Note that passing in both an ax and sharex=True will alter all x axis\\n        labels for all subplots in a figure.\\n    sharey : bool, default False\\n        In case subplots=True, share y axis and set some y axis labels to\\n        invisible.\\n    figsize : tuple\\n        The size in inches of the figure to create. Uses the value in\\n        `matplotlib.rcParams` by default.\\n    layout : tuple, optional\\n        Tuple of (rows, columns) for the layout of the histograms.\\n    bins : integer or sequence, default 10\\n        Number of histogram bins to be used. If an integer is given, bins + 1\\n        bin edges are calculated and returned. If bins is a sequence, gives\\n        bin edges, including left edge of first bin and right edge of last\\n        bin. In this case, bins is returned unmodified.\\n    **kwds\\n        All other plotting keyword arguments to be passed to\\n        :meth:`matplotlib.pyplot.hist`.\\n\\n    Returns\\n    -------\\n    matplotlib.AxesSubplot or numpy.ndarray of them\\n\\n    See Also\\n    --------\\n    matplotlib.pyplot.hist : Plot a histogram using matplotlib.\\n\\n    Examples\\n    --------\\n\\n    .. plot::\\n        :context: close-figs\\n\\n        This example draws a histogram based on the length and width of\\n        some animals, displayed in three bins\\n\\n        >>> df = pd.DataFrame({\\n        ...     \\'length\\': [1.5, 0.5, 1.2, 0.9, 3],\\n        ...     \\'width\\': [0.7, 0.2, 0.15, 0.2, 1.1]\\n        ...     }, index= [\\'pig\\', \\'rabbit\\', \\'duck\\', \\'chicken\\', \\'horse\\'])\\n        >>> hist = df.hist(bins=3)\\n    \"\"\"\\n    _raise_if_no_mpl()\\n    _converter._WARN = False\\n    if by is not None:\\n        axes = grouped_hist(data, column=column, by=by, ax=ax, grid=grid,\\n                            figsize=figsize, sharex=sharex, sharey=sharey,\\n                            layout=layout, bins=bins, xlabelsize=xlabelsize,\\n                            xrot=xrot, ylabelsize=ylabelsize,\\n                            yrot=yrot, **kwds)\\n        return axes\\n\\n    if column is not None:\\n        if not isinstance(column, (list, np.ndarray, ABCIndexClass)):\\n            column = [column]\\n        data = data[column]\\n    data = data._get_numeric_data()\\n    naxes = len(data.columns)\\n\\n    fig, axes = _subplots(naxes=naxes, ax=ax, squeeze=False,\\n                          sharex=sharex, sharey=sharey, figsize=figsize,\\n                          layout=layout)\\n    _axes = _flatten(axes)\\n\\n    for i, col in enumerate(com.try_sort(data.columns)):\\n        ax = _axes[i]\\n        ax.hist(data[col].dropna().values, bins=bins, **kwds)\\n        ax.set_title(col)\\n        ax.grid(grid)\\n\\n    _set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot,\\n                     ylabelsize=ylabelsize, yrot=yrot)\\n    fig.subplots_adjust(wspace=0.3, hspace=0.3)\\n\\n    return axes',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'FramePlotMethods.line',\n",
       "  'docstring': \"Plot DataFrame columns as lines.\\n\\n        This function is useful to plot lines using DataFrame's values\\n        as coordinates.\\n\\n        Parameters\\n        ----------\\n        x : int or str, optional\\n            Columns to use for the horizontal axis.\\n            Either the location or the label of the columns to be used.\\n            By default, it will use the DataFrame indices.\\n        y : int, str, or list of them, optional\\n            The values to be plotted.\\n            Either the location or the label of the columns to be used.\\n            By default, it will use the remaining DataFrame numeric columns.\\n        **kwds\\n            Keyword arguments to pass on to :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        :class:`matplotlib.axes.Axes` or :class:`numpy.ndarray`\\n            Return an ndarray when ``subplots=True``.\\n\\n        See Also\\n        --------\\n        matplotlib.pyplot.plot : Plot y versus x as lines and/or markers.\\n\\n        Examples\\n        --------\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            The following example shows the populations for some animals\\n            over the years.\\n\\n            >>> df = pd.DataFrame({\\n            ...    'pig': [20, 18, 489, 675, 1776],\\n            ...    'horse': [4, 25, 281, 600, 1900]\\n            ...    }, index=[1990, 1997, 2003, 2009, 2014])\\n            >>> lines = df.plot.line()\\n\\n        .. plot::\\n           :context: close-figs\\n\\n           An example with subplots, so an array of axes is returned.\\n\\n           >>> axes = df.plot.line(subplots=True)\\n           >>> type(axes)\\n           <class 'numpy.ndarray'>\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            The following example shows the relationship between both\\n            populations.\\n\\n            >>> lines = df.plot.line(x='pig', y='horse')\",\n",
       "  'code': 'def line(self, x=None, y=None, **kwds):\\n        \"\"\"\\n        Plot DataFrame columns as lines.\\n\\n        This function is useful to plot lines using DataFrame\\'s values\\n        as coordinates.\\n\\n        Parameters\\n        ----------\\n        x : int or str, optional\\n            Columns to use for the horizontal axis.\\n            Either the location or the label of the columns to be used.\\n            By default, it will use the DataFrame indices.\\n        y : int, str, or list of them, optional\\n            The values to be plotted.\\n            Either the location or the label of the columns to be used.\\n            By default, it will use the remaining DataFrame numeric columns.\\n        **kwds\\n            Keyword arguments to pass on to :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        :class:`matplotlib.axes.Axes` or :class:`numpy.ndarray`\\n            Return an ndarray when ``subplots=True``.\\n\\n        See Also\\n        --------\\n        matplotlib.pyplot.plot : Plot y versus x as lines and/or markers.\\n\\n        Examples\\n        --------\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            The following example shows the populations for some animals\\n            over the years.\\n\\n            >>> df = pd.DataFrame({\\n            ...    \\'pig\\': [20, 18, 489, 675, 1776],\\n            ...    \\'horse\\': [4, 25, 281, 600, 1900]\\n            ...    }, index=[1990, 1997, 2003, 2009, 2014])\\n            >>> lines = df.plot.line()\\n\\n        .. plot::\\n           :context: close-figs\\n\\n           An example with subplots, so an array of axes is returned.\\n\\n           >>> axes = df.plot.line(subplots=True)\\n           >>> type(axes)\\n           <class \\'numpy.ndarray\\'>\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            The following example shows the relationship between both\\n            populations.\\n\\n            >>> lines = df.plot.line(x=\\'pig\\', y=\\'horse\\')\\n        \"\"\"\\n        return self(kind=\\'line\\', x=x, y=y, **kwds)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'FramePlotMethods.bar',\n",
       "  'docstring': \"Vertical bar plot.\\n\\n        A bar plot is a plot that presents categorical data with\\n        rectangular bars with lengths proportional to the values that they\\n        represent. A bar plot shows comparisons among discrete categories. One\\n        axis of the plot shows the specific categories being compared, and the\\n        other axis represents a measured value.\\n\\n        Parameters\\n        ----------\\n        x : label or position, optional\\n            Allows plotting of one column versus another. If not specified,\\n            the index of the DataFrame is used.\\n        y : label or position, optional\\n            Allows plotting of one column versus another. If not specified,\\n            all numerical columns are used.\\n        **kwds\\n            Additional keyword arguments are documented in\\n            :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        matplotlib.axes.Axes or np.ndarray of them\\n            An ndarray is returned with one :class:`matplotlib.axes.Axes`\\n            per column when ``subplots=True``.\\n\\n        See Also\\n        --------\\n        DataFrame.plot.barh : Horizontal bar plot.\\n        DataFrame.plot : Make plots of a DataFrame.\\n        matplotlib.pyplot.bar : Make a bar plot with matplotlib.\\n\\n        Examples\\n        --------\\n        Basic plot.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\\n            >>> ax = df.plot.bar(x='lab', y='val', rot=0)\\n\\n        Plot a whole dataframe to a bar plot. Each column is assigned a\\n        distinct color, and each row is nested in a group along the\\n        horizontal axis.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = ['snail', 'pig', 'elephant',\\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\\n            >>> df = pd.DataFrame({'speed': speed,\\n            ...                    'lifespan': lifespan}, index=index)\\n            >>> ax = df.plot.bar(rot=0)\\n\\n        Instead of nesting, the figure can be split by column with\\n        ``subplots=True``. In this case, a :class:`numpy.ndarray` of\\n        :class:`matplotlib.axes.Axes` are returned.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> axes = df.plot.bar(rot=0, subplots=True)\\n            >>> axes[1].legend(loc=2)  # doctest: +SKIP\\n\\n        Plot a single column.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.bar(y='speed', rot=0)\\n\\n        Plot only selected categories for the DataFrame.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.bar(x='lifespan', rot=0)\",\n",
       "  'code': 'def bar(self, x=None, y=None, **kwds):\\n        \"\"\"\\n        Vertical bar plot.\\n\\n        A bar plot is a plot that presents categorical data with\\n        rectangular bars with lengths proportional to the values that they\\n        represent. A bar plot shows comparisons among discrete categories. One\\n        axis of the plot shows the specific categories being compared, and the\\n        other axis represents a measured value.\\n\\n        Parameters\\n        ----------\\n        x : label or position, optional\\n            Allows plotting of one column versus another. If not specified,\\n            the index of the DataFrame is used.\\n        y : label or position, optional\\n            Allows plotting of one column versus another. If not specified,\\n            all numerical columns are used.\\n        **kwds\\n            Additional keyword arguments are documented in\\n            :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        matplotlib.axes.Axes or np.ndarray of them\\n            An ndarray is returned with one :class:`matplotlib.axes.Axes`\\n            per column when ``subplots=True``.\\n\\n        See Also\\n        --------\\n        DataFrame.plot.barh : Horizontal bar plot.\\n        DataFrame.plot : Make plots of a DataFrame.\\n        matplotlib.pyplot.bar : Make a bar plot with matplotlib.\\n\\n        Examples\\n        --------\\n        Basic plot.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({\\'lab\\':[\\'A\\', \\'B\\', \\'C\\'], \\'val\\':[10, 30, 20]})\\n            >>> ax = df.plot.bar(x=\\'lab\\', y=\\'val\\', rot=0)\\n\\n        Plot a whole dataframe to a bar plot. Each column is assigned a\\n        distinct color, and each row is nested in a group along the\\n        horizontal axis.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = [\\'snail\\', \\'pig\\', \\'elephant\\',\\n            ...          \\'rabbit\\', \\'giraffe\\', \\'coyote\\', \\'horse\\']\\n            >>> df = pd.DataFrame({\\'speed\\': speed,\\n            ...                    \\'lifespan\\': lifespan}, index=index)\\n            >>> ax = df.plot.bar(rot=0)\\n\\n        Instead of nesting, the figure can be split by column with\\n        ``subplots=True``. In this case, a :class:`numpy.ndarray` of\\n        :class:`matplotlib.axes.Axes` are returned.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> axes = df.plot.bar(rot=0, subplots=True)\\n            >>> axes[1].legend(loc=2)  # doctest: +SKIP\\n\\n        Plot a single column.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.bar(y=\\'speed\\', rot=0)\\n\\n        Plot only selected categories for the DataFrame.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.bar(x=\\'lifespan\\', rot=0)\\n        \"\"\"\\n        return self(kind=\\'bar\\', x=x, y=y, **kwds)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'FramePlotMethods.barh',\n",
       "  'docstring': \"Make a horizontal bar plot.\\n\\n        A horizontal bar plot is a plot that presents quantitative data with\\n        rectangular bars with lengths proportional to the values that they\\n        represent. A bar plot shows comparisons among discrete categories. One\\n        axis of the plot shows the specific categories being compared, and the\\n        other axis represents a measured value.\\n\\n        Parameters\\n        ----------\\n        x : label or position, default DataFrame.index\\n            Column to be used for categories.\\n        y : label or position, default All numeric columns in dataframe\\n            Columns to be plotted from the DataFrame.\\n        **kwds\\n            Keyword arguments to pass on to :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        :class:`matplotlib.axes.Axes` or numpy.ndarray of them\\n\\n        See Also\\n        --------\\n        DataFrame.plot.bar: Vertical bar plot.\\n        DataFrame.plot : Make plots of DataFrame using matplotlib.\\n        matplotlib.axes.Axes.bar : Plot a vertical bar plot using matplotlib.\\n\\n        Examples\\n        --------\\n        Basic example\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\\n            >>> ax = df.plot.barh(x='lab', y='val')\\n\\n        Plot a whole DataFrame to a horizontal bar plot\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = ['snail', 'pig', 'elephant',\\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\\n            >>> df = pd.DataFrame({'speed': speed,\\n            ...                    'lifespan': lifespan}, index=index)\\n            >>> ax = df.plot.barh()\\n\\n        Plot a column of the DataFrame to a horizontal bar plot\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = ['snail', 'pig', 'elephant',\\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\\n            >>> df = pd.DataFrame({'speed': speed,\\n            ...                    'lifespan': lifespan}, index=index)\\n            >>> ax = df.plot.barh(y='speed')\\n\\n        Plot DataFrame versus the desired column\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = ['snail', 'pig', 'elephant',\\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\\n            >>> df = pd.DataFrame({'speed': speed,\\n            ...                    'lifespan': lifespan}, index=index)\\n            >>> ax = df.plot.barh(x='lifespan')\",\n",
       "  'code': 'def barh(self, x=None, y=None, **kwds):\\n        \"\"\"\\n        Make a horizontal bar plot.\\n\\n        A horizontal bar plot is a plot that presents quantitative data with\\n        rectangular bars with lengths proportional to the values that they\\n        represent. A bar plot shows comparisons among discrete categories. One\\n        axis of the plot shows the specific categories being compared, and the\\n        other axis represents a measured value.\\n\\n        Parameters\\n        ----------\\n        x : label or position, default DataFrame.index\\n            Column to be used for categories.\\n        y : label or position, default All numeric columns in dataframe\\n            Columns to be plotted from the DataFrame.\\n        **kwds\\n            Keyword arguments to pass on to :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        :class:`matplotlib.axes.Axes` or numpy.ndarray of them\\n\\n        See Also\\n        --------\\n        DataFrame.plot.bar: Vertical bar plot.\\n        DataFrame.plot : Make plots of DataFrame using matplotlib.\\n        matplotlib.axes.Axes.bar : Plot a vertical bar plot using matplotlib.\\n\\n        Examples\\n        --------\\n        Basic example\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({\\'lab\\':[\\'A\\', \\'B\\', \\'C\\'], \\'val\\':[10, 30, 20]})\\n            >>> ax = df.plot.barh(x=\\'lab\\', y=\\'val\\')\\n\\n        Plot a whole DataFrame to a horizontal bar plot\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = [\\'snail\\', \\'pig\\', \\'elephant\\',\\n            ...          \\'rabbit\\', \\'giraffe\\', \\'coyote\\', \\'horse\\']\\n            >>> df = pd.DataFrame({\\'speed\\': speed,\\n            ...                    \\'lifespan\\': lifespan}, index=index)\\n            >>> ax = df.plot.barh()\\n\\n        Plot a column of the DataFrame to a horizontal bar plot\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = [\\'snail\\', \\'pig\\', \\'elephant\\',\\n            ...          \\'rabbit\\', \\'giraffe\\', \\'coyote\\', \\'horse\\']\\n            >>> df = pd.DataFrame({\\'speed\\': speed,\\n            ...                    \\'lifespan\\': lifespan}, index=index)\\n            >>> ax = df.plot.barh(y=\\'speed\\')\\n\\n        Plot DataFrame versus the desired column\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = [\\'snail\\', \\'pig\\', \\'elephant\\',\\n            ...          \\'rabbit\\', \\'giraffe\\', \\'coyote\\', \\'horse\\']\\n            >>> df = pd.DataFrame({\\'speed\\': speed,\\n            ...                    \\'lifespan\\': lifespan}, index=index)\\n            >>> ax = df.plot.barh(x=\\'lifespan\\')\\n        \"\"\"\\n        return self(kind=\\'barh\\', x=x, y=y, **kwds)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'FramePlotMethods.area',\n",
       "  'docstring': \"Draw a stacked area plot.\\n\\n        An area plot displays quantitative data visually.\\n        This function wraps the matplotlib area function.\\n\\n        Parameters\\n        ----------\\n        x : label or position, optional\\n            Coordinates for the X axis. By default uses the index.\\n        y : label or position, optional\\n            Column to plot. By default uses all columns.\\n        stacked : bool, default True\\n            Area plots are stacked by default. Set to False to create a\\n            unstacked plot.\\n        **kwds : optional\\n            Additional keyword arguments are documented in\\n            :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        matplotlib.axes.Axes or numpy.ndarray\\n            Area plot, or array of area plots if subplots is True.\\n\\n        See Also\\n        --------\\n        DataFrame.plot : Make plots of DataFrame using matplotlib / pylab.\\n\\n        Examples\\n        --------\\n        Draw an area plot based on basic business metrics:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({\\n            ...     'sales': [3, 2, 3, 9, 10, 6],\\n            ...     'signups': [5, 5, 6, 12, 14, 13],\\n            ...     'visits': [20, 42, 28, 62, 81, 50],\\n            ... }, index=pd.date_range(start='2018/01/01', end='2018/07/01',\\n            ...                        freq='M'))\\n            >>> ax = df.plot.area()\\n\\n        Area plots are stacked by default. To produce an unstacked plot,\\n        pass ``stacked=False``:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.area(stacked=False)\\n\\n        Draw an area plot for a single column:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.area(y='sales')\\n\\n        Draw with a different `x`:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({\\n            ...     'sales': [3, 2, 3],\\n            ...     'visits': [20, 42, 28],\\n            ...     'day': [1, 2, 3],\\n            ... })\\n            >>> ax = df.plot.area(x='day')\",\n",
       "  'code': 'def area(self, x=None, y=None, **kwds):\\n        \"\"\"\\n        Draw a stacked area plot.\\n\\n        An area plot displays quantitative data visually.\\n        This function wraps the matplotlib area function.\\n\\n        Parameters\\n        ----------\\n        x : label or position, optional\\n            Coordinates for the X axis. By default uses the index.\\n        y : label or position, optional\\n            Column to plot. By default uses all columns.\\n        stacked : bool, default True\\n            Area plots are stacked by default. Set to False to create a\\n            unstacked plot.\\n        **kwds : optional\\n            Additional keyword arguments are documented in\\n            :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        matplotlib.axes.Axes or numpy.ndarray\\n            Area plot, or array of area plots if subplots is True.\\n\\n        See Also\\n        --------\\n        DataFrame.plot : Make plots of DataFrame using matplotlib / pylab.\\n\\n        Examples\\n        --------\\n        Draw an area plot based on basic business metrics:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({\\n            ...     \\'sales\\': [3, 2, 3, 9, 10, 6],\\n            ...     \\'signups\\': [5, 5, 6, 12, 14, 13],\\n            ...     \\'visits\\': [20, 42, 28, 62, 81, 50],\\n            ... }, index=pd.date_range(start=\\'2018/01/01\\', end=\\'2018/07/01\\',\\n            ...                        freq=\\'M\\'))\\n            >>> ax = df.plot.area()\\n\\n        Area plots are stacked by default. To produce an unstacked plot,\\n        pass ``stacked=False``:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.area(stacked=False)\\n\\n        Draw an area plot for a single column:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.area(y=\\'sales\\')\\n\\n        Draw with a different `x`:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({\\n            ...     \\'sales\\': [3, 2, 3],\\n            ...     \\'visits\\': [20, 42, 28],\\n            ...     \\'day\\': [1, 2, 3],\\n            ... })\\n            >>> ax = df.plot.area(x=\\'day\\')\\n        \"\"\"\\n        return self(kind=\\'area\\', x=x, y=y, **kwds)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'FramePlotMethods.scatter',\n",
       "  'docstring': \"Create a scatter plot with varying marker point size and color.\\n\\n        The coordinates of each point are defined by two dataframe columns and\\n        filled circles are used to represent each point. This kind of plot is\\n        useful to see complex correlations between two variables. Points could\\n        be for instance natural 2D coordinates like longitude and latitude in\\n        a map or, in general, any pair of metrics that can be plotted against\\n        each other.\\n\\n        Parameters\\n        ----------\\n        x : int or str\\n            The column name or column position to be used as horizontal\\n            coordinates for each point.\\n        y : int or str\\n            The column name or column position to be used as vertical\\n            coordinates for each point.\\n        s : scalar or array_like, optional\\n            The size of each point. Possible values are:\\n\\n            - A single scalar so all points have the same size.\\n\\n            - A sequence of scalars, which will be used for each point's size\\n              recursively. For instance, when passing [2,14] all points size\\n              will be either 2 or 14, alternatively.\\n\\n        c : str, int or array_like, optional\\n            The color of each point. Possible values are:\\n\\n            - A single color string referred to by name, RGB or RGBA code,\\n              for instance 'red' or '#a98d19'.\\n\\n            - A sequence of color strings referred to by name, RGB or RGBA\\n              code, which will be used for each point's color recursively. For\\n              instance ['green','yellow'] all points will be filled in green or\\n              yellow, alternatively.\\n\\n            - A column name or position whose values will be used to color the\\n              marker points according to a colormap.\\n\\n        **kwds\\n            Keyword arguments to pass on to :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        :class:`matplotlib.axes.Axes` or numpy.ndarray of them\\n\\n        See Also\\n        --------\\n        matplotlib.pyplot.scatter : Scatter plot using multiple input data\\n            formats.\\n\\n        Examples\\n        --------\\n        Let's see how to draw a scatter plot using coordinates from the values\\n        in a DataFrame's columns.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],\\n            ...                    [6.4, 3.2, 1], [5.9, 3.0, 2]],\\n            ...                   columns=['length', 'width', 'species'])\\n            >>> ax1 = df.plot.scatter(x='length',\\n            ...                       y='width',\\n            ...                       c='DarkBlue')\\n\\n        And now with the color determined by a column as well.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax2 = df.plot.scatter(x='length',\\n            ...                       y='width',\\n            ...                       c='species',\\n            ...                       colormap='viridis')\",\n",
       "  'code': 'def scatter(self, x, y, s=None, c=None, **kwds):\\n        \"\"\"\\n        Create a scatter plot with varying marker point size and color.\\n\\n        The coordinates of each point are defined by two dataframe columns and\\n        filled circles are used to represent each point. This kind of plot is\\n        useful to see complex correlations between two variables. Points could\\n        be for instance natural 2D coordinates like longitude and latitude in\\n        a map or, in general, any pair of metrics that can be plotted against\\n        each other.\\n\\n        Parameters\\n        ----------\\n        x : int or str\\n            The column name or column position to be used as horizontal\\n            coordinates for each point.\\n        y : int or str\\n            The column name or column position to be used as vertical\\n            coordinates for each point.\\n        s : scalar or array_like, optional\\n            The size of each point. Possible values are:\\n\\n            - A single scalar so all points have the same size.\\n\\n            - A sequence of scalars, which will be used for each point\\'s size\\n              recursively. For instance, when passing [2,14] all points size\\n              will be either 2 or 14, alternatively.\\n\\n        c : str, int or array_like, optional\\n            The color of each point. Possible values are:\\n\\n            - A single color string referred to by name, RGB or RGBA code,\\n              for instance \\'red\\' or \\'#a98d19\\'.\\n\\n            - A sequence of color strings referred to by name, RGB or RGBA\\n              code, which will be used for each point\\'s color recursively. For\\n              instance [\\'green\\',\\'yellow\\'] all points will be filled in green or\\n              yellow, alternatively.\\n\\n            - A column name or position whose values will be used to color the\\n              marker points according to a colormap.\\n\\n        **kwds\\n            Keyword arguments to pass on to :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        :class:`matplotlib.axes.Axes` or numpy.ndarray of them\\n\\n        See Also\\n        --------\\n        matplotlib.pyplot.scatter : Scatter plot using multiple input data\\n            formats.\\n\\n        Examples\\n        --------\\n        Let\\'s see how to draw a scatter plot using coordinates from the values\\n        in a DataFrame\\'s columns.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],\\n            ...                    [6.4, 3.2, 1], [5.9, 3.0, 2]],\\n            ...                   columns=[\\'length\\', \\'width\\', \\'species\\'])\\n            >>> ax1 = df.plot.scatter(x=\\'length\\',\\n            ...                       y=\\'width\\',\\n            ...                       c=\\'DarkBlue\\')\\n\\n        And now with the color determined by a column as well.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax2 = df.plot.scatter(x=\\'length\\',\\n            ...                       y=\\'width\\',\\n            ...                       c=\\'species\\',\\n            ...                       colormap=\\'viridis\\')\\n        \"\"\"\\n        return self(kind=\\'scatter\\', x=x, y=y, c=c, s=s, **kwds)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'FramePlotMethods.hexbin',\n",
       "  'docstring': 'Generate a hexagonal binning plot.\\n\\n        Generate a hexagonal binning plot of `x` versus `y`. If `C` is `None`\\n        (the default), this is a histogram of the number of occurrences\\n        of the observations at ``(x[i], y[i])``.\\n\\n        If `C` is specified, specifies values at given coordinates\\n        ``(x[i], y[i])``. These values are accumulated for each hexagonal\\n        bin and then reduced according to `reduce_C_function`,\\n        having as default the NumPy\\'s mean function (:meth:`numpy.mean`).\\n        (If `C` is specified, it must also be a 1-D sequence\\n        of the same length as `x` and `y`, or a column label.)\\n\\n        Parameters\\n        ----------\\n        x : int or str\\n            The column label or position for x points.\\n        y : int or str\\n            The column label or position for y points.\\n        C : int or str, optional\\n            The column label or position for the value of `(x, y)` point.\\n        reduce_C_function : callable, default `np.mean`\\n            Function of one argument that reduces all the values in a bin to\\n            a single number (e.g. `np.mean`, `np.max`, `np.sum`, `np.std`).\\n        gridsize : int or tuple of (int, int), default 100\\n            The number of hexagons in the x-direction.\\n            The corresponding number of hexagons in the y-direction is\\n            chosen in a way that the hexagons are approximately regular.\\n            Alternatively, gridsize can be a tuple with two elements\\n            specifying the number of hexagons in the x-direction and the\\n            y-direction.\\n        **kwds\\n            Additional keyword arguments are documented in\\n            :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        matplotlib.AxesSubplot\\n            The matplotlib ``Axes`` on which the hexbin is plotted.\\n\\n        See Also\\n        --------\\n        DataFrame.plot : Make plots of a DataFrame.\\n        matplotlib.pyplot.hexbin : Hexagonal binning plot using matplotlib,\\n            the matplotlib function that is used under the hood.\\n\\n        Examples\\n        --------\\n        The following examples are generated with random data from\\n        a normal distribution.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> n = 10000\\n            >>> df = pd.DataFrame({\\'x\\': np.random.randn(n),\\n            ...                    \\'y\\': np.random.randn(n)})\\n            >>> ax = df.plot.hexbin(x=\\'x\\', y=\\'y\\', gridsize=20)\\n\\n        The next example uses `C` and `np.sum` as `reduce_C_function`.\\n        Note that `\\'observations\\'` values ranges from 1 to 5 but the result\\n        plot shows values up to more than 25. This is because of the\\n        `reduce_C_function`.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> n = 500\\n            >>> df = pd.DataFrame({\\n            ...     \\'coord_x\\': np.random.uniform(-3, 3, size=n),\\n            ...     \\'coord_y\\': np.random.uniform(30, 50, size=n),\\n            ...     \\'observations\\': np.random.randint(1,5, size=n)\\n            ...     })\\n            >>> ax = df.plot.hexbin(x=\\'coord_x\\',\\n            ...                     y=\\'coord_y\\',\\n            ...                     C=\\'observations\\',\\n            ...                     reduce_C_function=np.sum,\\n            ...                     gridsize=10,\\n            ...                     cmap=\"viridis\")',\n",
       "  'code': 'def hexbin(self, x, y, C=None, reduce_C_function=None, gridsize=None,\\n               **kwds):\\n        \"\"\"\\n        Generate a hexagonal binning plot.\\n\\n        Generate a hexagonal binning plot of `x` versus `y`. If `C` is `None`\\n        (the default), this is a histogram of the number of occurrences\\n        of the observations at ``(x[i], y[i])``.\\n\\n        If `C` is specified, specifies values at given coordinates\\n        ``(x[i], y[i])``. These values are accumulated for each hexagonal\\n        bin and then reduced according to `reduce_C_function`,\\n        having as default the NumPy\\'s mean function (:meth:`numpy.mean`).\\n        (If `C` is specified, it must also be a 1-D sequence\\n        of the same length as `x` and `y`, or a column label.)\\n\\n        Parameters\\n        ----------\\n        x : int or str\\n            The column label or position for x points.\\n        y : int or str\\n            The column label or position for y points.\\n        C : int or str, optional\\n            The column label or position for the value of `(x, y)` point.\\n        reduce_C_function : callable, default `np.mean`\\n            Function of one argument that reduces all the values in a bin to\\n            a single number (e.g. `np.mean`, `np.max`, `np.sum`, `np.std`).\\n        gridsize : int or tuple of (int, int), default 100\\n            The number of hexagons in the x-direction.\\n            The corresponding number of hexagons in the y-direction is\\n            chosen in a way that the hexagons are approximately regular.\\n            Alternatively, gridsize can be a tuple with two elements\\n            specifying the number of hexagons in the x-direction and the\\n            y-direction.\\n        **kwds\\n            Additional keyword arguments are documented in\\n            :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        matplotlib.AxesSubplot\\n            The matplotlib ``Axes`` on which the hexbin is plotted.\\n\\n        See Also\\n        --------\\n        DataFrame.plot : Make plots of a DataFrame.\\n        matplotlib.pyplot.hexbin : Hexagonal binning plot using matplotlib,\\n            the matplotlib function that is used under the hood.\\n\\n        Examples\\n        --------\\n        The following examples are generated with random data from\\n        a normal distribution.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> n = 10000\\n            >>> df = pd.DataFrame({\\'x\\': np.random.randn(n),\\n            ...                    \\'y\\': np.random.randn(n)})\\n            >>> ax = df.plot.hexbin(x=\\'x\\', y=\\'y\\', gridsize=20)\\n\\n        The next example uses `C` and `np.sum` as `reduce_C_function`.\\n        Note that `\\'observations\\'` values ranges from 1 to 5 but the result\\n        plot shows values up to more than 25. This is because of the\\n        `reduce_C_function`.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> n = 500\\n            >>> df = pd.DataFrame({\\n            ...     \\'coord_x\\': np.random.uniform(-3, 3, size=n),\\n            ...     \\'coord_y\\': np.random.uniform(30, 50, size=n),\\n            ...     \\'observations\\': np.random.randint(1,5, size=n)\\n            ...     })\\n            >>> ax = df.plot.hexbin(x=\\'coord_x\\',\\n            ...                     y=\\'coord_y\\',\\n            ...                     C=\\'observations\\',\\n            ...                     reduce_C_function=np.sum,\\n            ...                     gridsize=10,\\n            ...                     cmap=\"viridis\")\\n        \"\"\"\\n        if reduce_C_function is not None:\\n            kwds[\\'reduce_C_function\\'] = reduce_C_function\\n        if gridsize is not None:\\n            kwds[\\'gridsize\\'] = gridsize\\n        return self(kind=\\'hexbin\\', x=x, y=y, C=C, **kwds)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_get_objs_combined_axis',\n",
       "  'docstring': 'Extract combined index: return intersection or union (depending on the\\n    value of \"intersect\") of indexes on given axis, or None if all objects\\n    lack indexes (e.g. they are numpy arrays).\\n\\n    Parameters\\n    ----------\\n    objs : list of objects\\n        Each object will only be considered if it has a _get_axis\\n        attribute.\\n    intersect : bool, default False\\n        If True, calculate the intersection between indexes. Otherwise,\\n        calculate the union.\\n    axis : {0 or \\'index\\', 1 or \\'outer\\'}, default 0\\n        The axis to extract indexes from.\\n    sort : bool, default True\\n        Whether the result index should come out sorted or not.\\n\\n    Returns\\n    -------\\n    Index',\n",
       "  'code': 'def _get_objs_combined_axis(objs, intersect=False, axis=0, sort=True):\\n    \"\"\"\\n    Extract combined index: return intersection or union (depending on the\\n    value of \"intersect\") of indexes on given axis, or None if all objects\\n    lack indexes (e.g. they are numpy arrays).\\n\\n    Parameters\\n    ----------\\n    objs : list of objects\\n        Each object will only be considered if it has a _get_axis\\n        attribute.\\n    intersect : bool, default False\\n        If True, calculate the intersection between indexes. Otherwise,\\n        calculate the union.\\n    axis : {0 or \\'index\\', 1 or \\'outer\\'}, default 0\\n        The axis to extract indexes from.\\n    sort : bool, default True\\n        Whether the result index should come out sorted or not.\\n\\n    Returns\\n    -------\\n    Index\\n    \"\"\"\\n    obs_idxes = [obj._get_axis(axis) for obj in objs\\n                 if hasattr(obj, \\'_get_axis\\')]\\n    if obs_idxes:\\n        return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_get_codes_for_values',\n",
       "  'docstring': 'utility routine to turn values into codes given the specified categories',\n",
       "  'code': 'def _get_codes_for_values(values, categories):\\n    \"\"\"\\n    utility routine to turn values into codes given the specified categories\\n    \"\"\"\\n    from pandas.core.algorithms import _get_data_algo, _hashtables\\n    dtype_equal = is_dtype_equal(values.dtype, categories.dtype)\\n\\n    if dtype_equal:\\n        # To prevent erroneous dtype coercion in _get_data_algo, retrieve\\n        # the underlying numpy array. gh-22702\\n        values = getattr(values, \\'_ndarray_values\\', values)\\n        categories = getattr(categories, \\'_ndarray_values\\', categories)\\n    elif (is_extension_array_dtype(categories.dtype) and\\n          is_object_dtype(values)):\\n        # Support inferring the correct extension dtype from an array of\\n        # scalar objects. e.g.\\n        # Categorical(array[Period, Period], categories=PeriodIndex(...))\\n        try:\\n            values = (\\n                categories.dtype.construct_array_type()._from_sequence(values)\\n            )\\n        except Exception:\\n            # but that may fail for any reason, so fall back to object\\n            values = ensure_object(values)\\n            categories = ensure_object(categories)\\n    else:\\n        values = ensure_object(values)\\n        categories = ensure_object(categories)\\n\\n    (hash_klass, vec_klass), vals = _get_data_algo(values, _hashtables)\\n    (_, _), cats = _get_data_algo(categories, _hashtables)\\n    t = hash_klass(len(cats))\\n    t.map_locations(cats)\\n    return coerce_indexer_dtype(t.lookup(vals), cats)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Categorical.astype',\n",
       "  'docstring': 'Coerce this type to another dtype\\n\\n        Parameters\\n        ----------\\n        dtype : numpy dtype or pandas type\\n        copy : bool, default True\\n            By default, astype always returns a newly allocated object.\\n            If copy is set to False and dtype is categorical, the original\\n            object is returned.\\n\\n            .. versionadded:: 0.19.0',\n",
       "  'code': 'def astype(self, dtype, copy=True):\\n        \"\"\"\\n        Coerce this type to another dtype\\n\\n        Parameters\\n        ----------\\n        dtype : numpy dtype or pandas type\\n        copy : bool, default True\\n            By default, astype always returns a newly allocated object.\\n            If copy is set to False and dtype is categorical, the original\\n            object is returned.\\n\\n            .. versionadded:: 0.19.0\\n\\n        \"\"\"\\n        if is_categorical_dtype(dtype):\\n            # GH 10696/18593\\n            dtype = self.dtype.update_dtype(dtype)\\n            self = self.copy() if copy else self\\n            if dtype == self.dtype:\\n                return self\\n            return self._set_dtype(dtype)\\n        return np.array(self, dtype=dtype, copy=copy)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Categorical.memory_usage',\n",
       "  'docstring': 'Memory usage of my values\\n\\n        Parameters\\n        ----------\\n        deep : bool\\n            Introspect the data deeply, interrogate\\n            `object` dtypes for system-level memory consumption\\n\\n        Returns\\n        -------\\n        bytes used\\n\\n        Notes\\n        -----\\n        Memory usage does not include memory consumed by elements that\\n        are not components of the array if deep=False\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes',\n",
       "  'code': 'def memory_usage(self, deep=False):\\n        \"\"\"\\n        Memory usage of my values\\n\\n        Parameters\\n        ----------\\n        deep : bool\\n            Introspect the data deeply, interrogate\\n            `object` dtypes for system-level memory consumption\\n\\n        Returns\\n        -------\\n        bytes used\\n\\n        Notes\\n        -----\\n        Memory usage does not include memory consumed by elements that\\n        are not components of the array if deep=False\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes\\n        \"\"\"\\n        return self._codes.nbytes + self.dtype.categories.memory_usage(\\n            deep=deep)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Categorical.value_counts',\n",
       "  'docstring': \"Return a Series containing counts of each category.\\n\\n        Every category will have an entry, even those with a count of 0.\\n\\n        Parameters\\n        ----------\\n        dropna : bool, default True\\n            Don't include counts of NaN.\\n\\n        Returns\\n        -------\\n        counts : Series\\n\\n        See Also\\n        --------\\n        Series.value_counts\",\n",
       "  'code': 'def value_counts(self, dropna=True):\\n        \"\"\"\\n        Return a Series containing counts of each category.\\n\\n        Every category will have an entry, even those with a count of 0.\\n\\n        Parameters\\n        ----------\\n        dropna : bool, default True\\n            Don\\'t include counts of NaN.\\n\\n        Returns\\n        -------\\n        counts : Series\\n\\n        See Also\\n        --------\\n        Series.value_counts\\n\\n        \"\"\"\\n        from numpy import bincount\\n        from pandas import Series, CategoricalIndex\\n\\n        code, cat = self._codes, self.categories\\n        ncat, mask = len(cat), 0 <= code\\n        ix, clean = np.arange(ncat), mask.all()\\n\\n        if dropna or clean:\\n            obs = code if clean else code[mask]\\n            count = bincount(obs, minlength=ncat or None)\\n        else:\\n            count = bincount(np.where(mask, code, ncat))\\n            ix = np.append(ix, -1)\\n\\n        ix = self._constructor(ix, dtype=self.dtype,\\n                               fastpath=True)\\n\\n        return Series(count, index=CategoricalIndex(ix), dtype=\\'int64\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Categorical.get_values',\n",
       "  'docstring': 'Return the values.\\n\\n        For internal compatibility with pandas formatting.\\n\\n        Returns\\n        -------\\n        numpy.array\\n            A numpy array of the same dtype as categorical.categories.dtype or\\n            Index if datetime / periods.',\n",
       "  'code': 'def get_values(self):\\n        \"\"\"\\n        Return the values.\\n\\n        For internal compatibility with pandas formatting.\\n\\n        Returns\\n        -------\\n        numpy.array\\n            A numpy array of the same dtype as categorical.categories.dtype or\\n            Index if datetime / periods.\\n        \"\"\"\\n        # if we are a datetime and period index, return Index to keep metadata\\n        if is_datetimelike(self.categories):\\n            return self.categories.take(self._codes, fill_value=np.nan)\\n        elif is_integer_dtype(self.categories) and -1 in self._codes:\\n            return self.categories.astype(\"object\").take(self._codes,\\n                                                         fill_value=np.nan)\\n        return np.array(self)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Categorical._values_for_rank',\n",
       "  'docstring': 'For correctly ranking ordered categorical data. See GH#15420\\n\\n        Ordered categorical data should be ranked on the basis of\\n        codes with -1 translated to NaN.\\n\\n        Returns\\n        -------\\n        numpy.array',\n",
       "  'code': 'def _values_for_rank(self):\\n        \"\"\"\\n        For correctly ranking ordered categorical data. See GH#15420\\n\\n        Ordered categorical data should be ranked on the basis of\\n        codes with -1 translated to NaN.\\n\\n        Returns\\n        -------\\n        numpy.array\\n\\n        \"\"\"\\n        from pandas import Series\\n        if self.ordered:\\n            values = self.codes\\n            mask = values == -1\\n            if mask.any():\\n                values = values.astype(\\'float64\\')\\n                values[mask] = np.nan\\n        elif self.categories.is_numeric():\\n            values = np.array(self)\\n        else:\\n            #  reorder the categories (so rank can use the float codes)\\n            #  instead of passing an object array to rank\\n            values = np.array(\\n                self.rename_categories(Series(self.categories).rank().values)\\n            )\\n        return values',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Categorical.take_nd',\n",
       "  'docstring': 'Take elements from the Categorical.\\n\\n        Parameters\\n        ----------\\n        indexer : sequence of int\\n            The indices in `self` to take. The meaning of negative values in\\n            `indexer` depends on the value of `allow_fill`.\\n        allow_fill : bool, default None\\n            How to handle negative values in `indexer`.\\n\\n            * False: negative values in `indices` indicate positional indices\\n              from the right. This is similar to\\n              :func:`numpy.take`.\\n\\n            * True: negative values in `indices` indicate missing values\\n              (the default). These values are set to `fill_value`. Any other\\n              other negative values raise a ``ValueError``.\\n\\n            .. versionchanged:: 0.23.0\\n\\n               Deprecated the default value of `allow_fill`. The deprecated\\n               default is ``True``. In the future, this will change to\\n               ``False``.\\n\\n        fill_value : object\\n            The value to use for `indices` that are missing (-1), when\\n            ``allow_fill=True``. This should be the category, i.e. a value\\n            in ``self.categories``, not a code.\\n\\n        Returns\\n        -------\\n        Categorical\\n            This Categorical will have the same categories and ordered as\\n            `self`.\\n\\n        See Also\\n        --------\\n        Series.take : Similar method for Series.\\n        numpy.ndarray.take : Similar method for NumPy arrays.\\n\\n        Examples\\n        --------\\n        >>> cat = pd.Categorical([\\'a\\', \\'a\\', \\'b\\'])\\n        >>> cat\\n        [a, a, b]\\n        Categories (2, object): [a, b]\\n\\n        Specify ``allow_fill==False`` to have negative indices mean indexing\\n        from the right.\\n\\n        >>> cat.take([0, -1, -2], allow_fill=False)\\n        [a, b, a]\\n        Categories (2, object): [a, b]\\n\\n        With ``allow_fill=True``, indices equal to ``-1`` mean \"missing\"\\n        values that should be filled with the `fill_value`, which is\\n        ``np.nan`` by default.\\n\\n        >>> cat.take([0, -1, -1], allow_fill=True)\\n        [a, NaN, NaN]\\n        Categories (2, object): [a, b]\\n\\n        The fill value can be specified.\\n\\n        >>> cat.take([0, -1, -1], allow_fill=True, fill_value=\\'a\\')\\n        [a, a, a]\\n        Categories (3, object): [a, b]\\n\\n        Specifying a fill value that\\'s not in ``self.categories``\\n        will raise a ``TypeError``.',\n",
       "  'code': 'def take_nd(self, indexer, allow_fill=None, fill_value=None):\\n        \"\"\"\\n        Take elements from the Categorical.\\n\\n        Parameters\\n        ----------\\n        indexer : sequence of int\\n            The indices in `self` to take. The meaning of negative values in\\n            `indexer` depends on the value of `allow_fill`.\\n        allow_fill : bool, default None\\n            How to handle negative values in `indexer`.\\n\\n            * False: negative values in `indices` indicate positional indices\\n              from the right. This is similar to\\n              :func:`numpy.take`.\\n\\n            * True: negative values in `indices` indicate missing values\\n              (the default). These values are set to `fill_value`. Any other\\n              other negative values raise a ``ValueError``.\\n\\n            .. versionchanged:: 0.23.0\\n\\n               Deprecated the default value of `allow_fill`. The deprecated\\n               default is ``True``. In the future, this will change to\\n               ``False``.\\n\\n        fill_value : object\\n            The value to use for `indices` that are missing (-1), when\\n            ``allow_fill=True``. This should be the category, i.e. a value\\n            in ``self.categories``, not a code.\\n\\n        Returns\\n        -------\\n        Categorical\\n            This Categorical will have the same categories and ordered as\\n            `self`.\\n\\n        See Also\\n        --------\\n        Series.take : Similar method for Series.\\n        numpy.ndarray.take : Similar method for NumPy arrays.\\n\\n        Examples\\n        --------\\n        >>> cat = pd.Categorical([\\'a\\', \\'a\\', \\'b\\'])\\n        >>> cat\\n        [a, a, b]\\n        Categories (2, object): [a, b]\\n\\n        Specify ``allow_fill==False`` to have negative indices mean indexing\\n        from the right.\\n\\n        >>> cat.take([0, -1, -2], allow_fill=False)\\n        [a, b, a]\\n        Categories (2, object): [a, b]\\n\\n        With ``allow_fill=True``, indices equal to ``-1`` mean \"missing\"\\n        values that should be filled with the `fill_value`, which is\\n        ``np.nan`` by default.\\n\\n        >>> cat.take([0, -1, -1], allow_fill=True)\\n        [a, NaN, NaN]\\n        Categories (2, object): [a, b]\\n\\n        The fill value can be specified.\\n\\n        >>> cat.take([0, -1, -1], allow_fill=True, fill_value=\\'a\\')\\n        [a, a, a]\\n        Categories (3, object): [a, b]\\n\\n        Specifying a fill value that\\'s not in ``self.categories``\\n        will raise a ``TypeError``.\\n        \"\"\"\\n        indexer = np.asarray(indexer, dtype=np.intp)\\n        if allow_fill is None:\\n            if (indexer < 0).any():\\n                warn(_take_msg, FutureWarning, stacklevel=2)\\n                allow_fill = True\\n\\n        dtype = self.dtype\\n\\n        if isna(fill_value):\\n            fill_value = -1\\n        elif allow_fill:\\n            # convert user-provided `fill_value` to codes\\n            if fill_value in self.categories:\\n                fill_value = self.categories.get_loc(fill_value)\\n            else:\\n                msg = (\\n                    \"\\'fill_value\\' (\\'{}\\') is not in this Categorical\\'s \"\\n                    \"categories.\"\\n                )\\n                raise TypeError(msg.format(fill_value))\\n\\n        codes = take(self._codes, indexer, allow_fill=allow_fill,\\n                     fill_value=fill_value)\\n        result = type(self).from_codes(codes, dtype=dtype)\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Categorical._slice',\n",
       "  'docstring': 'Return a slice of myself.\\n\\n        For internal compatibility with numpy arrays.',\n",
       "  'code': 'def _slice(self, slicer):\\n        \"\"\"\\n        Return a slice of myself.\\n\\n        For internal compatibility with numpy arrays.\\n        \"\"\"\\n\\n        # only allow 1 dimensional slicing, but can\\n        # in a 2-d case be passd (slice(None),....)\\n        if isinstance(slicer, tuple) and len(slicer) == 2:\\n            if not com.is_null_slice(slicer[0]):\\n                raise AssertionError(\"invalid slicing for a 1-ndim \"\\n                                     \"categorical\")\\n            slicer = slicer[1]\\n\\n        codes = self._codes[slicer]\\n        return self._constructor(values=codes, dtype=self.dtype, fastpath=True)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Categorical.isin',\n",
       "  'docstring': \"Check whether `values` are contained in Categorical.\\n\\n        Return a boolean NumPy Array showing whether each element in\\n        the Categorical matches an element in the passed sequence of\\n        `values` exactly.\\n\\n        Parameters\\n        ----------\\n        values : set or list-like\\n            The sequence of values to test. Passing in a single string will\\n            raise a ``TypeError``. Instead, turn a single string into a\\n            list of one element.\\n\\n        Returns\\n        -------\\n        isin : numpy.ndarray (bool dtype)\\n\\n        Raises\\n        ------\\n        TypeError\\n          * If `values` is not a set or list-like\\n\\n        See Also\\n        --------\\n        pandas.Series.isin : Equivalent method on Series.\\n\\n        Examples\\n        --------\\n\\n        >>> s = pd.Categorical(['lama', 'cow', 'lama', 'beetle', 'lama',\\n        ...                'hippo'])\\n        >>> s.isin(['cow', 'lama'])\\n        array([ True,  True,  True, False,  True, False])\\n\\n        Passing a single string as ``s.isin('lama')`` will raise an error. Use\\n        a list of one element instead:\\n\\n        >>> s.isin(['lama'])\\n        array([ True, False,  True, False,  True, False])\",\n",
       "  'code': 'def isin(self, values):\\n        \"\"\"\\n        Check whether `values` are contained in Categorical.\\n\\n        Return a boolean NumPy Array showing whether each element in\\n        the Categorical matches an element in the passed sequence of\\n        `values` exactly.\\n\\n        Parameters\\n        ----------\\n        values : set or list-like\\n            The sequence of values to test. Passing in a single string will\\n            raise a ``TypeError``. Instead, turn a single string into a\\n            list of one element.\\n\\n        Returns\\n        -------\\n        isin : numpy.ndarray (bool dtype)\\n\\n        Raises\\n        ------\\n        TypeError\\n          * If `values` is not a set or list-like\\n\\n        See Also\\n        --------\\n        pandas.Series.isin : Equivalent method on Series.\\n\\n        Examples\\n        --------\\n\\n        >>> s = pd.Categorical([\\'lama\\', \\'cow\\', \\'lama\\', \\'beetle\\', \\'lama\\',\\n        ...                \\'hippo\\'])\\n        >>> s.isin([\\'cow\\', \\'lama\\'])\\n        array([ True,  True,  True, False,  True, False])\\n\\n        Passing a single string as ``s.isin(\\'lama\\')`` will raise an error. Use\\n        a list of one element instead:\\n\\n        >>> s.isin([\\'lama\\'])\\n        array([ True, False,  True, False,  True, False])\\n        \"\"\"\\n        from pandas.core.internals.construction import sanitize_array\\n        if not is_list_like(values):\\n            raise TypeError(\"only list-like objects are allowed to be passed\"\\n                            \" to isin(), you passed a [{values_type}]\"\\n                            .format(values_type=type(values).__name__))\\n        values = sanitize_array(values, None, None)\\n        null_mask = np.asarray(isna(values))\\n        code_values = self.categories.get_indexer(values)\\n        code_values = code_values[null_mask | (code_values >= 0)]\\n        return algorithms.isin(self.codes, code_values)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'to_timedelta',\n",
       "  'docstring': \"Convert argument to timedelta.\\n\\n    Timedeltas are absolute differences in times, expressed in difference\\n    units (e.g. days, hours, minutes, seconds). This method converts\\n    an argument from a recognized timedelta format / value into\\n    a Timedelta type.\\n\\n    Parameters\\n    ----------\\n    arg : str, timedelta, list-like or Series\\n        The data to be converted to timedelta.\\n    unit : str, default 'ns'\\n        Denotes the unit of the arg. Possible values:\\n        ('Y', 'M', 'W', 'D', 'days', 'day', 'hours', hour', 'hr',\\n        'h', 'm', 'minute', 'min', 'minutes', 'T', 'S', 'seconds',\\n        'sec', 'second', 'ms', 'milliseconds', 'millisecond',\\n        'milli', 'millis', 'L', 'us', 'microseconds', 'microsecond',\\n        'micro', 'micros', 'U', 'ns', 'nanoseconds', 'nano', 'nanos',\\n        'nanosecond', 'N').\\n    box : bool, default True\\n        - If True returns a Timedelta/TimedeltaIndex of the results.\\n        - If False returns a numpy.timedelta64 or numpy.darray of\\n          values of dtype timedelta64[ns].\\n\\n        .. deprecated:: 0.25.0\\n            Use :meth:`.to_numpy` or :meth:`Timedelta.to_timedelta64`\\n            instead to get an ndarray of values or numpy.timedelta64,\\n            respectively.\\n\\n    errors : {'ignore', 'raise', 'coerce'}, default 'raise'\\n        - If 'raise', then invalid parsing will raise an exception.\\n        - If 'coerce', then invalid parsing will be set as NaT.\\n        - If 'ignore', then invalid parsing will return the input.\\n\\n    Returns\\n    -------\\n    timedelta64 or numpy.array of timedelta64\\n        Output type returned if parsing succeeded.\\n\\n    See Also\\n    --------\\n    DataFrame.astype : Cast argument to a specified dtype.\\n    to_datetime : Convert argument to datetime.\\n\\n    Examples\\n    --------\\n\\n    Parsing a single string to a Timedelta:\\n\\n    >>> pd.to_timedelta('1 days 06:05:01.00003')\\n    Timedelta('1 days 06:05:01.000030')\\n    >>> pd.to_timedelta('15.5us')\\n    Timedelta('0 days 00:00:00.000015')\\n\\n    Parsing a list or array of strings:\\n\\n    >>> pd.to_timedelta(['1 days 06:05:01.00003', '15.5us', 'nan'])\\n    TimedeltaIndex(['1 days 06:05:01.000030', '0 days 00:00:00.000015', NaT],\\n                   dtype='timedelta64[ns]', freq=None)\\n\\n    Converting numbers by specifying the `unit` keyword argument:\\n\\n    >>> pd.to_timedelta(np.arange(5), unit='s')\\n    TimedeltaIndex(['00:00:00', '00:00:01', '00:00:02',\\n                    '00:00:03', '00:00:04'],\\n                   dtype='timedelta64[ns]', freq=None)\\n    >>> pd.to_timedelta(np.arange(5), unit='d')\\n    TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'],\\n                   dtype='timedelta64[ns]', freq=None)\\n\\n    Returning an ndarray by using the 'box' keyword argument:\\n\\n    >>> pd.to_timedelta(np.arange(5), box=False)\\n    array([0, 1, 2, 3, 4], dtype='timedelta64[ns]')\",\n",
       "  'code': 'def to_timedelta(arg, unit=\\'ns\\', box=True, errors=\\'raise\\'):\\n    \"\"\"\\n    Convert argument to timedelta.\\n\\n    Timedeltas are absolute differences in times, expressed in difference\\n    units (e.g. days, hours, minutes, seconds). This method converts\\n    an argument from a recognized timedelta format / value into\\n    a Timedelta type.\\n\\n    Parameters\\n    ----------\\n    arg : str, timedelta, list-like or Series\\n        The data to be converted to timedelta.\\n    unit : str, default \\'ns\\'\\n        Denotes the unit of the arg. Possible values:\\n        (\\'Y\\', \\'M\\', \\'W\\', \\'D\\', \\'days\\', \\'day\\', \\'hours\\', hour\\', \\'hr\\',\\n        \\'h\\', \\'m\\', \\'minute\\', \\'min\\', \\'minutes\\', \\'T\\', \\'S\\', \\'seconds\\',\\n        \\'sec\\', \\'second\\', \\'ms\\', \\'milliseconds\\', \\'millisecond\\',\\n        \\'milli\\', \\'millis\\', \\'L\\', \\'us\\', \\'microseconds\\', \\'microsecond\\',\\n        \\'micro\\', \\'micros\\', \\'U\\', \\'ns\\', \\'nanoseconds\\', \\'nano\\', \\'nanos\\',\\n        \\'nanosecond\\', \\'N\\').\\n    box : bool, default True\\n        - If True returns a Timedelta/TimedeltaIndex of the results.\\n        - If False returns a numpy.timedelta64 or numpy.darray of\\n          values of dtype timedelta64[ns].\\n\\n        .. deprecated:: 0.25.0\\n            Use :meth:`.to_numpy` or :meth:`Timedelta.to_timedelta64`\\n            instead to get an ndarray of values or numpy.timedelta64,\\n            respectively.\\n\\n    errors : {\\'ignore\\', \\'raise\\', \\'coerce\\'}, default \\'raise\\'\\n        - If \\'raise\\', then invalid parsing will raise an exception.\\n        - If \\'coerce\\', then invalid parsing will be set as NaT.\\n        - If \\'ignore\\', then invalid parsing will return the input.\\n\\n    Returns\\n    -------\\n    timedelta64 or numpy.array of timedelta64\\n        Output type returned if parsing succeeded.\\n\\n    See Also\\n    --------\\n    DataFrame.astype : Cast argument to a specified dtype.\\n    to_datetime : Convert argument to datetime.\\n\\n    Examples\\n    --------\\n\\n    Parsing a single string to a Timedelta:\\n\\n    >>> pd.to_timedelta(\\'1 days 06:05:01.00003\\')\\n    Timedelta(\\'1 days 06:05:01.000030\\')\\n    >>> pd.to_timedelta(\\'15.5us\\')\\n    Timedelta(\\'0 days 00:00:00.000015\\')\\n\\n    Parsing a list or array of strings:\\n\\n    >>> pd.to_timedelta([\\'1 days 06:05:01.00003\\', \\'15.5us\\', \\'nan\\'])\\n    TimedeltaIndex([\\'1 days 06:05:01.000030\\', \\'0 days 00:00:00.000015\\', NaT],\\n                   dtype=\\'timedelta64[ns]\\', freq=None)\\n\\n    Converting numbers by specifying the `unit` keyword argument:\\n\\n    >>> pd.to_timedelta(np.arange(5), unit=\\'s\\')\\n    TimedeltaIndex([\\'00:00:00\\', \\'00:00:01\\', \\'00:00:02\\',\\n                    \\'00:00:03\\', \\'00:00:04\\'],\\n                   dtype=\\'timedelta64[ns]\\', freq=None)\\n    >>> pd.to_timedelta(np.arange(5), unit=\\'d\\')\\n    TimedeltaIndex([\\'0 days\\', \\'1 days\\', \\'2 days\\', \\'3 days\\', \\'4 days\\'],\\n                   dtype=\\'timedelta64[ns]\\', freq=None)\\n\\n    Returning an ndarray by using the \\'box\\' keyword argument:\\n\\n    >>> pd.to_timedelta(np.arange(5), box=False)\\n    array([0, 1, 2, 3, 4], dtype=\\'timedelta64[ns]\\')\\n    \"\"\"\\n    unit = parse_timedelta_unit(unit)\\n\\n    if errors not in (\\'ignore\\', \\'raise\\', \\'coerce\\'):\\n        raise ValueError(\"errors must be one of \\'ignore\\', \"\\n                         \"\\'raise\\', or \\'coerce\\'}\")\\n\\n    if unit in {\\'Y\\', \\'y\\', \\'M\\'}:\\n        warnings.warn(\"M and Y units are deprecated and \"\\n                      \"will be removed in a future version.\",\\n                      FutureWarning, stacklevel=2)\\n\\n    if arg is None:\\n        return arg\\n    elif isinstance(arg, ABCSeries):\\n        values = _convert_listlike(arg._values, unit=unit,\\n                                   box=False, errors=errors)\\n        return arg._constructor(values, index=arg.index, name=arg.name)\\n    elif isinstance(arg, ABCIndexClass):\\n        return _convert_listlike(arg, unit=unit, box=box,\\n                                 errors=errors, name=arg.name)\\n    elif isinstance(arg, np.ndarray) and arg.ndim == 0:\\n        # extract array scalar and process below\\n        arg = arg.item()\\n    elif is_list_like(arg) and getattr(arg, \\'ndim\\', 1) == 1:\\n        return _convert_listlike(arg, unit=unit, box=box, errors=errors)\\n    elif getattr(arg, \\'ndim\\', 1) > 1:\\n        raise TypeError(\\'arg must be a string, timedelta, list, tuple, \\'\\n                        \\'1-d array, or Series\\')\\n\\n    # ...so it must be a scalar value. Return scalar.\\n    return _coerce_scalar_to_timedelta_type(arg, unit=unit,\\n                                            box=box, errors=errors)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_cast_inplace',\n",
       "  'docstring': \"Cast an expression inplace.\\n\\n    Parameters\\n    ----------\\n    terms : Op\\n        The expression that should cast.\\n    acceptable_dtypes : list of acceptable numpy.dtype\\n        Will not cast if term's dtype in this list.\\n\\n        .. versionadded:: 0.19.0\\n\\n    dtype : str or numpy.dtype\\n        The dtype to cast to.\",\n",
       "  'code': 'def _cast_inplace(terms, acceptable_dtypes, dtype):\\n    \"\"\"Cast an expression inplace.\\n\\n    Parameters\\n    ----------\\n    terms : Op\\n        The expression that should cast.\\n    acceptable_dtypes : list of acceptable numpy.dtype\\n        Will not cast if term\\'s dtype in this list.\\n\\n        .. versionadded:: 0.19.0\\n\\n    dtype : str or numpy.dtype\\n        The dtype to cast to.\\n    \"\"\"\\n    dt = np.dtype(dtype)\\n    for term in terms:\\n        if term.type in acceptable_dtypes:\\n            continue\\n\\n        try:\\n            new_value = term.value.astype(dt)\\n        except AttributeError:\\n            new_value = dt.type(term.value)\\n        term.update(new_value)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'cut',\n",
       "  'docstring': 'Bin values into discrete intervals.\\n\\n    Use `cut` when you need to segment and sort data values into bins. This\\n    function is also useful for going from a continuous variable to a\\n    categorical variable. For example, `cut` could convert ages to groups of\\n    age ranges. Supports binning into an equal number of bins, or a\\n    pre-specified array of bins.\\n\\n    Parameters\\n    ----------\\n    x : array-like\\n        The input array to be binned. Must be 1-dimensional.\\n    bins : int, sequence of scalars, or IntervalIndex\\n        The criteria to bin by.\\n\\n        * int : Defines the number of equal-width bins in the range of `x`. The\\n          range of `x` is extended by .1% on each side to include the minimum\\n          and maximum values of `x`.\\n        * sequence of scalars : Defines the bin edges allowing for non-uniform\\n          width. No extension of the range of `x` is done.\\n        * IntervalIndex : Defines the exact bins to be used. Note that\\n          IntervalIndex for `bins` must be non-overlapping.\\n\\n    right : bool, default True\\n        Indicates whether `bins` includes the rightmost edge or not. If\\n        ``right == True`` (the default), then the `bins` ``[1, 2, 3, 4]``\\n        indicate (1,2], (2,3], (3,4]. This argument is ignored when\\n        `bins` is an IntervalIndex.\\n    labels : array or bool, optional\\n        Specifies the labels for the returned bins. Must be the same length as\\n        the resulting bins. If False, returns only integer indicators of the\\n        bins. This affects the type of the output container (see below).\\n        This argument is ignored when `bins` is an IntervalIndex.\\n    retbins : bool, default False\\n        Whether to return the bins or not. Useful when bins is provided\\n        as a scalar.\\n    precision : int, default 3\\n        The precision at which to store and display the bins labels.\\n    include_lowest : bool, default False\\n        Whether the first interval should be left-inclusive or not.\\n    duplicates : {default \\'raise\\', \\'drop\\'}, optional\\n        If bin edges are not unique, raise ValueError or drop non-uniques.\\n\\n        .. versionadded:: 0.23.0\\n\\n    Returns\\n    -------\\n    out : Categorical, Series, or ndarray\\n        An array-like object representing the respective bin for each value\\n        of `x`. The type depends on the value of `labels`.\\n\\n        * True (default) : returns a Series for Series `x` or a\\n          Categorical for all other inputs. The values stored within\\n          are Interval dtype.\\n\\n        * sequence of scalars : returns a Series for Series `x` or a\\n          Categorical for all other inputs. The values stored within\\n          are whatever the type in the sequence is.\\n\\n        * False : returns an ndarray of integers.\\n\\n    bins : numpy.ndarray or IntervalIndex.\\n        The computed or specified bins. Only returned when `retbins=True`.\\n        For scalar or sequence `bins`, this is an ndarray with the computed\\n        bins. If set `duplicates=drop`, `bins` will drop non-unique bin. For\\n        an IntervalIndex `bins`, this is equal to `bins`.\\n\\n    See Also\\n    --------\\n    qcut : Discretize variable into equal-sized buckets based on rank\\n        or based on sample quantiles.\\n    Categorical : Array type for storing data that come from a\\n        fixed set of values.\\n    Series : One-dimensional array with axis labels (including time series).\\n    IntervalIndex : Immutable Index implementing an ordered, sliceable set.\\n\\n    Notes\\n    -----\\n    Any NA values will be NA in the result. Out of bounds values will be NA in\\n    the resulting Series or Categorical object.\\n\\n    Examples\\n    --------\\n    Discretize into three equal-sized bins.\\n\\n    >>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3)\\n    ... # doctest: +ELLIPSIS\\n    [(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\\n    Categories (3, interval[float64]): [(0.994, 3.0] < (3.0, 5.0] ...\\n\\n    >>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, retbins=True)\\n    ... # doctest: +ELLIPSIS\\n    ([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\\n    Categories (3, interval[float64]): [(0.994, 3.0] < (3.0, 5.0] ...\\n    array([0.994, 3.   , 5.   , 7.   ]))\\n\\n    Discovers the same bins, but assign them specific labels. Notice that\\n    the returned Categorical\\'s categories are `labels` and is ordered.\\n\\n    >>> pd.cut(np.array([1, 7, 5, 4, 6, 3]),\\n    ...        3, labels=[\"bad\", \"medium\", \"good\"])\\n    [bad, good, medium, medium, good, bad]\\n    Categories (3, object): [bad < medium < good]\\n\\n    ``labels=False`` implies you just want the bins back.\\n\\n    >>> pd.cut([0, 1, 1, 2], bins=4, labels=False)\\n    array([0, 1, 1, 3])\\n\\n    Passing a Series as an input returns a Series with categorical dtype:\\n\\n    >>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\\n    ...               index=[\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'])\\n    >>> pd.cut(s, 3)\\n    ... # doctest: +ELLIPSIS\\n    a    (1.992, 4.667]\\n    b    (1.992, 4.667]\\n    c    (4.667, 7.333]\\n    d     (7.333, 10.0]\\n    e     (7.333, 10.0]\\n    dtype: category\\n    Categories (3, interval[float64]): [(1.992, 4.667] < (4.667, ...\\n\\n    Passing a Series as an input returns a Series with mapping value.\\n    It is used to map numerically to intervals based on bins.\\n\\n    >>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\\n    ...               index=[\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'])\\n    >>> pd.cut(s, [0, 2, 4, 6, 8, 10], labels=False, retbins=True, right=False)\\n    ... # doctest: +ELLIPSIS\\n    (a    0.0\\n     b    1.0\\n     c    2.0\\n     d    3.0\\n     e    4.0\\n     dtype: float64, array([0, 2, 4, 6, 8]))\\n\\n    Use `drop` optional when bins is not unique\\n\\n    >>> pd.cut(s, [0, 2, 4, 6, 10, 10], labels=False, retbins=True,\\n    ...        right=False, duplicates=\\'drop\\')\\n    ... # doctest: +ELLIPSIS\\n    (a    0.0\\n     b    1.0\\n     c    2.0\\n     d    3.0\\n     e    3.0\\n     dtype: float64, array([0, 2, 4, 6, 8]))\\n\\n    Passing an IntervalIndex for `bins` results in those categories exactly.\\n    Notice that values not covered by the IntervalIndex are set to NaN. 0\\n    is to the left of the first bin (which is closed on the right), and 1.5\\n    falls between two bins.\\n\\n    >>> bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])\\n    >>> pd.cut([0, 0.5, 1.5, 2.5, 4.5], bins)\\n    [NaN, (0, 1], NaN, (2, 3], (4, 5]]\\n    Categories (3, interval[int64]): [(0, 1] < (2, 3] < (4, 5]]',\n",
       "  'code': 'def cut(x, bins, right=True, labels=None, retbins=False, precision=3,\\n        include_lowest=False, duplicates=\\'raise\\'):\\n    \"\"\"\\n    Bin values into discrete intervals.\\n\\n    Use `cut` when you need to segment and sort data values into bins. This\\n    function is also useful for going from a continuous variable to a\\n    categorical variable. For example, `cut` could convert ages to groups of\\n    age ranges. Supports binning into an equal number of bins, or a\\n    pre-specified array of bins.\\n\\n    Parameters\\n    ----------\\n    x : array-like\\n        The input array to be binned. Must be 1-dimensional.\\n    bins : int, sequence of scalars, or IntervalIndex\\n        The criteria to bin by.\\n\\n        * int : Defines the number of equal-width bins in the range of `x`. The\\n          range of `x` is extended by .1% on each side to include the minimum\\n          and maximum values of `x`.\\n        * sequence of scalars : Defines the bin edges allowing for non-uniform\\n          width. No extension of the range of `x` is done.\\n        * IntervalIndex : Defines the exact bins to be used. Note that\\n          IntervalIndex for `bins` must be non-overlapping.\\n\\n    right : bool, default True\\n        Indicates whether `bins` includes the rightmost edge or not. If\\n        ``right == True`` (the default), then the `bins` ``[1, 2, 3, 4]``\\n        indicate (1,2], (2,3], (3,4]. This argument is ignored when\\n        `bins` is an IntervalIndex.\\n    labels : array or bool, optional\\n        Specifies the labels for the returned bins. Must be the same length as\\n        the resulting bins. If False, returns only integer indicators of the\\n        bins. This affects the type of the output container (see below).\\n        This argument is ignored when `bins` is an IntervalIndex.\\n    retbins : bool, default False\\n        Whether to return the bins or not. Useful when bins is provided\\n        as a scalar.\\n    precision : int, default 3\\n        The precision at which to store and display the bins labels.\\n    include_lowest : bool, default False\\n        Whether the first interval should be left-inclusive or not.\\n    duplicates : {default \\'raise\\', \\'drop\\'}, optional\\n        If bin edges are not unique, raise ValueError or drop non-uniques.\\n\\n        .. versionadded:: 0.23.0\\n\\n    Returns\\n    -------\\n    out : Categorical, Series, or ndarray\\n        An array-like object representing the respective bin for each value\\n        of `x`. The type depends on the value of `labels`.\\n\\n        * True (default) : returns a Series for Series `x` or a\\n          Categorical for all other inputs. The values stored within\\n          are Interval dtype.\\n\\n        * sequence of scalars : returns a Series for Series `x` or a\\n          Categorical for all other inputs. The values stored within\\n          are whatever the type in the sequence is.\\n\\n        * False : returns an ndarray of integers.\\n\\n    bins : numpy.ndarray or IntervalIndex.\\n        The computed or specified bins. Only returned when `retbins=True`.\\n        For scalar or sequence `bins`, this is an ndarray with the computed\\n        bins. If set `duplicates=drop`, `bins` will drop non-unique bin. For\\n        an IntervalIndex `bins`, this is equal to `bins`.\\n\\n    See Also\\n    --------\\n    qcut : Discretize variable into equal-sized buckets based on rank\\n        or based on sample quantiles.\\n    Categorical : Array type for storing data that come from a\\n        fixed set of values.\\n    Series : One-dimensional array with axis labels (including time series).\\n    IntervalIndex : Immutable Index implementing an ordered, sliceable set.\\n\\n    Notes\\n    -----\\n    Any NA values will be NA in the result. Out of bounds values will be NA in\\n    the resulting Series or Categorical object.\\n\\n    Examples\\n    --------\\n    Discretize into three equal-sized bins.\\n\\n    >>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3)\\n    ... # doctest: +ELLIPSIS\\n    [(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\\n    Categories (3, interval[float64]): [(0.994, 3.0] < (3.0, 5.0] ...\\n\\n    >>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, retbins=True)\\n    ... # doctest: +ELLIPSIS\\n    ([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\\n    Categories (3, interval[float64]): [(0.994, 3.0] < (3.0, 5.0] ...\\n    array([0.994, 3.   , 5.   , 7.   ]))\\n\\n    Discovers the same bins, but assign them specific labels. Notice that\\n    the returned Categorical\\'s categories are `labels` and is ordered.\\n\\n    >>> pd.cut(np.array([1, 7, 5, 4, 6, 3]),\\n    ...        3, labels=[\"bad\", \"medium\", \"good\"])\\n    [bad, good, medium, medium, good, bad]\\n    Categories (3, object): [bad < medium < good]\\n\\n    ``labels=False`` implies you just want the bins back.\\n\\n    >>> pd.cut([0, 1, 1, 2], bins=4, labels=False)\\n    array([0, 1, 1, 3])\\n\\n    Passing a Series as an input returns a Series with categorical dtype:\\n\\n    >>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\\n    ...               index=[\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'])\\n    >>> pd.cut(s, 3)\\n    ... # doctest: +ELLIPSIS\\n    a    (1.992, 4.667]\\n    b    (1.992, 4.667]\\n    c    (4.667, 7.333]\\n    d     (7.333, 10.0]\\n    e     (7.333, 10.0]\\n    dtype: category\\n    Categories (3, interval[float64]): [(1.992, 4.667] < (4.667, ...\\n\\n    Passing a Series as an input returns a Series with mapping value.\\n    It is used to map numerically to intervals based on bins.\\n\\n    >>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\\n    ...               index=[\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'])\\n    >>> pd.cut(s, [0, 2, 4, 6, 8, 10], labels=False, retbins=True, right=False)\\n    ... # doctest: +ELLIPSIS\\n    (a    0.0\\n     b    1.0\\n     c    2.0\\n     d    3.0\\n     e    4.0\\n     dtype: float64, array([0, 2, 4, 6, 8]))\\n\\n    Use `drop` optional when bins is not unique\\n\\n    >>> pd.cut(s, [0, 2, 4, 6, 10, 10], labels=False, retbins=True,\\n    ...        right=False, duplicates=\\'drop\\')\\n    ... # doctest: +ELLIPSIS\\n    (a    0.0\\n     b    1.0\\n     c    2.0\\n     d    3.0\\n     e    3.0\\n     dtype: float64, array([0, 2, 4, 6, 8]))\\n\\n    Passing an IntervalIndex for `bins` results in those categories exactly.\\n    Notice that values not covered by the IntervalIndex are set to NaN. 0\\n    is to the left of the first bin (which is closed on the right), and 1.5\\n    falls between two bins.\\n\\n    >>> bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])\\n    >>> pd.cut([0, 0.5, 1.5, 2.5, 4.5], bins)\\n    [NaN, (0, 1], NaN, (2, 3], (4, 5]]\\n    Categories (3, interval[int64]): [(0, 1] < (2, 3] < (4, 5]]\\n    \"\"\"\\n    # NOTE: this binning code is changed a bit from histogram for var(x) == 0\\n\\n    # for handling the cut for datetime and timedelta objects\\n    x_is_series, series_index, name, x = _preprocess_for_cut(x)\\n    x, dtype = _coerce_to_type(x)\\n\\n    if not np.iterable(bins):\\n        if is_scalar(bins) and bins < 1:\\n            raise ValueError(\"`bins` should be a positive integer.\")\\n\\n        try:  # for array-like\\n            sz = x.size\\n        except AttributeError:\\n            x = np.asarray(x)\\n            sz = x.size\\n\\n        if sz == 0:\\n            raise ValueError(\\'Cannot cut empty array\\')\\n\\n        rng = (nanops.nanmin(x), nanops.nanmax(x))\\n        mn, mx = [mi + 0.0 for mi in rng]\\n\\n        if np.isinf(mn) or np.isinf(mx):\\n            # GH 24314\\n            raise ValueError(\\'cannot specify integer `bins` when input data \\'\\n                             \\'contains infinity\\')\\n        elif mn == mx:  # adjust end points before binning\\n            mn -= .001 * abs(mn) if mn != 0 else .001\\n            mx += .001 * abs(mx) if mx != 0 else .001\\n            bins = np.linspace(mn, mx, bins + 1, endpoint=True)\\n        else:  # adjust end points after binning\\n            bins = np.linspace(mn, mx, bins + 1, endpoint=True)\\n            adj = (mx - mn) * 0.001  # 0.1% of the range\\n            if right:\\n                bins[0] -= adj\\n            else:\\n                bins[-1] += adj\\n\\n    elif isinstance(bins, IntervalIndex):\\n        if bins.is_overlapping:\\n            raise ValueError(\\'Overlapping IntervalIndex is not accepted.\\')\\n\\n    else:\\n        if is_datetime64tz_dtype(bins):\\n            bins = np.asarray(bins, dtype=_NS_DTYPE)\\n        else:\\n            bins = np.asarray(bins)\\n        bins = _convert_bin_to_numeric_type(bins, dtype)\\n\\n        # GH 26045: cast to float64 to avoid an overflow\\n        if (np.diff(bins.astype(\\'float64\\')) < 0).any():\\n            raise ValueError(\\'bins must increase monotonically.\\')\\n\\n    fac, bins = _bins_to_cuts(x, bins, right=right, labels=labels,\\n                              precision=precision,\\n                              include_lowest=include_lowest,\\n                              dtype=dtype,\\n                              duplicates=duplicates)\\n\\n    return _postprocess_for_cut(fac, bins, retbins, x_is_series,\\n                                series_index, name, dtype)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_preprocess_for_cut',\n",
       "  'docstring': 'handles preprocessing for cut where we convert passed\\n    input to array, strip the index information and store it\\n    separately',\n",
       "  'code': 'def _preprocess_for_cut(x):\\n    \"\"\"\\n    handles preprocessing for cut where we convert passed\\n    input to array, strip the index information and store it\\n    separately\\n    \"\"\"\\n    x_is_series = isinstance(x, Series)\\n    series_index = None\\n    name = None\\n\\n    if x_is_series:\\n        series_index = x.index\\n        name = x.name\\n\\n    # Check that the passed array is a Pandas or Numpy object\\n    # We don\\'t want to strip away a Pandas data-type here (e.g. datetimetz)\\n    ndim = getattr(x, \\'ndim\\', None)\\n    if ndim is None:\\n        x = np.asarray(x)\\n    if x.ndim != 1:\\n        raise ValueError(\"Input array must be 1 dimensional\")\\n\\n    return x_is_series, series_index, name, x',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_check_for_default_values',\n",
       "  'docstring': 'Check that the keys in `arg_val_dict` are mapped to their\\n    default values as specified in `compat_args`.\\n\\n    Note that this function is to be called only when it has been\\n    checked that arg_val_dict.keys() is a subset of compat_args',\n",
       "  'code': 'def _check_for_default_values(fname, arg_val_dict, compat_args):\\n    \"\"\"\\n    Check that the keys in `arg_val_dict` are mapped to their\\n    default values as specified in `compat_args`.\\n\\n    Note that this function is to be called only when it has been\\n    checked that arg_val_dict.keys() is a subset of compat_args\\n\\n    \"\"\"\\n    for key in arg_val_dict:\\n        # try checking equality directly with \\'=\\' operator,\\n        # as comparison may have been overridden for the left\\n        # hand object\\n        try:\\n            v1 = arg_val_dict[key]\\n            v2 = compat_args[key]\\n\\n            # check for None-ness otherwise we could end up\\n            # comparing a numpy array vs None\\n            if (v1 is not None and v2 is None) or \\\\\\n               (v1 is None and v2 is not None):\\n                match = False\\n            else:\\n                match = (v1 == v2)\\n\\n            if not is_bool(match):\\n                raise ValueError(\"\\'match\\' is not a boolean\")\\n\\n        # could not compare them directly, so try comparison\\n        # using the \\'is\\' operator\\n        except ValueError:\\n            match = (arg_val_dict[key] is compat_args[key])\\n\\n        if not match:\\n            raise ValueError((\"the \\'{arg}\\' parameter is not \"\\n                              \"supported in the pandas \"\\n                              \"implementation of {fname}()\".\\n                              format(fname=fname, arg=key)))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'validate_args',\n",
       "  'docstring': 'Checks whether the length of the `*args` argument passed into a function\\n    has at most `len(compat_args)` arguments and whether or not all of these\\n    elements in `args` are set to their default values.\\n\\n    fname: str\\n        The name of the function being passed the `*args` parameter\\n\\n    args: tuple\\n        The `*args` parameter passed into a function\\n\\n    max_fname_arg_count: int\\n        The maximum number of arguments that the function `fname`\\n        can accept, excluding those in `args`. Used for displaying\\n        appropriate error messages. Must be non-negative.\\n\\n    compat_args: OrderedDict\\n        A ordered dictionary of keys and their associated default values.\\n        In order to accommodate buggy behaviour in some versions of `numpy`,\\n        where a signature displayed keyword arguments but then passed those\\n        arguments **positionally** internally when calling downstream\\n        implementations, an ordered dictionary ensures that the original\\n        order of the keyword arguments is enforced. Note that if there is\\n        only one key, a generic dict can be passed in as well.\\n\\n    Raises\\n    ------\\n    TypeError if `args` contains more values than there are `compat_args`\\n    ValueError if `args` contains values that do not correspond to those\\n    of the default values specified in `compat_args`',\n",
       "  'code': 'def validate_args(fname, args, max_fname_arg_count, compat_args):\\n    \"\"\"\\n    Checks whether the length of the `*args` argument passed into a function\\n    has at most `len(compat_args)` arguments and whether or not all of these\\n    elements in `args` are set to their default values.\\n\\n    fname: str\\n        The name of the function being passed the `*args` parameter\\n\\n    args: tuple\\n        The `*args` parameter passed into a function\\n\\n    max_fname_arg_count: int\\n        The maximum number of arguments that the function `fname`\\n        can accept, excluding those in `args`. Used for displaying\\n        appropriate error messages. Must be non-negative.\\n\\n    compat_args: OrderedDict\\n        A ordered dictionary of keys and their associated default values.\\n        In order to accommodate buggy behaviour in some versions of `numpy`,\\n        where a signature displayed keyword arguments but then passed those\\n        arguments **positionally** internally when calling downstream\\n        implementations, an ordered dictionary ensures that the original\\n        order of the keyword arguments is enforced. Note that if there is\\n        only one key, a generic dict can be passed in as well.\\n\\n    Raises\\n    ------\\n    TypeError if `args` contains more values than there are `compat_args`\\n    ValueError if `args` contains values that do not correspond to those\\n    of the default values specified in `compat_args`\\n\\n    \"\"\"\\n    _check_arg_length(fname, args, max_fname_arg_count, compat_args)\\n\\n    # We do this so that we can provide a more informative\\n    # error message about the parameters that we are not\\n    # supporting in the pandas implementation of \\'fname\\'\\n    kwargs = dict(zip(compat_args, args))\\n    _check_for_default_values(fname, kwargs, compat_args)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'hash_array',\n",
       "  'docstring': \"Given a 1d array, return an array of deterministic integers.\\n\\n    .. versionadded:: 0.19.2\\n\\n    Parameters\\n    ----------\\n    vals : ndarray, Categorical\\n    encoding : string, default 'utf8'\\n        encoding for data & key when strings\\n    hash_key : string key to encode, default to _default_hash_key\\n    categorize : bool, default True\\n        Whether to first categorize object arrays before hashing. This is more\\n        efficient when the array contains duplicate values.\\n\\n        .. versionadded:: 0.20.0\\n\\n    Returns\\n    -------\\n    1d uint64 numpy array of hash values, same length as the vals\",\n",
       "  'code': 'def hash_array(vals, encoding=\\'utf8\\', hash_key=None, categorize=True):\\n    \"\"\"\\n    Given a 1d array, return an array of deterministic integers.\\n\\n    .. versionadded:: 0.19.2\\n\\n    Parameters\\n    ----------\\n    vals : ndarray, Categorical\\n    encoding : string, default \\'utf8\\'\\n        encoding for data & key when strings\\n    hash_key : string key to encode, default to _default_hash_key\\n    categorize : bool, default True\\n        Whether to first categorize object arrays before hashing. This is more\\n        efficient when the array contains duplicate values.\\n\\n        .. versionadded:: 0.20.0\\n\\n    Returns\\n    -------\\n    1d uint64 numpy array of hash values, same length as the vals\\n    \"\"\"\\n\\n    if not hasattr(vals, \\'dtype\\'):\\n        raise TypeError(\"must pass a ndarray-like\")\\n    dtype = vals.dtype\\n\\n    if hash_key is None:\\n        hash_key = _default_hash_key\\n\\n    # For categoricals, we hash the categories, then remap the codes to the\\n    # hash values. (This check is above the complex check so that we don\\'t ask\\n    # numpy if categorical is a subdtype of complex, as it will choke).\\n    if is_categorical_dtype(dtype):\\n        return _hash_categorical(vals, encoding, hash_key)\\n    elif is_extension_array_dtype(dtype):\\n        vals, _ = vals._values_for_factorize()\\n        dtype = vals.dtype\\n\\n    # we\\'ll be working with everything as 64-bit values, so handle this\\n    # 128-bit value early\\n    if np.issubdtype(dtype, np.complex128):\\n        return hash_array(vals.real) + 23 * hash_array(vals.imag)\\n\\n    # First, turn whatever array this is into unsigned 64-bit ints, if we can\\n    # manage it.\\n    elif isinstance(dtype, np.bool):\\n        vals = vals.astype(\\'u8\\')\\n    elif issubclass(dtype.type, (np.datetime64, np.timedelta64)):\\n        vals = vals.view(\\'i8\\').astype(\\'u8\\', copy=False)\\n    elif issubclass(dtype.type, np.number) and dtype.itemsize <= 8:\\n        vals = vals.view(\\'u{}\\'.format(vals.dtype.itemsize)).astype(\\'u8\\')\\n    else:\\n        # With repeated values, its MUCH faster to categorize object dtypes,\\n        # then hash and rename categories. We allow skipping the categorization\\n        # when the values are known/likely to be unique.\\n        if categorize:\\n            from pandas import factorize, Categorical, Index\\n            codes, categories = factorize(vals, sort=False)\\n            cat = Categorical(codes, Index(categories),\\n                              ordered=False, fastpath=True)\\n            return _hash_categorical(cat, encoding, hash_key)\\n\\n        try:\\n            vals = hashing.hash_object_array(vals, hash_key, encoding)\\n        except TypeError:\\n            # we have mixed types\\n            vals = hashing.hash_object_array(vals.astype(str).astype(object),\\n                                             hash_key, encoding)\\n\\n    # Then, redistribute these 64-bit ints within the space of 64-bit ints\\n    vals ^= vals >> 30\\n    vals *= np.uint64(0xbf58476d1ce4e5b9)\\n    vals ^= vals >> 27\\n    vals *= np.uint64(0x94d049bb133111eb)\\n    vals ^= vals >> 31\\n    return vals',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_hash_scalar',\n",
       "  'docstring': 'Hash scalar value\\n\\n    Returns\\n    -------\\n    1d uint64 numpy array of hash value, of length 1',\n",
       "  'code': 'def _hash_scalar(val, encoding=\\'utf8\\', hash_key=None):\\n    \"\"\"\\n    Hash scalar value\\n\\n    Returns\\n    -------\\n    1d uint64 numpy array of hash value, of length 1\\n    \"\"\"\\n\\n    if isna(val):\\n        # this is to be consistent with the _hash_categorical implementation\\n        return np.array([np.iinfo(np.uint64).max], dtype=\\'u8\\')\\n\\n    if getattr(val, \\'tzinfo\\', None) is not None:\\n        # for tz-aware datetimes, we need the underlying naive UTC value and\\n        # not the tz aware object or pd extension type (as\\n        # infer_dtype_from_scalar would do)\\n        if not isinstance(val, tslibs.Timestamp):\\n            val = tslibs.Timestamp(val)\\n        val = val.tz_convert(None)\\n\\n    dtype, val = infer_dtype_from_scalar(val)\\n    vals = np.array([val], dtype=dtype)\\n\\n    return hash_array(vals, hash_key=hash_key, encoding=encoding,\\n                      categorize=False)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_dtype_to_default_stata_fmt',\n",
       "  'docstring': 'Map numpy dtype to stata\\'s default format for this type. Not terribly\\n    important since users can change this in Stata. Semantics are\\n\\n    object  -> \"%DDs\" where DD is the length of the string.  If not a string,\\n                raise ValueError\\n    float64 -> \"%10.0g\"\\n    float32 -> \"%9.0g\"\\n    int64   -> \"%9.0g\"\\n    int32   -> \"%12.0g\"\\n    int16   -> \"%8.0g\"\\n    int8    -> \"%8.0g\"\\n    strl    -> \"%9s\"',\n",
       "  'code': 'def _dtype_to_default_stata_fmt(dtype, column, dta_version=114,\\n                                force_strl=False):\\n    \"\"\"\\n    Map numpy dtype to stata\\'s default format for this type. Not terribly\\n    important since users can change this in Stata. Semantics are\\n\\n    object  -> \"%DDs\" where DD is the length of the string.  If not a string,\\n                raise ValueError\\n    float64 -> \"%10.0g\"\\n    float32 -> \"%9.0g\"\\n    int64   -> \"%9.0g\"\\n    int32   -> \"%12.0g\"\\n    int16   -> \"%8.0g\"\\n    int8    -> \"%8.0g\"\\n    strl    -> \"%9s\"\\n    \"\"\"\\n    # TODO: Refactor to combine type with format\\n    # TODO: expand this to handle a default datetime format?\\n    if dta_version < 117:\\n        max_str_len = 244\\n    else:\\n        max_str_len = 2045\\n        if force_strl:\\n            return \\'%9s\\'\\n    if dtype.type == np.object_:\\n        inferred_dtype = infer_dtype(column, skipna=True)\\n        if not (inferred_dtype in (\\'string\\', \\'unicode\\') or\\n                len(column) == 0):\\n            raise ValueError(\\'Column `{col}` cannot be exported.\\\\n\\\\nOnly \\'\\n                             \\'string-like object arrays containing all \\'\\n                             \\'strings or a mix of strings and None can be \\'\\n                             \\'exported. Object arrays containing only null \\'\\n                             \\'values are prohibited. Other object types\\'\\n                             \\'cannot be exported and must first be converted \\'\\n                             \\'to one of the supported \\'\\n                             \\'types.\\'.format(col=column.name))\\n        itemsize = max_len_string_array(ensure_object(column.values))\\n        if itemsize > max_str_len:\\n            if dta_version >= 117:\\n                return \\'%9s\\'\\n            else:\\n                raise ValueError(excessive_string_length_error % column.name)\\n        return \"%\" + str(max(itemsize, 1)) + \"s\"\\n    elif dtype == np.float64:\\n        return \"%10.0g\"\\n    elif dtype == np.float32:\\n        return \"%9.0g\"\\n    elif dtype == np.int32:\\n        return \"%12.0g\"\\n    elif dtype == np.int8 or dtype == np.int16:\\n        return \"%8.0g\"\\n    else:  # pragma : no cover\\n        raise NotImplementedError(\\n            \"Data type {dtype} not supported.\".format(dtype=dtype))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'StataReader._setup_dtype',\n",
       "  'docstring': 'Map between numpy and state dtypes',\n",
       "  'code': 'def _setup_dtype(self):\\n        \"\"\"Map between numpy and state dtypes\"\"\"\\n        if self._dtype is not None:\\n            return self._dtype\\n\\n        dtype = []  # Convert struct data types to numpy data type\\n        for i, typ in enumerate(self.typlist):\\n            if typ in self.NUMPY_TYPE_MAP:\\n                dtype.append((\\'s\\' + str(i), self.byteorder +\\n                              self.NUMPY_TYPE_MAP[typ]))\\n            else:\\n                dtype.append((\\'s\\' + str(i), \\'S\\' + str(typ)))\\n        dtype = np.dtype(dtype)\\n        self._dtype = dtype\\n\\n        return self._dtype',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'convert_json_field_to_pandas_type',\n",
       "  'docstring': \"Converts a JSON field descriptor into its corresponding NumPy / pandas type\\n\\n    Parameters\\n    ----------\\n    field\\n        A JSON field descriptor\\n\\n    Returns\\n    -------\\n    dtype\\n\\n    Raises\\n    -----\\n    ValueError\\n        If the type of the provided field is unknown or currently unsupported\\n\\n    Examples\\n    --------\\n    >>> convert_json_field_to_pandas_type({'name': 'an_int',\\n                                           'type': 'integer'})\\n    'int64'\\n    >>> convert_json_field_to_pandas_type({'name': 'a_categorical',\\n                                           'type': 'any',\\n                                           'contraints': {'enum': [\\n                                                          'a', 'b', 'c']},\\n                                           'ordered': True})\\n    'CategoricalDtype(categories=['a', 'b', 'c'], ordered=True)'\\n    >>> convert_json_field_to_pandas_type({'name': 'a_datetime',\\n                                           'type': 'datetime'})\\n    'datetime64[ns]'\\n    >>> convert_json_field_to_pandas_type({'name': 'a_datetime_with_tz',\\n                                           'type': 'datetime',\\n                                           'tz': 'US/Central'})\\n    'datetime64[ns, US/Central]'\",\n",
       "  'code': 'def convert_json_field_to_pandas_type(field):\\n    \"\"\"\\n    Converts a JSON field descriptor into its corresponding NumPy / pandas type\\n\\n    Parameters\\n    ----------\\n    field\\n        A JSON field descriptor\\n\\n    Returns\\n    -------\\n    dtype\\n\\n    Raises\\n    -----\\n    ValueError\\n        If the type of the provided field is unknown or currently unsupported\\n\\n    Examples\\n    --------\\n    >>> convert_json_field_to_pandas_type({\\'name\\': \\'an_int\\',\\n                                           \\'type\\': \\'integer\\'})\\n    \\'int64\\'\\n    >>> convert_json_field_to_pandas_type({\\'name\\': \\'a_categorical\\',\\n                                           \\'type\\': \\'any\\',\\n                                           \\'contraints\\': {\\'enum\\': [\\n                                                          \\'a\\', \\'b\\', \\'c\\']},\\n                                           \\'ordered\\': True})\\n    \\'CategoricalDtype(categories=[\\'a\\', \\'b\\', \\'c\\'], ordered=True)\\'\\n    >>> convert_json_field_to_pandas_type({\\'name\\': \\'a_datetime\\',\\n                                           \\'type\\': \\'datetime\\'})\\n    \\'datetime64[ns]\\'\\n    >>> convert_json_field_to_pandas_type({\\'name\\': \\'a_datetime_with_tz\\',\\n                                           \\'type\\': \\'datetime\\',\\n                                           \\'tz\\': \\'US/Central\\'})\\n    \\'datetime64[ns, US/Central]\\'\\n    \"\"\"\\n    typ = field[\\'type\\']\\n    if typ == \\'string\\':\\n        return \\'object\\'\\n    elif typ == \\'integer\\':\\n        return \\'int64\\'\\n    elif typ == \\'number\\':\\n        return \\'float64\\'\\n    elif typ == \\'boolean\\':\\n        return \\'bool\\'\\n    elif typ == \\'duration\\':\\n        return \\'timedelta64\\'\\n    elif typ == \\'datetime\\':\\n        if field.get(\\'tz\\'):\\n            return \\'datetime64[ns, {tz}]\\'.format(tz=field[\\'tz\\'])\\n        else:\\n            return \\'datetime64[ns]\\'\\n    elif typ == \\'any\\':\\n        if \\'constraints\\' in field and \\'ordered\\' in field:\\n            return CategoricalDtype(categories=field[\\'constraints\\'][\\'enum\\'],\\n                                    ordered=field[\\'ordered\\'])\\n        else:\\n            return \\'object\\'\\n\\n    raise ValueError(\"Unsupported or invalid field type: {}\".format(typ))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'maybe_upcast_for_op',\n",
       "  'docstring': 'Cast non-pandas objects to pandas types to unify behavior of arithmetic\\n    and comparison operations.\\n\\n    Parameters\\n    ----------\\n    obj: object\\n\\n    Returns\\n    -------\\n    out : object\\n\\n    Notes\\n    -----\\n    Be careful to call this *after* determining the `name` attribute to be\\n    attached to the result of the arithmetic operation.',\n",
       "  'code': 'def maybe_upcast_for_op(obj):\\n    \"\"\"\\n    Cast non-pandas objects to pandas types to unify behavior of arithmetic\\n    and comparison operations.\\n\\n    Parameters\\n    ----------\\n    obj: object\\n\\n    Returns\\n    -------\\n    out : object\\n\\n    Notes\\n    -----\\n    Be careful to call this *after* determining the `name` attribute to be\\n    attached to the result of the arithmetic operation.\\n    \"\"\"\\n    if type(obj) is datetime.timedelta:\\n        # GH#22390  cast up to Timedelta to rely on Timedelta\\n        # implementation; otherwise operation against numeric-dtype\\n        # raises TypeError\\n        return pd.Timedelta(obj)\\n    elif isinstance(obj, np.timedelta64) and not isna(obj):\\n        # In particular non-nanosecond timedelta64 needs to be cast to\\n        #  nanoseconds, or else we get undesired behavior like\\n        #  np.timedelta64(3, \\'D\\') / 2 == np.timedelta64(1, \\'D\\')\\n        # The isna check is to avoid casting timedelta64(\"NaT\"), which would\\n        #  return NaT and incorrectly be treated as a datetime-NaT.\\n        return pd.Timedelta(obj)\\n    elif isinstance(obj, np.ndarray) and is_timedelta64_dtype(obj):\\n        # GH#22390 Unfortunately we need to special-case right-hand\\n        # timedelta64 dtypes because numpy casts integer dtypes to\\n        # timedelta64 when operating with timedelta64\\n        return pd.TimedeltaIndex(obj)\\n    return obj',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'should_series_dispatch',\n",
       "  'docstring': 'Identify cases where a DataFrame operation should dispatch to its\\n    Series counterpart.\\n\\n    Parameters\\n    ----------\\n    left : DataFrame\\n    right : DataFrame\\n    op : binary operator\\n\\n    Returns\\n    -------\\n    override : bool',\n",
       "  'code': 'def should_series_dispatch(left, right, op):\\n    \"\"\"\\n    Identify cases where a DataFrame operation should dispatch to its\\n    Series counterpart.\\n\\n    Parameters\\n    ----------\\n    left : DataFrame\\n    right : DataFrame\\n    op : binary operator\\n\\n    Returns\\n    -------\\n    override : bool\\n    \"\"\"\\n    if left._is_mixed_type or right._is_mixed_type:\\n        return True\\n\\n    if not len(left.columns) or not len(right.columns):\\n        # ensure obj.dtypes[0] exists for each obj\\n        return False\\n\\n    ldtype = left.dtypes.iloc[0]\\n    rdtype = right.dtypes.iloc[0]\\n\\n    if ((is_timedelta64_dtype(ldtype) and is_integer_dtype(rdtype)) or\\n            (is_timedelta64_dtype(rdtype) and is_integer_dtype(ldtype))):\\n        # numpy integer dtypes as timedelta64 dtypes in this scenario\\n        return True\\n\\n    if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):\\n        # in particular case where right is an array of DateOffsets\\n        return True\\n\\n    return False',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_comp_method_SERIES',\n",
       "  'docstring': 'Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.',\n",
       "  'code': 'def _comp_method_SERIES(cls, op, special):\\n    \"\"\"\\n    Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.\\n    \"\"\"\\n    op_name = _get_op_name(op, special)\\n    masker = _gen_eval_kwargs(op_name).get(\\'masker\\', False)\\n\\n    def na_op(x, y):\\n        # TODO:\\n        # should have guarantess on what x, y can be type-wise\\n        # Extension Dtypes are not called here\\n\\n        # Checking that cases that were once handled here are no longer\\n        # reachable.\\n        assert not (is_categorical_dtype(y) and not is_scalar(y))\\n\\n        if is_object_dtype(x.dtype):\\n            result = _comp_method_OBJECT_ARRAY(op, x, y)\\n\\n        elif is_datetimelike_v_numeric(x, y):\\n            return invalid_comparison(x, y, op)\\n\\n        else:\\n\\n            # we want to compare like types\\n            # we only want to convert to integer like if\\n            # we are not NotImplemented, otherwise\\n            # we would allow datetime64 (but viewed as i8) against\\n            # integer comparisons\\n\\n            # we have a datetime/timedelta and may need to convert\\n            assert not needs_i8_conversion(x)\\n            mask = None\\n            if not is_scalar(y) and needs_i8_conversion(y):\\n                mask = isna(x) | isna(y)\\n                y = y.view(\\'i8\\')\\n                x = x.view(\\'i8\\')\\n\\n            method = getattr(x, op_name, None)\\n            if method is not None:\\n                with np.errstate(all=\\'ignore\\'):\\n                    result = method(y)\\n                if result is NotImplemented:\\n                    return invalid_comparison(x, y, op)\\n            else:\\n                result = op(x, y)\\n\\n            if mask is not None and mask.any():\\n                result[mask] = masker\\n\\n        return result\\n\\n    def wrapper(self, other, axis=None):\\n        # Validate the axis parameter\\n        if axis is not None:\\n            self._get_axis_number(axis)\\n\\n        res_name = get_op_result_name(self, other)\\n\\n        if isinstance(other, list):\\n            # TODO: same for tuples?\\n            other = np.asarray(other)\\n\\n        if isinstance(other, ABCDataFrame):  # pragma: no cover\\n            # Defer to DataFrame implementation; fail early\\n            return NotImplemented\\n\\n        elif isinstance(other, ABCSeries) and not self._indexed_same(other):\\n            raise ValueError(\"Can only compare identically-labeled \"\\n                             \"Series objects\")\\n\\n        elif is_categorical_dtype(self):\\n            # Dispatch to Categorical implementation; pd.CategoricalIndex\\n            # behavior is non-canonical GH#19513\\n            res_values = dispatch_to_index_op(op, self, other, pd.Categorical)\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name)\\n\\n        elif is_datetime64_dtype(self) or is_datetime64tz_dtype(self):\\n            # Dispatch to DatetimeIndex to ensure identical\\n            # Series/Index behavior\\n            if (isinstance(other, datetime.date) and\\n                    not isinstance(other, datetime.datetime)):\\n                # https://github.com/pandas-dev/pandas/issues/21152\\n                # Compatibility for difference between Series comparison w/\\n                # datetime and date\\n                msg = (\\n                    \"Comparing Series of datetimes with \\'datetime.date\\'.  \"\\n                    \"Currently, the \\'datetime.date\\' is coerced to a \"\\n                    \"datetime. In the future pandas will not coerce, \"\\n                    \"and {future}. \"\\n                    \"To retain the current behavior, \"\\n                    \"convert the \\'datetime.date\\' to a datetime with \"\\n                    \"\\'pd.Timestamp\\'.\"\\n                )\\n\\n                if op in {operator.lt, operator.le, operator.gt, operator.ge}:\\n                    future = \"a TypeError will be raised\"\\n                else:\\n                    future = (\\n                        \"\\'the values will not compare equal to the \"\\n                        \"\\'datetime.date\\'\"\\n                    )\\n                msg = \\'\\\\n\\'.join(textwrap.wrap(msg.format(future=future)))\\n                warnings.warn(msg, FutureWarning, stacklevel=2)\\n                other = pd.Timestamp(other)\\n\\n            res_values = dispatch_to_index_op(op, self, other,\\n                                              pd.DatetimeIndex)\\n\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name)\\n\\n        elif is_timedelta64_dtype(self):\\n            res_values = dispatch_to_index_op(op, self, other,\\n                                              pd.TimedeltaIndex)\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name)\\n\\n        elif (is_extension_array_dtype(self) or\\n              (is_extension_array_dtype(other) and not is_scalar(other))):\\n            # Note: the `not is_scalar(other)` condition rules out\\n            # e.g. other == \"category\"\\n            return dispatch_to_extension_op(op, self, other)\\n\\n        elif isinstance(other, ABCSeries):\\n            # By this point we have checked that self._indexed_same(other)\\n            res_values = na_op(self.values, other.values)\\n            # rename is needed in case res_name is None and res_values.name\\n            # is not.\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name).rename(res_name)\\n\\n        elif isinstance(other, (np.ndarray, pd.Index)):\\n            # do not check length of zerodim array\\n            # as it will broadcast\\n            if other.ndim != 0 and len(self) != len(other):\\n                raise ValueError(\\'Lengths must match to compare\\')\\n\\n            res_values = na_op(self.values, np.asarray(other))\\n            result = self._constructor(res_values, index=self.index)\\n            # rename is needed in case res_name is None and self.name\\n            # is not.\\n            return result.__finalize__(self).rename(res_name)\\n\\n        elif is_scalar(other) and isna(other):\\n            # numpy does not like comparisons vs None\\n            if op is operator.ne:\\n                res_values = np.ones(len(self), dtype=bool)\\n            else:\\n                res_values = np.zeros(len(self), dtype=bool)\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name, dtype=\\'bool\\')\\n\\n        else:\\n            values = self.get_values()\\n\\n            with np.errstate(all=\\'ignore\\'):\\n                res = na_op(values, other)\\n            if is_scalar(res):\\n                raise TypeError(\\'Could not compare {typ} type with Series\\'\\n                                .format(typ=type(other)))\\n\\n            # always return a full value series here\\n            res_values = com.values_from_object(res)\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name, dtype=\\'bool\\')\\n\\n    wrapper.__name__ = op_name\\n    return wrapper',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatetimeLikeArrayMixin.repeat',\n",
       "  'docstring': 'Repeat elements of an array.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.repeat',\n",
       "  'code': 'def repeat(self, repeats, *args, **kwargs):\\n        \"\"\"\\n        Repeat elements of an array.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.repeat\\n        \"\"\"\\n        nv.validate_repeat(args, kwargs)\\n        values = self._data.repeat(repeats)\\n        return type(self)(values.view(\\'i8\\'), dtype=self.dtype)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatetimeLikeArrayMixin._add_delta',\n",
       "  'docstring': \"Add a timedelta-like, Tick or TimedeltaIndex-like object\\n        to self, yielding an int64 numpy array\\n\\n        Parameters\\n        ----------\\n        delta : {timedelta, np.timedelta64, Tick,\\n                 TimedeltaIndex, ndarray[timedelta64]}\\n\\n        Returns\\n        -------\\n        result : ndarray[int64]\\n\\n        Notes\\n        -----\\n        The result's name is set outside of _add_delta by the calling\\n        method (__add__ or __sub__), if necessary (i.e. for Indexes).\",\n",
       "  'code': 'def _add_delta(self, other):\\n        \"\"\"\\n        Add a timedelta-like, Tick or TimedeltaIndex-like object\\n        to self, yielding an int64 numpy array\\n\\n        Parameters\\n        ----------\\n        delta : {timedelta, np.timedelta64, Tick,\\n                 TimedeltaIndex, ndarray[timedelta64]}\\n\\n        Returns\\n        -------\\n        result : ndarray[int64]\\n\\n        Notes\\n        -----\\n        The result\\'s name is set outside of _add_delta by the calling\\n        method (__add__ or __sub__), if necessary (i.e. for Indexes).\\n        \"\"\"\\n        if isinstance(other, (Tick, timedelta, np.timedelta64)):\\n            new_values = self._add_timedeltalike_scalar(other)\\n        elif is_timedelta64_dtype(other):\\n            # ndarray[timedelta64] or TimedeltaArray/index\\n            new_values = self._add_delta_tdi(other)\\n\\n        return new_values',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatetimeLikeArrayMixin._addsub_offset_array',\n",
       "  'docstring': 'Add or subtract array-like of DateOffset objects\\n\\n        Parameters\\n        ----------\\n        other : Index, np.ndarray\\n            object-dtype containing pd.DateOffset objects\\n        op : {operator.add, operator.sub}\\n\\n        Returns\\n        -------\\n        result : same class as self',\n",
       "  'code': 'def _addsub_offset_array(self, other, op):\\n        \"\"\"\\n        Add or subtract array-like of DateOffset objects\\n\\n        Parameters\\n        ----------\\n        other : Index, np.ndarray\\n            object-dtype containing pd.DateOffset objects\\n        op : {operator.add, operator.sub}\\n\\n        Returns\\n        -------\\n        result : same class as self\\n        \"\"\"\\n        assert op in [operator.add, operator.sub]\\n        if len(other) == 1:\\n            return op(self, other[0])\\n\\n        warnings.warn(\"Adding/subtracting array of DateOffsets to \"\\n                      \"{cls} not vectorized\"\\n                      .format(cls=type(self).__name__), PerformanceWarning)\\n\\n        # For EA self.astype(\\'O\\') returns a numpy array, not an Index\\n        left = lib.values_from_object(self.astype(\\'O\\'))\\n\\n        res_values = op(left, np.array(other))\\n        kwargs = {}\\n        if not is_period_dtype(self):\\n            kwargs[\\'freq\\'] = \\'infer\\'\\n        return self._from_sequence(res_values, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatetimeLikeArrayMixin.min',\n",
       "  'docstring': 'Return the minimum value of the Array or minimum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.min\\n        Index.min : Return the minimum value in an Index.\\n        Series.min : Return the minimum value in a Series.',\n",
       "  'code': 'def min(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Return the minimum value of the Array or minimum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.min\\n        Index.min : Return the minimum value in an Index.\\n        Series.min : Return the minimum value in a Series.\\n        \"\"\"\\n        nv.validate_min(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())\\n        if isna(result):\\n            # Period._from_ordinal does not handle np.nan gracefully\\n            return NaT\\n        return self._box_func(result)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatetimeLikeArrayMixin.max',\n",
       "  'docstring': 'Return the maximum value of the Array or maximum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.max\\n        Index.max : Return the maximum value in an Index.\\n        Series.max : Return the maximum value in a Series.',\n",
       "  'code': 'def max(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Return the maximum value of the Array or maximum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.max\\n        Index.max : Return the maximum value in an Index.\\n        Series.max : Return the maximum value in a Series.\\n        \"\"\"\\n        # TODO: skipna is broken with max.\\n        # See https://github.com/pandas-dev/pandas/issues/24265\\n        nv.validate_max(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        mask = self.isna()\\n        if skipna:\\n            values = self[~mask].asi8\\n        elif mask.any():\\n            return NaT\\n        else:\\n            values = self.asi8\\n\\n        if not len(values):\\n            # short-circut for empty max / min\\n            return NaT\\n\\n        result = nanops.nanmax(values, skipna=skipna)\\n        # Don\\'t have to worry about NA `result`, since no NA went in.\\n        return self._box_func(result)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'PeriodArray._check_timedeltalike_freq_compat',\n",
       "  'docstring': 'Arithmetic operations with timedelta-like scalars or array `other`\\n        are only valid if `other` is an integer multiple of `self.freq`.\\n        If the operation is valid, find that integer multiple.  Otherwise,\\n        raise because the operation is invalid.\\n\\n        Parameters\\n        ----------\\n        other : timedelta, np.timedelta64, Tick,\\n                ndarray[timedelta64], TimedeltaArray, TimedeltaIndex\\n\\n        Returns\\n        -------\\n        multiple : int or ndarray[int64]\\n\\n        Raises\\n        ------\\n        IncompatibleFrequency',\n",
       "  'code': 'def _check_timedeltalike_freq_compat(self, other):\\n        \"\"\"\\n        Arithmetic operations with timedelta-like scalars or array `other`\\n        are only valid if `other` is an integer multiple of `self.freq`.\\n        If the operation is valid, find that integer multiple.  Otherwise,\\n        raise because the operation is invalid.\\n\\n        Parameters\\n        ----------\\n        other : timedelta, np.timedelta64, Tick,\\n                ndarray[timedelta64], TimedeltaArray, TimedeltaIndex\\n\\n        Returns\\n        -------\\n        multiple : int or ndarray[int64]\\n\\n        Raises\\n        ------\\n        IncompatibleFrequency\\n        \"\"\"\\n        assert isinstance(self.freq, Tick)  # checked by calling function\\n        own_offset = frequencies.to_offset(self.freq.rule_code)\\n        base_nanos = delta_to_nanoseconds(own_offset)\\n\\n        if isinstance(other, (timedelta, np.timedelta64, Tick)):\\n            nanos = delta_to_nanoseconds(other)\\n\\n        elif isinstance(other, np.ndarray):\\n            # numpy timedelta64 array; all entries must be compatible\\n            assert other.dtype.kind == \\'m\\'\\n            if other.dtype != _TD_DTYPE:\\n                # i.e. non-nano unit\\n                # TODO: disallow unit-less timedelta64\\n                other = other.astype(_TD_DTYPE)\\n            nanos = other.view(\\'i8\\')\\n        else:\\n            # TimedeltaArray/Index\\n            nanos = other.asi8\\n\\n        if np.all(nanos % base_nanos == 0):\\n            # nanos being added is an integer multiple of the\\n            #  base-frequency to self.freq\\n            delta = nanos // base_nanos\\n            # delta is the integer (or integer-array) number of periods\\n            # by which will be added to self.\\n            return delta\\n\\n        _raise_on_incompatible(self, other)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_use_inf_as_na',\n",
       "  'docstring': 'Option change callback for na/inf behaviour\\n    Choose which replacement for numpy.isnan / -numpy.isfinite is used.\\n\\n    Parameters\\n    ----------\\n    flag: bool\\n        True means treat None, NaN, INF, -INF as null (old way),\\n        False means None and NaN are null, but INF, -INF are not null\\n        (new way).\\n\\n    Notes\\n    -----\\n    This approach to setting global module values is discussed and\\n    approved here:\\n\\n    * http://stackoverflow.com/questions/4859217/\\n      programmatically-creating-variables-in-python/4859312#4859312',\n",
       "  'code': 'def _use_inf_as_na(key):\\n    \"\"\"Option change callback for na/inf behaviour\\n    Choose which replacement for numpy.isnan / -numpy.isfinite is used.\\n\\n    Parameters\\n    ----------\\n    flag: bool\\n        True means treat None, NaN, INF, -INF as null (old way),\\n        False means None and NaN are null, but INF, -INF are not null\\n        (new way).\\n\\n    Notes\\n    -----\\n    This approach to setting global module values is discussed and\\n    approved here:\\n\\n    * http://stackoverflow.com/questions/4859217/\\n      programmatically-creating-variables-in-python/4859312#4859312\\n    \"\"\"\\n    from pandas._config import get_option\\n    flag = get_option(key)\\n    if flag:\\n        globals()[\\'_isna\\'] = _isna_old\\n    else:\\n        globals()[\\'_isna\\'] = _isna_new',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_isna_compat',\n",
       "  'docstring': 'Parameters\\n    ----------\\n    arr: a numpy array\\n    fill_value: fill value, default to np.nan\\n\\n    Returns\\n    -------\\n    True if we can fill using this fill_value',\n",
       "  'code': 'def _isna_compat(arr, fill_value=np.nan):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    arr: a numpy array\\n    fill_value: fill value, default to np.nan\\n\\n    Returns\\n    -------\\n    True if we can fill using this fill_value\\n    \"\"\"\\n    dtype = arr.dtype\\n    if isna(fill_value):\\n        return not (is_bool_dtype(dtype) or\\n                    is_integer_dtype(dtype))\\n    return True',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'array_equivalent',\n",
       "  'docstring': 'True if two arrays, left and right, have equal non-NaN elements, and NaNs\\n    in corresponding locations.  False otherwise. It is assumed that left and\\n    right are NumPy arrays of the same dtype. The behavior of this function\\n    (particularly with respect to NaNs) is not defined if the dtypes are\\n    different.\\n\\n    Parameters\\n    ----------\\n    left, right : ndarrays\\n    strict_nan : bool, default False\\n        If True, consider NaN and None to be different.\\n\\n    Returns\\n    -------\\n    b : bool\\n        Returns True if the arrays are equivalent.\\n\\n    Examples\\n    --------\\n    >>> array_equivalent(\\n    ...     np.array([1, 2, np.nan]),\\n    ...     np.array([1, 2, np.nan]))\\n    True\\n    >>> array_equivalent(\\n    ...     np.array([1, np.nan, 2]),\\n    ...     np.array([1, 2, np.nan]))\\n    False',\n",
       "  'code': 'def array_equivalent(left, right, strict_nan=False):\\n    \"\"\"\\n    True if two arrays, left and right, have equal non-NaN elements, and NaNs\\n    in corresponding locations.  False otherwise. It is assumed that left and\\n    right are NumPy arrays of the same dtype. The behavior of this function\\n    (particularly with respect to NaNs) is not defined if the dtypes are\\n    different.\\n\\n    Parameters\\n    ----------\\n    left, right : ndarrays\\n    strict_nan : bool, default False\\n        If True, consider NaN and None to be different.\\n\\n    Returns\\n    -------\\n    b : bool\\n        Returns True if the arrays are equivalent.\\n\\n    Examples\\n    --------\\n    >>> array_equivalent(\\n    ...     np.array([1, 2, np.nan]),\\n    ...     np.array([1, 2, np.nan]))\\n    True\\n    >>> array_equivalent(\\n    ...     np.array([1, np.nan, 2]),\\n    ...     np.array([1, 2, np.nan]))\\n    False\\n    \"\"\"\\n\\n    left, right = np.asarray(left), np.asarray(right)\\n\\n    # shape compat\\n    if left.shape != right.shape:\\n        return False\\n\\n    # Object arrays can contain None, NaN and NaT.\\n    # string dtypes must be come to this path for NumPy 1.7.1 compat\\n    if is_string_dtype(left) or is_string_dtype(right):\\n\\n        if not strict_nan:\\n            # isna considers NaN and None to be equivalent.\\n            return lib.array_equivalent_object(\\n                ensure_object(left.ravel()), ensure_object(right.ravel()))\\n\\n        for left_value, right_value in zip(left, right):\\n            if left_value is NaT and right_value is not NaT:\\n                return False\\n\\n            elif isinstance(left_value, float) and np.isnan(left_value):\\n                if (not isinstance(right_value, float) or\\n                        not np.isnan(right_value)):\\n                    return False\\n            else:\\n                if left_value != right_value:\\n                    return False\\n        return True\\n\\n    # NaNs can occur in float and complex arrays.\\n    if is_float_dtype(left) or is_complex_dtype(left):\\n\\n        # empty\\n        if not (np.prod(left.shape) and np.prod(right.shape)):\\n            return True\\n        return ((left == right) | (isna(left) & isna(right))).all()\\n\\n    # numpy will will not allow this type of datetimelike vs integer comparison\\n    elif is_datetimelike_v_numeric(left, right):\\n        return False\\n\\n    # M8/m8\\n    elif needs_i8_conversion(left) and needs_i8_conversion(right):\\n        if not is_dtype_equal(left.dtype, right.dtype):\\n            return False\\n\\n        left = left.view(\\'i8\\')\\n        right = right.view(\\'i8\\')\\n\\n    # if we have structured dtypes, compare first\\n    if (left.dtype.type is np.void or\\n            right.dtype.type is np.void):\\n        if left.dtype != right.dtype:\\n            return False\\n\\n    return np.array_equal(left, right)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_subplots',\n",
       "  'docstring': \"Create a figure with a set of subplots already made.\\n\\n    This utility wrapper makes it convenient to create common layouts of\\n    subplots, including the enclosing figure object, in a single call.\\n\\n    Keyword arguments:\\n\\n    naxes : int\\n      Number of required axes. Exceeded axes are set invisible. Default is\\n      nrows * ncols.\\n\\n    sharex : bool\\n      If True, the X axis will be shared amongst all subplots.\\n\\n    sharey : bool\\n      If True, the Y axis will be shared amongst all subplots.\\n\\n    squeeze : bool\\n\\n      If True, extra dimensions are squeezed out from the returned axis object:\\n        - if only one subplot is constructed (nrows=ncols=1), the resulting\\n        single Axis object is returned as a scalar.\\n        - for Nx1 or 1xN subplots, the returned object is a 1-d numpy object\\n        array of Axis objects are returned as numpy 1-d arrays.\\n        - for NxM subplots with N>1 and M>1 are returned as a 2d array.\\n\\n      If False, no squeezing is done: the returned axis object is always\\n      a 2-d array containing Axis instances, even if it ends up being 1x1.\\n\\n    subplot_kw : dict\\n      Dict with keywords passed to the add_subplot() call used to create each\\n      subplots.\\n\\n    ax : Matplotlib axis object, optional\\n\\n    layout : tuple\\n      Number of rows and columns of the subplot grid.\\n      If not specified, calculated from naxes and layout_type\\n\\n    layout_type : {'box', 'horziontal', 'vertical'}, default 'box'\\n      Specify how to layout the subplot grid.\\n\\n    fig_kw : Other keyword arguments to be passed to the figure() call.\\n        Note that all keywords not recognized above will be\\n        automatically included here.\\n\\n    Returns:\\n\\n    fig, ax : tuple\\n      - fig is the Matplotlib Figure object\\n      - ax can be either a single axis object or an array of axis objects if\\n      more than one subplot was created.  The dimensions of the resulting array\\n      can be controlled with the squeeze keyword, see above.\\n\\n    **Examples:**\\n\\n    x = np.linspace(0, 2*np.pi, 400)\\n    y = np.sin(x**2)\\n\\n    # Just a figure and one subplot\\n    f, ax = plt.subplots()\\n    ax.plot(x, y)\\n    ax.set_title('Simple plot')\\n\\n    # Two subplots, unpack the output array immediately\\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\\n    ax1.plot(x, y)\\n    ax1.set_title('Sharing Y axis')\\n    ax2.scatter(x, y)\\n\\n    # Four polar axes\\n    plt.subplots(2, 2, subplot_kw=dict(polar=True))\",\n",
       "  'code': 'def _subplots(naxes=None, sharex=False, sharey=False, squeeze=True,\\n              subplot_kw=None, ax=None, layout=None, layout_type=\\'box\\',\\n              **fig_kw):\\n    \"\"\"Create a figure with a set of subplots already made.\\n\\n    This utility wrapper makes it convenient to create common layouts of\\n    subplots, including the enclosing figure object, in a single call.\\n\\n    Keyword arguments:\\n\\n    naxes : int\\n      Number of required axes. Exceeded axes are set invisible. Default is\\n      nrows * ncols.\\n\\n    sharex : bool\\n      If True, the X axis will be shared amongst all subplots.\\n\\n    sharey : bool\\n      If True, the Y axis will be shared amongst all subplots.\\n\\n    squeeze : bool\\n\\n      If True, extra dimensions are squeezed out from the returned axis object:\\n        - if only one subplot is constructed (nrows=ncols=1), the resulting\\n        single Axis object is returned as a scalar.\\n        - for Nx1 or 1xN subplots, the returned object is a 1-d numpy object\\n        array of Axis objects are returned as numpy 1-d arrays.\\n        - for NxM subplots with N>1 and M>1 are returned as a 2d array.\\n\\n      If False, no squeezing is done: the returned axis object is always\\n      a 2-d array containing Axis instances, even if it ends up being 1x1.\\n\\n    subplot_kw : dict\\n      Dict with keywords passed to the add_subplot() call used to create each\\n      subplots.\\n\\n    ax : Matplotlib axis object, optional\\n\\n    layout : tuple\\n      Number of rows and columns of the subplot grid.\\n      If not specified, calculated from naxes and layout_type\\n\\n    layout_type : {\\'box\\', \\'horziontal\\', \\'vertical\\'}, default \\'box\\'\\n      Specify how to layout the subplot grid.\\n\\n    fig_kw : Other keyword arguments to be passed to the figure() call.\\n        Note that all keywords not recognized above will be\\n        automatically included here.\\n\\n    Returns:\\n\\n    fig, ax : tuple\\n      - fig is the Matplotlib Figure object\\n      - ax can be either a single axis object or an array of axis objects if\\n      more than one subplot was created.  The dimensions of the resulting array\\n      can be controlled with the squeeze keyword, see above.\\n\\n    **Examples:**\\n\\n    x = np.linspace(0, 2*np.pi, 400)\\n    y = np.sin(x**2)\\n\\n    # Just a figure and one subplot\\n    f, ax = plt.subplots()\\n    ax.plot(x, y)\\n    ax.set_title(\\'Simple plot\\')\\n\\n    # Two subplots, unpack the output array immediately\\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\\n    ax1.plot(x, y)\\n    ax1.set_title(\\'Sharing Y axis\\')\\n    ax2.scatter(x, y)\\n\\n    # Four polar axes\\n    plt.subplots(2, 2, subplot_kw=dict(polar=True))\\n    \"\"\"\\n    import matplotlib.pyplot as plt\\n\\n    if subplot_kw is None:\\n        subplot_kw = {}\\n\\n    if ax is None:\\n        fig = plt.figure(**fig_kw)\\n    else:\\n        if is_list_like(ax):\\n            ax = _flatten(ax)\\n            if layout is not None:\\n                warnings.warn(\"When passing multiple axes, layout keyword is \"\\n                              \"ignored\", UserWarning)\\n            if sharex or sharey:\\n                warnings.warn(\"When passing multiple axes, sharex and sharey \"\\n                              \"are ignored. These settings must be specified \"\\n                              \"when creating axes\", UserWarning,\\n                              stacklevel=4)\\n            if len(ax) == naxes:\\n                fig = ax[0].get_figure()\\n                return fig, ax\\n            else:\\n                raise ValueError(\"The number of passed axes must be {0}, the \"\\n                                 \"same as the output plot\".format(naxes))\\n\\n        fig = ax.get_figure()\\n        # if ax is passed and a number of subplots is 1, return ax as it is\\n        if naxes == 1:\\n            if squeeze:\\n                return fig, ax\\n            else:\\n                return fig, _flatten(ax)\\n        else:\\n            warnings.warn(\"To output multiple subplots, the figure containing \"\\n                          \"the passed axes is being cleared\", UserWarning,\\n                          stacklevel=4)\\n            fig.clear()\\n\\n    nrows, ncols = _get_layout(naxes, layout=layout, layout_type=layout_type)\\n    nplots = nrows * ncols\\n\\n    # Create empty object array to hold all axes.  It\\'s easiest to make it 1-d\\n    # so we can just append subplots upon creation, and then\\n    axarr = np.empty(nplots, dtype=object)\\n\\n    # Create first subplot separately, so we can share it if requested\\n    ax0 = fig.add_subplot(nrows, ncols, 1, **subplot_kw)\\n\\n    if sharex:\\n        subplot_kw[\\'sharex\\'] = ax0\\n    if sharey:\\n        subplot_kw[\\'sharey\\'] = ax0\\n    axarr[0] = ax0\\n\\n    # Note off-by-one counting because add_subplot uses the MATLAB 1-based\\n    # convention.\\n    for i in range(1, nplots):\\n        kwds = subplot_kw.copy()\\n        # Set sharex and sharey to None for blank/dummy axes, these can\\n        # interfere with proper axis limits on the visible axes if\\n        # they share axes e.g. issue #7528\\n        if i >= naxes:\\n            kwds[\\'sharex\\'] = None\\n            kwds[\\'sharey\\'] = None\\n        ax = fig.add_subplot(nrows, ncols, i + 1, **kwds)\\n        axarr[i] = ax\\n\\n    if naxes != nplots:\\n        for ax in axarr[naxes:]:\\n            ax.set_visible(False)\\n\\n    _handle_shared_axes(axarr, nplots, naxes, nrows, ncols, sharex, sharey)\\n\\n    if squeeze:\\n        # Reshape the array to have the final desired dimension (nrow,ncol),\\n        # though discarding unneeded dimensions that equal 1.  If we only have\\n        # one subplot, just return it instead of a 1-element array.\\n        if nplots == 1:\\n            axes = axarr[0]\\n        else:\\n            axes = axarr.reshape(nrows, ncols).squeeze()\\n    else:\\n        # returned axis array will be always 2-d, even if nrows=ncols=1\\n        axes = axarr.reshape(nrows, ncols)\\n\\n    return fig, axes',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'maybe_cythonize',\n",
       "  'docstring': 'Render tempita templates before calling cythonize',\n",
       "  'code': 'def maybe_cythonize(extensions, *args, **kwargs):\\n    \"\"\"\\n    Render tempita templates before calling cythonize\\n    \"\"\"\\n    if len(sys.argv) > 1 and \\'clean\\' in sys.argv:\\n        # Avoid running cythonize on `python setup.py clean`\\n        # See https://github.com/cython/cython/issues/1495\\n        return extensions\\n    if not cython:\\n        # Avoid trying to look up numpy when installing from sdist\\n        # https://github.com/pandas-dev/pandas/issues/25193\\n        # TODO: See if this can be removed after pyproject.toml added.\\n        return extensions\\n\\n    numpy_incl = pkg_resources.resource_filename(\\'numpy\\', \\'core/include\\')\\n    # TODO: Is this really necessary here?\\n    for ext in extensions:\\n        if (hasattr(ext, \\'include_dirs\\') and\\n                numpy_incl not in ext.include_dirs):\\n            ext.include_dirs.append(numpy_incl)\\n\\n    build_ext.render_templates(_pxifiles)\\n    return cythonize(extensions, *args, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'extract_array',\n",
       "  'docstring': \"Extract the ndarray or ExtensionArray from a Series or Index.\\n\\n    For all other types, `obj` is just returned as is.\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        For Series / Index, the underlying ExtensionArray is unboxed.\\n        For Numpy-backed ExtensionArrays, the ndarray is extracted.\\n\\n    extract_numpy : bool, default False\\n        Whether to extract the ndarray from a PandasArray\\n\\n    Returns\\n    -------\\n    arr : object\\n\\n    Examples\\n    --------\\n    >>> extract_array(pd.Series(['a', 'b', 'c'], dtype='category'))\\n    [a, b, c]\\n    Categories (3, object): [a, b, c]\\n\\n    Other objects like lists, arrays, and DataFrames are just passed through.\\n\\n    >>> extract_array([1, 2, 3])\\n    [1, 2, 3]\\n\\n    For an ndarray-backed Series / Index a PandasArray is returned.\\n\\n    >>> extract_array(pd.Series([1, 2, 3]))\\n    <PandasArray>\\n    [1, 2, 3]\\n    Length: 3, dtype: int64\\n\\n    To extract all the way down to the ndarray, pass ``extract_numpy=True``.\\n\\n    >>> extract_array(pd.Series([1, 2, 3]), extract_numpy=True)\\n    array([1, 2, 3])\",\n",
       "  'code': 'def extract_array(obj, extract_numpy=False):\\n    \"\"\"\\n    Extract the ndarray or ExtensionArray from a Series or Index.\\n\\n    For all other types, `obj` is just returned as is.\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        For Series / Index, the underlying ExtensionArray is unboxed.\\n        For Numpy-backed ExtensionArrays, the ndarray is extracted.\\n\\n    extract_numpy : bool, default False\\n        Whether to extract the ndarray from a PandasArray\\n\\n    Returns\\n    -------\\n    arr : object\\n\\n    Examples\\n    --------\\n    >>> extract_array(pd.Series([\\'a\\', \\'b\\', \\'c\\'], dtype=\\'category\\'))\\n    [a, b, c]\\n    Categories (3, object): [a, b, c]\\n\\n    Other objects like lists, arrays, and DataFrames are just passed through.\\n\\n    >>> extract_array([1, 2, 3])\\n    [1, 2, 3]\\n\\n    For an ndarray-backed Series / Index a PandasArray is returned.\\n\\n    >>> extract_array(pd.Series([1, 2, 3]))\\n    <PandasArray>\\n    [1, 2, 3]\\n    Length: 3, dtype: int64\\n\\n    To extract all the way down to the ndarray, pass ``extract_numpy=True``.\\n\\n    >>> extract_array(pd.Series([1, 2, 3]), extract_numpy=True)\\n    array([1, 2, 3])\\n    \"\"\"\\n    if isinstance(obj, (ABCIndexClass, ABCSeries)):\\n        obj = obj.array\\n\\n    if extract_numpy and isinstance(obj, ABCPandasArray):\\n        obj = obj.to_numpy()\\n\\n    return obj',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'cast_scalar_indexer',\n",
       "  'docstring': 'To avoid numpy DeprecationWarnings, cast float to integer where valid.\\n\\n    Parameters\\n    ----------\\n    val : scalar\\n\\n    Returns\\n    -------\\n    outval : scalar',\n",
       "  'code': 'def cast_scalar_indexer(val):\\n    \"\"\"\\n    To avoid numpy DeprecationWarnings, cast float to integer where valid.\\n\\n    Parameters\\n    ----------\\n    val : scalar\\n\\n    Returns\\n    -------\\n    outval : scalar\\n    \"\"\"\\n    # assumes lib.is_scalar(val)\\n    if lib.is_float(val) and val == int(val):\\n        return int(val)\\n    return val',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'random_state',\n",
       "  'docstring': 'Helper function for processing random_state arguments.\\n\\n    Parameters\\n    ----------\\n    state : int, np.random.RandomState, None.\\n        If receives an int, passes to np.random.RandomState() as seed.\\n        If receives an np.random.RandomState object, just returns object.\\n        If receives `None`, returns np.random.\\n        If receives anything else, raises an informative ValueError.\\n        Default None.\\n\\n    Returns\\n    -------\\n    np.random.RandomState',\n",
       "  'code': 'def random_state(state=None):\\n    \"\"\"\\n    Helper function for processing random_state arguments.\\n\\n    Parameters\\n    ----------\\n    state : int, np.random.RandomState, None.\\n        If receives an int, passes to np.random.RandomState() as seed.\\n        If receives an np.random.RandomState object, just returns object.\\n        If receives `None`, returns np.random.\\n        If receives anything else, raises an informative ValueError.\\n        Default None.\\n\\n    Returns\\n    -------\\n    np.random.RandomState\\n    \"\"\"\\n\\n    if is_integer(state):\\n        return np.random.RandomState(state)\\n    elif isinstance(state, np.random.RandomState):\\n        return state\\n    elif state is None:\\n        return np.random\\n    else:\\n        raise ValueError(\"random_state must be an integer, a numpy \"\\n                         \"RandomState, or None\")',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'nanmedian',\n",
       "  'docstring': 'Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : float\\n        Unless input is a float array, in which case use the same\\n        precision as the input array.\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, np.nan, 2, 2])\\n    >>> nanops.nanmedian(s)\\n    2.0',\n",
       "  'code': 'def nanmedian(values, axis=None, skipna=True, mask=None):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : float\\n        Unless input is a float array, in which case use the same\\n        precision as the input array.\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, np.nan, 2, 2])\\n    >>> nanops.nanmedian(s)\\n    2.0\\n    \"\"\"\\n    def get_median(x):\\n        mask = notna(x)\\n        if not skipna and not mask.all():\\n            return np.nan\\n        return np.nanmedian(x[mask])\\n\\n    values, mask, dtype, dtype_max, _ = _get_values(values, skipna, mask=mask)\\n    if not is_float_dtype(values):\\n        values = values.astype(\\'f8\\')\\n        values[mask] = np.nan\\n\\n    if axis is None:\\n        values = values.ravel()\\n\\n    notempty = values.size\\n\\n    # an array from a frame\\n    if values.ndim > 1:\\n\\n        # there\\'s a non-empty array to apply over otherwise numpy raises\\n        if notempty:\\n            if not skipna:\\n                return _wrap_results(\\n                    np.apply_along_axis(get_median, axis, values), dtype)\\n\\n            # fastpath for the skipna case\\n            return _wrap_results(np.nanmedian(values, axis), dtype)\\n\\n        # must return the correct shape, but median is not defined for the\\n        # empty set so return nans of shape \"everything but the passed axis\"\\n        # since \"axis\" is where the reduction would occur if we had a nonempty\\n        # array\\n        shp = np.array(values.shape)\\n        dims = np.arange(values.ndim)\\n        ret = np.empty(shp[dims != axis])\\n        ret.fill(np.nan)\\n        return _wrap_results(ret, dtype)\\n\\n    # otherwise return a scalar value\\n    return _wrap_results(get_median(values) if notempty else np.nan, dtype)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_concat_compat',\n",
       "  'docstring': \"provide concatenation of an array of arrays each of which is a single\\n    'normalized' dtypes (in that for example, if it's object, then it is a\\n    non-datetimelike and provide a combined dtype for the resulting array that\\n    preserves the overall dtype if possible)\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : axis to provide concatenation\\n\\n    Returns\\n    -------\\n    a single array, preserving the combined dtypes\",\n",
       "  'code': 'def _concat_compat(to_concat, axis=0):\\n    \"\"\"\\n    provide concatenation of an array of arrays each of which is a single\\n    \\'normalized\\' dtypes (in that for example, if it\\'s object, then it is a\\n    non-datetimelike and provide a combined dtype for the resulting array that\\n    preserves the overall dtype if possible)\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : axis to provide concatenation\\n\\n    Returns\\n    -------\\n    a single array, preserving the combined dtypes\\n    \"\"\"\\n\\n    # filter empty arrays\\n    # 1-d dtypes always are included here\\n    def is_nonempty(x):\\n        try:\\n            return x.shape[axis] > 0\\n        except Exception:\\n            return True\\n\\n    # If all arrays are empty, there\\'s nothing to convert, just short-cut to\\n    # the concatenation, #3121.\\n    #\\n    # Creating an empty array directly is tempting, but the winnings would be\\n    # marginal given that it would still require shape & dtype calculation and\\n    # np.concatenate which has them both implemented is compiled.\\n\\n    typs = get_dtype_kinds(to_concat)\\n    _contains_datetime = any(typ.startswith(\\'datetime\\') for typ in typs)\\n    _contains_period = any(typ.startswith(\\'period\\') for typ in typs)\\n\\n    if \\'category\\' in typs:\\n        # this must be priort to _concat_datetime,\\n        # to support Categorical + datetime-like\\n        return _concat_categorical(to_concat, axis=axis)\\n\\n    elif _contains_datetime or \\'timedelta\\' in typs or _contains_period:\\n        return _concat_datetime(to_concat, axis=axis, typs=typs)\\n\\n    # these are mandated to handle empties as well\\n    elif \\'sparse\\' in typs:\\n        return _concat_sparse(to_concat, axis=axis, typs=typs)\\n\\n    all_empty = all(not is_nonempty(x) for x in to_concat)\\n    if any(is_extension_array_dtype(x) for x in to_concat) and axis == 1:\\n        to_concat = [np.atleast_2d(x.astype(\\'object\\')) for x in to_concat]\\n\\n    if all_empty:\\n        # we have all empties, but may need to coerce the result dtype to\\n        # object if we have non-numeric type operands (numpy would otherwise\\n        # cast this to float)\\n        typs = get_dtype_kinds(to_concat)\\n        if len(typs) != 1:\\n\\n            if (not len(typs - {\\'i\\', \\'u\\', \\'f\\'}) or\\n                    not len(typs - {\\'bool\\', \\'i\\', \\'u\\'})):\\n                # let numpy coerce\\n                pass\\n            else:\\n                # coerce to object\\n                to_concat = [x.astype(\\'object\\') for x in to_concat]\\n\\n    return np.concatenate(to_concat, axis=axis)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'np_datetime64_compat',\n",
       "  'docstring': \"provide compat for construction of strings to numpy datetime64's with\\n    tz-changes in 1.11 that make '2015-01-01 09:00:00Z' show a deprecation\\n    warning, when need to pass '2015-01-01 09:00:00'\",\n",
       "  'code': 'def np_datetime64_compat(s, *args, **kwargs):\\n    \"\"\"\\n    provide compat for construction of strings to numpy datetime64\\'s with\\n    tz-changes in 1.11 that make \\'2015-01-01 09:00:00Z\\' show a deprecation\\n    warning, when need to pass \\'2015-01-01 09:00:00\\'\\n    \"\"\"\\n    s = tz_replacer(s)\\n    return np.datetime64(s, *args, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Int64Index._assert_safe_casting',\n",
       "  'docstring': 'Ensure incoming data can be represented as ints.',\n",
       "  'code': 'def _assert_safe_casting(cls, data, subarr):\\n        \"\"\"\\n        Ensure incoming data can be represented as ints.\\n        \"\"\"\\n        if not issubclass(data.dtype.type, np.signedinteger):\\n            if not np.array_equal(data, subarr):\\n                raise TypeError(\\'Unsafe NumPy casting, you must \\'\\n                                \\'explicitly cast\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_convert_string_array',\n",
       "  'docstring': 'we take a string-like that is object dtype and coerce to a fixed size\\n    string type\\n\\n    Parameters\\n    ----------\\n    data : a numpy array of object dtype\\n    encoding : None or string-encoding\\n    errors : handler for encoding errors\\n    itemsize : integer, optional, defaults to the max length of the strings\\n\\n    Returns\\n    -------\\n    data in a fixed-length string dtype, encoded to bytes if needed',\n",
       "  'code': 'def _convert_string_array(data, encoding, errors, itemsize=None):\\n    \"\"\"\\n    we take a string-like that is object dtype and coerce to a fixed size\\n    string type\\n\\n    Parameters\\n    ----------\\n    data : a numpy array of object dtype\\n    encoding : None or string-encoding\\n    errors : handler for encoding errors\\n    itemsize : integer, optional, defaults to the max length of the strings\\n\\n    Returns\\n    -------\\n    data in a fixed-length string dtype, encoded to bytes if needed\\n    \"\"\"\\n\\n    # encode if needed\\n    if encoding is not None and len(data):\\n        data = Series(data.ravel()).str.encode(\\n            encoding, errors).values.reshape(data.shape)\\n\\n    # create the sized dtype\\n    if itemsize is None:\\n        ensured = ensure_object(data.ravel())\\n        itemsize = max(1, libwriters.max_len_string_array(ensured))\\n\\n    data = np.asarray(data, dtype=\"S{size}\".format(size=itemsize))\\n    return data',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ExtensionArray.astype',\n",
       "  'docstring': \"Cast to a NumPy array with 'dtype'.\\n\\n        Parameters\\n        ----------\\n        dtype : str or dtype\\n            Typecode or data-type to which the array is cast.\\n        copy : bool, default True\\n            Whether to copy the data, even if not necessary. If False,\\n            a copy is made only if the old dtype does not match the\\n            new dtype.\\n\\n        Returns\\n        -------\\n        array : ndarray\\n            NumPy ndarray with 'dtype' for its dtype.\",\n",
       "  'code': 'def astype(self, dtype, copy=True):\\n        \"\"\"\\n        Cast to a NumPy array with \\'dtype\\'.\\n\\n        Parameters\\n        ----------\\n        dtype : str or dtype\\n            Typecode or data-type to which the array is cast.\\n        copy : bool, default True\\n            Whether to copy the data, even if not necessary. If False,\\n            a copy is made only if the old dtype does not match the\\n            new dtype.\\n\\n        Returns\\n        -------\\n        array : ndarray\\n            NumPy ndarray with \\'dtype\\' for its dtype.\\n        \"\"\"\\n        return np.array(self, dtype=dtype, copy=copy)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ExtensionArray.argsort',\n",
       "  'docstring': \"Return the indices that would sort this array.\\n\\n        Parameters\\n        ----------\\n        ascending : bool, default True\\n            Whether the indices should result in an ascending\\n            or descending sort.\\n        kind : {'quicksort', 'mergesort', 'heapsort'}, optional\\n            Sorting algorithm.\\n        *args, **kwargs:\\n            passed through to :func:`numpy.argsort`.\\n\\n        Returns\\n        -------\\n        index_array : ndarray\\n            Array of indices that sort ``self``.\\n\\n        See Also\\n        --------\\n        numpy.argsort : Sorting implementation used internally.\",\n",
       "  'code': 'def argsort(self, ascending=True, kind=\\'quicksort\\', *args, **kwargs):\\n        \"\"\"\\n        Return the indices that would sort this array.\\n\\n        Parameters\\n        ----------\\n        ascending : bool, default True\\n            Whether the indices should result in an ascending\\n            or descending sort.\\n        kind : {\\'quicksort\\', \\'mergesort\\', \\'heapsort\\'}, optional\\n            Sorting algorithm.\\n        *args, **kwargs:\\n            passed through to :func:`numpy.argsort`.\\n\\n        Returns\\n        -------\\n        index_array : ndarray\\n            Array of indices that sort ``self``.\\n\\n        See Also\\n        --------\\n        numpy.argsort : Sorting implementation used internally.\\n        \"\"\"\\n        # Implementor note: You have two places to override the behavior of\\n        # argsort.\\n        # 1. _values_for_argsort : construct the values passed to np.argsort\\n        # 2. argsort : total control over sorting.\\n        ascending = nv.validate_argsort_with_ascending(ascending, args, kwargs)\\n        values = self._values_for_argsort()\\n        result = np.argsort(values, kind=kind, **kwargs)\\n        if not ascending:\\n            result = result[::-1]\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ExtensionArray.searchsorted',\n",
       "  'docstring': \"Find indices where elements should be inserted to maintain order.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Find the indices into a sorted array `self` (a) such that, if the\\n        corresponding elements in `value` were inserted before the indices,\\n        the order of `self` would be preserved.\\n\\n        Assuming that `self` is sorted:\\n\\n        ======  ================================\\n        `side`  returned index `i` satisfies\\n        ======  ================================\\n        left    ``self[i-1] < value <= self[i]``\\n        right   ``self[i-1] <= value < self[i]``\\n        ======  ================================\\n\\n        Parameters\\n        ----------\\n        value : array_like\\n            Values to insert into `self`.\\n        side : {'left', 'right'}, optional\\n            If 'left', the index of the first suitable location found is given.\\n            If 'right', return the last such index.  If there is no suitable\\n            index, return either 0 or N (where N is the length of `self`).\\n        sorter : 1-D array_like, optional\\n            Optional array of integer indices that sort array a into ascending\\n            order. They are typically the result of argsort.\\n\\n        Returns\\n        -------\\n        array of ints\\n            Array of insertion points with the same shape as `value`.\\n\\n        See Also\\n        --------\\n        numpy.searchsorted : Similar method from NumPy.\",\n",
       "  'code': 'def searchsorted(self, value, side=\"left\", sorter=None):\\n        \"\"\"\\n        Find indices where elements should be inserted to maintain order.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Find the indices into a sorted array `self` (a) such that, if the\\n        corresponding elements in `value` were inserted before the indices,\\n        the order of `self` would be preserved.\\n\\n        Assuming that `self` is sorted:\\n\\n        ======  ================================\\n        `side`  returned index `i` satisfies\\n        ======  ================================\\n        left    ``self[i-1] < value <= self[i]``\\n        right   ``self[i-1] <= value < self[i]``\\n        ======  ================================\\n\\n        Parameters\\n        ----------\\n        value : array_like\\n            Values to insert into `self`.\\n        side : {\\'left\\', \\'right\\'}, optional\\n            If \\'left\\', the index of the first suitable location found is given.\\n            If \\'right\\', return the last such index.  If there is no suitable\\n            index, return either 0 or N (where N is the length of `self`).\\n        sorter : 1-D array_like, optional\\n            Optional array of integer indices that sort array a into ascending\\n            order. They are typically the result of argsort.\\n\\n        Returns\\n        -------\\n        array of ints\\n            Array of insertion points with the same shape as `value`.\\n\\n        See Also\\n        --------\\n        numpy.searchsorted : Similar method from NumPy.\\n        \"\"\"\\n        # Note: the base tests provided by pandas only test the basics.\\n        # We do not test\\n        # 1. Values outside the range of the `data_for_sorting` fixture\\n        # 2. Values between the values in the `data_for_sorting` fixture\\n        # 3. Missing values.\\n        arr = self.astype(object)\\n        return arr.searchsorted(value, side=side, sorter=sorter)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ExtensionArray.factorize',\n",
       "  'docstring': \"Encode the extension array as an enumerated type.\\n\\n        Parameters\\n        ----------\\n        na_sentinel : int, default -1\\n            Value to use in the `labels` array to indicate missing values.\\n\\n        Returns\\n        -------\\n        labels : ndarray\\n            An integer NumPy array that's an indexer into the original\\n            ExtensionArray.\\n        uniques : ExtensionArray\\n            An ExtensionArray containing the unique values of `self`.\\n\\n            .. note::\\n\\n               uniques will *not* contain an entry for the NA value of\\n               the ExtensionArray if there are any missing values present\\n               in `self`.\\n\\n        See Also\\n        --------\\n        pandas.factorize : Top-level factorize method that dispatches here.\\n\\n        Notes\\n        -----\\n        :meth:`pandas.factorize` offers a `sort` keyword as well.\",\n",
       "  'code': 'def factorize(\\n            self,\\n            na_sentinel: int = -1,\\n    ) -> Tuple[np.ndarray, ABCExtensionArray]:\\n        \"\"\"\\n        Encode the extension array as an enumerated type.\\n\\n        Parameters\\n        ----------\\n        na_sentinel : int, default -1\\n            Value to use in the `labels` array to indicate missing values.\\n\\n        Returns\\n        -------\\n        labels : ndarray\\n            An integer NumPy array that\\'s an indexer into the original\\n            ExtensionArray.\\n        uniques : ExtensionArray\\n            An ExtensionArray containing the unique values of `self`.\\n\\n            .. note::\\n\\n               uniques will *not* contain an entry for the NA value of\\n               the ExtensionArray if there are any missing values present\\n               in `self`.\\n\\n        See Also\\n        --------\\n        pandas.factorize : Top-level factorize method that dispatches here.\\n\\n        Notes\\n        -----\\n        :meth:`pandas.factorize` offers a `sort` keyword as well.\\n        \"\"\"\\n        # Impelmentor note: There are two ways to override the behavior of\\n        # pandas.factorize\\n        # 1. _values_for_factorize and _from_factorize.\\n        #    Specify the values passed to pandas\\' internal factorization\\n        #    routines, and how to convert from those values back to the\\n        #    original ExtensionArray.\\n        # 2. ExtensionArray.factorize.\\n        #    Complete control over factorization.\\n        from pandas.core.algorithms import _factorize_array\\n\\n        arr, na_value = self._values_for_factorize()\\n\\n        labels, uniques = _factorize_array(arr, na_sentinel=na_sentinel,\\n                                           na_value=na_value)\\n\\n        uniques = self._from_factorized(uniques, self)\\n        return labels, uniques',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ExtensionArray.take',\n",
       "  'docstring': 'Take elements from an array.\\n\\n        Parameters\\n        ----------\\n        indices : sequence of integers\\n            Indices to be taken.\\n        allow_fill : bool, default False\\n            How to handle negative values in `indices`.\\n\\n            * False: negative values in `indices` indicate positional indices\\n              from the right (the default). This is similar to\\n              :func:`numpy.take`.\\n\\n            * True: negative values in `indices` indicate\\n              missing values. These values are set to `fill_value`. Any other\\n              other negative values raise a ``ValueError``.\\n\\n        fill_value : any, optional\\n            Fill value to use for NA-indices when `allow_fill` is True.\\n            This may be ``None``, in which case the default NA value for\\n            the type, ``self.dtype.na_value``, is used.\\n\\n            For many ExtensionArrays, there will be two representations of\\n            `fill_value`: a user-facing \"boxed\" scalar, and a low-level\\n            physical NA value. `fill_value` should be the user-facing version,\\n            and the implementation should handle translating that to the\\n            physical version for processing the take if necessary.\\n\\n        Returns\\n        -------\\n        ExtensionArray\\n\\n        Raises\\n        ------\\n        IndexError\\n            When the indices are out of bounds for the array.\\n        ValueError\\n            When `indices` contains negative values other than ``-1``\\n            and `allow_fill` is True.\\n\\n        Notes\\n        -----\\n        ExtensionArray.take is called by ``Series.__getitem__``, ``.loc``,\\n        ``iloc``, when `indices` is a sequence of values. Additionally,\\n        it\\'s called by :meth:`Series.reindex`, or any other method\\n        that causes realignment, with a `fill_value`.\\n\\n        See Also\\n        --------\\n        numpy.take\\n        pandas.api.extensions.take\\n\\n        Examples\\n        --------\\n        Here\\'s an example implementation, which relies on casting the\\n        extension array to object dtype. This uses the helper method\\n        :func:`pandas.api.extensions.take`.\\n\\n        .. code-block:: python\\n\\n           def take(self, indices, allow_fill=False, fill_value=None):\\n               from pandas.core.algorithms import take\\n\\n               # If the ExtensionArray is backed by an ndarray, then\\n               # just pass that here instead of coercing to object.\\n               data = self.astype(object)\\n\\n               if allow_fill and fill_value is None:\\n                   fill_value = self.dtype.na_value\\n\\n               # fill value should always be translated from the scalar\\n               # type for the array, to the physical storage type for\\n               # the data, before passing to take.\\n\\n               result = take(data, indices, fill_value=fill_value,\\n                             allow_fill=allow_fill)\\n               return self._from_sequence(result, dtype=self.dtype)',\n",
       "  'code': 'def take(\\n            self,\\n            indices: Sequence[int],\\n            allow_fill: bool = False,\\n            fill_value: Any = None\\n    ) -> ABCExtensionArray:\\n        \"\"\"\\n        Take elements from an array.\\n\\n        Parameters\\n        ----------\\n        indices : sequence of integers\\n            Indices to be taken.\\n        allow_fill : bool, default False\\n            How to handle negative values in `indices`.\\n\\n            * False: negative values in `indices` indicate positional indices\\n              from the right (the default). This is similar to\\n              :func:`numpy.take`.\\n\\n            * True: negative values in `indices` indicate\\n              missing values. These values are set to `fill_value`. Any other\\n              other negative values raise a ``ValueError``.\\n\\n        fill_value : any, optional\\n            Fill value to use for NA-indices when `allow_fill` is True.\\n            This may be ``None``, in which case the default NA value for\\n            the type, ``self.dtype.na_value``, is used.\\n\\n            For many ExtensionArrays, there will be two representations of\\n            `fill_value`: a user-facing \"boxed\" scalar, and a low-level\\n            physical NA value. `fill_value` should be the user-facing version,\\n            and the implementation should handle translating that to the\\n            physical version for processing the take if necessary.\\n\\n        Returns\\n        -------\\n        ExtensionArray\\n\\n        Raises\\n        ------\\n        IndexError\\n            When the indices are out of bounds for the array.\\n        ValueError\\n            When `indices` contains negative values other than ``-1``\\n            and `allow_fill` is True.\\n\\n        Notes\\n        -----\\n        ExtensionArray.take is called by ``Series.__getitem__``, ``.loc``,\\n        ``iloc``, when `indices` is a sequence of values. Additionally,\\n        it\\'s called by :meth:`Series.reindex`, or any other method\\n        that causes realignment, with a `fill_value`.\\n\\n        See Also\\n        --------\\n        numpy.take\\n        pandas.api.extensions.take\\n\\n        Examples\\n        --------\\n        Here\\'s an example implementation, which relies on casting the\\n        extension array to object dtype. This uses the helper method\\n        :func:`pandas.api.extensions.take`.\\n\\n        .. code-block:: python\\n\\n           def take(self, indices, allow_fill=False, fill_value=None):\\n               from pandas.core.algorithms import take\\n\\n               # If the ExtensionArray is backed by an ndarray, then\\n               # just pass that here instead of coercing to object.\\n               data = self.astype(object)\\n\\n               if allow_fill and fill_value is None:\\n                   fill_value = self.dtype.na_value\\n\\n               # fill value should always be translated from the scalar\\n               # type for the array, to the physical storage type for\\n               # the data, before passing to take.\\n\\n               result = take(data, indices, fill_value=fill_value,\\n                             allow_fill=allow_fill)\\n               return self._from_sequence(result, dtype=self.dtype)\\n        \"\"\"\\n        # Implementer note: The `fill_value` parameter should be a user-facing\\n        # value, an instance of self.dtype.type. When passed `fill_value=None`,\\n        # the default of `self.dtype.na_value` should be used.\\n        # This may differ from the physical storage type your ExtensionArray\\n        # uses. In this case, your implementation is responsible for casting\\n        # the user-facing type to the storage type, before using\\n        # pandas.api.extensions.take\\n        raise AbstractMethodError(self)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ExtensionScalarOpsMixin._create_method',\n",
       "  'docstring': \"A class method that returns a method that will correspond to an\\n        operator for an ExtensionArray subclass, by dispatching to the\\n        relevant operator defined on the individual elements of the\\n        ExtensionArray.\\n\\n        Parameters\\n        ----------\\n        op : function\\n            An operator that takes arguments op(a, b)\\n        coerce_to_dtype :  bool, default True\\n            boolean indicating whether to attempt to convert\\n            the result to the underlying ExtensionArray dtype.\\n            If it's not possible to create a new ExtensionArray with the\\n            values, an ndarray is returned instead.\\n\\n        Returns\\n        -------\\n        Callable[[Any, Any], Union[ndarray, ExtensionArray]]\\n            A method that can be bound to a class. When used, the method\\n            receives the two arguments, one of which is the instance of\\n            this class, and should return an ExtensionArray or an ndarray.\\n\\n            Returning an ndarray may be necessary when the result of the\\n            `op` cannot be stored in the ExtensionArray. The dtype of the\\n            ndarray uses NumPy's normal inference rules.\\n\\n        Example\\n        -------\\n        Given an ExtensionArray subclass called MyExtensionArray, use\\n\\n        >>> __add__ = cls._create_method(operator.add)\\n\\n        in the class definition of MyExtensionArray to create the operator\\n        for addition, that will be based on the operator implementation\\n        of the underlying elements of the ExtensionArray\",\n",
       "  'code': 'def _create_method(cls, op, coerce_to_dtype=True):\\n        \"\"\"\\n        A class method that returns a method that will correspond to an\\n        operator for an ExtensionArray subclass, by dispatching to the\\n        relevant operator defined on the individual elements of the\\n        ExtensionArray.\\n\\n        Parameters\\n        ----------\\n        op : function\\n            An operator that takes arguments op(a, b)\\n        coerce_to_dtype :  bool, default True\\n            boolean indicating whether to attempt to convert\\n            the result to the underlying ExtensionArray dtype.\\n            If it\\'s not possible to create a new ExtensionArray with the\\n            values, an ndarray is returned instead.\\n\\n        Returns\\n        -------\\n        Callable[[Any, Any], Union[ndarray, ExtensionArray]]\\n            A method that can be bound to a class. When used, the method\\n            receives the two arguments, one of which is the instance of\\n            this class, and should return an ExtensionArray or an ndarray.\\n\\n            Returning an ndarray may be necessary when the result of the\\n            `op` cannot be stored in the ExtensionArray. The dtype of the\\n            ndarray uses NumPy\\'s normal inference rules.\\n\\n        Example\\n        -------\\n        Given an ExtensionArray subclass called MyExtensionArray, use\\n\\n        >>> __add__ = cls._create_method(operator.add)\\n\\n        in the class definition of MyExtensionArray to create the operator\\n        for addition, that will be based on the operator implementation\\n        of the underlying elements of the ExtensionArray\\n        \"\"\"\\n\\n        def _binop(self, other):\\n            def convert_values(param):\\n                if isinstance(param, ExtensionArray) or is_list_like(param):\\n                    ovalues = param\\n                else:  # Assume its an object\\n                    ovalues = [param] * len(self)\\n                return ovalues\\n\\n            if isinstance(other, (ABCSeries, ABCIndexClass)):\\n                # rely on pandas to unbox and dispatch to us\\n                return NotImplemented\\n\\n            lvalues = self\\n            rvalues = convert_values(other)\\n\\n            # If the operator is not defined for the underlying objects,\\n            # a TypeError should be raised\\n            res = [op(a, b) for (a, b) in zip(lvalues, rvalues)]\\n\\n            def _maybe_convert(arr):\\n                if coerce_to_dtype:\\n                    # https://github.com/pandas-dev/pandas/issues/22850\\n                    # We catch all regular exceptions here, and fall back\\n                    # to an ndarray.\\n                    try:\\n                        res = self._from_sequence(arr)\\n                    except Exception:\\n                        res = np.asarray(arr)\\n                else:\\n                    res = np.asarray(arr)\\n                return res\\n\\n            if op.__name__ in {\\'divmod\\', \\'rdivmod\\'}:\\n                a, b = zip(*res)\\n                res = _maybe_convert(a), _maybe_convert(b)\\n            else:\\n                res = _maybe_convert(res)\\n            return res\\n\\n        op_name = ops._get_op_name(op, True)\\n        return set_function_name(_binop, op_name, cls)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatetimeIndexOpsMixin.min',\n",
       "  'docstring': 'Return the minimum value of the Index or minimum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.min\\n        Series.min : Return the minimum value in a Series.',\n",
       "  'code': 'def min(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Return the minimum value of the Index or minimum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.min\\n        Series.min : Return the minimum value in a Series.\\n        \"\"\"\\n        nv.validate_min(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        if not len(self):\\n            return self._na_value\\n\\n        i8 = self.asi8\\n        try:\\n            # quick check\\n            if len(i8) and self.is_monotonic:\\n                if i8[0] != iNaT:\\n                    return self._box_func(i8[0])\\n\\n            if self.hasnans:\\n                if skipna:\\n                    min_stamp = self[~self._isnan].asi8.min()\\n                else:\\n                    return self._na_value\\n            else:\\n                min_stamp = i8.min()\\n            return self._box_func(min_stamp)\\n        except ValueError:\\n            return self._na_value',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatetimeIndexOpsMixin.argmin',\n",
       "  'docstring': 'Returns the indices of the minimum values along an axis.\\n\\n        See `numpy.ndarray.argmin` for more information on the\\n        `axis` parameter.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmin',\n",
       "  'code': 'def argmin(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Returns the indices of the minimum values along an axis.\\n\\n        See `numpy.ndarray.argmin` for more information on the\\n        `axis` parameter.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmin\\n        \"\"\"\\n        nv.validate_argmin(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        i8 = self.asi8\\n        if self.hasnans:\\n            mask = self._isnan\\n            if mask.all() or not skipna:\\n                return -1\\n            i8 = i8.copy()\\n            i8[mask] = np.iinfo(\\'int64\\').max\\n        return i8.argmin()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatetimeIndexOpsMixin.max',\n",
       "  'docstring': 'Return the maximum value of the Index or maximum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.max\\n        Series.max : Return the maximum value in a Series.',\n",
       "  'code': 'def max(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Return the maximum value of the Index or maximum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.max\\n        Series.max : Return the maximum value in a Series.\\n        \"\"\"\\n        nv.validate_max(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        if not len(self):\\n            return self._na_value\\n\\n        i8 = self.asi8\\n        try:\\n            # quick check\\n            if len(i8) and self.is_monotonic:\\n                if i8[-1] != iNaT:\\n                    return self._box_func(i8[-1])\\n\\n            if self.hasnans:\\n                if skipna:\\n                    max_stamp = self[~self._isnan].asi8.max()\\n                else:\\n                    return self._na_value\\n            else:\\n                max_stamp = i8.max()\\n            return self._box_func(max_stamp)\\n        except ValueError:\\n            return self._na_value',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatetimeIndexOpsMixin.argmax',\n",
       "  'docstring': 'Returns the indices of the maximum values along an axis.\\n\\n        See `numpy.ndarray.argmax` for more information on the\\n        `axis` parameter.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmax',\n",
       "  'code': 'def argmax(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Returns the indices of the maximum values along an axis.\\n\\n        See `numpy.ndarray.argmax` for more information on the\\n        `axis` parameter.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmax\\n        \"\"\"\\n        nv.validate_argmax(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        i8 = self.asi8\\n        if self.hasnans:\\n            mask = self._isnan\\n            if mask.all() or not skipna:\\n                return -1\\n            i8 = i8.copy()\\n            i8[mask] = 0\\n        return i8.argmax()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame.equals',\n",
       "  'docstring': 'Test whether two objects contain the same elements.\\n\\n        This function allows two Series or DataFrames to be compared against\\n        each other to see if they have the same shape and elements. NaNs in\\n        the same location are considered equal. The column headers do not\\n        need to have the same type, but the elements within the columns must\\n        be the same dtype.\\n\\n        Parameters\\n        ----------\\n        other : Series or DataFrame\\n            The other Series or DataFrame to be compared with the first.\\n\\n        Returns\\n        -------\\n        bool\\n            True if all elements are the same in both objects, False\\n            otherwise.\\n\\n        See Also\\n        --------\\n        Series.eq : Compare two Series objects of the same length\\n            and return a Series where each element is True if the element\\n            in each Series is equal, False otherwise.\\n        DataFrame.eq : Compare two DataFrame objects of the same shape and\\n            return a DataFrame where each element is True if the respective\\n            element in each DataFrame is equal, False otherwise.\\n        assert_series_equal : Return True if left and right Series are equal,\\n            False otherwise.\\n        assert_frame_equal : Return True if left and right DataFrames are\\n            equal, False otherwise.\\n        numpy.array_equal : Return True if two arrays have the same shape\\n            and elements, False otherwise.\\n\\n        Notes\\n        -----\\n        This function requires that the elements have the same dtype as their\\n        respective elements in the other Series or DataFrame. However, the\\n        column labels do not need to have the same type, as long as they are\\n        still considered equal.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({1: [10], 2: [20]})\\n        >>> df\\n            1   2\\n        0  10  20\\n\\n        DataFrames df and exactly_equal have the same types and values for\\n        their elements and column labels, which will return True.\\n\\n        >>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})\\n        >>> exactly_equal\\n            1   2\\n        0  10  20\\n        >>> df.equals(exactly_equal)\\n        True\\n\\n        DataFrames df and different_column_type have the same element\\n        types and values, but have different types for the column labels,\\n        which will still return True.\\n\\n        >>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\\n        >>> different_column_type\\n           1.0  2.0\\n        0   10   20\\n        >>> df.equals(different_column_type)\\n        True\\n\\n        DataFrames df and different_data_type have different types for the\\n        same values for their elements, and will return False even though\\n        their column labels are the same values and types.\\n\\n        >>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\\n        >>> different_data_type\\n              1     2\\n        0  10.0  20.0\\n        >>> df.equals(different_data_type)\\n        False',\n",
       "  'code': 'def equals(self, other):\\n        \"\"\"\\n        Test whether two objects contain the same elements.\\n\\n        This function allows two Series or DataFrames to be compared against\\n        each other to see if they have the same shape and elements. NaNs in\\n        the same location are considered equal. The column headers do not\\n        need to have the same type, but the elements within the columns must\\n        be the same dtype.\\n\\n        Parameters\\n        ----------\\n        other : Series or DataFrame\\n            The other Series or DataFrame to be compared with the first.\\n\\n        Returns\\n        -------\\n        bool\\n            True if all elements are the same in both objects, False\\n            otherwise.\\n\\n        See Also\\n        --------\\n        Series.eq : Compare two Series objects of the same length\\n            and return a Series where each element is True if the element\\n            in each Series is equal, False otherwise.\\n        DataFrame.eq : Compare two DataFrame objects of the same shape and\\n            return a DataFrame where each element is True if the respective\\n            element in each DataFrame is equal, False otherwise.\\n        assert_series_equal : Return True if left and right Series are equal,\\n            False otherwise.\\n        assert_frame_equal : Return True if left and right DataFrames are\\n            equal, False otherwise.\\n        numpy.array_equal : Return True if two arrays have the same shape\\n            and elements, False otherwise.\\n\\n        Notes\\n        -----\\n        This function requires that the elements have the same dtype as their\\n        respective elements in the other Series or DataFrame. However, the\\n        column labels do not need to have the same type, as long as they are\\n        still considered equal.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({1: [10], 2: [20]})\\n        >>> df\\n            1   2\\n        0  10  20\\n\\n        DataFrames df and exactly_equal have the same types and values for\\n        their elements and column labels, which will return True.\\n\\n        >>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})\\n        >>> exactly_equal\\n            1   2\\n        0  10  20\\n        >>> df.equals(exactly_equal)\\n        True\\n\\n        DataFrames df and different_column_type have the same element\\n        types and values, but have different types for the column labels,\\n        which will still return True.\\n\\n        >>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\\n        >>> different_column_type\\n           1.0  2.0\\n        0   10   20\\n        >>> df.equals(different_column_type)\\n        True\\n\\n        DataFrames df and different_data_type have different types for the\\n        same values for their elements, and will return False even though\\n        their column labels are the same values and types.\\n\\n        >>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\\n        >>> different_data_type\\n              1     2\\n        0  10.0  20.0\\n        >>> df.equals(different_data_type)\\n        False\\n        \"\"\"\\n        if not isinstance(other, self._constructor):\\n            return False\\n        return self._data.equals(other._data)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame._take',\n",
       "  'docstring': 'Return the elements in the given *positional* indices along an axis.\\n\\n        This means that we are not indexing according to actual values in\\n        the index attribute of the object. We are indexing according to the\\n        actual position of the element in the object.\\n\\n        This is the internal version of ``.take()`` and will contain a wider\\n        selection of parameters useful for internal use but not as suitable\\n        for public usage.\\n\\n        Parameters\\n        ----------\\n        indices : array-like\\n            An array of ints indicating which positions to take.\\n        axis : int, default 0\\n            The axis on which to select elements. \"0\" means that we are\\n            selecting rows, \"1\" means that we are selecting columns, etc.\\n        is_copy : bool, default True\\n            Whether to return a copy of the original object or not.\\n\\n        Returns\\n        -------\\n        taken : same type as caller\\n            An array-like containing the elements taken from the object.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.take\\n        numpy.take',\n",
       "  'code': 'def _take(self, indices, axis=0, is_copy=True):\\n        \"\"\"\\n        Return the elements in the given *positional* indices along an axis.\\n\\n        This means that we are not indexing according to actual values in\\n        the index attribute of the object. We are indexing according to the\\n        actual position of the element in the object.\\n\\n        This is the internal version of ``.take()`` and will contain a wider\\n        selection of parameters useful for internal use but not as suitable\\n        for public usage.\\n\\n        Parameters\\n        ----------\\n        indices : array-like\\n            An array of ints indicating which positions to take.\\n        axis : int, default 0\\n            The axis on which to select elements. \"0\" means that we are\\n            selecting rows, \"1\" means that we are selecting columns, etc.\\n        is_copy : bool, default True\\n            Whether to return a copy of the original object or not.\\n\\n        Returns\\n        -------\\n        taken : same type as caller\\n            An array-like containing the elements taken from the object.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.take\\n        numpy.take\\n        \"\"\"\\n        self._consolidate_inplace()\\n\\n        new_data = self._data.take(indices,\\n                                   axis=self._get_block_manager_axis(axis),\\n                                   verify=True)\\n        result = self._constructor(new_data).__finalize__(self)\\n\\n        # Maybe set copy if we didn\\'t actually change the index.\\n        if is_copy:\\n            if not result._get_axis(axis).equals(self._get_axis(axis)):\\n                result._set_is_copy(self)\\n\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame.take',\n",
       "  'docstring': \"Return the elements in the given *positional* indices along an axis.\\n\\n        This means that we are not indexing according to actual values in\\n        the index attribute of the object. We are indexing according to the\\n        actual position of the element in the object.\\n\\n        Parameters\\n        ----------\\n        indices : array-like\\n            An array of ints indicating which positions to take.\\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\\n            The axis on which to select elements. ``0`` means that we are\\n            selecting rows, ``1`` means that we are selecting columns.\\n        convert : bool, default True\\n            Whether to convert negative indices into positive ones.\\n            For example, ``-1`` would map to the ``len(axis) - 1``.\\n            The conversions are similar to the behavior of indexing a\\n            regular Python list.\\n\\n            .. deprecated:: 0.21.0\\n               In the future, negative indices will always be converted.\\n\\n        is_copy : bool, default True\\n            Whether to return a copy of the original object or not.\\n        **kwargs\\n            For compatibility with :meth:`numpy.take`. Has no effect on the\\n            output.\\n\\n        Returns\\n        -------\\n        taken : same type as caller\\n            An array-like containing the elements taken from the object.\\n\\n        See Also\\n        --------\\n        DataFrame.loc : Select a subset of a DataFrame by labels.\\n        DataFrame.iloc : Select a subset of a DataFrame by positions.\\n        numpy.take : Take elements from an array along an axis.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([('falcon', 'bird',    389.0),\\n        ...                    ('parrot', 'bird',     24.0),\\n        ...                    ('lion',   'mammal',   80.5),\\n        ...                    ('monkey', 'mammal', np.nan)],\\n        ...                    columns=['name', 'class', 'max_speed'],\\n        ...                    index=[0, 2, 3, 1])\\n        >>> df\\n             name   class  max_speed\\n        0  falcon    bird      389.0\\n        2  parrot    bird       24.0\\n        3    lion  mammal       80.5\\n        1  monkey  mammal        NaN\\n\\n        Take elements at positions 0 and 3 along the axis 0 (default).\\n\\n        Note how the actual indices selected (0 and 1) do not correspond to\\n        our selected indices 0 and 3. That's because we are selecting the 0th\\n        and 3rd rows, not rows whose indices equal 0 and 3.\\n\\n        >>> df.take([0, 3])\\n             name   class  max_speed\\n        0  falcon    bird      389.0\\n        1  monkey  mammal        NaN\\n\\n        Take elements at indices 1 and 2 along the axis 1 (column selection).\\n\\n        >>> df.take([1, 2], axis=1)\\n            class  max_speed\\n        0    bird      389.0\\n        2    bird       24.0\\n        3  mammal       80.5\\n        1  mammal        NaN\\n\\n        We may take elements using negative integers for positive indices,\\n        starting from the end of the object, just like with Python lists.\\n\\n        >>> df.take([-1, -2])\\n             name   class  max_speed\\n        1  monkey  mammal        NaN\\n        3    lion  mammal       80.5\",\n",
       "  'code': 'def take(self, indices, axis=0, convert=None, is_copy=True, **kwargs):\\n        \"\"\"\\n        Return the elements in the given *positional* indices along an axis.\\n\\n        This means that we are not indexing according to actual values in\\n        the index attribute of the object. We are indexing according to the\\n        actual position of the element in the object.\\n\\n        Parameters\\n        ----------\\n        indices : array-like\\n            An array of ints indicating which positions to take.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\', None}, default 0\\n            The axis on which to select elements. ``0`` means that we are\\n            selecting rows, ``1`` means that we are selecting columns.\\n        convert : bool, default True\\n            Whether to convert negative indices into positive ones.\\n            For example, ``-1`` would map to the ``len(axis) - 1``.\\n            The conversions are similar to the behavior of indexing a\\n            regular Python list.\\n\\n            .. deprecated:: 0.21.0\\n               In the future, negative indices will always be converted.\\n\\n        is_copy : bool, default True\\n            Whether to return a copy of the original object or not.\\n        **kwargs\\n            For compatibility with :meth:`numpy.take`. Has no effect on the\\n            output.\\n\\n        Returns\\n        -------\\n        taken : same type as caller\\n            An array-like containing the elements taken from the object.\\n\\n        See Also\\n        --------\\n        DataFrame.loc : Select a subset of a DataFrame by labels.\\n        DataFrame.iloc : Select a subset of a DataFrame by positions.\\n        numpy.take : Take elements from an array along an axis.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([(\\'falcon\\', \\'bird\\',    389.0),\\n        ...                    (\\'parrot\\', \\'bird\\',     24.0),\\n        ...                    (\\'lion\\',   \\'mammal\\',   80.5),\\n        ...                    (\\'monkey\\', \\'mammal\\', np.nan)],\\n        ...                    columns=[\\'name\\', \\'class\\', \\'max_speed\\'],\\n        ...                    index=[0, 2, 3, 1])\\n        >>> df\\n             name   class  max_speed\\n        0  falcon    bird      389.0\\n        2  parrot    bird       24.0\\n        3    lion  mammal       80.5\\n        1  monkey  mammal        NaN\\n\\n        Take elements at positions 0 and 3 along the axis 0 (default).\\n\\n        Note how the actual indices selected (0 and 1) do not correspond to\\n        our selected indices 0 and 3. That\\'s because we are selecting the 0th\\n        and 3rd rows, not rows whose indices equal 0 and 3.\\n\\n        >>> df.take([0, 3])\\n             name   class  max_speed\\n        0  falcon    bird      389.0\\n        1  monkey  mammal        NaN\\n\\n        Take elements at indices 1 and 2 along the axis 1 (column selection).\\n\\n        >>> df.take([1, 2], axis=1)\\n            class  max_speed\\n        0    bird      389.0\\n        2    bird       24.0\\n        3  mammal       80.5\\n        1  mammal        NaN\\n\\n        We may take elements using negative integers for positive indices,\\n        starting from the end of the object, just like with Python lists.\\n\\n        >>> df.take([-1, -2])\\n             name   class  max_speed\\n        1  monkey  mammal        NaN\\n        3    lion  mammal       80.5\\n        \"\"\"\\n        if convert is not None:\\n            msg = (\"The \\'convert\\' parameter is deprecated \"\\n                   \"and will be removed in a future version.\")\\n            warnings.warn(msg, FutureWarning, stacklevel=2)\\n\\n        nv.validate_take(tuple(), kwargs)\\n        return self._take(indices, axis=axis, is_copy=is_copy)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame.sample',\n",
       "  'docstring': \"Return a random sample of items from an axis of object.\\n\\n        You can use `random_state` for reproducibility.\\n\\n        Parameters\\n        ----------\\n        n : int, optional\\n            Number of items from axis to return. Cannot be used with `frac`.\\n            Default = 1 if `frac` = None.\\n        frac : float, optional\\n            Fraction of axis items to return. Cannot be used with `n`.\\n        replace : bool, default False\\n            Sample with or without replacement.\\n        weights : str or ndarray-like, optional\\n            Default 'None' results in equal probability weighting.\\n            If passed a Series, will align with target object on index. Index\\n            values in weights not found in sampled object will be ignored and\\n            index values in sampled object not in weights will be assigned\\n            weights of zero.\\n            If called on a DataFrame, will accept the name of a column\\n            when axis = 0.\\n            Unless weights are a Series, weights must be same length as axis\\n            being sampled.\\n            If weights do not sum to 1, they will be normalized to sum to 1.\\n            Missing values in the weights column will be treated as zero.\\n            Infinite values not allowed.\\n        random_state : int or numpy.random.RandomState, optional\\n            Seed for the random number generator (if int), or numpy RandomState\\n            object.\\n        axis : int or string, optional\\n            Axis to sample. Accepts axis number or name. Default is stat axis\\n            for given data type (0 for Series and DataFrames, 1 for Panels).\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            A new object of same type as caller containing `n` items randomly\\n            sampled from the caller object.\\n\\n        See Also\\n        --------\\n        numpy.random.choice: Generates a random sample from a given 1-D numpy\\n            array.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\\n        ...                    'num_wings': [2, 0, 0, 0],\\n        ...                    'num_specimen_seen': [10, 2, 1, 8]},\\n        ...                   index=['falcon', 'dog', 'spider', 'fish'])\\n        >>> df\\n                num_legs  num_wings  num_specimen_seen\\n        falcon         2          2                 10\\n        dog            4          0                  2\\n        spider         8          0                  1\\n        fish           0          0                  8\\n\\n        Extract 3 random elements from the ``Series`` ``df['num_legs']``:\\n        Note that we use `random_state` to ensure the reproducibility of\\n        the examples.\\n\\n        >>> df['num_legs'].sample(n=3, random_state=1)\\n        fish      0\\n        spider    8\\n        falcon    2\\n        Name: num_legs, dtype: int64\\n\\n        A random 50% sample of the ``DataFrame`` with replacement:\\n\\n        >>> df.sample(frac=0.5, replace=True, random_state=1)\\n              num_legs  num_wings  num_specimen_seen\\n        dog          4          0                  2\\n        fish         0          0                  8\\n\\n        Using a DataFrame column as weights. Rows with larger value in the\\n        `num_specimen_seen` column are more likely to be sampled.\\n\\n        >>> df.sample(n=2, weights='num_specimen_seen', random_state=1)\\n                num_legs  num_wings  num_specimen_seen\\n        falcon         2          2                 10\\n        fish           0          0                  8\",\n",
       "  'code': 'def sample(self, n=None, frac=None, replace=False, weights=None,\\n               random_state=None, axis=None):\\n        \"\"\"\\n        Return a random sample of items from an axis of object.\\n\\n        You can use `random_state` for reproducibility.\\n\\n        Parameters\\n        ----------\\n        n : int, optional\\n            Number of items from axis to return. Cannot be used with `frac`.\\n            Default = 1 if `frac` = None.\\n        frac : float, optional\\n            Fraction of axis items to return. Cannot be used with `n`.\\n        replace : bool, default False\\n            Sample with or without replacement.\\n        weights : str or ndarray-like, optional\\n            Default \\'None\\' results in equal probability weighting.\\n            If passed a Series, will align with target object on index. Index\\n            values in weights not found in sampled object will be ignored and\\n            index values in sampled object not in weights will be assigned\\n            weights of zero.\\n            If called on a DataFrame, will accept the name of a column\\n            when axis = 0.\\n            Unless weights are a Series, weights must be same length as axis\\n            being sampled.\\n            If weights do not sum to 1, they will be normalized to sum to 1.\\n            Missing values in the weights column will be treated as zero.\\n            Infinite values not allowed.\\n        random_state : int or numpy.random.RandomState, optional\\n            Seed for the random number generator (if int), or numpy RandomState\\n            object.\\n        axis : int or string, optional\\n            Axis to sample. Accepts axis number or name. Default is stat axis\\n            for given data type (0 for Series and DataFrames, 1 for Panels).\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            A new object of same type as caller containing `n` items randomly\\n            sampled from the caller object.\\n\\n        See Also\\n        --------\\n        numpy.random.choice: Generates a random sample from a given 1-D numpy\\n            array.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'num_legs\\': [2, 4, 8, 0],\\n        ...                    \\'num_wings\\': [2, 0, 0, 0],\\n        ...                    \\'num_specimen_seen\\': [10, 2, 1, 8]},\\n        ...                   index=[\\'falcon\\', \\'dog\\', \\'spider\\', \\'fish\\'])\\n        >>> df\\n                num_legs  num_wings  num_specimen_seen\\n        falcon         2          2                 10\\n        dog            4          0                  2\\n        spider         8          0                  1\\n        fish           0          0                  8\\n\\n        Extract 3 random elements from the ``Series`` ``df[\\'num_legs\\']``:\\n        Note that we use `random_state` to ensure the reproducibility of\\n        the examples.\\n\\n        >>> df[\\'num_legs\\'].sample(n=3, random_state=1)\\n        fish      0\\n        spider    8\\n        falcon    2\\n        Name: num_legs, dtype: int64\\n\\n        A random 50% sample of the ``DataFrame`` with replacement:\\n\\n        >>> df.sample(frac=0.5, replace=True, random_state=1)\\n              num_legs  num_wings  num_specimen_seen\\n        dog          4          0                  2\\n        fish         0          0                  8\\n\\n        Using a DataFrame column as weights. Rows with larger value in the\\n        `num_specimen_seen` column are more likely to be sampled.\\n\\n        >>> df.sample(n=2, weights=\\'num_specimen_seen\\', random_state=1)\\n                num_legs  num_wings  num_specimen_seen\\n        falcon         2          2                 10\\n        fish           0          0                  8\\n        \"\"\"\\n\\n        if axis is None:\\n            axis = self._stat_axis_number\\n\\n        axis = self._get_axis_number(axis)\\n        axis_length = self.shape[axis]\\n\\n        # Process random_state argument\\n        rs = com.random_state(random_state)\\n\\n        # Check weights for compliance\\n        if weights is not None:\\n\\n            # If a series, align with frame\\n            if isinstance(weights, pd.Series):\\n                weights = weights.reindex(self.axes[axis])\\n\\n            # Strings acceptable if a dataframe and axis = 0\\n            if isinstance(weights, str):\\n                if isinstance(self, pd.DataFrame):\\n                    if axis == 0:\\n                        try:\\n                            weights = self[weights]\\n                        except KeyError:\\n                            raise KeyError(\"String passed to weights not a \"\\n                                           \"valid column\")\\n                    else:\\n                        raise ValueError(\"Strings can only be passed to \"\\n                                         \"weights when sampling from rows on \"\\n                                         \"a DataFrame\")\\n                else:\\n                    raise ValueError(\"Strings cannot be passed as weights \"\\n                                     \"when sampling from a Series or Panel.\")\\n\\n            weights = pd.Series(weights, dtype=\\'float64\\')\\n\\n            if len(weights) != axis_length:\\n                raise ValueError(\"Weights and axis to be sampled must be of \"\\n                                 \"same length\")\\n\\n            if (weights == np.inf).any() or (weights == -np.inf).any():\\n                raise ValueError(\"weight vector may not include `inf` values\")\\n\\n            if (weights < 0).any():\\n                raise ValueError(\"weight vector many not include negative \"\\n                                 \"values\")\\n\\n            # If has nan, set to zero.\\n            weights = weights.fillna(0)\\n\\n            # Renormalize if don\\'t sum to 1\\n            if weights.sum() != 1:\\n                if weights.sum() != 0:\\n                    weights = weights / weights.sum()\\n                else:\\n                    raise ValueError(\"Invalid weights: weights sum to zero\")\\n\\n            weights = weights.values\\n\\n        # If no frac or n, default to n=1.\\n        if n is None and frac is None:\\n            n = 1\\n        elif n is not None and frac is None and n % 1 != 0:\\n            raise ValueError(\"Only integers accepted as `n` values\")\\n        elif n is None and frac is not None:\\n            n = int(round(frac * axis_length))\\n        elif n is not None and frac is not None:\\n            raise ValueError(\\'Please enter a value for `frac` OR `n`, not \\'\\n                             \\'both\\')\\n\\n        # Check for negative sizes\\n        if n < 0:\\n            raise ValueError(\"A negative number of rows requested. Please \"\\n                             \"provide positive value.\")\\n\\n        locs = rs.choice(axis_length, size=n, replace=replace, p=weights)\\n        return self.take(locs, axis=axis, is_copy=False)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame.as_matrix',\n",
       "  'docstring': \"Convert the frame to its Numpy-array representation.\\n\\n        .. deprecated:: 0.23.0\\n            Use :meth:`DataFrame.values` instead.\\n\\n        Parameters\\n        ----------\\n        columns : list, optional, default:None\\n            If None, return all columns, otherwise, returns specified columns.\\n\\n        Returns\\n        -------\\n        values : ndarray\\n            If the caller is heterogeneous and contains booleans or objects,\\n            the result will be of dtype=object. See Notes.\\n\\n        See Also\\n        --------\\n        DataFrame.values\\n\\n        Notes\\n        -----\\n        Return is NOT a Numpy-matrix, rather, a Numpy-array.\\n\\n        The dtype will be a lower-common-denominator dtype (implicit\\n        upcasting); that is to say if the dtypes (even of numeric types)\\n        are mixed, the one that accommodates all will be chosen. Use this\\n        with care if you are not dealing with the blocks.\\n\\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\\n        float32.  If dtypes are int32 and uint8, dtype will be upcase to\\n        int32. By numpy.find_common_type convention, mixing int64 and uint64\\n        will result in a float64 dtype.\\n\\n        This method is provided for backwards compatibility. Generally,\\n        it is recommended to use '.values'.\",\n",
       "  'code': 'def as_matrix(self, columns=None):\\n        \"\"\"\\n        Convert the frame to its Numpy-array representation.\\n\\n        .. deprecated:: 0.23.0\\n            Use :meth:`DataFrame.values` instead.\\n\\n        Parameters\\n        ----------\\n        columns : list, optional, default:None\\n            If None, return all columns, otherwise, returns specified columns.\\n\\n        Returns\\n        -------\\n        values : ndarray\\n            If the caller is heterogeneous and contains booleans or objects,\\n            the result will be of dtype=object. See Notes.\\n\\n        See Also\\n        --------\\n        DataFrame.values\\n\\n        Notes\\n        -----\\n        Return is NOT a Numpy-matrix, rather, a Numpy-array.\\n\\n        The dtype will be a lower-common-denominator dtype (implicit\\n        upcasting); that is to say if the dtypes (even of numeric types)\\n        are mixed, the one that accommodates all will be chosen. Use this\\n        with care if you are not dealing with the blocks.\\n\\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\\n        float32.  If dtypes are int32 and uint8, dtype will be upcase to\\n        int32. By numpy.find_common_type convention, mixing int64 and uint64\\n        will result in a float64 dtype.\\n\\n        This method is provided for backwards compatibility. Generally,\\n        it is recommended to use \\'.values\\'.\\n        \"\"\"\\n        warnings.warn(\"Method .as_matrix will be removed in a future version. \"\\n                      \"Use .values instead.\", FutureWarning, stacklevel=2)\\n        self._consolidate_inplace()\\n        return self._data.as_array(transpose=self._AXIS_REVERSED,\\n                                   items=columns)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame.values',\n",
       "  'docstring': \"Return a Numpy representation of the DataFrame.\\n\\n        .. warning::\\n\\n           We recommend using :meth:`DataFrame.to_numpy` instead.\\n\\n        Only the values in the DataFrame will be returned, the axes labels\\n        will be removed.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n            The values of the DataFrame.\\n\\n        See Also\\n        --------\\n        DataFrame.to_numpy : Recommended alternative to this method.\\n        DataFrame.index : Retrieve the index labels.\\n        DataFrame.columns : Retrieving the column names.\\n\\n        Notes\\n        -----\\n        The dtype will be a lower-common-denominator dtype (implicit\\n        upcasting); that is to say if the dtypes (even of numeric types)\\n        are mixed, the one that accommodates all will be chosen. Use this\\n        with care if you are not dealing with the blocks.\\n\\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\\n        float32.  If dtypes are int32 and uint8, dtype will be upcast to\\n        int32. By :func:`numpy.find_common_type` convention, mixing int64\\n        and uint64 will result in a float64 dtype.\\n\\n        Examples\\n        --------\\n        A DataFrame where all columns are the same type (e.g., int64) results\\n        in an array of the same type.\\n\\n        >>> df = pd.DataFrame({'age':    [ 3,  29],\\n        ...                    'height': [94, 170],\\n        ...                    'weight': [31, 115]})\\n        >>> df\\n           age  height  weight\\n        0    3      94      31\\n        1   29     170     115\\n        >>> df.dtypes\\n        age       int64\\n        height    int64\\n        weight    int64\\n        dtype: object\\n        >>> df.values\\n        array([[  3,  94,  31],\\n               [ 29, 170, 115]], dtype=int64)\\n\\n        A DataFrame with mixed type columns(e.g., str/object, int64, float32)\\n        results in an ndarray of the broadest type that accommodates these\\n        mixed types (e.g., object).\\n\\n        >>> df2 = pd.DataFrame([('parrot',   24.0, 'second'),\\n        ...                     ('lion',     80.5, 1),\\n        ...                     ('monkey', np.nan, None)],\\n        ...                   columns=('name', 'max_speed', 'rank'))\\n        >>> df2.dtypes\\n        name          object\\n        max_speed    float64\\n        rank          object\\n        dtype: object\\n        >>> df2.values\\n        array([['parrot', 24.0, 'second'],\\n               ['lion', 80.5, 1],\\n               ['monkey', nan, None]], dtype=object)\",\n",
       "  'code': 'def values(self):\\n        \"\"\"\\n        Return a Numpy representation of the DataFrame.\\n\\n        .. warning::\\n\\n           We recommend using :meth:`DataFrame.to_numpy` instead.\\n\\n        Only the values in the DataFrame will be returned, the axes labels\\n        will be removed.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n            The values of the DataFrame.\\n\\n        See Also\\n        --------\\n        DataFrame.to_numpy : Recommended alternative to this method.\\n        DataFrame.index : Retrieve the index labels.\\n        DataFrame.columns : Retrieving the column names.\\n\\n        Notes\\n        -----\\n        The dtype will be a lower-common-denominator dtype (implicit\\n        upcasting); that is to say if the dtypes (even of numeric types)\\n        are mixed, the one that accommodates all will be chosen. Use this\\n        with care if you are not dealing with the blocks.\\n\\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\\n        float32.  If dtypes are int32 and uint8, dtype will be upcast to\\n        int32. By :func:`numpy.find_common_type` convention, mixing int64\\n        and uint64 will result in a float64 dtype.\\n\\n        Examples\\n        --------\\n        A DataFrame where all columns are the same type (e.g., int64) results\\n        in an array of the same type.\\n\\n        >>> df = pd.DataFrame({\\'age\\':    [ 3,  29],\\n        ...                    \\'height\\': [94, 170],\\n        ...                    \\'weight\\': [31, 115]})\\n        >>> df\\n           age  height  weight\\n        0    3      94      31\\n        1   29     170     115\\n        >>> df.dtypes\\n        age       int64\\n        height    int64\\n        weight    int64\\n        dtype: object\\n        >>> df.values\\n        array([[  3,  94,  31],\\n               [ 29, 170, 115]], dtype=int64)\\n\\n        A DataFrame with mixed type columns(e.g., str/object, int64, float32)\\n        results in an ndarray of the broadest type that accommodates these\\n        mixed types (e.g., object).\\n\\n        >>> df2 = pd.DataFrame([(\\'parrot\\',   24.0, \\'second\\'),\\n        ...                     (\\'lion\\',     80.5, 1),\\n        ...                     (\\'monkey\\', np.nan, None)],\\n        ...                   columns=(\\'name\\', \\'max_speed\\', \\'rank\\'))\\n        >>> df2.dtypes\\n        name          object\\n        max_speed    float64\\n        rank          object\\n        dtype: object\\n        >>> df2.values\\n        array([[\\'parrot\\', 24.0, \\'second\\'],\\n               [\\'lion\\', 80.5, 1],\\n               [\\'monkey\\', nan, None]], dtype=object)\\n        \"\"\"\\n        self._consolidate_inplace()\\n        return self._data.as_array(transpose=self._AXIS_REVERSED)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame.astype',\n",
       "  'docstring': \"Cast a pandas object to a specified dtype ``dtype``.\\n\\n        Parameters\\n        ----------\\n        dtype : data type, or dict of column name -> data type\\n            Use a numpy.dtype or Python type to cast entire pandas object to\\n            the same type. Alternatively, use {col: dtype, ...}, where col is a\\n            column label and dtype is a numpy.dtype or Python type to cast one\\n            or more of the DataFrame's columns to column-specific types.\\n        copy : bool, default True\\n            Return a copy when ``copy=True`` (be very careful setting\\n            ``copy=False`` as changes to values then may propagate to other\\n            pandas objects).\\n        errors : {'raise', 'ignore'}, default 'raise'\\n            Control raising of exceptions on invalid data for provided dtype.\\n\\n            - ``raise`` : allow exceptions to be raised\\n            - ``ignore`` : suppress exceptions. On error return original object\\n\\n            .. versionadded:: 0.20.0\\n\\n        kwargs : keyword arguments to pass on to the constructor\\n\\n        Returns\\n        -------\\n        casted : same type as caller\\n\\n        See Also\\n        --------\\n        to_datetime : Convert argument to datetime.\\n        to_timedelta : Convert argument to timedelta.\\n        to_numeric : Convert argument to a numeric type.\\n        numpy.ndarray.astype : Cast a numpy array to a specified type.\\n\\n        Examples\\n        --------\\n        >>> ser = pd.Series([1, 2], dtype='int32')\\n        >>> ser\\n        0    1\\n        1    2\\n        dtype: int32\\n        >>> ser.astype('int64')\\n        0    1\\n        1    2\\n        dtype: int64\\n\\n        Convert to categorical type:\\n\\n        >>> ser.astype('category')\\n        0    1\\n        1    2\\n        dtype: category\\n        Categories (2, int64): [1, 2]\\n\\n        Convert to ordered categorical type with custom ordering:\\n\\n        >>> cat_dtype = pd.api.types.CategoricalDtype(\\n        ...                     categories=[2, 1], ordered=True)\\n        >>> ser.astype(cat_dtype)\\n        0    1\\n        1    2\\n        dtype: category\\n        Categories (2, int64): [2 < 1]\\n\\n        Note that using ``copy=False`` and changing data on a new\\n        pandas object may propagate changes:\\n\\n        >>> s1 = pd.Series([1,2])\\n        >>> s2 = s1.astype('int64', copy=False)\\n        >>> s2[0] = 10\\n        >>> s1  # note that s1[0] has changed too\\n        0    10\\n        1     2\\n        dtype: int64\",\n",
       "  'code': 'def astype(self, dtype, copy=True, errors=\\'raise\\', **kwargs):\\n        \"\"\"\\n        Cast a pandas object to a specified dtype ``dtype``.\\n\\n        Parameters\\n        ----------\\n        dtype : data type, or dict of column name -> data type\\n            Use a numpy.dtype or Python type to cast entire pandas object to\\n            the same type. Alternatively, use {col: dtype, ...}, where col is a\\n            column label and dtype is a numpy.dtype or Python type to cast one\\n            or more of the DataFrame\\'s columns to column-specific types.\\n        copy : bool, default True\\n            Return a copy when ``copy=True`` (be very careful setting\\n            ``copy=False`` as changes to values then may propagate to other\\n            pandas objects).\\n        errors : {\\'raise\\', \\'ignore\\'}, default \\'raise\\'\\n            Control raising of exceptions on invalid data for provided dtype.\\n\\n            - ``raise`` : allow exceptions to be raised\\n            - ``ignore`` : suppress exceptions. On error return original object\\n\\n            .. versionadded:: 0.20.0\\n\\n        kwargs : keyword arguments to pass on to the constructor\\n\\n        Returns\\n        -------\\n        casted : same type as caller\\n\\n        See Also\\n        --------\\n        to_datetime : Convert argument to datetime.\\n        to_timedelta : Convert argument to timedelta.\\n        to_numeric : Convert argument to a numeric type.\\n        numpy.ndarray.astype : Cast a numpy array to a specified type.\\n\\n        Examples\\n        --------\\n        >>> ser = pd.Series([1, 2], dtype=\\'int32\\')\\n        >>> ser\\n        0    1\\n        1    2\\n        dtype: int32\\n        >>> ser.astype(\\'int64\\')\\n        0    1\\n        1    2\\n        dtype: int64\\n\\n        Convert to categorical type:\\n\\n        >>> ser.astype(\\'category\\')\\n        0    1\\n        1    2\\n        dtype: category\\n        Categories (2, int64): [1, 2]\\n\\n        Convert to ordered categorical type with custom ordering:\\n\\n        >>> cat_dtype = pd.api.types.CategoricalDtype(\\n        ...                     categories=[2, 1], ordered=True)\\n        >>> ser.astype(cat_dtype)\\n        0    1\\n        1    2\\n        dtype: category\\n        Categories (2, int64): [2 < 1]\\n\\n        Note that using ``copy=False`` and changing data on a new\\n        pandas object may propagate changes:\\n\\n        >>> s1 = pd.Series([1,2])\\n        >>> s2 = s1.astype(\\'int64\\', copy=False)\\n        >>> s2[0] = 10\\n        >>> s1  # note that s1[0] has changed too\\n        0    10\\n        1     2\\n        dtype: int64\\n        \"\"\"\\n        if is_dict_like(dtype):\\n            if self.ndim == 1:  # i.e. Series\\n                if len(dtype) > 1 or self.name not in dtype:\\n                    raise KeyError(\\'Only the Series name can be used for \\'\\n                                   \\'the key in Series dtype mappings.\\')\\n                new_type = dtype[self.name]\\n                return self.astype(new_type, copy, errors, **kwargs)\\n            elif self.ndim > 2:\\n                raise NotImplementedError(\\n                    \\'astype() only accepts a dtype arg of type dict when \\'\\n                    \\'invoked on Series and DataFrames. A single dtype must be \\'\\n                    \\'specified when invoked on a Panel.\\'\\n                )\\n            for col_name in dtype.keys():\\n                if col_name not in self:\\n                    raise KeyError(\\'Only a column name can be used for the \\'\\n                                   \\'key in a dtype mappings argument.\\')\\n            results = []\\n            for col_name, col in self.iteritems():\\n                if col_name in dtype:\\n                    results.append(col.astype(dtype=dtype[col_name], copy=copy,\\n                                              errors=errors, **kwargs))\\n                else:\\n                    results.append(results.append(col.copy() if copy else col))\\n\\n        elif is_extension_array_dtype(dtype) and self.ndim > 1:\\n            # GH 18099/22869: columnwise conversion to extension dtype\\n            # GH 24704: use iloc to handle duplicate column names\\n            results = (self.iloc[:, i].astype(dtype, copy=copy)\\n                       for i in range(len(self.columns)))\\n\\n        else:\\n            # else, only a single dtype is given\\n            new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\\n                                         **kwargs)\\n            return self._constructor(new_data).__finalize__(self)\\n\\n        # GH 19920: retain column metadata after concat\\n        result = pd.concat(results, axis=1, copy=False)\\n        result.columns = self.columns\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame.copy',\n",
       "  'docstring': 'Make a copy of this object\\'s indices and data.\\n\\n        When ``deep=True`` (default), a new object will be created with a\\n        copy of the calling object\\'s data and indices. Modifications to\\n        the data or indices of the copy will not be reflected in the\\n        original object (see notes below).\\n\\n        When ``deep=False``, a new object will be created without copying\\n        the calling object\\'s data or index (only references to the data\\n        and index are copied). Any changes to the data of the original\\n        will be reflected in the shallow copy (and vice versa).\\n\\n        Parameters\\n        ----------\\n        deep : bool, default True\\n            Make a deep copy, including a copy of the data and the indices.\\n            With ``deep=False`` neither the indices nor the data are copied.\\n\\n        Returns\\n        -------\\n        copy : Series, DataFrame or Panel\\n            Object type matches caller.\\n\\n        Notes\\n        -----\\n        When ``deep=True``, data is copied but actual Python objects\\n        will not be copied recursively, only the reference to the object.\\n        This is in contrast to `copy.deepcopy` in the Standard Library,\\n        which recursively copies object data (see examples below).\\n\\n        While ``Index`` objects are copied when ``deep=True``, the underlying\\n        numpy array is not copied for performance reasons. Since ``Index`` is\\n        immutable, the underlying data can be safely shared and a copy\\n        is not needed.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\\n        >>> s\\n        a    1\\n        b    2\\n        dtype: int64\\n\\n        >>> s_copy = s.copy()\\n        >>> s_copy\\n        a    1\\n        b    2\\n        dtype: int64\\n\\n        **Shallow copy versus default (deep) copy:**\\n\\n        >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\\n        >>> deep = s.copy()\\n        >>> shallow = s.copy(deep=False)\\n\\n        Shallow copy shares data and index with original.\\n\\n        >>> s is shallow\\n        False\\n        >>> s.values is shallow.values and s.index is shallow.index\\n        True\\n\\n        Deep copy has own copy of data and index.\\n\\n        >>> s is deep\\n        False\\n        >>> s.values is deep.values or s.index is deep.index\\n        False\\n\\n        Updates to the data shared by shallow copy and original is reflected\\n        in both; deep copy remains unchanged.\\n\\n        >>> s[0] = 3\\n        >>> shallow[1] = 4\\n        >>> s\\n        a    3\\n        b    4\\n        dtype: int64\\n        >>> shallow\\n        a    3\\n        b    4\\n        dtype: int64\\n        >>> deep\\n        a    1\\n        b    2\\n        dtype: int64\\n\\n        Note that when copying an object containing Python objects, a deep copy\\n        will copy the data, but will not do so recursively. Updating a nested\\n        data object will be reflected in the deep copy.\\n\\n        >>> s = pd.Series([[1, 2], [3, 4]])\\n        >>> deep = s.copy()\\n        >>> s[0][0] = 10\\n        >>> s\\n        0    [10, 2]\\n        1     [3, 4]\\n        dtype: object\\n        >>> deep\\n        0    [10, 2]\\n        1     [3, 4]\\n        dtype: object',\n",
       "  'code': 'def copy(self, deep=True):\\n        \"\"\"\\n        Make a copy of this object\\'s indices and data.\\n\\n        When ``deep=True`` (default), a new object will be created with a\\n        copy of the calling object\\'s data and indices. Modifications to\\n        the data or indices of the copy will not be reflected in the\\n        original object (see notes below).\\n\\n        When ``deep=False``, a new object will be created without copying\\n        the calling object\\'s data or index (only references to the data\\n        and index are copied). Any changes to the data of the original\\n        will be reflected in the shallow copy (and vice versa).\\n\\n        Parameters\\n        ----------\\n        deep : bool, default True\\n            Make a deep copy, including a copy of the data and the indices.\\n            With ``deep=False`` neither the indices nor the data are copied.\\n\\n        Returns\\n        -------\\n        copy : Series, DataFrame or Panel\\n            Object type matches caller.\\n\\n        Notes\\n        -----\\n        When ``deep=True``, data is copied but actual Python objects\\n        will not be copied recursively, only the reference to the object.\\n        This is in contrast to `copy.deepcopy` in the Standard Library,\\n        which recursively copies object data (see examples below).\\n\\n        While ``Index`` objects are copied when ``deep=True``, the underlying\\n        numpy array is not copied for performance reasons. Since ``Index`` is\\n        immutable, the underlying data can be safely shared and a copy\\n        is not needed.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\\n        >>> s\\n        a    1\\n        b    2\\n        dtype: int64\\n\\n        >>> s_copy = s.copy()\\n        >>> s_copy\\n        a    1\\n        b    2\\n        dtype: int64\\n\\n        **Shallow copy versus default (deep) copy:**\\n\\n        >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\\n        >>> deep = s.copy()\\n        >>> shallow = s.copy(deep=False)\\n\\n        Shallow copy shares data and index with original.\\n\\n        >>> s is shallow\\n        False\\n        >>> s.values is shallow.values and s.index is shallow.index\\n        True\\n\\n        Deep copy has own copy of data and index.\\n\\n        >>> s is deep\\n        False\\n        >>> s.values is deep.values or s.index is deep.index\\n        False\\n\\n        Updates to the data shared by shallow copy and original is reflected\\n        in both; deep copy remains unchanged.\\n\\n        >>> s[0] = 3\\n        >>> shallow[1] = 4\\n        >>> s\\n        a    3\\n        b    4\\n        dtype: int64\\n        >>> shallow\\n        a    3\\n        b    4\\n        dtype: int64\\n        >>> deep\\n        a    1\\n        b    2\\n        dtype: int64\\n\\n        Note that when copying an object containing Python objects, a deep copy\\n        will copy the data, but will not do so recursively. Updating a nested\\n        data object will be reflected in the deep copy.\\n\\n        >>> s = pd.Series([[1, 2], [3, 4]])\\n        >>> deep = s.copy()\\n        >>> s[0][0] = 10\\n        >>> s\\n        0    [10, 2]\\n        1     [3, 4]\\n        dtype: object\\n        >>> deep\\n        0    [10, 2]\\n        1     [3, 4]\\n        dtype: object\\n        \"\"\"\\n        data = self._data.copy(deep=deep)\\n        return self._constructor(data).__finalize__(self)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame.infer_objects',\n",
       "  'docstring': 'Attempt to infer better dtypes for object columns.\\n\\n        Attempts soft conversion of object-dtyped\\n        columns, leaving non-object and unconvertible\\n        columns unchanged. The inference rules are the\\n        same as during normal Series/DataFrame construction.\\n\\n        .. versionadded:: 0.21.0\\n\\n        Returns\\n        -------\\n        converted : same type as input object\\n\\n        See Also\\n        --------\\n        to_datetime : Convert argument to datetime.\\n        to_timedelta : Convert argument to timedelta.\\n        to_numeric : Convert argument to numeric type.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]})\\n        >>> df = df.iloc[1:]\\n        >>> df\\n           A\\n        1  1\\n        2  2\\n        3  3\\n\\n        >>> df.dtypes\\n        A    object\\n        dtype: object\\n\\n        >>> df.infer_objects().dtypes\\n        A    int64\\n        dtype: object',\n",
       "  'code': 'def infer_objects(self):\\n        \"\"\"\\n        Attempt to infer better dtypes for object columns.\\n\\n        Attempts soft conversion of object-dtyped\\n        columns, leaving non-object and unconvertible\\n        columns unchanged. The inference rules are the\\n        same as during normal Series/DataFrame construction.\\n\\n        .. versionadded:: 0.21.0\\n\\n        Returns\\n        -------\\n        converted : same type as input object\\n\\n        See Also\\n        --------\\n        to_datetime : Convert argument to datetime.\\n        to_timedelta : Convert argument to timedelta.\\n        to_numeric : Convert argument to numeric type.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]})\\n        >>> df = df.iloc[1:]\\n        >>> df\\n           A\\n        1  1\\n        2  2\\n        3  3\\n\\n        >>> df.dtypes\\n        A    object\\n        dtype: object\\n\\n        >>> df.infer_objects().dtypes\\n        A    int64\\n        dtype: object\\n        \"\"\"\\n        # numeric=False necessary to only soft convert;\\n        # python objects will still be converted to\\n        # native numpy numeric types\\n        return self._constructor(\\n            self._data.convert(datetime=True, numeric=False,\\n                               timedelta=True, coerce=False,\\n                               copy=True)).__finalize__(self)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame.clip',\n",
       "  'docstring': \"Trim values at input threshold(s).\\n\\n        Assigns values outside boundary to boundary values. Thresholds\\n        can be singular values or array like, and in the latter case\\n        the clipping is performed element-wise in the specified axis.\\n\\n        Parameters\\n        ----------\\n        lower : float or array_like, default None\\n            Minimum threshold value. All values below this\\n            threshold will be set to it.\\n        upper : float or array_like, default None\\n            Maximum threshold value. All values above this\\n            threshold will be set to it.\\n        axis : int or str axis name, optional\\n            Align object with lower and upper along the given axis.\\n        inplace : bool, default False\\n            Whether to perform the operation in place on the data.\\n\\n            .. versionadded:: 0.21.0\\n        *args, **kwargs\\n            Additional keywords have no effect but might be accepted\\n            for compatibility with numpy.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Same type as calling object with the values outside the\\n            clip boundaries replaced.\\n\\n        Examples\\n        --------\\n        >>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\\n        >>> df = pd.DataFrame(data)\\n        >>> df\\n           col_0  col_1\\n        0      9     -2\\n        1     -3     -7\\n        2      0      6\\n        3     -1      8\\n        4      5     -5\\n\\n        Clips per column using lower and upper thresholds:\\n\\n        >>> df.clip(-4, 6)\\n           col_0  col_1\\n        0      6     -2\\n        1     -3     -4\\n        2      0      6\\n        3     -1      6\\n        4      5     -4\\n\\n        Clips using specific lower and upper thresholds per column element:\\n\\n        >>> t = pd.Series([2, -4, -1, 6, 3])\\n        >>> t\\n        0    2\\n        1   -4\\n        2   -1\\n        3    6\\n        4    3\\n        dtype: int64\\n\\n        >>> df.clip(t, t + 4, axis=0)\\n           col_0  col_1\\n        0      6      2\\n        1     -3     -4\\n        2      0      3\\n        3      6      8\\n        4      5      3\",\n",
       "  'code': 'def clip(self, lower=None, upper=None, axis=None, inplace=False,\\n             *args, **kwargs):\\n        \"\"\"\\n        Trim values at input threshold(s).\\n\\n        Assigns values outside boundary to boundary values. Thresholds\\n        can be singular values or array like, and in the latter case\\n        the clipping is performed element-wise in the specified axis.\\n\\n        Parameters\\n        ----------\\n        lower : float or array_like, default None\\n            Minimum threshold value. All values below this\\n            threshold will be set to it.\\n        upper : float or array_like, default None\\n            Maximum threshold value. All values above this\\n            threshold will be set to it.\\n        axis : int or str axis name, optional\\n            Align object with lower and upper along the given axis.\\n        inplace : bool, default False\\n            Whether to perform the operation in place on the data.\\n\\n            .. versionadded:: 0.21.0\\n        *args, **kwargs\\n            Additional keywords have no effect but might be accepted\\n            for compatibility with numpy.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Same type as calling object with the values outside the\\n            clip boundaries replaced.\\n\\n        Examples\\n        --------\\n        >>> data = {\\'col_0\\': [9, -3, 0, -1, 5], \\'col_1\\': [-2, -7, 6, 8, -5]}\\n        >>> df = pd.DataFrame(data)\\n        >>> df\\n           col_0  col_1\\n        0      9     -2\\n        1     -3     -7\\n        2      0      6\\n        3     -1      8\\n        4      5     -5\\n\\n        Clips per column using lower and upper thresholds:\\n\\n        >>> df.clip(-4, 6)\\n           col_0  col_1\\n        0      6     -2\\n        1     -3     -4\\n        2      0      6\\n        3     -1      6\\n        4      5     -4\\n\\n        Clips using specific lower and upper thresholds per column element:\\n\\n        >>> t = pd.Series([2, -4, -1, 6, 3])\\n        >>> t\\n        0    2\\n        1   -4\\n        2   -1\\n        3    6\\n        4    3\\n        dtype: int64\\n\\n        >>> df.clip(t, t + 4, axis=0)\\n           col_0  col_1\\n        0      6      2\\n        1     -3     -4\\n        2      0      3\\n        3      6      8\\n        4      5      3\\n        \"\"\"\\n        if isinstance(self, ABCPanel):\\n            raise NotImplementedError(\"clip is not supported yet for panels\")\\n\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n\\n        axis = nv.validate_clip_with_axis(axis, args, kwargs)\\n        if axis is not None:\\n            axis = self._get_axis_number(axis)\\n\\n        # GH 17276\\n        # numpy doesn\\'t like NaN as a clip value\\n        # so ignore\\n        # GH 19992\\n        # numpy doesn\\'t drop a list-like bound containing NaN\\n        if not is_list_like(lower) and np.any(pd.isnull(lower)):\\n            lower = None\\n        if not is_list_like(upper) and np.any(pd.isnull(upper)):\\n            upper = None\\n\\n        # GH 2747 (arguments were reversed)\\n        if lower is not None and upper is not None:\\n            if is_scalar(lower) and is_scalar(upper):\\n                lower, upper = min(lower, upper), max(lower, upper)\\n\\n        # fast-path for scalars\\n        if ((lower is None or (is_scalar(lower) and is_number(lower))) and\\n                (upper is None or (is_scalar(upper) and is_number(upper)))):\\n            return self._clip_with_scalar(lower, upper, inplace=inplace)\\n\\n        result = self\\n        if lower is not None:\\n            result = result._clip_with_one_bound(lower, method=self.ge,\\n                                                 axis=axis, inplace=inplace)\\n        if upper is not None:\\n            if inplace:\\n                result = self\\n            result = result._clip_with_one_bound(upper, method=self.le,\\n                                                 axis=axis, inplace=inplace)\\n\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame.describe',\n",
       "  'docstring': 'Generate descriptive statistics that summarize the central tendency,\\n        dispersion and shape of a dataset\\'s distribution, excluding\\n        ``NaN`` values.\\n\\n        Analyzes both numeric and object series, as well\\n        as ``DataFrame`` column sets of mixed data types. The output\\n        will vary depending on what is provided. Refer to the notes\\n        below for more detail.\\n\\n        Parameters\\n        ----------\\n        percentiles : list-like of numbers, optional\\n            The percentiles to include in the output. All should\\n            fall between 0 and 1. The default is\\n            ``[.25, .5, .75]``, which returns the 25th, 50th, and\\n            75th percentiles.\\n        include : \\'all\\', list-like of dtypes or None (default), optional\\n            A white list of data types to include in the result. Ignored\\n            for ``Series``. Here are the options:\\n\\n            - \\'all\\' : All columns of the input will be included in the output.\\n            - A list-like of dtypes : Limits the results to the\\n              provided data types.\\n              To limit the result to numeric types submit\\n              ``numpy.number``. To limit it instead to object columns submit\\n              the ``numpy.object`` data type. Strings\\n              can also be used in the style of\\n              ``select_dtypes`` (e.g. ``df.describe(include=[\\'O\\'])``). To\\n              select pandas categorical columns, use ``\\'category\\'``\\n            - None (default) : The result will include all numeric columns.\\n        exclude : list-like of dtypes or None (default), optional,\\n            A black list of data types to omit from the result. Ignored\\n            for ``Series``. Here are the options:\\n\\n            - A list-like of dtypes : Excludes the provided data types\\n              from the result. To exclude numeric types submit\\n              ``numpy.number``. To exclude object columns submit the data\\n              type ``numpy.object``. Strings can also be used in the style of\\n              ``select_dtypes`` (e.g. ``df.describe(include=[\\'O\\'])``). To\\n              exclude pandas categorical columns, use ``\\'category\\'``\\n            - None (default) : The result will exclude nothing.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Summary statistics of the Series or Dataframe provided.\\n\\n        See Also\\n        --------\\n        DataFrame.count: Count number of non-NA/null observations.\\n        DataFrame.max: Maximum of the values in the object.\\n        DataFrame.min: Minimum of the values in the object.\\n        DataFrame.mean: Mean of the values.\\n        DataFrame.std: Standard deviation of the obersvations.\\n        DataFrame.select_dtypes: Subset of a DataFrame including/excluding\\n            columns based on their dtype.\\n\\n        Notes\\n        -----\\n        For numeric data, the result\\'s index will include ``count``,\\n        ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\\n        upper percentiles. By default the lower percentile is ``25`` and the\\n        upper percentile is ``75``. The ``50`` percentile is the\\n        same as the median.\\n\\n        For object data (e.g. strings or timestamps), the result\\'s index\\n        will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\\n        is the most common value. The ``freq`` is the most common value\\'s\\n        frequency. Timestamps also include the ``first`` and ``last`` items.\\n\\n        If multiple object values have the highest count, then the\\n        ``count`` and ``top`` results will be arbitrarily chosen from\\n        among those with the highest count.\\n\\n        For mixed data types provided via a ``DataFrame``, the default is to\\n        return only an analysis of numeric columns. If the dataframe consists\\n        only of object and categorical data without any numeric columns, the\\n        default is to return an analysis of both the object and categorical\\n        columns. If ``include=\\'all\\'`` is provided as an option, the result\\n        will include a union of attributes of each type.\\n\\n        The `include` and `exclude` parameters can be used to limit\\n        which columns in a ``DataFrame`` are analyzed for the output.\\n        The parameters are ignored when analyzing a ``Series``.\\n\\n        Examples\\n        --------\\n        Describing a numeric ``Series``.\\n\\n        >>> s = pd.Series([1, 2, 3])\\n        >>> s.describe()\\n        count    3.0\\n        mean     2.0\\n        std      1.0\\n        min      1.0\\n        25%      1.5\\n        50%      2.0\\n        75%      2.5\\n        max      3.0\\n        dtype: float64\\n\\n        Describing a categorical ``Series``.\\n\\n        >>> s = pd.Series([\\'a\\', \\'a\\', \\'b\\', \\'c\\'])\\n        >>> s.describe()\\n        count     4\\n        unique    3\\n        top       a\\n        freq      2\\n        dtype: object\\n\\n        Describing a timestamp ``Series``.\\n\\n        >>> s = pd.Series([\\n        ...   np.datetime64(\"2000-01-01\"),\\n        ...   np.datetime64(\"2010-01-01\"),\\n        ...   np.datetime64(\"2010-01-01\")\\n        ... ])\\n        >>> s.describe()\\n        count                       3\\n        unique                      2\\n        top       2010-01-01 00:00:00\\n        freq                        2\\n        first     2000-01-01 00:00:00\\n        last      2010-01-01 00:00:00\\n        dtype: object\\n\\n        Describing a ``DataFrame``. By default only numeric fields\\n        are returned.\\n\\n        >>> df = pd.DataFrame({\\'categorical\\': pd.Categorical([\\'d\\',\\'e\\',\\'f\\']),\\n        ...                    \\'numeric\\': [1, 2, 3],\\n        ...                    \\'object\\': [\\'a\\', \\'b\\', \\'c\\']\\n        ...                   })\\n        >>> df.describe()\\n               numeric\\n        count      3.0\\n        mean       2.0\\n        std        1.0\\n        min        1.0\\n        25%        1.5\\n        50%        2.0\\n        75%        2.5\\n        max        3.0\\n\\n        Describing all columns of a ``DataFrame`` regardless of data type.\\n\\n        >>> df.describe(include=\\'all\\')\\n                categorical  numeric object\\n        count            3      3.0      3\\n        unique           3      NaN      3\\n        top              f      NaN      c\\n        freq             1      NaN      1\\n        mean           NaN      2.0    NaN\\n        std            NaN      1.0    NaN\\n        min            NaN      1.0    NaN\\n        25%            NaN      1.5    NaN\\n        50%            NaN      2.0    NaN\\n        75%            NaN      2.5    NaN\\n        max            NaN      3.0    NaN\\n\\n        Describing a column from a ``DataFrame`` by accessing it as\\n        an attribute.\\n\\n        >>> df.numeric.describe()\\n        count    3.0\\n        mean     2.0\\n        std      1.0\\n        min      1.0\\n        25%      1.5\\n        50%      2.0\\n        75%      2.5\\n        max      3.0\\n        Name: numeric, dtype: float64\\n\\n        Including only numeric columns in a ``DataFrame`` description.\\n\\n        >>> df.describe(include=[np.number])\\n               numeric\\n        count      3.0\\n        mean       2.0\\n        std        1.0\\n        min        1.0\\n        25%        1.5\\n        50%        2.0\\n        75%        2.5\\n        max        3.0\\n\\n        Including only string columns in a ``DataFrame`` description.\\n\\n        >>> df.describe(include=[np.object])\\n               object\\n        count       3\\n        unique      3\\n        top         c\\n        freq        1\\n\\n        Including only categorical columns from a ``DataFrame`` description.\\n\\n        >>> df.describe(include=[\\'category\\'])\\n               categorical\\n        count            3\\n        unique           3\\n        top              f\\n        freq             1\\n\\n        Excluding numeric columns from a ``DataFrame`` description.\\n\\n        >>> df.describe(exclude=[np.number])\\n               categorical object\\n        count            3      3\\n        unique           3      3\\n        top              f      c\\n        freq             1      1\\n\\n        Excluding object columns from a ``DataFrame`` description.\\n\\n        >>> df.describe(exclude=[np.object])\\n               categorical  numeric\\n        count            3      3.0\\n        unique           3      NaN\\n        top              f      NaN\\n        freq             1      NaN\\n        mean           NaN      2.0\\n        std            NaN      1.0\\n        min            NaN      1.0\\n        25%            NaN      1.5\\n        50%            NaN      2.0\\n        75%            NaN      2.5\\n        max            NaN      3.0',\n",
       "  'code': 'def describe(self, percentiles=None, include=None, exclude=None):\\n        \"\"\"\\n        Generate descriptive statistics that summarize the central tendency,\\n        dispersion and shape of a dataset\\'s distribution, excluding\\n        ``NaN`` values.\\n\\n        Analyzes both numeric and object series, as well\\n        as ``DataFrame`` column sets of mixed data types. The output\\n        will vary depending on what is provided. Refer to the notes\\n        below for more detail.\\n\\n        Parameters\\n        ----------\\n        percentiles : list-like of numbers, optional\\n            The percentiles to include in the output. All should\\n            fall between 0 and 1. The default is\\n            ``[.25, .5, .75]``, which returns the 25th, 50th, and\\n            75th percentiles.\\n        include : \\'all\\', list-like of dtypes or None (default), optional\\n            A white list of data types to include in the result. Ignored\\n            for ``Series``. Here are the options:\\n\\n            - \\'all\\' : All columns of the input will be included in the output.\\n            - A list-like of dtypes : Limits the results to the\\n              provided data types.\\n              To limit the result to numeric types submit\\n              ``numpy.number``. To limit it instead to object columns submit\\n              the ``numpy.object`` data type. Strings\\n              can also be used in the style of\\n              ``select_dtypes`` (e.g. ``df.describe(include=[\\'O\\'])``). To\\n              select pandas categorical columns, use ``\\'category\\'``\\n            - None (default) : The result will include all numeric columns.\\n        exclude : list-like of dtypes or None (default), optional,\\n            A black list of data types to omit from the result. Ignored\\n            for ``Series``. Here are the options:\\n\\n            - A list-like of dtypes : Excludes the provided data types\\n              from the result. To exclude numeric types submit\\n              ``numpy.number``. To exclude object columns submit the data\\n              type ``numpy.object``. Strings can also be used in the style of\\n              ``select_dtypes`` (e.g. ``df.describe(include=[\\'O\\'])``). To\\n              exclude pandas categorical columns, use ``\\'category\\'``\\n            - None (default) : The result will exclude nothing.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Summary statistics of the Series or Dataframe provided.\\n\\n        See Also\\n        --------\\n        DataFrame.count: Count number of non-NA/null observations.\\n        DataFrame.max: Maximum of the values in the object.\\n        DataFrame.min: Minimum of the values in the object.\\n        DataFrame.mean: Mean of the values.\\n        DataFrame.std: Standard deviation of the obersvations.\\n        DataFrame.select_dtypes: Subset of a DataFrame including/excluding\\n            columns based on their dtype.\\n\\n        Notes\\n        -----\\n        For numeric data, the result\\'s index will include ``count``,\\n        ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\\n        upper percentiles. By default the lower percentile is ``25`` and the\\n        upper percentile is ``75``. The ``50`` percentile is the\\n        same as the median.\\n\\n        For object data (e.g. strings or timestamps), the result\\'s index\\n        will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\\n        is the most common value. The ``freq`` is the most common value\\'s\\n        frequency. Timestamps also include the ``first`` and ``last`` items.\\n\\n        If multiple object values have the highest count, then the\\n        ``count`` and ``top`` results will be arbitrarily chosen from\\n        among those with the highest count.\\n\\n        For mixed data types provided via a ``DataFrame``, the default is to\\n        return only an analysis of numeric columns. If the dataframe consists\\n        only of object and categorical data without any numeric columns, the\\n        default is to return an analysis of both the object and categorical\\n        columns. If ``include=\\'all\\'`` is provided as an option, the result\\n        will include a union of attributes of each type.\\n\\n        The `include` and `exclude` parameters can be used to limit\\n        which columns in a ``DataFrame`` are analyzed for the output.\\n        The parameters are ignored when analyzing a ``Series``.\\n\\n        Examples\\n        --------\\n        Describing a numeric ``Series``.\\n\\n        >>> s = pd.Series([1, 2, 3])\\n        >>> s.describe()\\n        count    3.0\\n        mean     2.0\\n        std      1.0\\n        min      1.0\\n        25%      1.5\\n        50%      2.0\\n        75%      2.5\\n        max      3.0\\n        dtype: float64\\n\\n        Describing a categorical ``Series``.\\n\\n        >>> s = pd.Series([\\'a\\', \\'a\\', \\'b\\', \\'c\\'])\\n        >>> s.describe()\\n        count     4\\n        unique    3\\n        top       a\\n        freq      2\\n        dtype: object\\n\\n        Describing a timestamp ``Series``.\\n\\n        >>> s = pd.Series([\\n        ...   np.datetime64(\"2000-01-01\"),\\n        ...   np.datetime64(\"2010-01-01\"),\\n        ...   np.datetime64(\"2010-01-01\")\\n        ... ])\\n        >>> s.describe()\\n        count                       3\\n        unique                      2\\n        top       2010-01-01 00:00:00\\n        freq                        2\\n        first     2000-01-01 00:00:00\\n        last      2010-01-01 00:00:00\\n        dtype: object\\n\\n        Describing a ``DataFrame``. By default only numeric fields\\n        are returned.\\n\\n        >>> df = pd.DataFrame({\\'categorical\\': pd.Categorical([\\'d\\',\\'e\\',\\'f\\']),\\n        ...                    \\'numeric\\': [1, 2, 3],\\n        ...                    \\'object\\': [\\'a\\', \\'b\\', \\'c\\']\\n        ...                   })\\n        >>> df.describe()\\n               numeric\\n        count      3.0\\n        mean       2.0\\n        std        1.0\\n        min        1.0\\n        25%        1.5\\n        50%        2.0\\n        75%        2.5\\n        max        3.0\\n\\n        Describing all columns of a ``DataFrame`` regardless of data type.\\n\\n        >>> df.describe(include=\\'all\\')\\n                categorical  numeric object\\n        count            3      3.0      3\\n        unique           3      NaN      3\\n        top              f      NaN      c\\n        freq             1      NaN      1\\n        mean           NaN      2.0    NaN\\n        std            NaN      1.0    NaN\\n        min            NaN      1.0    NaN\\n        25%            NaN      1.5    NaN\\n        50%            NaN      2.0    NaN\\n        75%            NaN      2.5    NaN\\n        max            NaN      3.0    NaN\\n\\n        Describing a column from a ``DataFrame`` by accessing it as\\n        an attribute.\\n\\n        >>> df.numeric.describe()\\n        count    3.0\\n        mean     2.0\\n        std      1.0\\n        min      1.0\\n        25%      1.5\\n        50%      2.0\\n        75%      2.5\\n        max      3.0\\n        Name: numeric, dtype: float64\\n\\n        Including only numeric columns in a ``DataFrame`` description.\\n\\n        >>> df.describe(include=[np.number])\\n               numeric\\n        count      3.0\\n        mean       2.0\\n        std        1.0\\n        min        1.0\\n        25%        1.5\\n        50%        2.0\\n        75%        2.5\\n        max        3.0\\n\\n        Including only string columns in a ``DataFrame`` description.\\n\\n        >>> df.describe(include=[np.object])\\n               object\\n        count       3\\n        unique      3\\n        top         c\\n        freq        1\\n\\n        Including only categorical columns from a ``DataFrame`` description.\\n\\n        >>> df.describe(include=[\\'category\\'])\\n               categorical\\n        count            3\\n        unique           3\\n        top              f\\n        freq             1\\n\\n        Excluding numeric columns from a ``DataFrame`` description.\\n\\n        >>> df.describe(exclude=[np.number])\\n               categorical object\\n        count            3      3\\n        unique           3      3\\n        top              f      c\\n        freq             1      1\\n\\n        Excluding object columns from a ``DataFrame`` description.\\n\\n        >>> df.describe(exclude=[np.object])\\n               categorical  numeric\\n        count            3      3.0\\n        unique           3      NaN\\n        top              f      NaN\\n        freq             1      NaN\\n        mean           NaN      2.0\\n        std            NaN      1.0\\n        min            NaN      1.0\\n        25%            NaN      1.5\\n        50%            NaN      2.0\\n        75%            NaN      2.5\\n        max            NaN      3.0\\n        \"\"\"\\n        if self.ndim >= 3:\\n            msg = \"describe is not implemented on Panel objects.\"\\n            raise NotImplementedError(msg)\\n        elif self.ndim == 2 and self.columns.size == 0:\\n            raise ValueError(\"Cannot describe a DataFrame without columns\")\\n\\n        if percentiles is not None:\\n            # explicit conversion of `percentiles` to list\\n            percentiles = list(percentiles)\\n\\n            # get them all to be in [0, 1]\\n            self._check_percentile(percentiles)\\n\\n            # median should always be included\\n            if 0.5 not in percentiles:\\n                percentiles.append(0.5)\\n            percentiles = np.asarray(percentiles)\\n        else:\\n            percentiles = np.array([0.25, 0.5, 0.75])\\n\\n        # sort and check for duplicates\\n        unique_pcts = np.unique(percentiles)\\n        if len(unique_pcts) < len(percentiles):\\n            raise ValueError(\"percentiles cannot contain duplicates\")\\n        percentiles = unique_pcts\\n\\n        formatted_percentiles = format_percentiles(percentiles)\\n\\n        def describe_numeric_1d(series):\\n            stat_index = ([\\'count\\', \\'mean\\', \\'std\\', \\'min\\'] +\\n                          formatted_percentiles + [\\'max\\'])\\n            d = ([series.count(), series.mean(), series.std(), series.min()] +\\n                 series.quantile(percentiles).tolist() + [series.max()])\\n            return pd.Series(d, index=stat_index, name=series.name)\\n\\n        def describe_categorical_1d(data):\\n            names = [\\'count\\', \\'unique\\']\\n            objcounts = data.value_counts()\\n            count_unique = len(objcounts[objcounts != 0])\\n            result = [data.count(), count_unique]\\n            if result[1] > 0:\\n                top, freq = objcounts.index[0], objcounts.iloc[0]\\n\\n                if is_datetime64_any_dtype(data):\\n                    tz = data.dt.tz\\n                    asint = data.dropna().values.view(\\'i8\\')\\n                    top = Timestamp(top)\\n                    if top.tzinfo is not None and tz is not None:\\n                        # Don\\'t tz_localize(None) if key is already tz-aware\\n                        top = top.tz_convert(tz)\\n                    else:\\n                        top = top.tz_localize(tz)\\n                    names += [\\'top\\', \\'freq\\', \\'first\\', \\'last\\']\\n                    result += [top, freq,\\n                               Timestamp(asint.min(), tz=tz),\\n                               Timestamp(asint.max(), tz=tz)]\\n                else:\\n                    names += [\\'top\\', \\'freq\\']\\n                    result += [top, freq]\\n\\n            return pd.Series(result, index=names, name=data.name)\\n\\n        def describe_1d(data):\\n            if is_bool_dtype(data):\\n                return describe_categorical_1d(data)\\n            elif is_numeric_dtype(data):\\n                return describe_numeric_1d(data)\\n            elif is_timedelta64_dtype(data):\\n                return describe_numeric_1d(data)\\n            else:\\n                return describe_categorical_1d(data)\\n\\n        if self.ndim == 1:\\n            return describe_1d(self)\\n        elif (include is None) and (exclude is None):\\n            # when some numerics are found, keep only numerics\\n            data = self.select_dtypes(include=[np.number])\\n            if len(data.columns) == 0:\\n                data = self\\n        elif include == \\'all\\':\\n            if exclude is not None:\\n                msg = \"exclude must be None when include is \\'all\\'\"\\n                raise ValueError(msg)\\n            data = self\\n        else:\\n            data = self.select_dtypes(include=include, exclude=exclude)\\n\\n        ldesc = [describe_1d(s) for _, s in data.iteritems()]\\n        # set a convenient order for rows\\n        names = []\\n        ldesc_indexes = sorted((x.index for x in ldesc), key=len)\\n        for idxnames in ldesc_indexes:\\n            for name in idxnames:\\n                if name not in names:\\n                    names.append(name)\\n\\n        d = pd.concat(ldesc, join_axes=pd.Index([names]), axis=1)\\n        d.columns = data.columns.copy()\\n        return d',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame._add_numeric_operations',\n",
       "  'docstring': 'Add the operations to the cls; evaluate the doc strings again',\n",
       "  'code': 'def _add_numeric_operations(cls):\\n        \"\"\"\\n        Add the operations to the cls; evaluate the doc strings again\\n        \"\"\"\\n\\n        axis_descr, name, name2 = _doc_parms(cls)\\n\\n        cls.any = _make_logical_function(\\n            cls, \\'any\\', name, name2, axis_descr, _any_desc, nanops.nanany,\\n            _any_see_also, _any_examples, empty_value=False)\\n        cls.all = _make_logical_function(\\n            cls, \\'all\\', name, name2, axis_descr, _all_desc, nanops.nanall,\\n            _all_see_also, _all_examples, empty_value=True)\\n\\n        @Substitution(desc=\"Return the mean absolute deviation of the values \"\\n                           \"for the requested axis.\",\\n                      name1=name, name2=name2, axis_descr=axis_descr,\\n                      min_count=\\'\\', see_also=\\'\\', examples=\\'\\')\\n        @Appender(_num_doc)\\n        def mad(self, axis=None, skipna=None, level=None):\\n            if skipna is None:\\n                skipna = True\\n            if axis is None:\\n                axis = self._stat_axis_number\\n            if level is not None:\\n                return self._agg_by_level(\\'mad\\', axis=axis, level=level,\\n                                          skipna=skipna)\\n\\n            data = self._get_numeric_data()\\n            if axis == 0:\\n                demeaned = data - data.mean(axis=0)\\n            else:\\n                demeaned = data.sub(data.mean(axis=1), axis=0)\\n            return np.abs(demeaned).mean(axis=axis, skipna=skipna)\\n\\n        cls.mad = mad\\n\\n        cls.sem = _make_stat_function_ddof(\\n            cls, \\'sem\\', name, name2, axis_descr,\\n            \"Return unbiased standard error of the mean over requested \"\\n            \"axis.\\\\n\\\\nNormalized by N-1 by default. This can be changed \"\\n            \"using the ddof argument\",\\n            nanops.nansem)\\n        cls.var = _make_stat_function_ddof(\\n            cls, \\'var\\', name, name2, axis_descr,\\n            \"Return unbiased variance over requested axis.\\\\n\\\\nNormalized by \"\\n            \"N-1 by default. This can be changed using the ddof argument\",\\n            nanops.nanvar)\\n        cls.std = _make_stat_function_ddof(\\n            cls, \\'std\\', name, name2, axis_descr,\\n            \"Return sample standard deviation over requested axis.\"\\n            \"\\\\n\\\\nNormalized by N-1 by default. This can be changed using the \"\\n            \"ddof argument\",\\n            nanops.nanstd)\\n\\n        @Substitution(desc=\"Return the compound percentage of the values for \"\\n                      \"the requested axis.\", name1=name, name2=name2,\\n                      axis_descr=axis_descr,\\n                      min_count=\\'\\', see_also=\\'\\', examples=\\'\\')\\n        @Appender(_num_doc)\\n        def compound(self, axis=None, skipna=None, level=None):\\n            if skipna is None:\\n                skipna = True\\n            return (1 + self).prod(axis=axis, skipna=skipna, level=level) - 1\\n\\n        cls.compound = compound\\n\\n        cls.cummin = _make_cum_function(\\n            cls, \\'cummin\\', name, name2, axis_descr, \"minimum\",\\n            lambda y, axis: np.minimum.accumulate(y, axis), \"min\",\\n            np.inf, np.nan, _cummin_examples)\\n        cls.cumsum = _make_cum_function(\\n            cls, \\'cumsum\\', name, name2, axis_descr, \"sum\",\\n            lambda y, axis: y.cumsum(axis), \"sum\", 0.,\\n            np.nan, _cumsum_examples)\\n        cls.cumprod = _make_cum_function(\\n            cls, \\'cumprod\\', name, name2, axis_descr, \"product\",\\n            lambda y, axis: y.cumprod(axis), \"prod\", 1.,\\n            np.nan, _cumprod_examples)\\n        cls.cummax = _make_cum_function(\\n            cls, \\'cummax\\', name, name2, axis_descr, \"maximum\",\\n            lambda y, axis: np.maximum.accumulate(y, axis), \"max\",\\n            -np.inf, np.nan, _cummax_examples)\\n\\n        cls.sum = _make_min_count_stat_function(\\n            cls, \\'sum\\', name, name2, axis_descr,\\n            \"\"\"Return the sum of the values for the requested axis.\\\\n\\n            This is equivalent to the method ``numpy.sum``.\"\"\",\\n            nanops.nansum, _stat_func_see_also, _sum_examples)\\n        cls.mean = _make_stat_function(\\n            cls, \\'mean\\', name, name2, axis_descr,\\n            \\'Return the mean of the values for the requested axis.\\',\\n            nanops.nanmean)\\n        cls.skew = _make_stat_function(\\n            cls, \\'skew\\', name, name2, axis_descr,\\n            \\'Return unbiased skew over requested axis\\\\nNormalized by N-1.\\',\\n            nanops.nanskew)\\n        cls.kurt = _make_stat_function(\\n            cls, \\'kurt\\', name, name2, axis_descr,\\n            \"Return unbiased kurtosis over requested axis using Fisher\\'s \"\\n            \"definition of\\\\nkurtosis (kurtosis of normal == 0.0). Normalized \"\\n            \"by N-1.\",\\n            nanops.nankurt)\\n        cls.kurtosis = cls.kurt\\n        cls.prod = _make_min_count_stat_function(\\n            cls, \\'prod\\', name, name2, axis_descr,\\n            \\'Return the product of the values for the requested axis.\\',\\n            nanops.nanprod, examples=_prod_examples)\\n        cls.product = cls.prod\\n        cls.median = _make_stat_function(\\n            cls, \\'median\\', name, name2, axis_descr,\\n            \\'Return the median of the values for the requested axis.\\',\\n            nanops.nanmedian)\\n        cls.max = _make_stat_function(\\n            cls, \\'max\\', name, name2, axis_descr,\\n            \"\"\"Return the maximum of the values for the requested axis.\\\\n\\n            If you want the *index* of the maximum, use ``idxmax``. This is\\n            the equivalent of the ``numpy.ndarray`` method ``argmax``.\"\"\",\\n            nanops.nanmax, _stat_func_see_also, _max_examples)\\n        cls.min = _make_stat_function(\\n            cls, \\'min\\', name, name2, axis_descr,\\n            \"\"\"Return the minimum of the values for the requested axis.\\\\n\\n            If you want the *index* of the minimum, use ``idxmin``. This is\\n            the equivalent of the ``numpy.ndarray`` method ``argmin``.\"\"\",\\n            nanops.nanmin, _stat_func_see_also, _min_examples)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'NDFrame._add_series_only_operations',\n",
       "  'docstring': 'Add the series only operations to the cls; evaluate the doc\\n        strings again.',\n",
       "  'code': 'def _add_series_only_operations(cls):\\n        \"\"\"\\n        Add the series only operations to the cls; evaluate the doc\\n        strings again.\\n        \"\"\"\\n\\n        axis_descr, name, name2 = _doc_parms(cls)\\n\\n        def nanptp(values, axis=0, skipna=True):\\n            nmax = nanops.nanmax(values, axis, skipna)\\n            nmin = nanops.nanmin(values, axis, skipna)\\n            warnings.warn(\"Method .ptp is deprecated and will be removed \"\\n                          \"in a future version. Use numpy.ptp instead.\",\\n                          FutureWarning, stacklevel=4)\\n            return nmax - nmin\\n\\n        cls.ptp = _make_stat_function(\\n            cls, \\'ptp\\', name, name2, axis_descr,\\n            \"\"\"Return the difference between the maximum value and the\\n            minimum value in the object. This is the equivalent of the\\n            ``numpy.ndarray`` method ``ptp``.\\\\n\\\\n.. deprecated:: 0.24.0\\n                Use numpy.ptp instead\"\"\",\\n            nanptp)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SelectionMixin._try_aggregate_string_function',\n",
       "  'docstring': 'if arg is a string, then try to operate on it:\\n        - try to find a function (or attribute) on ourselves\\n        - try to find a numpy function\\n        - raise',\n",
       "  'code': 'def _try_aggregate_string_function(self, arg, *args, **kwargs):\\n        \"\"\"\\n        if arg is a string, then try to operate on it:\\n        - try to find a function (or attribute) on ourselves\\n        - try to find a numpy function\\n        - raise\\n\\n        \"\"\"\\n        assert isinstance(arg, str)\\n\\n        f = getattr(self, arg, None)\\n        if f is not None:\\n            if callable(f):\\n                return f(*args, **kwargs)\\n\\n            # people may try to aggregate on a non-callable attribute\\n            # but don\\'t let them think they can pass args to it\\n            assert len(args) == 0\\n            assert len([kwarg for kwarg in kwargs\\n                        if kwarg not in [\\'axis\\', \\'_level\\']]) == 0\\n            return f\\n\\n        f = getattr(np, arg, None)\\n        if f is not None:\\n            return f(self, *args, **kwargs)\\n\\n        raise ValueError(\"{arg} is an unknown string function\".format(arg=arg))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'IndexOpsMixin.array',\n",
       "  'docstring': \"The ExtensionArray of the data backing this Series or Index.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        ExtensionArray\\n            An ExtensionArray of the values stored within. For extension\\n            types, this is the actual array. For NumPy native types, this\\n            is a thin (no copy) wrapper around :class:`numpy.ndarray`.\\n\\n            ``.array`` differs ``.values`` which may require converting the\\n            data to a different form.\\n\\n        See Also\\n        --------\\n        Index.to_numpy : Similar method that always returns a NumPy array.\\n        Series.to_numpy : Similar method that always returns a NumPy array.\\n\\n        Notes\\n        -----\\n        This table lays out the different array types for each extension\\n        dtype within pandas.\\n\\n        ================== =============================\\n        dtype              array type\\n        ================== =============================\\n        category           Categorical\\n        period             PeriodArray\\n        interval           IntervalArray\\n        IntegerNA          IntegerArray\\n        datetime64[ns, tz] DatetimeArray\\n        ================== =============================\\n\\n        For any 3rd-party extension types, the array type will be an\\n        ExtensionArray.\\n\\n        For all remaining dtypes ``.array`` will be a\\n        :class:`arrays.NumpyExtensionArray` wrapping the actual ndarray\\n        stored within. If you absolutely need a NumPy array (possibly with\\n        copying / coercing data), then use :meth:`Series.to_numpy` instead.\\n\\n        Examples\\n        --------\\n\\n        For regular NumPy types like int, and float, a PandasArray\\n        is returned.\\n\\n        >>> pd.Series([1, 2, 3]).array\\n        <PandasArray>\\n        [1, 2, 3]\\n        Length: 3, dtype: int64\\n\\n        For extension types, like Categorical, the actual ExtensionArray\\n        is returned\\n\\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\\n        >>> ser.array\\n        [a, b, a]\\n        Categories (2, object): [a, b]\",\n",
       "  'code': 'def array(self) -> ExtensionArray:\\n        \"\"\"\\n        The ExtensionArray of the data backing this Series or Index.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        ExtensionArray\\n            An ExtensionArray of the values stored within. For extension\\n            types, this is the actual array. For NumPy native types, this\\n            is a thin (no copy) wrapper around :class:`numpy.ndarray`.\\n\\n            ``.array`` differs ``.values`` which may require converting the\\n            data to a different form.\\n\\n        See Also\\n        --------\\n        Index.to_numpy : Similar method that always returns a NumPy array.\\n        Series.to_numpy : Similar method that always returns a NumPy array.\\n\\n        Notes\\n        -----\\n        This table lays out the different array types for each extension\\n        dtype within pandas.\\n\\n        ================== =============================\\n        dtype              array type\\n        ================== =============================\\n        category           Categorical\\n        period             PeriodArray\\n        interval           IntervalArray\\n        IntegerNA          IntegerArray\\n        datetime64[ns, tz] DatetimeArray\\n        ================== =============================\\n\\n        For any 3rd-party extension types, the array type will be an\\n        ExtensionArray.\\n\\n        For all remaining dtypes ``.array`` will be a\\n        :class:`arrays.NumpyExtensionArray` wrapping the actual ndarray\\n        stored within. If you absolutely need a NumPy array (possibly with\\n        copying / coercing data), then use :meth:`Series.to_numpy` instead.\\n\\n        Examples\\n        --------\\n\\n        For regular NumPy types like int, and float, a PandasArray\\n        is returned.\\n\\n        >>> pd.Series([1, 2, 3]).array\\n        <PandasArray>\\n        [1, 2, 3]\\n        Length: 3, dtype: int64\\n\\n        For extension types, like Categorical, the actual ExtensionArray\\n        is returned\\n\\n        >>> ser = pd.Series(pd.Categorical([\\'a\\', \\'b\\', \\'a\\']))\\n        >>> ser.array\\n        [a, b, a]\\n        Categories (2, object): [a, b]\\n        \"\"\"\\n        result = self._values\\n\\n        if is_datetime64_ns_dtype(result.dtype):\\n            from pandas.arrays import DatetimeArray\\n            result = DatetimeArray(result)\\n        elif is_timedelta64_ns_dtype(result.dtype):\\n            from pandas.arrays import TimedeltaArray\\n            result = TimedeltaArray(result)\\n\\n        elif not is_extension_array_dtype(result.dtype):\\n            from pandas.core.arrays.numpy_ import PandasArray\\n            result = PandasArray(result)\\n\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'IndexOpsMixin.to_numpy',\n",
       "  'docstring': 'A NumPy ndarray representing the values in this Series or Index.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Parameters\\n        ----------\\n        dtype : str or numpy.dtype, optional\\n            The dtype to pass to :meth:`numpy.asarray`\\n        copy : bool, default False\\n            Whether to ensure that the returned value is a not a view on\\n            another array. Note that ``copy=False`` does not *ensure* that\\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\\n            a copy is made, even if not strictly necessary.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n\\n        See Also\\n        --------\\n        Series.array : Get the actual data stored within.\\n        Index.array : Get the actual data stored within.\\n        DataFrame.to_numpy : Similar method for DataFrame.\\n\\n        Notes\\n        -----\\n        The returned array will be the same up to equality (values equal\\n        in `self` will be equal in the returned array; likewise for values\\n        that are not equal). When `self` contains an ExtensionArray, the\\n        dtype may be different. For example, for a category-dtype Series,\\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\\n        will be lost.\\n\\n        For NumPy dtypes, this will be a reference to the actual data stored\\n        in this Series or Index (assuming ``copy=False``). Modifying the result\\n        in place will modify the data stored in the Series or Index (not that\\n        we recommend doing that).\\n\\n        For extension types, ``to_numpy()`` *may* require copying data and\\n        coercing the result to a NumPy type (possibly object), which may be\\n        expensive. When you need a no-copy reference to the underlying data,\\n        :attr:`Series.array` should be used instead.\\n\\n        This table lays out the different dtypes and default return types of\\n        ``to_numpy()`` for various dtypes within pandas.\\n\\n        ================== ================================\\n        dtype              array type\\n        ================== ================================\\n        category[T]        ndarray[T] (same dtype as input)\\n        period             ndarray[object] (Periods)\\n        interval           ndarray[object] (Intervals)\\n        IntegerNA          ndarray[object]\\n        datetime64[ns]     datetime64[ns]\\n        datetime64[ns, tz] ndarray[object] (Timestamps)\\n        ================== ================================\\n\\n        Examples\\n        --------\\n        >>> ser = pd.Series(pd.Categorical([\\'a\\', \\'b\\', \\'a\\']))\\n        >>> ser.to_numpy()\\n        array([\\'a\\', \\'b\\', \\'a\\'], dtype=object)\\n\\n        Specify the `dtype` to control how datetime-aware data is represented.\\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\\n        objects, each with the correct ``tz``.\\n\\n        >>> ser = pd.Series(pd.date_range(\\'2000\\', periods=2, tz=\"CET\"))\\n        >>> ser.to_numpy(dtype=object)\\n        array([Timestamp(\\'2000-01-01 00:00:00+0100\\', tz=\\'CET\\', freq=\\'D\\'),\\n               Timestamp(\\'2000-01-02 00:00:00+0100\\', tz=\\'CET\\', freq=\\'D\\')],\\n              dtype=object)\\n\\n        Or ``dtype=\\'datetime64[ns]\\'`` to return an ndarray of native\\n        datetime64 values. The values are converted to UTC and the timezone\\n        info is dropped.\\n\\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\\n        ... # doctest: +ELLIPSIS\\n        array([\\'1999-12-31T23:00:00.000000000\\', \\'2000-01-01T23:00:00...\\'],\\n              dtype=\\'datetime64[ns]\\')',\n",
       "  'code': 'def to_numpy(self, dtype=None, copy=False):\\n        \"\"\"\\n        A NumPy ndarray representing the values in this Series or Index.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Parameters\\n        ----------\\n        dtype : str or numpy.dtype, optional\\n            The dtype to pass to :meth:`numpy.asarray`\\n        copy : bool, default False\\n            Whether to ensure that the returned value is a not a view on\\n            another array. Note that ``copy=False`` does not *ensure* that\\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\\n            a copy is made, even if not strictly necessary.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n\\n        See Also\\n        --------\\n        Series.array : Get the actual data stored within.\\n        Index.array : Get the actual data stored within.\\n        DataFrame.to_numpy : Similar method for DataFrame.\\n\\n        Notes\\n        -----\\n        The returned array will be the same up to equality (values equal\\n        in `self` will be equal in the returned array; likewise for values\\n        that are not equal). When `self` contains an ExtensionArray, the\\n        dtype may be different. For example, for a category-dtype Series,\\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\\n        will be lost.\\n\\n        For NumPy dtypes, this will be a reference to the actual data stored\\n        in this Series or Index (assuming ``copy=False``). Modifying the result\\n        in place will modify the data stored in the Series or Index (not that\\n        we recommend doing that).\\n\\n        For extension types, ``to_numpy()`` *may* require copying data and\\n        coercing the result to a NumPy type (possibly object), which may be\\n        expensive. When you need a no-copy reference to the underlying data,\\n        :attr:`Series.array` should be used instead.\\n\\n        This table lays out the different dtypes and default return types of\\n        ``to_numpy()`` for various dtypes within pandas.\\n\\n        ================== ================================\\n        dtype              array type\\n        ================== ================================\\n        category[T]        ndarray[T] (same dtype as input)\\n        period             ndarray[object] (Periods)\\n        interval           ndarray[object] (Intervals)\\n        IntegerNA          ndarray[object]\\n        datetime64[ns]     datetime64[ns]\\n        datetime64[ns, tz] ndarray[object] (Timestamps)\\n        ================== ================================\\n\\n        Examples\\n        --------\\n        >>> ser = pd.Series(pd.Categorical([\\'a\\', \\'b\\', \\'a\\']))\\n        >>> ser.to_numpy()\\n        array([\\'a\\', \\'b\\', \\'a\\'], dtype=object)\\n\\n        Specify the `dtype` to control how datetime-aware data is represented.\\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\\n        objects, each with the correct ``tz``.\\n\\n        >>> ser = pd.Series(pd.date_range(\\'2000\\', periods=2, tz=\"CET\"))\\n        >>> ser.to_numpy(dtype=object)\\n        array([Timestamp(\\'2000-01-01 00:00:00+0100\\', tz=\\'CET\\', freq=\\'D\\'),\\n               Timestamp(\\'2000-01-02 00:00:00+0100\\', tz=\\'CET\\', freq=\\'D\\')],\\n              dtype=object)\\n\\n        Or ``dtype=\\'datetime64[ns]\\'`` to return an ndarray of native\\n        datetime64 values. The values are converted to UTC and the timezone\\n        info is dropped.\\n\\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\\n        ... # doctest: +ELLIPSIS\\n        array([\\'1999-12-31T23:00:00.000000000\\', \\'2000-01-01T23:00:00...\\'],\\n              dtype=\\'datetime64[ns]\\')\\n        \"\"\"\\n        if is_datetime64tz_dtype(self.dtype) and dtype is None:\\n            # note: this is going to change very soon.\\n            # I have a WIP PR making this unnecessary, but it\\'s\\n            # a bit out of scope for the DatetimeArray PR.\\n            dtype = \"object\"\\n\\n        result = np.asarray(self._values, dtype=dtype)\\n        # TODO(GH-24345): Avoid potential double copy\\n        if copy:\\n            result = result.copy()\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'IndexOpsMixin.max',\n",
       "  'docstring': \"Return the maximum value of the Index.\\n\\n        Parameters\\n        ----------\\n        axis : int, optional\\n            For compatibility with NumPy. Only 0 or None are allowed.\\n        skipna : bool, default True\\n\\n        Returns\\n        -------\\n        scalar\\n            Maximum value.\\n\\n        See Also\\n        --------\\n        Index.min : Return the minimum value in an Index.\\n        Series.max : Return the maximum value in a Series.\\n        DataFrame.max : Return the maximum values in a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.Index([3, 2, 1])\\n        >>> idx.max()\\n        3\\n\\n        >>> idx = pd.Index(['c', 'b', 'a'])\\n        >>> idx.max()\\n        'c'\\n\\n        For a MultiIndex, the maximum is determined lexicographically.\\n\\n        >>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\\n        >>> idx.max()\\n        ('b', 2)\",\n",
       "  'code': 'def max(self, axis=None, skipna=True):\\n        \"\"\"\\n        Return the maximum value of the Index.\\n\\n        Parameters\\n        ----------\\n        axis : int, optional\\n            For compatibility with NumPy. Only 0 or None are allowed.\\n        skipna : bool, default True\\n\\n        Returns\\n        -------\\n        scalar\\n            Maximum value.\\n\\n        See Also\\n        --------\\n        Index.min : Return the minimum value in an Index.\\n        Series.max : Return the maximum value in a Series.\\n        DataFrame.max : Return the maximum values in a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> idx = pd.Index([3, 2, 1])\\n        >>> idx.max()\\n        3\\n\\n        >>> idx = pd.Index([\\'c\\', \\'b\\', \\'a\\'])\\n        >>> idx.max()\\n        \\'c\\'\\n\\n        For a MultiIndex, the maximum is determined lexicographically.\\n\\n        >>> idx = pd.MultiIndex.from_product([(\\'a\\', \\'b\\'), (2, 1)])\\n        >>> idx.max()\\n        (\\'b\\', 2)\\n        \"\"\"\\n        nv.validate_minmax_axis(axis)\\n        return nanops.nanmax(self._values, skipna=skipna)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'IndexOpsMixin.argmax',\n",
       "  'docstring': 'Return an ndarray of the maximum argument indexer.\\n\\n        Parameters\\n        ----------\\n        axis : {None}\\n            Dummy argument for consistency with Series\\n        skipna : bool, default True\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmax',\n",
       "  'code': 'def argmax(self, axis=None, skipna=True):\\n        \"\"\"\\n        Return an ndarray of the maximum argument indexer.\\n\\n        Parameters\\n        ----------\\n        axis : {None}\\n            Dummy argument for consistency with Series\\n        skipna : bool, default True\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmax\\n        \"\"\"\\n        nv.validate_minmax_axis(axis)\\n        return nanops.nanargmax(self._values, skipna=skipna)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'IndexOpsMixin.argmin',\n",
       "  'docstring': 'Return a ndarray of the minimum argument indexer.\\n\\n        Parameters\\n        ----------\\n        axis : {None}\\n            Dummy argument for consistency with Series\\n        skipna : bool, default True\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmin',\n",
       "  'code': 'def argmin(self, axis=None, skipna=True):\\n        \"\"\"\\n        Return a ndarray of the minimum argument indexer.\\n\\n        Parameters\\n        ----------\\n        axis : {None}\\n            Dummy argument for consistency with Series\\n        skipna : bool, default True\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmin\\n        \"\"\"\\n        nv.validate_minmax_axis(axis)\\n        return nanops.nanargmin(self._values, skipna=skipna)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'IndexOpsMixin.tolist',\n",
       "  'docstring': 'Return a list of the values.\\n\\n        These are each a scalar type, which is a Python scalar\\n        (for str, int, float) or a pandas scalar\\n        (for Timestamp/Timedelta/Interval/Period)\\n\\n        Returns\\n        -------\\n        list\\n\\n        See Also\\n        --------\\n        numpy.ndarray.tolist',\n",
       "  'code': 'def tolist(self):\\n        \"\"\"\\n        Return a list of the values.\\n\\n        These are each a scalar type, which is a Python scalar\\n        (for str, int, float) or a pandas scalar\\n        (for Timestamp/Timedelta/Interval/Period)\\n\\n        Returns\\n        -------\\n        list\\n\\n        See Also\\n        --------\\n        numpy.ndarray.tolist\\n        \"\"\"\\n        if is_datetimelike(self._values):\\n            return [com.maybe_box_datetimelike(x) for x in self._values]\\n        elif is_extension_array_dtype(self._values):\\n            return list(self._values)\\n        else:\\n            return self._values.tolist()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'IndexOpsMixin.memory_usage',\n",
       "  'docstring': 'Memory usage of the values\\n\\n        Parameters\\n        ----------\\n        deep : bool\\n            Introspect the data deeply, interrogate\\n            `object` dtypes for system-level memory consumption\\n\\n        Returns\\n        -------\\n        bytes used\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes\\n\\n        Notes\\n        -----\\n        Memory usage does not include memory consumed by elements that\\n        are not components of the array if deep=False or if used on PyPy',\n",
       "  'code': 'def memory_usage(self, deep=False):\\n        \"\"\"\\n        Memory usage of the values\\n\\n        Parameters\\n        ----------\\n        deep : bool\\n            Introspect the data deeply, interrogate\\n            `object` dtypes for system-level memory consumption\\n\\n        Returns\\n        -------\\n        bytes used\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes\\n\\n        Notes\\n        -----\\n        Memory usage does not include memory consumed by elements that\\n        are not components of the array if deep=False or if used on PyPy\\n        \"\"\"\\n        if hasattr(self.array, \\'memory_usage\\'):\\n            return self.array.memory_usage(deep=deep)\\n\\n        v = self.array.nbytes\\n        if deep and is_object_dtype(self) and not PYPY:\\n            v += lib.memory_usage_of_objects(self.array)\\n        return v',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'sequence_to_td64ns',\n",
       "  'docstring': 'Parameters\\n    ----------\\n    array : list-like\\n    copy : bool, default False\\n    unit : str, default \"ns\"\\n        The timedelta unit to treat integers as multiples of.\\n    errors : {\"raise\", \"coerce\", \"ignore\"}, default \"raise\"\\n        How to handle elements that cannot be converted to timedelta64[ns].\\n        See ``pandas.to_timedelta`` for details.\\n\\n    Returns\\n    -------\\n    converted : numpy.ndarray\\n        The sequence converted to a numpy array with dtype ``timedelta64[ns]``.\\n    inferred_freq : Tick or None\\n        The inferred frequency of the sequence.\\n\\n    Raises\\n    ------\\n    ValueError : Data cannot be converted to timedelta64[ns].\\n\\n    Notes\\n    -----\\n    Unlike `pandas.to_timedelta`, if setting ``errors=ignore`` will not cause\\n    errors to be ignored; they are caught and subsequently ignored at a\\n    higher level.',\n",
       "  'code': 'def sequence_to_td64ns(data, copy=False, unit=\"ns\", errors=\"raise\"):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    array : list-like\\n    copy : bool, default False\\n    unit : str, default \"ns\"\\n        The timedelta unit to treat integers as multiples of.\\n    errors : {\"raise\", \"coerce\", \"ignore\"}, default \"raise\"\\n        How to handle elements that cannot be converted to timedelta64[ns].\\n        See ``pandas.to_timedelta`` for details.\\n\\n    Returns\\n    -------\\n    converted : numpy.ndarray\\n        The sequence converted to a numpy array with dtype ``timedelta64[ns]``.\\n    inferred_freq : Tick or None\\n        The inferred frequency of the sequence.\\n\\n    Raises\\n    ------\\n    ValueError : Data cannot be converted to timedelta64[ns].\\n\\n    Notes\\n    -----\\n    Unlike `pandas.to_timedelta`, if setting ``errors=ignore`` will not cause\\n    errors to be ignored; they are caught and subsequently ignored at a\\n    higher level.\\n    \"\"\"\\n    inferred_freq = None\\n    unit = parse_timedelta_unit(unit)\\n\\n    # Unwrap whatever we have into a np.ndarray\\n    if not hasattr(data, \\'dtype\\'):\\n        # e.g. list, tuple\\n        if np.ndim(data) == 0:\\n            # i.e. generator\\n            data = list(data)\\n        data = np.array(data, copy=False)\\n    elif isinstance(data, ABCSeries):\\n        data = data._values\\n    elif isinstance(data, (ABCTimedeltaIndex, TimedeltaArray)):\\n        inferred_freq = data.freq\\n        data = data._data\\n\\n    # Convert whatever we have into timedelta64[ns] dtype\\n    if is_object_dtype(data.dtype) or is_string_dtype(data.dtype):\\n        # no need to make a copy, need to convert if string-dtyped\\n        data = objects_to_td64ns(data, unit=unit, errors=errors)\\n        copy = False\\n\\n    elif is_integer_dtype(data.dtype):\\n        # treat as multiples of the given unit\\n        data, copy_made = ints_to_td64ns(data, unit=unit)\\n        copy = copy and not copy_made\\n\\n    elif is_float_dtype(data.dtype):\\n        # cast the unit, multiply base/frace separately\\n        # to avoid precision issues from float -> int\\n        mask = np.isnan(data)\\n        m, p = precision_from_unit(unit)\\n        base = data.astype(np.int64)\\n        frac = data - base\\n        if p:\\n            frac = np.round(frac, p)\\n        data = (base * m + (frac * m).astype(np.int64)).view(\\'timedelta64[ns]\\')\\n        data[mask] = iNaT\\n        copy = False\\n\\n    elif is_timedelta64_dtype(data.dtype):\\n        if data.dtype != _TD_DTYPE:\\n            # non-nano unit\\n            # TODO: watch out for overflows\\n            data = data.astype(_TD_DTYPE)\\n            copy = False\\n\\n    elif is_datetime64_dtype(data):\\n        # GH#23539\\n        warnings.warn(\"Passing datetime64-dtype data to TimedeltaIndex is \"\\n                      \"deprecated, will raise a TypeError in a future \"\\n                      \"version\",\\n                      FutureWarning, stacklevel=4)\\n        data = ensure_int64(data).view(_TD_DTYPE)\\n\\n    else:\\n        raise TypeError(\"dtype {dtype} cannot be converted to timedelta64[ns]\"\\n                        .format(dtype=data.dtype))\\n\\n    data = np.array(data, copy=copy)\\n    if data.ndim != 1:\\n        raise ValueError(\"Only 1-dimensional input arrays are supported.\")\\n\\n    assert data.dtype == \\'m8[ns]\\', data\\n    return data, inferred_freq',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ints_to_td64ns',\n",
       "  'docstring': 'Convert an ndarray with integer-dtype to timedelta64[ns] dtype, treating\\n    the integers as multiples of the given timedelta unit.\\n\\n    Parameters\\n    ----------\\n    data : numpy.ndarray with integer-dtype\\n    unit : str, default \"ns\"\\n        The timedelta unit to treat integers as multiples of.\\n\\n    Returns\\n    -------\\n    numpy.ndarray : timedelta64[ns] array converted from data\\n    bool : whether a copy was made',\n",
       "  'code': 'def ints_to_td64ns(data, unit=\"ns\"):\\n    \"\"\"\\n    Convert an ndarray with integer-dtype to timedelta64[ns] dtype, treating\\n    the integers as multiples of the given timedelta unit.\\n\\n    Parameters\\n    ----------\\n    data : numpy.ndarray with integer-dtype\\n    unit : str, default \"ns\"\\n        The timedelta unit to treat integers as multiples of.\\n\\n    Returns\\n    -------\\n    numpy.ndarray : timedelta64[ns] array converted from data\\n    bool : whether a copy was made\\n    \"\"\"\\n    copy_made = False\\n    unit = unit if unit is not None else \"ns\"\\n\\n    if data.dtype != np.int64:\\n        # converting to int64 makes a copy, so we can avoid\\n        # re-copying later\\n        data = data.astype(np.int64)\\n        copy_made = True\\n\\n    if unit != \"ns\":\\n        dtype_str = \"timedelta64[{unit}]\".format(unit=unit)\\n        data = data.view(dtype_str)\\n\\n        # TODO: watch out for overflows when converting from lower-resolution\\n        data = data.astype(\"timedelta64[ns]\")\\n        # the astype conversion makes a copy, so we can avoid re-copying later\\n        copy_made = True\\n\\n    else:\\n        data = data.view(\"timedelta64[ns]\")\\n\\n    return data, copy_made',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'objects_to_td64ns',\n",
       "  'docstring': 'Convert a object-dtyped or string-dtyped array into an\\n    timedelta64[ns]-dtyped array.\\n\\n    Parameters\\n    ----------\\n    data : ndarray or Index\\n    unit : str, default \"ns\"\\n        The timedelta unit to treat integers as multiples of.\\n    errors : {\"raise\", \"coerce\", \"ignore\"}, default \"raise\"\\n        How to handle elements that cannot be converted to timedelta64[ns].\\n        See ``pandas.to_timedelta`` for details.\\n\\n    Returns\\n    -------\\n    numpy.ndarray : timedelta64[ns] array converted from data\\n\\n    Raises\\n    ------\\n    ValueError : Data cannot be converted to timedelta64[ns].\\n\\n    Notes\\n    -----\\n    Unlike `pandas.to_timedelta`, if setting `errors=ignore` will not cause\\n    errors to be ignored; they are caught and subsequently ignored at a\\n    higher level.',\n",
       "  'code': 'def objects_to_td64ns(data, unit=\"ns\", errors=\"raise\"):\\n    \"\"\"\\n    Convert a object-dtyped or string-dtyped array into an\\n    timedelta64[ns]-dtyped array.\\n\\n    Parameters\\n    ----------\\n    data : ndarray or Index\\n    unit : str, default \"ns\"\\n        The timedelta unit to treat integers as multiples of.\\n    errors : {\"raise\", \"coerce\", \"ignore\"}, default \"raise\"\\n        How to handle elements that cannot be converted to timedelta64[ns].\\n        See ``pandas.to_timedelta`` for details.\\n\\n    Returns\\n    -------\\n    numpy.ndarray : timedelta64[ns] array converted from data\\n\\n    Raises\\n    ------\\n    ValueError : Data cannot be converted to timedelta64[ns].\\n\\n    Notes\\n    -----\\n    Unlike `pandas.to_timedelta`, if setting `errors=ignore` will not cause\\n    errors to be ignored; they are caught and subsequently ignored at a\\n    higher level.\\n    \"\"\"\\n    # coerce Index to np.ndarray, converting string-dtype if necessary\\n    values = np.array(data, dtype=np.object_, copy=False)\\n\\n    result = array_to_timedelta64(values,\\n                                  unit=unit, errors=errors)\\n    return result.view(\\'timedelta64[ns]\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.compress',\n",
       "  'docstring': 'Return selected slices of an array along given axis as a Series.\\n\\n        .. deprecated:: 0.24.0\\n\\n        See Also\\n        --------\\n        numpy.ndarray.compress',\n",
       "  'code': 'def compress(self, condition, *args, **kwargs):\\n        \"\"\"\\n        Return selected slices of an array along given axis as a Series.\\n\\n        .. deprecated:: 0.24.0\\n\\n        See Also\\n        --------\\n        numpy.ndarray.compress\\n        \"\"\"\\n        msg = (\"Series.compress(condition) is deprecated. \"\\n               \"Use \\'Series[condition]\\' or \"\\n               \"\\'np.asarray(series).compress(condition)\\' instead.\")\\n        warnings.warn(msg, FutureWarning, stacklevel=2)\\n        nv.validate_compress(args, kwargs)\\n        return self[condition]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.nonzero',\n",
       "  'docstring': \"Return the *integer* indices of the elements that are non-zero.\\n\\n        .. deprecated:: 0.24.0\\n           Please use .to_numpy().nonzero() as a replacement.\\n\\n        This method is equivalent to calling `numpy.nonzero` on the\\n        series data. For compatibility with NumPy, the return value is\\n        the same (a tuple with an array of indices for each dimension),\\n        but it will always be a one-item tuple because series only have\\n        one dimension.\\n\\n        See Also\\n        --------\\n        numpy.nonzero\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([0, 3, 0, 4])\\n        >>> s.nonzero()\\n        (array([1, 3]),)\\n        >>> s.iloc[s.nonzero()[0]]\\n        1    3\\n        3    4\\n        dtype: int64\\n\\n        >>> s = pd.Series([0, 3, 0, 4], index=['a', 'b', 'c', 'd'])\\n        # same return although index of s is different\\n        >>> s.nonzero()\\n        (array([1, 3]),)\\n        >>> s.iloc[s.nonzero()[0]]\\n        b    3\\n        d    4\\n        dtype: int64\",\n",
       "  'code': 'def nonzero(self):\\n        \"\"\"\\n        Return the *integer* indices of the elements that are non-zero.\\n\\n        .. deprecated:: 0.24.0\\n           Please use .to_numpy().nonzero() as a replacement.\\n\\n        This method is equivalent to calling `numpy.nonzero` on the\\n        series data. For compatibility with NumPy, the return value is\\n        the same (a tuple with an array of indices for each dimension),\\n        but it will always be a one-item tuple because series only have\\n        one dimension.\\n\\n        See Also\\n        --------\\n        numpy.nonzero\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([0, 3, 0, 4])\\n        >>> s.nonzero()\\n        (array([1, 3]),)\\n        >>> s.iloc[s.nonzero()[0]]\\n        1    3\\n        3    4\\n        dtype: int64\\n\\n        >>> s = pd.Series([0, 3, 0, 4], index=[\\'a\\', \\'b\\', \\'c\\', \\'d\\'])\\n        # same return although index of s is different\\n        >>> s.nonzero()\\n        (array([1, 3]),)\\n        >>> s.iloc[s.nonzero()[0]]\\n        b    3\\n        d    4\\n        dtype: int64\\n        \"\"\"\\n        msg = (\"Series.nonzero() is deprecated \"\\n               \"and will be removed in a future version.\"\\n               \"Use Series.to_numpy().nonzero() instead\")\\n        warnings.warn(msg, FutureWarning, stacklevel=2)\\n        return self._values.nonzero()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.view',\n",
       "  'docstring': \"Create a new view of the Series.\\n\\n        This function will return a new Series with a view of the same\\n        underlying values in memory, optionally reinterpreted with a new data\\n        type. The new data type must preserve the same size in bytes as to not\\n        cause index misalignment.\\n\\n        Parameters\\n        ----------\\n        dtype : data type\\n            Data type object or one of their string representations.\\n\\n        Returns\\n        -------\\n        Series\\n            A new Series object as a view of the same data in memory.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.view : Equivalent numpy function to create a new view of\\n            the same data in memory.\\n\\n        Notes\\n        -----\\n        Series are instantiated with ``dtype=float64`` by default. While\\n        ``numpy.ndarray.view()`` will return a view with the same data type as\\n        the original array, ``Series.view()`` (without specified dtype)\\n        will try using ``float64`` and may fail if the original data type size\\n        in bytes is not the same.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')\\n        >>> s\\n        0   -2\\n        1   -1\\n        2    0\\n        3    1\\n        4    2\\n        dtype: int8\\n\\n        The 8 bit signed integer representation of `-1` is `0b11111111`, but\\n        the same bytes represent 255 if read as an 8 bit unsigned integer:\\n\\n        >>> us = s.view('uint8')\\n        >>> us\\n        0    254\\n        1    255\\n        2      0\\n        3      1\\n        4      2\\n        dtype: uint8\\n\\n        The views share the same underlying values:\\n\\n        >>> us[0] = 128\\n        >>> s\\n        0   -128\\n        1     -1\\n        2      0\\n        3      1\\n        4      2\\n        dtype: int8\",\n",
       "  'code': 'def view(self, dtype=None):\\n        \"\"\"\\n        Create a new view of the Series.\\n\\n        This function will return a new Series with a view of the same\\n        underlying values in memory, optionally reinterpreted with a new data\\n        type. The new data type must preserve the same size in bytes as to not\\n        cause index misalignment.\\n\\n        Parameters\\n        ----------\\n        dtype : data type\\n            Data type object or one of their string representations.\\n\\n        Returns\\n        -------\\n        Series\\n            A new Series object as a view of the same data in memory.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.view : Equivalent numpy function to create a new view of\\n            the same data in memory.\\n\\n        Notes\\n        -----\\n        Series are instantiated with ``dtype=float64`` by default. While\\n        ``numpy.ndarray.view()`` will return a view with the same data type as\\n        the original array, ``Series.view()`` (without specified dtype)\\n        will try using ``float64`` and may fail if the original data type size\\n        in bytes is not the same.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([-2, -1, 0, 1, 2], dtype=\\'int8\\')\\n        >>> s\\n        0   -2\\n        1   -1\\n        2    0\\n        3    1\\n        4    2\\n        dtype: int8\\n\\n        The 8 bit signed integer representation of `-1` is `0b11111111`, but\\n        the same bytes represent 255 if read as an 8 bit unsigned integer:\\n\\n        >>> us = s.view(\\'uint8\\')\\n        >>> us\\n        0    254\\n        1    255\\n        2      0\\n        3      1\\n        4      2\\n        dtype: uint8\\n\\n        The views share the same underlying values:\\n\\n        >>> us[0] = 128\\n        >>> s\\n        0   -128\\n        1     -1\\n        2      0\\n        3      1\\n        4      2\\n        dtype: int8\\n        \"\"\"\\n        return self._constructor(self._values.view(dtype),\\n                                 index=self.index).__finalize__(self)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.repeat',\n",
       "  'docstring': \"Repeat elements of a Series.\\n\\n        Returns a new Series where each element of the current Series\\n        is repeated consecutively a given number of times.\\n\\n        Parameters\\n        ----------\\n        repeats : int or array of ints\\n            The number of repetitions for each element. This should be a\\n            non-negative integer. Repeating 0 times will return an empty\\n            Series.\\n        axis : None\\n            Must be ``None``. Has no effect but is accepted for compatibility\\n            with numpy.\\n\\n        Returns\\n        -------\\n        Series\\n            Newly created Series with repeated elements.\\n\\n        See Also\\n        --------\\n        Index.repeat : Equivalent function for Index.\\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series(['a', 'b', 'c'])\\n        >>> s\\n        0    a\\n        1    b\\n        2    c\\n        dtype: object\\n        >>> s.repeat(2)\\n        0    a\\n        0    a\\n        1    b\\n        1    b\\n        2    c\\n        2    c\\n        dtype: object\\n        >>> s.repeat([1, 2, 3])\\n        0    a\\n        1    b\\n        1    b\\n        2    c\\n        2    c\\n        2    c\\n        dtype: object\",\n",
       "  'code': 'def repeat(self, repeats, axis=None):\\n        \"\"\"\\n        Repeat elements of a Series.\\n\\n        Returns a new Series where each element of the current Series\\n        is repeated consecutively a given number of times.\\n\\n        Parameters\\n        ----------\\n        repeats : int or array of ints\\n            The number of repetitions for each element. This should be a\\n            non-negative integer. Repeating 0 times will return an empty\\n            Series.\\n        axis : None\\n            Must be ``None``. Has no effect but is accepted for compatibility\\n            with numpy.\\n\\n        Returns\\n        -------\\n        Series\\n            Newly created Series with repeated elements.\\n\\n        See Also\\n        --------\\n        Index.repeat : Equivalent function for Index.\\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([\\'a\\', \\'b\\', \\'c\\'])\\n        >>> s\\n        0    a\\n        1    b\\n        2    c\\n        dtype: object\\n        >>> s.repeat(2)\\n        0    a\\n        0    a\\n        1    b\\n        1    b\\n        2    c\\n        2    c\\n        dtype: object\\n        >>> s.repeat([1, 2, 3])\\n        0    a\\n        1    b\\n        1    b\\n        2    c\\n        2    c\\n        2    c\\n        dtype: object\\n        \"\"\"\\n        nv.validate_repeat(tuple(), dict(axis=axis))\\n        new_index = self.index.repeat(repeats)\\n        new_values = self._values.repeat(repeats)\\n        return self._constructor(new_values,\\n                                 index=new_index).__finalize__(self)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.idxmin',\n",
       "  'docstring': \"Return the row label of the minimum value.\\n\\n        If multiple values equal the minimum, the first row label with that\\n        value is returned.\\n\\n        Parameters\\n        ----------\\n        skipna : bool, default True\\n            Exclude NA/null values. If the entire Series is NA, the result\\n            will be NA.\\n        axis : int, default 0\\n            For compatibility with DataFrame.idxmin. Redundant for application\\n            on Series.\\n        *args, **kwargs\\n            Additional keywords have no effect but might be accepted\\n            for compatibility with NumPy.\\n\\n        Returns\\n        -------\\n        Index\\n            Label of the minimum value.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If the Series is empty.\\n\\n        See Also\\n        --------\\n        numpy.argmin : Return indices of the minimum values\\n            along the given axis.\\n        DataFrame.idxmin : Return index of first occurrence of minimum\\n            over requested axis.\\n        Series.idxmax : Return index *label* of the first occurrence\\n            of maximum of values.\\n\\n        Notes\\n        -----\\n        This method is the Series version of ``ndarray.argmin``. This method\\n        returns the label of the minimum, while ``ndarray.argmin`` returns\\n        the position. To get the position, use ``series.values.argmin()``.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series(data=[1, None, 4, 1],\\n        ...               index=['A', 'B', 'C', 'D'])\\n        >>> s\\n        A    1.0\\n        B    NaN\\n        C    4.0\\n        D    1.0\\n        dtype: float64\\n\\n        >>> s.idxmin()\\n        'A'\\n\\n        If `skipna` is False and there is an NA value in the data,\\n        the function returns ``nan``.\\n\\n        >>> s.idxmin(skipna=False)\\n        nan\",\n",
       "  'code': 'def idxmin(self, axis=0, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Return the row label of the minimum value.\\n\\n        If multiple values equal the minimum, the first row label with that\\n        value is returned.\\n\\n        Parameters\\n        ----------\\n        skipna : bool, default True\\n            Exclude NA/null values. If the entire Series is NA, the result\\n            will be NA.\\n        axis : int, default 0\\n            For compatibility with DataFrame.idxmin. Redundant for application\\n            on Series.\\n        *args, **kwargs\\n            Additional keywords have no effect but might be accepted\\n            for compatibility with NumPy.\\n\\n        Returns\\n        -------\\n        Index\\n            Label of the minimum value.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If the Series is empty.\\n\\n        See Also\\n        --------\\n        numpy.argmin : Return indices of the minimum values\\n            along the given axis.\\n        DataFrame.idxmin : Return index of first occurrence of minimum\\n            over requested axis.\\n        Series.idxmax : Return index *label* of the first occurrence\\n            of maximum of values.\\n\\n        Notes\\n        -----\\n        This method is the Series version of ``ndarray.argmin``. This method\\n        returns the label of the minimum, while ``ndarray.argmin`` returns\\n        the position. To get the position, use ``series.values.argmin()``.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series(data=[1, None, 4, 1],\\n        ...               index=[\\'A\\', \\'B\\', \\'C\\', \\'D\\'])\\n        >>> s\\n        A    1.0\\n        B    NaN\\n        C    4.0\\n        D    1.0\\n        dtype: float64\\n\\n        >>> s.idxmin()\\n        \\'A\\'\\n\\n        If `skipna` is False and there is an NA value in the data,\\n        the function returns ``nan``.\\n\\n        >>> s.idxmin(skipna=False)\\n        nan\\n        \"\"\"\\n        skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)\\n        i = nanops.nanargmin(com.values_from_object(self), skipna=skipna)\\n        if i == -1:\\n            return np.nan\\n        return self.index[i]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.idxmax',\n",
       "  'docstring': \"Return the row label of the maximum value.\\n\\n        If multiple values equal the maximum, the first row label with that\\n        value is returned.\\n\\n        Parameters\\n        ----------\\n        skipna : bool, default True\\n            Exclude NA/null values. If the entire Series is NA, the result\\n            will be NA.\\n        axis : int, default 0\\n            For compatibility with DataFrame.idxmax. Redundant for application\\n            on Series.\\n        *args, **kwargs\\n            Additional keywords have no effect but might be accepted\\n            for compatibility with NumPy.\\n\\n        Returns\\n        -------\\n        Index\\n            Label of the maximum value.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If the Series is empty.\\n\\n        See Also\\n        --------\\n        numpy.argmax : Return indices of the maximum values\\n            along the given axis.\\n        DataFrame.idxmax : Return index of first occurrence of maximum\\n            over requested axis.\\n        Series.idxmin : Return index *label* of the first occurrence\\n            of minimum of values.\\n\\n        Notes\\n        -----\\n        This method is the Series version of ``ndarray.argmax``. This method\\n        returns the label of the maximum, while ``ndarray.argmax`` returns\\n        the position. To get the position, use ``series.values.argmax()``.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series(data=[1, None, 4, 3, 4],\\n        ...               index=['A', 'B', 'C', 'D', 'E'])\\n        >>> s\\n        A    1.0\\n        B    NaN\\n        C    4.0\\n        D    3.0\\n        E    4.0\\n        dtype: float64\\n\\n        >>> s.idxmax()\\n        'C'\\n\\n        If `skipna` is False and there is an NA value in the data,\\n        the function returns ``nan``.\\n\\n        >>> s.idxmax(skipna=False)\\n        nan\",\n",
       "  'code': 'def idxmax(self, axis=0, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Return the row label of the maximum value.\\n\\n        If multiple values equal the maximum, the first row label with that\\n        value is returned.\\n\\n        Parameters\\n        ----------\\n        skipna : bool, default True\\n            Exclude NA/null values. If the entire Series is NA, the result\\n            will be NA.\\n        axis : int, default 0\\n            For compatibility with DataFrame.idxmax. Redundant for application\\n            on Series.\\n        *args, **kwargs\\n            Additional keywords have no effect but might be accepted\\n            for compatibility with NumPy.\\n\\n        Returns\\n        -------\\n        Index\\n            Label of the maximum value.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If the Series is empty.\\n\\n        See Also\\n        --------\\n        numpy.argmax : Return indices of the maximum values\\n            along the given axis.\\n        DataFrame.idxmax : Return index of first occurrence of maximum\\n            over requested axis.\\n        Series.idxmin : Return index *label* of the first occurrence\\n            of minimum of values.\\n\\n        Notes\\n        -----\\n        This method is the Series version of ``ndarray.argmax``. This method\\n        returns the label of the maximum, while ``ndarray.argmax`` returns\\n        the position. To get the position, use ``series.values.argmax()``.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series(data=[1, None, 4, 3, 4],\\n        ...               index=[\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'E\\'])\\n        >>> s\\n        A    1.0\\n        B    NaN\\n        C    4.0\\n        D    3.0\\n        E    4.0\\n        dtype: float64\\n\\n        >>> s.idxmax()\\n        \\'C\\'\\n\\n        If `skipna` is False and there is an NA value in the data,\\n        the function returns ``nan``.\\n\\n        >>> s.idxmax(skipna=False)\\n        nan\\n        \"\"\"\\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\\n        i = nanops.nanargmax(com.values_from_object(self), skipna=skipna)\\n        if i == -1:\\n            return np.nan\\n        return self.index[i]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.round',\n",
       "  'docstring': 'Round each value in a Series to the given number of decimals.\\n\\n        Parameters\\n        ----------\\n        decimals : int\\n            Number of decimal places to round to (default: 0).\\n            If decimals is negative, it specifies the number of\\n            positions to the left of the decimal point.\\n\\n        Returns\\n        -------\\n        Series\\n            Rounded values of the Series.\\n\\n        See Also\\n        --------\\n        numpy.around : Round values of an np.array.\\n        DataFrame.round : Round values of a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([0.1, 1.3, 2.7])\\n        >>> s.round()\\n        0    0.0\\n        1    1.0\\n        2    3.0\\n        dtype: float64',\n",
       "  'code': 'def round(self, decimals=0, *args, **kwargs):\\n        \"\"\"\\n        Round each value in a Series to the given number of decimals.\\n\\n        Parameters\\n        ----------\\n        decimals : int\\n            Number of decimal places to round to (default: 0).\\n            If decimals is negative, it specifies the number of\\n            positions to the left of the decimal point.\\n\\n        Returns\\n        -------\\n        Series\\n            Rounded values of the Series.\\n\\n        See Also\\n        --------\\n        numpy.around : Round values of an np.array.\\n        DataFrame.round : Round values of a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([0.1, 1.3, 2.7])\\n        >>> s.round()\\n        0    0.0\\n        1    1.0\\n        2    3.0\\n        dtype: float64\\n        \"\"\"\\n        nv.validate_round(args, kwargs)\\n        result = com.values_from_object(self).round(decimals)\\n        result = self._constructor(result, index=self.index).__finalize__(self)\\n\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.quantile',\n",
       "  'docstring': \"Return value at the given quantile.\\n\\n        Parameters\\n        ----------\\n        q : float or array-like, default 0.5 (50% quantile)\\n            0 <= q <= 1, the quantile(s) to compute.\\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\\n            .. versionadded:: 0.18.0\\n\\n            This optional parameter specifies the interpolation method to use,\\n            when the desired quantile lies between two data points `i` and `j`:\\n\\n                * linear: `i + (j - i) * fraction`, where `fraction` is the\\n                  fractional part of the index surrounded by `i` and `j`.\\n                * lower: `i`.\\n                * higher: `j`.\\n                * nearest: `i` or `j` whichever is nearest.\\n                * midpoint: (`i` + `j`) / 2.\\n\\n        Returns\\n        -------\\n        float or Series\\n            If ``q`` is an array, a Series will be returned where the\\n            index is ``q`` and the values are the quantiles, otherwise\\n            a float will be returned.\\n\\n        See Also\\n        --------\\n        core.window.Rolling.quantile\\n        numpy.percentile\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([1, 2, 3, 4])\\n        >>> s.quantile(.5)\\n        2.5\\n        >>> s.quantile([.25, .5, .75])\\n        0.25    1.75\\n        0.50    2.50\\n        0.75    3.25\\n        dtype: float64\",\n",
       "  'code': 'def quantile(self, q=0.5, interpolation=\\'linear\\'):\\n        \"\"\"\\n        Return value at the given quantile.\\n\\n        Parameters\\n        ----------\\n        q : float or array-like, default 0.5 (50% quantile)\\n            0 <= q <= 1, the quantile(s) to compute.\\n        interpolation : {\\'linear\\', \\'lower\\', \\'higher\\', \\'midpoint\\', \\'nearest\\'}\\n            .. versionadded:: 0.18.0\\n\\n            This optional parameter specifies the interpolation method to use,\\n            when the desired quantile lies between two data points `i` and `j`:\\n\\n                * linear: `i + (j - i) * fraction`, where `fraction` is the\\n                  fractional part of the index surrounded by `i` and `j`.\\n                * lower: `i`.\\n                * higher: `j`.\\n                * nearest: `i` or `j` whichever is nearest.\\n                * midpoint: (`i` + `j`) / 2.\\n\\n        Returns\\n        -------\\n        float or Series\\n            If ``q`` is an array, a Series will be returned where the\\n            index is ``q`` and the values are the quantiles, otherwise\\n            a float will be returned.\\n\\n        See Also\\n        --------\\n        core.window.Rolling.quantile\\n        numpy.percentile\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([1, 2, 3, 4])\\n        >>> s.quantile(.5)\\n        2.5\\n        >>> s.quantile([.25, .5, .75])\\n        0.25    1.75\\n        0.50    2.50\\n        0.75    3.25\\n        dtype: float64\\n        \"\"\"\\n\\n        self._check_percentile(q)\\n\\n        # We dispatch to DataFrame so that core.internals only has to worry\\n        #  about 2D cases.\\n        df = self.to_frame()\\n\\n        result = df.quantile(q=q, interpolation=interpolation,\\n                             numeric_only=False)\\n        if result.ndim == 2:\\n            result = result.iloc[:, 0]\\n\\n        if is_list_like(q):\\n            result.name = self.name\\n            return self._constructor(result,\\n                                     index=Float64Index(q),\\n                                     name=self.name)\\n        else:\\n            # scalar\\n            return result.iloc[0]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.dot',\n",
       "  'docstring': 'Compute the dot product between the Series and the columns of other.\\n\\n        This method computes the dot product between the Series and another\\n        one, or the Series and each columns of a DataFrame, or the Series and\\n        each columns of an array.\\n\\n        It can also be called using `self @ other` in Python >= 3.5.\\n\\n        Parameters\\n        ----------\\n        other : Series, DataFrame or array-like\\n            The other object to compute the dot product with its columns.\\n\\n        Returns\\n        -------\\n        scalar, Series or numpy.ndarray\\n            Return the dot product of the Series and other if other is a\\n            Series, the Series of the dot product of Series and each rows of\\n            other if other is a DataFrame or a numpy.ndarray between the Series\\n            and each columns of the numpy array.\\n\\n        See Also\\n        --------\\n        DataFrame.dot: Compute the matrix product with the DataFrame.\\n        Series.mul: Multiplication of series and other, element-wise.\\n\\n        Notes\\n        -----\\n        The Series and other has to share the same index if other is a Series\\n        or a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([0, 1, 2, 3])\\n        >>> other = pd.Series([-1, 2, -3, 4])\\n        >>> s.dot(other)\\n        8\\n        >>> s @ other\\n        8\\n        >>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\\n        >>> s.dot(df)\\n        0    24\\n        1    14\\n        dtype: int64\\n        >>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\\n        >>> s.dot(arr)\\n        array([24, 14])',\n",
       "  'code': 'def dot(self, other):\\n        \"\"\"\\n        Compute the dot product between the Series and the columns of other.\\n\\n        This method computes the dot product between the Series and another\\n        one, or the Series and each columns of a DataFrame, or the Series and\\n        each columns of an array.\\n\\n        It can also be called using `self @ other` in Python >= 3.5.\\n\\n        Parameters\\n        ----------\\n        other : Series, DataFrame or array-like\\n            The other object to compute the dot product with its columns.\\n\\n        Returns\\n        -------\\n        scalar, Series or numpy.ndarray\\n            Return the dot product of the Series and other if other is a\\n            Series, the Series of the dot product of Series and each rows of\\n            other if other is a DataFrame or a numpy.ndarray between the Series\\n            and each columns of the numpy array.\\n\\n        See Also\\n        --------\\n        DataFrame.dot: Compute the matrix product with the DataFrame.\\n        Series.mul: Multiplication of series and other, element-wise.\\n\\n        Notes\\n        -----\\n        The Series and other has to share the same index if other is a Series\\n        or a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([0, 1, 2, 3])\\n        >>> other = pd.Series([-1, 2, -3, 4])\\n        >>> s.dot(other)\\n        8\\n        >>> s @ other\\n        8\\n        >>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\\n        >>> s.dot(df)\\n        0    24\\n        1    14\\n        dtype: int64\\n        >>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\\n        >>> s.dot(arr)\\n        array([24, 14])\\n        \"\"\"\\n        from pandas.core.frame import DataFrame\\n        if isinstance(other, (Series, DataFrame)):\\n            common = self.index.union(other.index)\\n            if (len(common) > len(self.index) or\\n                    len(common) > len(other.index)):\\n                raise ValueError(\\'matrices are not aligned\\')\\n\\n            left = self.reindex(index=common, copy=False)\\n            right = other.reindex(index=common, copy=False)\\n            lvals = left.values\\n            rvals = right.values\\n        else:\\n            lvals = self.values\\n            rvals = np.asarray(other)\\n            if lvals.shape[0] != rvals.shape[0]:\\n                raise Exception(\\'Dot product shape mismatch, %s vs %s\\' %\\n                                (lvals.shape, rvals.shape))\\n\\n        if isinstance(other, DataFrame):\\n            return self._constructor(np.dot(lvals, rvals),\\n                                     index=other.columns).__finalize__(self)\\n        elif isinstance(other, Series):\\n            return np.dot(lvals, rvals)\\n        elif isinstance(rvals, np.ndarray):\\n            return np.dot(lvals, rvals)\\n        else:  # pragma: no cover\\n            raise TypeError(\\'unsupported type: %s\\' % type(other))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.sort_values',\n",
       "  'docstring': \"Sort by the values.\\n\\n        Sort a Series in ascending or descending order by some\\n        criterion.\\n\\n        Parameters\\n        ----------\\n        axis : {0 or 'index'}, default 0\\n            Axis to direct sorting. The value 'index' is accepted for\\n            compatibility with DataFrame.sort_values.\\n        ascending : bool, default True\\n            If True, sort values in ascending order, otherwise descending.\\n        inplace : bool, default False\\n            If True, perform operation in-place.\\n        kind : {'quicksort', 'mergesort' or 'heapsort'}, default 'quicksort'\\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\\n            information. 'mergesort' is the only stable  algorithm.\\n        na_position : {'first' or 'last'}, default 'last'\\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\\n            the end.\\n\\n        Returns\\n        -------\\n        Series\\n            Series ordered by values.\\n\\n        See Also\\n        --------\\n        Series.sort_index : Sort by the Series indices.\\n        DataFrame.sort_values : Sort DataFrame by the values along either axis.\\n        DataFrame.sort_index : Sort DataFrame by indices.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([np.nan, 1, 3, 10, 5])\\n        >>> s\\n        0     NaN\\n        1     1.0\\n        2     3.0\\n        3     10.0\\n        4     5.0\\n        dtype: float64\\n\\n        Sort values ascending order (default behaviour)\\n\\n        >>> s.sort_values(ascending=True)\\n        1     1.0\\n        2     3.0\\n        4     5.0\\n        3    10.0\\n        0     NaN\\n        dtype: float64\\n\\n        Sort values descending order\\n\\n        >>> s.sort_values(ascending=False)\\n        3    10.0\\n        4     5.0\\n        2     3.0\\n        1     1.0\\n        0     NaN\\n        dtype: float64\\n\\n        Sort values inplace\\n\\n        >>> s.sort_values(ascending=False, inplace=True)\\n        >>> s\\n        3    10.0\\n        4     5.0\\n        2     3.0\\n        1     1.0\\n        0     NaN\\n        dtype: float64\\n\\n        Sort values putting NAs first\\n\\n        >>> s.sort_values(na_position='first')\\n        0     NaN\\n        1     1.0\\n        2     3.0\\n        4     5.0\\n        3    10.0\\n        dtype: float64\\n\\n        Sort a series of strings\\n\\n        >>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\\n        >>> s\\n        0    z\\n        1    b\\n        2    d\\n        3    a\\n        4    c\\n        dtype: object\\n\\n        >>> s.sort_values()\\n        3    a\\n        1    b\\n        4    c\\n        2    d\\n        0    z\\n        dtype: object\",\n",
       "  'code': 'def sort_values(self, axis=0, ascending=True, inplace=False,\\n                    kind=\\'quicksort\\', na_position=\\'last\\'):\\n        \"\"\"\\n        Sort by the values.\\n\\n        Sort a Series in ascending or descending order by some\\n        criterion.\\n\\n        Parameters\\n        ----------\\n        axis : {0 or \\'index\\'}, default 0\\n            Axis to direct sorting. The value \\'index\\' is accepted for\\n            compatibility with DataFrame.sort_values.\\n        ascending : bool, default True\\n            If True, sort values in ascending order, otherwise descending.\\n        inplace : bool, default False\\n            If True, perform operation in-place.\\n        kind : {\\'quicksort\\', \\'mergesort\\' or \\'heapsort\\'}, default \\'quicksort\\'\\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\\n            information. \\'mergesort\\' is the only stable  algorithm.\\n        na_position : {\\'first\\' or \\'last\\'}, default \\'last\\'\\n            Argument \\'first\\' puts NaNs at the beginning, \\'last\\' puts NaNs at\\n            the end.\\n\\n        Returns\\n        -------\\n        Series\\n            Series ordered by values.\\n\\n        See Also\\n        --------\\n        Series.sort_index : Sort by the Series indices.\\n        DataFrame.sort_values : Sort DataFrame by the values along either axis.\\n        DataFrame.sort_index : Sort DataFrame by indices.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([np.nan, 1, 3, 10, 5])\\n        >>> s\\n        0     NaN\\n        1     1.0\\n        2     3.0\\n        3     10.0\\n        4     5.0\\n        dtype: float64\\n\\n        Sort values ascending order (default behaviour)\\n\\n        >>> s.sort_values(ascending=True)\\n        1     1.0\\n        2     3.0\\n        4     5.0\\n        3    10.0\\n        0     NaN\\n        dtype: float64\\n\\n        Sort values descending order\\n\\n        >>> s.sort_values(ascending=False)\\n        3    10.0\\n        4     5.0\\n        2     3.0\\n        1     1.0\\n        0     NaN\\n        dtype: float64\\n\\n        Sort values inplace\\n\\n        >>> s.sort_values(ascending=False, inplace=True)\\n        >>> s\\n        3    10.0\\n        4     5.0\\n        2     3.0\\n        1     1.0\\n        0     NaN\\n        dtype: float64\\n\\n        Sort values putting NAs first\\n\\n        >>> s.sort_values(na_position=\\'first\\')\\n        0     NaN\\n        1     1.0\\n        2     3.0\\n        4     5.0\\n        3    10.0\\n        dtype: float64\\n\\n        Sort a series of strings\\n\\n        >>> s = pd.Series([\\'z\\', \\'b\\', \\'d\\', \\'a\\', \\'c\\'])\\n        >>> s\\n        0    z\\n        1    b\\n        2    d\\n        3    a\\n        4    c\\n        dtype: object\\n\\n        >>> s.sort_values()\\n        3    a\\n        1    b\\n        4    c\\n        2    d\\n        0    z\\n        dtype: object\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        # Validate the axis parameter\\n        self._get_axis_number(axis)\\n\\n        # GH 5856/5853\\n        if inplace and self._is_cached:\\n            raise ValueError(\"This Series is a view of some other array, to \"\\n                             \"sort in-place you must create a copy\")\\n\\n        def _try_kind_sort(arr):\\n            # easier to ask forgiveness than permission\\n            try:\\n                # if kind==mergesort, it can fail for object dtype\\n                return arr.argsort(kind=kind)\\n            except TypeError:\\n                # stable sort not available for object dtype\\n                # uses the argsort default quicksort\\n                return arr.argsort(kind=\\'quicksort\\')\\n\\n        arr = self._values\\n        sortedIdx = np.empty(len(self), dtype=np.int32)\\n\\n        bad = isna(arr)\\n\\n        good = ~bad\\n        idx = ibase.default_index(len(self))\\n\\n        argsorted = _try_kind_sort(arr[good])\\n\\n        if is_list_like(ascending):\\n            if len(ascending) != 1:\\n                raise ValueError(\\'Length of ascending (%d) must be 1 \\'\\n                                 \\'for Series\\' % (len(ascending)))\\n            ascending = ascending[0]\\n\\n        if not is_bool(ascending):\\n            raise ValueError(\\'ascending must be boolean\\')\\n\\n        if not ascending:\\n            argsorted = argsorted[::-1]\\n\\n        if na_position == \\'last\\':\\n            n = good.sum()\\n            sortedIdx[:n] = idx[good][argsorted]\\n            sortedIdx[n:] = idx[bad]\\n        elif na_position == \\'first\\':\\n            n = bad.sum()\\n            sortedIdx[n:] = idx[good][argsorted]\\n            sortedIdx[:n] = idx[bad]\\n        else:\\n            raise ValueError(\\'invalid na_position: {!r}\\'.format(na_position))\\n\\n        result = self._constructor(arr[sortedIdx], index=self.index[sortedIdx])\\n\\n        if inplace:\\n            self._update_inplace(result)\\n        else:\\n            return result.__finalize__(self)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.sort_index',\n",
       "  'docstring': \"Sort Series by index labels.\\n\\n        Returns a new Series sorted by label if `inplace` argument is\\n        ``False``, otherwise updates the original series and returns None.\\n\\n        Parameters\\n        ----------\\n        axis : int, default 0\\n            Axis to direct sorting. This can only be 0 for Series.\\n        level : int, optional\\n            If not None, sort on values in specified index level(s).\\n        ascending : bool, default true\\n            Sort ascending vs. descending.\\n        inplace : bool, default False\\n            If True, perform operation in-place.\\n        kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\\n            information.  'mergesort' is the only stable algorithm. For\\n            DataFrames, this option is only applied when sorting on a single\\n            column or label.\\n        na_position : {'first', 'last'}, default 'last'\\n            If 'first' puts NaNs at the beginning, 'last' puts NaNs at the end.\\n            Not implemented for MultiIndex.\\n        sort_remaining : bool, default True\\n            If True and sorting by level and index is multilevel, sort by other\\n            levels too (in order) after sorting by specified level.\\n\\n        Returns\\n        -------\\n        Series\\n            The original Series sorted by the labels.\\n\\n        See Also\\n        --------\\n        DataFrame.sort_index: Sort DataFrame by the index.\\n        DataFrame.sort_values: Sort DataFrame by the value.\\n        Series.sort_values : Sort Series by the value.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\\n        >>> s.sort_index()\\n        1    c\\n        2    b\\n        3    a\\n        4    d\\n        dtype: object\\n\\n        Sort Descending\\n\\n        >>> s.sort_index(ascending=False)\\n        4    d\\n        3    a\\n        2    b\\n        1    c\\n        dtype: object\\n\\n        Sort Inplace\\n\\n        >>> s.sort_index(inplace=True)\\n        >>> s\\n        1    c\\n        2    b\\n        3    a\\n        4    d\\n        dtype: object\\n\\n        By default NaNs are put at the end, but use `na_position` to place\\n        them at the beginning\\n\\n        >>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\\n        >>> s.sort_index(na_position='first')\\n        NaN     d\\n         1.0    c\\n         2.0    b\\n         3.0    a\\n        dtype: object\\n\\n        Specify index level to sort\\n\\n        >>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\\n        ...                     'baz', 'baz', 'bar', 'bar']),\\n        ...           np.array(['two', 'one', 'two', 'one',\\n        ...                     'two', 'one', 'two', 'one'])]\\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\\n        >>> s.sort_index(level=1)\\n        bar  one    8\\n        baz  one    6\\n        foo  one    4\\n        qux  one    2\\n        bar  two    7\\n        baz  two    5\\n        foo  two    3\\n        qux  two    1\\n        dtype: int64\\n\\n        Does not sort by remaining levels when sorting by levels\\n\\n        >>> s.sort_index(level=1, sort_remaining=False)\\n        qux  one    2\\n        foo  one    4\\n        baz  one    6\\n        bar  one    8\\n        qux  two    1\\n        foo  two    3\\n        baz  two    5\\n        bar  two    7\\n        dtype: int64\",\n",
       "  'code': 'def sort_index(self, axis=0, level=None, ascending=True, inplace=False,\\n                   kind=\\'quicksort\\', na_position=\\'last\\', sort_remaining=True):\\n        \"\"\"\\n        Sort Series by index labels.\\n\\n        Returns a new Series sorted by label if `inplace` argument is\\n        ``False``, otherwise updates the original series and returns None.\\n\\n        Parameters\\n        ----------\\n        axis : int, default 0\\n            Axis to direct sorting. This can only be 0 for Series.\\n        level : int, optional\\n            If not None, sort on values in specified index level(s).\\n        ascending : bool, default true\\n            Sort ascending vs. descending.\\n        inplace : bool, default False\\n            If True, perform operation in-place.\\n        kind : {\\'quicksort\\', \\'mergesort\\', \\'heapsort\\'}, default \\'quicksort\\'\\n            Choice of sorting algorithm. See also :func:`numpy.sort` for more\\n            information.  \\'mergesort\\' is the only stable algorithm. For\\n            DataFrames, this option is only applied when sorting on a single\\n            column or label.\\n        na_position : {\\'first\\', \\'last\\'}, default \\'last\\'\\n            If \\'first\\' puts NaNs at the beginning, \\'last\\' puts NaNs at the end.\\n            Not implemented for MultiIndex.\\n        sort_remaining : bool, default True\\n            If True and sorting by level and index is multilevel, sort by other\\n            levels too (in order) after sorting by specified level.\\n\\n        Returns\\n        -------\\n        Series\\n            The original Series sorted by the labels.\\n\\n        See Also\\n        --------\\n        DataFrame.sort_index: Sort DataFrame by the index.\\n        DataFrame.sort_values: Sort DataFrame by the value.\\n        Series.sort_values : Sort Series by the value.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([\\'a\\', \\'b\\', \\'c\\', \\'d\\'], index=[3, 2, 1, 4])\\n        >>> s.sort_index()\\n        1    c\\n        2    b\\n        3    a\\n        4    d\\n        dtype: object\\n\\n        Sort Descending\\n\\n        >>> s.sort_index(ascending=False)\\n        4    d\\n        3    a\\n        2    b\\n        1    c\\n        dtype: object\\n\\n        Sort Inplace\\n\\n        >>> s.sort_index(inplace=True)\\n        >>> s\\n        1    c\\n        2    b\\n        3    a\\n        4    d\\n        dtype: object\\n\\n        By default NaNs are put at the end, but use `na_position` to place\\n        them at the beginning\\n\\n        >>> s = pd.Series([\\'a\\', \\'b\\', \\'c\\', \\'d\\'], index=[3, 2, 1, np.nan])\\n        >>> s.sort_index(na_position=\\'first\\')\\n        NaN     d\\n         1.0    c\\n         2.0    b\\n         3.0    a\\n        dtype: object\\n\\n        Specify index level to sort\\n\\n        >>> arrays = [np.array([\\'qux\\', \\'qux\\', \\'foo\\', \\'foo\\',\\n        ...                     \\'baz\\', \\'baz\\', \\'bar\\', \\'bar\\']),\\n        ...           np.array([\\'two\\', \\'one\\', \\'two\\', \\'one\\',\\n        ...                     \\'two\\', \\'one\\', \\'two\\', \\'one\\'])]\\n        >>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\\n        >>> s.sort_index(level=1)\\n        bar  one    8\\n        baz  one    6\\n        foo  one    4\\n        qux  one    2\\n        bar  two    7\\n        baz  two    5\\n        foo  two    3\\n        qux  two    1\\n        dtype: int64\\n\\n        Does not sort by remaining levels when sorting by levels\\n\\n        >>> s.sort_index(level=1, sort_remaining=False)\\n        qux  one    2\\n        foo  one    4\\n        baz  one    6\\n        bar  one    8\\n        qux  two    1\\n        foo  two    3\\n        baz  two    5\\n        bar  two    7\\n        dtype: int64\\n        \"\"\"\\n        # TODO: this can be combined with DataFrame.sort_index impl as\\n        # almost identical\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        # Validate the axis parameter\\n        self._get_axis_number(axis)\\n        index = self.index\\n\\n        if level is not None:\\n            new_index, indexer = index.sortlevel(level, ascending=ascending,\\n                                                 sort_remaining=sort_remaining)\\n        elif isinstance(index, MultiIndex):\\n            from pandas.core.sorting import lexsort_indexer\\n            labels = index._sort_levels_monotonic()\\n            indexer = lexsort_indexer(labels._get_codes_for_sorting(),\\n                                      orders=ascending,\\n                                      na_position=na_position)\\n        else:\\n            from pandas.core.sorting import nargsort\\n\\n            # Check monotonic-ness before sort an index\\n            # GH11080\\n            if ((ascending and index.is_monotonic_increasing) or\\n                    (not ascending and index.is_monotonic_decreasing)):\\n                if inplace:\\n                    return\\n                else:\\n                    return self.copy()\\n\\n            indexer = nargsort(index, kind=kind, ascending=ascending,\\n                               na_position=na_position)\\n\\n        indexer = ensure_platform_int(indexer)\\n        new_index = index.take(indexer)\\n        new_index = new_index._sort_levels_monotonic()\\n\\n        new_values = self._values.take(indexer)\\n        result = self._constructor(new_values, index=new_index)\\n\\n        if inplace:\\n            self._update_inplace(result)\\n        else:\\n            return result.__finalize__(self)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.argsort',\n",
       "  'docstring': \"Override ndarray.argsort. Argsorts the value, omitting NA/null values,\\n        and places the result in the same locations as the non-NA values.\\n\\n        Parameters\\n        ----------\\n        axis : int\\n            Has no effect but is accepted for compatibility with numpy.\\n        kind : {'mergesort', 'quicksort', 'heapsort'}, default 'quicksort'\\n            Choice of sorting algorithm. See np.sort for more\\n            information. 'mergesort' is the only stable algorithm\\n        order : None\\n            Has no effect but is accepted for compatibility with numpy.\\n\\n        Returns\\n        -------\\n        Series\\n            Positions of values within the sort order with -1 indicating\\n            nan values.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argsort\",\n",
       "  'code': 'def argsort(self, axis=0, kind=\\'quicksort\\', order=None):\\n        \"\"\"\\n        Override ndarray.argsort. Argsorts the value, omitting NA/null values,\\n        and places the result in the same locations as the non-NA values.\\n\\n        Parameters\\n        ----------\\n        axis : int\\n            Has no effect but is accepted for compatibility with numpy.\\n        kind : {\\'mergesort\\', \\'quicksort\\', \\'heapsort\\'}, default \\'quicksort\\'\\n            Choice of sorting algorithm. See np.sort for more\\n            information. \\'mergesort\\' is the only stable algorithm\\n        order : None\\n            Has no effect but is accepted for compatibility with numpy.\\n\\n        Returns\\n        -------\\n        Series\\n            Positions of values within the sort order with -1 indicating\\n            nan values.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argsort\\n        \"\"\"\\n        values = self._values\\n        mask = isna(values)\\n\\n        if mask.any():\\n            result = Series(-1, index=self.index, name=self.name,\\n                            dtype=\\'int64\\')\\n            notmask = ~mask\\n            result[notmask] = np.argsort(values[notmask], kind=kind)\\n            return self._constructor(result,\\n                                     index=self.index).__finalize__(self)\\n        else:\\n            return self._constructor(\\n                np.argsort(values, kind=kind), index=self.index,\\n                dtype=\\'int64\\').__finalize__(self)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.apply',\n",
       "  'docstring': \"Invoke function on values of Series.\\n\\n        Can be ufunc (a NumPy function that applies to the entire Series)\\n        or a Python function that only works on single values.\\n\\n        Parameters\\n        ----------\\n        func : function\\n            Python function or NumPy ufunc to apply.\\n        convert_dtype : bool, default True\\n            Try to find better dtype for elementwise function results. If\\n            False, leave as dtype=object.\\n        args : tuple\\n            Positional arguments passed to func after the series value.\\n        **kwds\\n            Additional keyword arguments passed to func.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            If func returns a Series object the result will be a DataFrame.\\n\\n        See Also\\n        --------\\n        Series.map: For element-wise operations.\\n        Series.agg: Only perform aggregating type operations.\\n        Series.transform: Only perform transforming type operations.\\n\\n        Examples\\n        --------\\n        Create a series with typical summer temperatures for each city.\\n\\n        >>> s = pd.Series([20, 21, 12],\\n        ...               index=['London', 'New York', 'Helsinki'])\\n        >>> s\\n        London      20\\n        New York    21\\n        Helsinki    12\\n        dtype: int64\\n\\n        Square the values by defining a function and passing it as an\\n        argument to ``apply()``.\\n\\n        >>> def square(x):\\n        ...     return x ** 2\\n        >>> s.apply(square)\\n        London      400\\n        New York    441\\n        Helsinki    144\\n        dtype: int64\\n\\n        Square the values by passing an anonymous function as an\\n        argument to ``apply()``.\\n\\n        >>> s.apply(lambda x: x ** 2)\\n        London      400\\n        New York    441\\n        Helsinki    144\\n        dtype: int64\\n\\n        Define a custom function that needs additional positional\\n        arguments and pass these additional arguments using the\\n        ``args`` keyword.\\n\\n        >>> def subtract_custom_value(x, custom_value):\\n        ...     return x - custom_value\\n\\n        >>> s.apply(subtract_custom_value, args=(5,))\\n        London      15\\n        New York    16\\n        Helsinki     7\\n        dtype: int64\\n\\n        Define a custom function that takes keyword arguments\\n        and pass these arguments to ``apply``.\\n\\n        >>> def add_custom_values(x, **kwargs):\\n        ...     for month in kwargs:\\n        ...         x += kwargs[month]\\n        ...     return x\\n\\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\\n        London      95\\n        New York    96\\n        Helsinki    87\\n        dtype: int64\\n\\n        Use a function from the Numpy library.\\n\\n        >>> s.apply(np.log)\\n        London      2.995732\\n        New York    3.044522\\n        Helsinki    2.484907\\n        dtype: float64\",\n",
       "  'code': 'def apply(self, func, convert_dtype=True, args=(), **kwds):\\n        \"\"\"\\n        Invoke function on values of Series.\\n\\n        Can be ufunc (a NumPy function that applies to the entire Series)\\n        or a Python function that only works on single values.\\n\\n        Parameters\\n        ----------\\n        func : function\\n            Python function or NumPy ufunc to apply.\\n        convert_dtype : bool, default True\\n            Try to find better dtype for elementwise function results. If\\n            False, leave as dtype=object.\\n        args : tuple\\n            Positional arguments passed to func after the series value.\\n        **kwds\\n            Additional keyword arguments passed to func.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            If func returns a Series object the result will be a DataFrame.\\n\\n        See Also\\n        --------\\n        Series.map: For element-wise operations.\\n        Series.agg: Only perform aggregating type operations.\\n        Series.transform: Only perform transforming type operations.\\n\\n        Examples\\n        --------\\n        Create a series with typical summer temperatures for each city.\\n\\n        >>> s = pd.Series([20, 21, 12],\\n        ...               index=[\\'London\\', \\'New York\\', \\'Helsinki\\'])\\n        >>> s\\n        London      20\\n        New York    21\\n        Helsinki    12\\n        dtype: int64\\n\\n        Square the values by defining a function and passing it as an\\n        argument to ``apply()``.\\n\\n        >>> def square(x):\\n        ...     return x ** 2\\n        >>> s.apply(square)\\n        London      400\\n        New York    441\\n        Helsinki    144\\n        dtype: int64\\n\\n        Square the values by passing an anonymous function as an\\n        argument to ``apply()``.\\n\\n        >>> s.apply(lambda x: x ** 2)\\n        London      400\\n        New York    441\\n        Helsinki    144\\n        dtype: int64\\n\\n        Define a custom function that needs additional positional\\n        arguments and pass these additional arguments using the\\n        ``args`` keyword.\\n\\n        >>> def subtract_custom_value(x, custom_value):\\n        ...     return x - custom_value\\n\\n        >>> s.apply(subtract_custom_value, args=(5,))\\n        London      15\\n        New York    16\\n        Helsinki     7\\n        dtype: int64\\n\\n        Define a custom function that takes keyword arguments\\n        and pass these arguments to ``apply``.\\n\\n        >>> def add_custom_values(x, **kwargs):\\n        ...     for month in kwargs:\\n        ...         x += kwargs[month]\\n        ...     return x\\n\\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\\n        London      95\\n        New York    96\\n        Helsinki    87\\n        dtype: int64\\n\\n        Use a function from the Numpy library.\\n\\n        >>> s.apply(np.log)\\n        London      2.995732\\n        New York    3.044522\\n        Helsinki    2.484907\\n        dtype: float64\\n        \"\"\"\\n        if len(self) == 0:\\n            return self._constructor(dtype=self.dtype,\\n                                     index=self.index).__finalize__(self)\\n\\n        # dispatch to agg\\n        if isinstance(func, (list, dict)):\\n            return self.aggregate(func, *args, **kwds)\\n\\n        # if we are a string, try to dispatch\\n        if isinstance(func, str):\\n            return self._try_aggregate_string_function(func, *args, **kwds)\\n\\n        # handle ufuncs and lambdas\\n        if kwds or args and not isinstance(func, np.ufunc):\\n            def f(x):\\n                return func(x, *args, **kwds)\\n        else:\\n            f = func\\n\\n        with np.errstate(all=\\'ignore\\'):\\n            if isinstance(f, np.ufunc):\\n                return f(self)\\n\\n            # row-wise access\\n            if is_extension_type(self.dtype):\\n                mapped = self._values.map(f)\\n            else:\\n                values = self.astype(object).values\\n                mapped = lib.map_infer(values, f, convert=convert_dtype)\\n\\n        if len(mapped) and isinstance(mapped[0], Series):\\n            from pandas.core.frame import DataFrame\\n            return DataFrame(mapped.tolist(), index=self.index)\\n        else:\\n            return self._constructor(mapped,\\n                                     index=self.index).__finalize__(self)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series._reduce',\n",
       "  'docstring': 'Perform a reduction operation.\\n\\n        If we have an ndarray as a value, then simply perform the operation,\\n        otherwise delegate to the object.',\n",
       "  'code': 'def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\\n                filter_type=None, **kwds):\\n        \"\"\"\\n        Perform a reduction operation.\\n\\n        If we have an ndarray as a value, then simply perform the operation,\\n        otherwise delegate to the object.\\n        \"\"\"\\n        delegate = self._values\\n\\n        if axis is not None:\\n            self._get_axis_number(axis)\\n\\n        if isinstance(delegate, Categorical):\\n            # TODO deprecate numeric_only argument for Categorical and use\\n            # skipna as well, see GH25303\\n            return delegate._reduce(name, numeric_only=numeric_only, **kwds)\\n        elif isinstance(delegate, ExtensionArray):\\n            # dispatch to ExtensionArray interface\\n            return delegate._reduce(name, skipna=skipna, **kwds)\\n        elif is_datetime64_dtype(delegate):\\n            # use DatetimeIndex implementation to handle skipna correctly\\n            delegate = DatetimeIndex(delegate)\\n\\n        # dispatch to numpy arrays\\n        elif isinstance(delegate, np.ndarray):\\n            if numeric_only:\\n                raise NotImplementedError(\\'Series.{0} does not implement \\'\\n                                          \\'numeric_only.\\'.format(name))\\n            with np.errstate(all=\\'ignore\\'):\\n                return op(delegate, skipna=skipna, **kwds)\\n\\n        # TODO(EA) dispatch to Index\\n        # remove once all internals extension types are\\n        # moved to ExtensionArrays\\n        return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,\\n                                numeric_only=numeric_only,\\n                                filter_type=filter_type, **kwds)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Series.memory_usage',\n",
       "  'docstring': 'Return the memory usage of the Series.\\n\\n        The memory usage can optionally include the contribution of\\n        the index and of elements of `object` dtype.\\n\\n        Parameters\\n        ----------\\n        index : bool, default True\\n            Specifies whether to include the memory usage of the Series index.\\n        deep : bool, default False\\n            If True, introspect the data deeply by interrogating\\n            `object` dtypes for system-level memory consumption, and include\\n            it in the returned value.\\n\\n        Returns\\n        -------\\n        int\\n            Bytes of memory consumed.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\\n            array.\\n        DataFrame.memory_usage : Bytes consumed by a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series(range(3))\\n        >>> s.memory_usage()\\n        104\\n\\n        Not including the index gives the size of the rest of the data, which\\n        is necessarily smaller:\\n\\n        >>> s.memory_usage(index=False)\\n        24\\n\\n        The memory footprint of `object` values is ignored by default:\\n\\n        >>> s = pd.Series([\"a\", \"b\"])\\n        >>> s.values\\n        array([\\'a\\', \\'b\\'], dtype=object)\\n        >>> s.memory_usage()\\n        96\\n        >>> s.memory_usage(deep=True)\\n        212',\n",
       "  'code': 'def memory_usage(self, index=True, deep=False):\\n        \"\"\"\\n        Return the memory usage of the Series.\\n\\n        The memory usage can optionally include the contribution of\\n        the index and of elements of `object` dtype.\\n\\n        Parameters\\n        ----------\\n        index : bool, default True\\n            Specifies whether to include the memory usage of the Series index.\\n        deep : bool, default False\\n            If True, introspect the data deeply by interrogating\\n            `object` dtypes for system-level memory consumption, and include\\n            it in the returned value.\\n\\n        Returns\\n        -------\\n        int\\n            Bytes of memory consumed.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\\n            array.\\n        DataFrame.memory_usage : Bytes consumed by a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series(range(3))\\n        >>> s.memory_usage()\\n        104\\n\\n        Not including the index gives the size of the rest of the data, which\\n        is necessarily smaller:\\n\\n        >>> s.memory_usage(index=False)\\n        24\\n\\n        The memory footprint of `object` values is ignored by default:\\n\\n        >>> s = pd.Series([\"a\", \"b\"])\\n        >>> s.values\\n        array([\\'a\\', \\'b\\'], dtype=object)\\n        >>> s.memory_usage()\\n        96\\n        >>> s.memory_usage(deep=True)\\n        212\\n        \"\"\"\\n        v = super().memory_usage(deep=deep)\\n        if index:\\n            v += self.index.memory_usage(deep=deep)\\n        return v',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'to_numeric',\n",
       "  'docstring': \"Convert argument to a numeric type.\\n\\n    The default return dtype is `float64` or `int64`\\n    depending on the data supplied. Use the `downcast` parameter\\n    to obtain other dtypes.\\n\\n    Please note that precision loss may occur if really large numbers\\n    are passed in. Due to the internal limitations of `ndarray`, if\\n    numbers smaller than `-9223372036854775808` (np.iinfo(np.int64).min)\\n    or larger than `18446744073709551615` (np.iinfo(np.uint64).max) are\\n    passed in, it is very likely they will be converted to float so that\\n    they can stored in an `ndarray`. These warnings apply similarly to\\n    `Series` since it internally leverages `ndarray`.\\n\\n    Parameters\\n    ----------\\n    arg : scalar, list, tuple, 1-d array, or Series\\n    errors : {'ignore', 'raise', 'coerce'}, default 'raise'\\n        - If 'raise', then invalid parsing will raise an exception\\n        - If 'coerce', then invalid parsing will be set as NaN\\n        - If 'ignore', then invalid parsing will return the input\\n    downcast : {'integer', 'signed', 'unsigned', 'float'} , default None\\n        If not None, and if the data has been successfully cast to a\\n        numerical dtype (or if the data was numeric to begin with),\\n        downcast that resulting data to the smallest numerical dtype\\n        possible according to the following rules:\\n\\n        - 'integer' or 'signed': smallest signed int dtype (min.: np.int8)\\n        - 'unsigned': smallest unsigned int dtype (min.: np.uint8)\\n        - 'float': smallest float dtype (min.: np.float32)\\n\\n        As this behaviour is separate from the core conversion to\\n        numeric values, any errors raised during the downcasting\\n        will be surfaced regardless of the value of the 'errors' input.\\n\\n        In addition, downcasting will only occur if the size\\n        of the resulting data's dtype is strictly larger than\\n        the dtype it is to be cast to, so if none of the dtypes\\n        checked satisfy that specification, no downcasting will be\\n        performed on the data.\\n\\n        .. versionadded:: 0.19.0\\n\\n    Returns\\n    -------\\n    ret : numeric if parsing succeeded.\\n        Return type depends on input.  Series if Series, otherwise ndarray.\\n\\n    See Also\\n    --------\\n    DataFrame.astype : Cast argument to a specified dtype.\\n    to_datetime : Convert argument to datetime.\\n    to_timedelta : Convert argument to timedelta.\\n    numpy.ndarray.astype : Cast a numpy array to a specified type.\\n\\n    Examples\\n    --------\\n    Take separate series and convert to numeric, coercing when told to\\n\\n    >>> s = pd.Series(['1.0', '2', -3])\\n    >>> pd.to_numeric(s)\\n    0    1.0\\n    1    2.0\\n    2   -3.0\\n    dtype: float64\\n    >>> pd.to_numeric(s, downcast='float')\\n    0    1.0\\n    1    2.0\\n    2   -3.0\\n    dtype: float32\\n    >>> pd.to_numeric(s, downcast='signed')\\n    0    1\\n    1    2\\n    2   -3\\n    dtype: int8\\n    >>> s = pd.Series(['apple', '1.0', '2', -3])\\n    >>> pd.to_numeric(s, errors='ignore')\\n    0    apple\\n    1      1.0\\n    2        2\\n    3       -3\\n    dtype: object\\n    >>> pd.to_numeric(s, errors='coerce')\\n    0    NaN\\n    1    1.0\\n    2    2.0\\n    3   -3.0\\n    dtype: float64\",\n",
       "  'code': 'def to_numeric(arg, errors=\\'raise\\', downcast=None):\\n    \"\"\"\\n    Convert argument to a numeric type.\\n\\n    The default return dtype is `float64` or `int64`\\n    depending on the data supplied. Use the `downcast` parameter\\n    to obtain other dtypes.\\n\\n    Please note that precision loss may occur if really large numbers\\n    are passed in. Due to the internal limitations of `ndarray`, if\\n    numbers smaller than `-9223372036854775808` (np.iinfo(np.int64).min)\\n    or larger than `18446744073709551615` (np.iinfo(np.uint64).max) are\\n    passed in, it is very likely they will be converted to float so that\\n    they can stored in an `ndarray`. These warnings apply similarly to\\n    `Series` since it internally leverages `ndarray`.\\n\\n    Parameters\\n    ----------\\n    arg : scalar, list, tuple, 1-d array, or Series\\n    errors : {\\'ignore\\', \\'raise\\', \\'coerce\\'}, default \\'raise\\'\\n        - If \\'raise\\', then invalid parsing will raise an exception\\n        - If \\'coerce\\', then invalid parsing will be set as NaN\\n        - If \\'ignore\\', then invalid parsing will return the input\\n    downcast : {\\'integer\\', \\'signed\\', \\'unsigned\\', \\'float\\'} , default None\\n        If not None, and if the data has been successfully cast to a\\n        numerical dtype (or if the data was numeric to begin with),\\n        downcast that resulting data to the smallest numerical dtype\\n        possible according to the following rules:\\n\\n        - \\'integer\\' or \\'signed\\': smallest signed int dtype (min.: np.int8)\\n        - \\'unsigned\\': smallest unsigned int dtype (min.: np.uint8)\\n        - \\'float\\': smallest float dtype (min.: np.float32)\\n\\n        As this behaviour is separate from the core conversion to\\n        numeric values, any errors raised during the downcasting\\n        will be surfaced regardless of the value of the \\'errors\\' input.\\n\\n        In addition, downcasting will only occur if the size\\n        of the resulting data\\'s dtype is strictly larger than\\n        the dtype it is to be cast to, so if none of the dtypes\\n        checked satisfy that specification, no downcasting will be\\n        performed on the data.\\n\\n        .. versionadded:: 0.19.0\\n\\n    Returns\\n    -------\\n    ret : numeric if parsing succeeded.\\n        Return type depends on input.  Series if Series, otherwise ndarray.\\n\\n    See Also\\n    --------\\n    DataFrame.astype : Cast argument to a specified dtype.\\n    to_datetime : Convert argument to datetime.\\n    to_timedelta : Convert argument to timedelta.\\n    numpy.ndarray.astype : Cast a numpy array to a specified type.\\n\\n    Examples\\n    --------\\n    Take separate series and convert to numeric, coercing when told to\\n\\n    >>> s = pd.Series([\\'1.0\\', \\'2\\', -3])\\n    >>> pd.to_numeric(s)\\n    0    1.0\\n    1    2.0\\n    2   -3.0\\n    dtype: float64\\n    >>> pd.to_numeric(s, downcast=\\'float\\')\\n    0    1.0\\n    1    2.0\\n    2   -3.0\\n    dtype: float32\\n    >>> pd.to_numeric(s, downcast=\\'signed\\')\\n    0    1\\n    1    2\\n    2   -3\\n    dtype: int8\\n    >>> s = pd.Series([\\'apple\\', \\'1.0\\', \\'2\\', -3])\\n    >>> pd.to_numeric(s, errors=\\'ignore\\')\\n    0    apple\\n    1      1.0\\n    2        2\\n    3       -3\\n    dtype: object\\n    >>> pd.to_numeric(s, errors=\\'coerce\\')\\n    0    NaN\\n    1    1.0\\n    2    2.0\\n    3   -3.0\\n    dtype: float64\\n    \"\"\"\\n    if downcast not in (None, \\'integer\\', \\'signed\\', \\'unsigned\\', \\'float\\'):\\n        raise ValueError(\\'invalid downcasting method provided\\')\\n\\n    is_series = False\\n    is_index = False\\n    is_scalars = False\\n\\n    if isinstance(arg, ABCSeries):\\n        is_series = True\\n        values = arg.values\\n    elif isinstance(arg, ABCIndexClass):\\n        is_index = True\\n        values = arg.asi8\\n        if values is None:\\n            values = arg.values\\n    elif isinstance(arg, (list, tuple)):\\n        values = np.array(arg, dtype=\\'O\\')\\n    elif is_scalar(arg):\\n        if is_decimal(arg):\\n            return float(arg)\\n        if is_number(arg):\\n            return arg\\n        is_scalars = True\\n        values = np.array([arg], dtype=\\'O\\')\\n    elif getattr(arg, \\'ndim\\', 1) > 1:\\n        raise TypeError(\\'arg must be a list, tuple, 1-d array, or Series\\')\\n    else:\\n        values = arg\\n\\n    try:\\n        if is_numeric_dtype(values):\\n            pass\\n        elif is_datetime_or_timedelta_dtype(values):\\n            values = values.astype(np.int64)\\n        else:\\n            values = ensure_object(values)\\n            coerce_numeric = errors not in (\\'ignore\\', \\'raise\\')\\n            values = lib.maybe_convert_numeric(values, set(),\\n                                               coerce_numeric=coerce_numeric)\\n\\n    except Exception:\\n        if errors == \\'raise\\':\\n            raise\\n\\n    # attempt downcast only if the data has been successfully converted\\n    # to a numerical dtype and if a downcast method has been specified\\n    if downcast is not None and is_numeric_dtype(values):\\n        typecodes = None\\n\\n        if downcast in (\\'integer\\', \\'signed\\'):\\n            typecodes = np.typecodes[\\'Integer\\']\\n        elif downcast == \\'unsigned\\' and np.min(values) >= 0:\\n            typecodes = np.typecodes[\\'UnsignedInteger\\']\\n        elif downcast == \\'float\\':\\n            typecodes = np.typecodes[\\'Float\\']\\n\\n            # pandas support goes only to np.float32,\\n            # as float dtypes smaller than that are\\n            # extremely rare and not well supported\\n            float_32_char = np.dtype(np.float32).char\\n            float_32_ind = typecodes.index(float_32_char)\\n            typecodes = typecodes[float_32_ind:]\\n\\n        if typecodes is not None:\\n            # from smallest to largest\\n            for dtype in typecodes:\\n                if np.dtype(dtype).itemsize <= values.dtype.itemsize:\\n                    values = maybe_downcast_to_dtype(values, dtype)\\n\\n                    # successful conversion\\n                    if values.dtype == dtype:\\n                        break\\n\\n    if is_series:\\n        return pd.Series(values, index=arg.index, name=arg.name)\\n    elif is_index:\\n        # because we want to coerce to numeric if possible,\\n        # do not use _shallow_copy_with_infer\\n        return pd.Index(values, name=arg.name)\\n    elif is_scalars:\\n        return values[0]\\n    else:\\n        return values',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'make_sparse',\n",
       "  'docstring': \"Convert ndarray to sparse format\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n    kind : {'block', 'integer'}\\n    fill_value : NaN or another value\\n    dtype : np.dtype, optional\\n    copy : bool, default False\\n\\n    Returns\\n    -------\\n    (sparse_values, index, fill_value) : (ndarray, SparseIndex, Scalar)\",\n",
       "  'code': 'def make_sparse(arr, kind=\\'block\\', fill_value=None, dtype=None, copy=False):\\n    \"\"\"\\n    Convert ndarray to sparse format\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n    kind : {\\'block\\', \\'integer\\'}\\n    fill_value : NaN or another value\\n    dtype : np.dtype, optional\\n    copy : bool, default False\\n\\n    Returns\\n    -------\\n    (sparse_values, index, fill_value) : (ndarray, SparseIndex, Scalar)\\n    \"\"\"\\n\\n    arr = _sanitize_values(arr)\\n\\n    if arr.ndim > 1:\\n        raise TypeError(\"expected dimension <= 1 data\")\\n\\n    if fill_value is None:\\n        fill_value = na_value_for_dtype(arr.dtype)\\n\\n    if isna(fill_value):\\n        mask = notna(arr)\\n    else:\\n        # cast to object comparison to be safe\\n        if is_string_dtype(arr):\\n            arr = arr.astype(object)\\n\\n        if is_object_dtype(arr.dtype):\\n            # element-wise equality check method in numpy doesn\\'t treat\\n            # each element type, eg. 0, 0.0, and False are treated as\\n            # same. So we have to check the both of its type and value.\\n            mask = splib.make_mask_object_ndarray(arr, fill_value)\\n        else:\\n            mask = arr != fill_value\\n\\n    length = len(arr)\\n    if length != len(mask):\\n        # the arr is a SparseArray\\n        indices = mask.sp_index.indices\\n    else:\\n        indices = mask.nonzero()[0].astype(np.int32)\\n\\n    index = _make_index(length, indices, kind)\\n    sparsified_values = arr[mask]\\n    if dtype is not None:\\n        sparsified_values = astype_nansafe(sparsified_values, dtype=dtype)\\n    # TODO: copy\\n    return sparsified_values, index, fill_value',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SparseArray.astype',\n",
       "  'docstring': 'Change the dtype of a SparseArray.\\n\\n        The output will always be a SparseArray. To convert to a dense\\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\\n\\n        Parameters\\n        ----------\\n        dtype : np.dtype or ExtensionDtype\\n            For SparseDtype, this changes the dtype of\\n            ``self.sp_values`` and the ``self.fill_value``.\\n\\n            For other dtypes, this only changes the dtype of\\n            ``self.sp_values``.\\n\\n        copy : bool, default True\\n            Whether to ensure a copy is made, even if not necessary.\\n\\n        Returns\\n        -------\\n        SparseArray\\n\\n        Examples\\n        --------\\n        >>> arr = SparseArray([0, 0, 1, 2])\\n        >>> arr\\n        [0, 0, 1, 2]\\n        Fill: 0\\n        IntIndex\\n        Indices: array([2, 3], dtype=int32)\\n\\n        >>> arr.astype(np.dtype(\\'int32\\'))\\n        [0, 0, 1, 2]\\n        Fill: 0\\n        IntIndex\\n        Indices: array([2, 3], dtype=int32)\\n\\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\\n        just ``self.sp_values``.\\n\\n        >>> arr.astype(np.dtype(\\'float64\\'))\\n        ... # doctest: +NORMALIZE_WHITESPACE\\n        [0, 0, 1.0, 2.0]\\n        Fill: 0\\n        IntIndex\\n        Indices: array([2, 3], dtype=int32)\\n\\n        Use a SparseDtype if you wish to be change the fill value as well.\\n\\n        >>> arr.astype(SparseDtype(\"float64\", fill_value=np.nan))\\n        ... # doctest: +NORMALIZE_WHITESPACE\\n        [nan, nan, 1.0, 2.0]\\n        Fill: nan\\n        IntIndex\\n        Indices: array([2, 3], dtype=int32)',\n",
       "  'code': 'def astype(self, dtype=None, copy=True):\\n        \"\"\"\\n        Change the dtype of a SparseArray.\\n\\n        The output will always be a SparseArray. To convert to a dense\\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\\n\\n        Parameters\\n        ----------\\n        dtype : np.dtype or ExtensionDtype\\n            For SparseDtype, this changes the dtype of\\n            ``self.sp_values`` and the ``self.fill_value``.\\n\\n            For other dtypes, this only changes the dtype of\\n            ``self.sp_values``.\\n\\n        copy : bool, default True\\n            Whether to ensure a copy is made, even if not necessary.\\n\\n        Returns\\n        -------\\n        SparseArray\\n\\n        Examples\\n        --------\\n        >>> arr = SparseArray([0, 0, 1, 2])\\n        >>> arr\\n        [0, 0, 1, 2]\\n        Fill: 0\\n        IntIndex\\n        Indices: array([2, 3], dtype=int32)\\n\\n        >>> arr.astype(np.dtype(\\'int32\\'))\\n        [0, 0, 1, 2]\\n        Fill: 0\\n        IntIndex\\n        Indices: array([2, 3], dtype=int32)\\n\\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\\n        just ``self.sp_values``.\\n\\n        >>> arr.astype(np.dtype(\\'float64\\'))\\n        ... # doctest: +NORMALIZE_WHITESPACE\\n        [0, 0, 1.0, 2.0]\\n        Fill: 0\\n        IntIndex\\n        Indices: array([2, 3], dtype=int32)\\n\\n        Use a SparseDtype if you wish to be change the fill value as well.\\n\\n        >>> arr.astype(SparseDtype(\"float64\", fill_value=np.nan))\\n        ... # doctest: +NORMALIZE_WHITESPACE\\n        [nan, nan, 1.0, 2.0]\\n        Fill: nan\\n        IntIndex\\n        Indices: array([2, 3], dtype=int32)\\n        \"\"\"\\n        dtype = self.dtype.update_dtype(dtype)\\n        subtype = dtype._subtype_with_str\\n        sp_values = astype_nansafe(self.sp_values,\\n                                   subtype,\\n                                   copy=copy)\\n        if sp_values is self.sp_values and copy:\\n            sp_values = sp_values.copy()\\n\\n        return self._simple_new(sp_values,\\n                                self.sp_index,\\n                                dtype)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SparseArray.all',\n",
       "  'docstring': 'Tests whether all elements evaluate True\\n\\n        Returns\\n        -------\\n        all : bool\\n\\n        See Also\\n        --------\\n        numpy.all',\n",
       "  'code': 'def all(self, axis=None, *args, **kwargs):\\n        \"\"\"\\n        Tests whether all elements evaluate True\\n\\n        Returns\\n        -------\\n        all : bool\\n\\n        See Also\\n        --------\\n        numpy.all\\n        \"\"\"\\n        nv.validate_all(args, kwargs)\\n\\n        values = self.sp_values\\n\\n        if len(values) != len(self) and not np.all(self.fill_value):\\n            return False\\n\\n        return values.all()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SparseArray.any',\n",
       "  'docstring': 'Tests whether at least one of elements evaluate True\\n\\n        Returns\\n        -------\\n        any : bool\\n\\n        See Also\\n        --------\\n        numpy.any',\n",
       "  'code': 'def any(self, axis=0, *args, **kwargs):\\n        \"\"\"\\n        Tests whether at least one of elements evaluate True\\n\\n        Returns\\n        -------\\n        any : bool\\n\\n        See Also\\n        --------\\n        numpy.any\\n        \"\"\"\\n        nv.validate_any(args, kwargs)\\n\\n        values = self.sp_values\\n\\n        if len(values) != len(self) and np.any(self.fill_value):\\n            return True\\n\\n        return values.any().item()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'FrozenNDArray.searchsorted',\n",
       "  'docstring': 'Find indices to insert `value` so as to maintain order.\\n\\n        For full documentation, see `numpy.searchsorted`\\n\\n        See Also\\n        --------\\n        numpy.searchsorted : Equivalent function.',\n",
       "  'code': 'def searchsorted(self, value, side=\"left\", sorter=None):\\n        \"\"\"\\n        Find indices to insert `value` so as to maintain order.\\n\\n        For full documentation, see `numpy.searchsorted`\\n\\n        See Also\\n        --------\\n        numpy.searchsorted : Equivalent function.\\n        \"\"\"\\n\\n        # We are much more performant if the searched\\n        # indexer is the same type as the array.\\n        #\\n        # This doesn\\'t matter for int64, but DOES\\n        # matter for smaller int dtypes.\\n        #\\n        # xref: https://github.com/numpy/numpy/issues/5370\\n        try:\\n            value = self.dtype.type(value)\\n        except ValueError:\\n            pass\\n\\n        return super().searchsorted(value, side=side, sorter=sorter)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'sanitize_array',\n",
       "  'docstring': 'Sanitize input data to an ndarray, copy if specified, coerce to the\\n    dtype if specified.',\n",
       "  'code': 'def sanitize_array(data, index, dtype=None, copy=False,\\n                   raise_cast_failure=False):\\n    \"\"\"\\n    Sanitize input data to an ndarray, copy if specified, coerce to the\\n    dtype if specified.\\n    \"\"\"\\n    if dtype is not None:\\n        dtype = pandas_dtype(dtype)\\n\\n    if isinstance(data, ma.MaskedArray):\\n        mask = ma.getmaskarray(data)\\n        if mask.any():\\n            data, fill_value = maybe_upcast(data, copy=True)\\n            data.soften_mask()  # set hardmask False if it was True\\n            data[mask] = fill_value\\n        else:\\n            data = data.copy()\\n\\n    data = extract_array(data, extract_numpy=True)\\n\\n    # GH#846\\n    if isinstance(data, np.ndarray):\\n\\n        if dtype is not None:\\n            subarr = np.array(data, copy=False)\\n\\n            # possibility of nan -> garbage\\n            if is_float_dtype(data.dtype) and is_integer_dtype(dtype):\\n                try:\\n                    subarr = _try_cast(data, True, dtype, copy,\\n                                       True)\\n                except ValueError:\\n                    if copy:\\n                        subarr = data.copy()\\n            else:\\n                subarr = _try_cast(data, True, dtype, copy, raise_cast_failure)\\n        elif isinstance(data, Index):\\n            # don\\'t coerce Index types\\n            # e.g. indexes can have different conversions (so don\\'t fast path\\n            # them)\\n            # GH#6140\\n            subarr = sanitize_index(data, index, copy=copy)\\n        else:\\n\\n            # we will try to copy be-definition here\\n            subarr = _try_cast(data, True, dtype, copy, raise_cast_failure)\\n\\n    elif isinstance(data, ExtensionArray):\\n        if isinstance(data, ABCPandasArray):\\n            # We don\\'t want to let people put our PandasArray wrapper\\n            # (the output of Series/Index.array), into a Series. So\\n            # we explicitly unwrap it here.\\n            subarr = data.to_numpy()\\n        else:\\n            subarr = data\\n\\n        # everything else in this block must also handle ndarray\\'s,\\n        # becuase we\\'ve unwrapped PandasArray into an ndarray.\\n\\n        if dtype is not None:\\n            subarr = data.astype(dtype)\\n\\n        if copy:\\n            subarr = data.copy()\\n        return subarr\\n\\n    elif isinstance(data, (list, tuple)) and len(data) > 0:\\n        if dtype is not None:\\n            try:\\n                subarr = _try_cast(data, False, dtype, copy,\\n                                   raise_cast_failure)\\n            except Exception:\\n                if raise_cast_failure:  # pragma: no cover\\n                    raise\\n                subarr = np.array(data, dtype=object, copy=copy)\\n                subarr = lib.maybe_convert_objects(subarr)\\n\\n        else:\\n            subarr = maybe_convert_platform(data)\\n\\n        subarr = maybe_cast_to_datetime(subarr, dtype)\\n\\n    elif isinstance(data, range):\\n        # GH#16804\\n        arr = np.arange(data.start, data.stop, data.step, dtype=\\'int64\\')\\n        subarr = _try_cast(arr, False, dtype, copy, raise_cast_failure)\\n    else:\\n        subarr = _try_cast(data, False, dtype, copy, raise_cast_failure)\\n\\n    # scalar like, GH\\n    if getattr(subarr, \\'ndim\\', 0) == 0:\\n        if isinstance(data, list):  # pragma: no cover\\n            subarr = np.array(data, dtype=object)\\n        elif index is not None:\\n            value = data\\n\\n            # figure out the dtype from the value (upcast if necessary)\\n            if dtype is None:\\n                dtype, value = infer_dtype_from_scalar(value)\\n            else:\\n                # need to possibly convert the value here\\n                value = maybe_cast_to_datetime(value, dtype)\\n\\n            subarr = construct_1d_arraylike_from_scalar(\\n                value, len(index), dtype)\\n\\n        else:\\n            return subarr.item()\\n\\n    # the result that we want\\n    elif subarr.ndim == 1:\\n        if index is not None:\\n\\n            # a 1-element ndarray\\n            if len(subarr) != len(index) and len(subarr) == 1:\\n                subarr = construct_1d_arraylike_from_scalar(\\n                    subarr[0], len(index), subarr.dtype)\\n\\n    elif subarr.ndim > 1:\\n        if isinstance(data, np.ndarray):\\n            raise Exception(\\'Data must be 1-dimensional\\')\\n        else:\\n            subarr = com.asarray_tuplesafe(data, dtype=dtype)\\n\\n    # This is to prevent mixed-type Series getting all casted to\\n    # NumPy string type, e.g. NaN --> \\'-1#IND\\'.\\n    if issubclass(subarr.dtype.type, str):\\n        # GH#16605\\n        # If not empty convert the data to dtype\\n        # GH#19853: If data is a scalar, subarr has already the result\\n        if not lib.is_scalar(data):\\n            if not np.all(isna(data)):\\n                data = np.array(data, dtype=dtype, copy=False)\\n            subarr = np.array(data, dtype=object, copy=copy)\\n\\n    if is_object_dtype(subarr.dtype) and dtype != \\'object\\':\\n        inferred = lib.infer_dtype(subarr, skipna=False)\\n        if inferred == \\'period\\':\\n            try:\\n                subarr = period_array(subarr)\\n            except IncompatibleFrequency:\\n                pass\\n\\n    return subarr',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'eval',\n",
       "  'docstring': \"Evaluate a Python expression as a string using various backends.\\n\\n    The following arithmetic operations are supported: ``+``, ``-``, ``*``,\\n    ``/``, ``**``, ``%``, ``//`` (python engine only) along with the following\\n    boolean operations: ``|`` (or), ``&`` (and), and ``~`` (not).\\n    Additionally, the ``'pandas'`` parser allows the use of :keyword:`and`,\\n    :keyword:`or`, and :keyword:`not` with the same semantics as the\\n    corresponding bitwise operators.  :class:`~pandas.Series` and\\n    :class:`~pandas.DataFrame` objects are supported and behave as they would\\n    with plain ol' Python evaluation.\\n\\n    Parameters\\n    ----------\\n    expr : str or unicode\\n        The expression to evaluate. This string cannot contain any Python\\n        `statements\\n        <https://docs.python.org/3/reference/simple_stmts.html#simple-statements>`__,\\n        only Python `expressions\\n        <https://docs.python.org/3/reference/simple_stmts.html#expression-statements>`__.\\n    parser : string, default 'pandas', {'pandas', 'python'}\\n        The parser to use to construct the syntax tree from the expression. The\\n        default of ``'pandas'`` parses code slightly different than standard\\n        Python. Alternatively, you can parse an expression using the\\n        ``'python'`` parser to retain strict Python semantics.  See the\\n        :ref:`enhancing performance <enhancingperf.eval>` documentation for\\n        more details.\\n    engine : string or None, default 'numexpr', {'python', 'numexpr'}\\n\\n        The engine used to evaluate the expression. Supported engines are\\n\\n        - None         : tries to use ``numexpr``, falls back to ``python``\\n        - ``'numexpr'``: This default engine evaluates pandas objects using\\n                         numexpr for large speed ups in complex expressions\\n                         with large frames.\\n        - ``'python'``: Performs operations as if you had ``eval``'d in top\\n                        level python. This engine is generally not that useful.\\n\\n        More backends may be available in the future.\\n\\n    truediv : bool, optional\\n        Whether to use true division, like in Python >= 3\\n    local_dict : dict or None, optional\\n        A dictionary of local variables, taken from locals() by default.\\n    global_dict : dict or None, optional\\n        A dictionary of global variables, taken from globals() by default.\\n    resolvers : list of dict-like or None, optional\\n        A list of objects implementing the ``__getitem__`` special method that\\n        you can use to inject an additional collection of namespaces to use for\\n        variable lookup. For example, this is used in the\\n        :meth:`~DataFrame.query` method to inject the\\n        ``DataFrame.index`` and ``DataFrame.columns``\\n        variables that refer to their respective :class:`~pandas.DataFrame`\\n        instance attributes.\\n    level : int, optional\\n        The number of prior stack frames to traverse and add to the current\\n        scope. Most users will **not** need to change this parameter.\\n    target : object, optional, default None\\n        This is the target object for assignment. It is used when there is\\n        variable assignment in the expression. If so, then `target` must\\n        support item assignment with string keys, and if a copy is being\\n        returned, it must also support `.copy()`.\\n    inplace : bool, default False\\n        If `target` is provided, and the expression mutates `target`, whether\\n        to modify `target` inplace. Otherwise, return a copy of `target` with\\n        the mutation.\\n\\n    Returns\\n    -------\\n    ndarray, numeric scalar, DataFrame, Series\\n\\n    Raises\\n    ------\\n    ValueError\\n        There are many instances where such an error can be raised:\\n\\n        - `target=None`, but the expression is multiline.\\n        - The expression is multiline, but not all them have item assignment.\\n          An example of such an arrangement is this:\\n\\n          a = b + 1\\n          a + 2\\n\\n          Here, there are expressions on different lines, making it multiline,\\n          but the last line has no variable assigned to the output of `a + 2`.\\n        - `inplace=True`, but the expression is missing item assignment.\\n        - Item assignment is provided, but the `target` does not support\\n          string item assignment.\\n        - Item assignment is provided and `inplace=False`, but the `target`\\n          does not support the `.copy()` method\\n\\n    See Also\\n    --------\\n    DataFrame.query\\n    DataFrame.eval\\n\\n    Notes\\n    -----\\n    The ``dtype`` of any objects involved in an arithmetic ``%`` operation are\\n    recursively cast to ``float64``.\\n\\n    See the :ref:`enhancing performance <enhancingperf.eval>` documentation for\\n    more details.\",\n",
       "  'code': 'def eval(expr, parser=\\'pandas\\', engine=None, truediv=True,\\n         local_dict=None, global_dict=None, resolvers=(), level=0,\\n         target=None, inplace=False):\\n    \"\"\"Evaluate a Python expression as a string using various backends.\\n\\n    The following arithmetic operations are supported: ``+``, ``-``, ``*``,\\n    ``/``, ``**``, ``%``, ``//`` (python engine only) along with the following\\n    boolean operations: ``|`` (or), ``&`` (and), and ``~`` (not).\\n    Additionally, the ``\\'pandas\\'`` parser allows the use of :keyword:`and`,\\n    :keyword:`or`, and :keyword:`not` with the same semantics as the\\n    corresponding bitwise operators.  :class:`~pandas.Series` and\\n    :class:`~pandas.DataFrame` objects are supported and behave as they would\\n    with plain ol\\' Python evaluation.\\n\\n    Parameters\\n    ----------\\n    expr : str or unicode\\n        The expression to evaluate. This string cannot contain any Python\\n        `statements\\n        <https://docs.python.org/3/reference/simple_stmts.html#simple-statements>`__,\\n        only Python `expressions\\n        <https://docs.python.org/3/reference/simple_stmts.html#expression-statements>`__.\\n    parser : string, default \\'pandas\\', {\\'pandas\\', \\'python\\'}\\n        The parser to use to construct the syntax tree from the expression. The\\n        default of ``\\'pandas\\'`` parses code slightly different than standard\\n        Python. Alternatively, you can parse an expression using the\\n        ``\\'python\\'`` parser to retain strict Python semantics.  See the\\n        :ref:`enhancing performance <enhancingperf.eval>` documentation for\\n        more details.\\n    engine : string or None, default \\'numexpr\\', {\\'python\\', \\'numexpr\\'}\\n\\n        The engine used to evaluate the expression. Supported engines are\\n\\n        - None         : tries to use ``numexpr``, falls back to ``python``\\n        - ``\\'numexpr\\'``: This default engine evaluates pandas objects using\\n                         numexpr for large speed ups in complex expressions\\n                         with large frames.\\n        - ``\\'python\\'``: Performs operations as if you had ``eval``\\'d in top\\n                        level python. This engine is generally not that useful.\\n\\n        More backends may be available in the future.\\n\\n    truediv : bool, optional\\n        Whether to use true division, like in Python >= 3\\n    local_dict : dict or None, optional\\n        A dictionary of local variables, taken from locals() by default.\\n    global_dict : dict or None, optional\\n        A dictionary of global variables, taken from globals() by default.\\n    resolvers : list of dict-like or None, optional\\n        A list of objects implementing the ``__getitem__`` special method that\\n        you can use to inject an additional collection of namespaces to use for\\n        variable lookup. For example, this is used in the\\n        :meth:`~DataFrame.query` method to inject the\\n        ``DataFrame.index`` and ``DataFrame.columns``\\n        variables that refer to their respective :class:`~pandas.DataFrame`\\n        instance attributes.\\n    level : int, optional\\n        The number of prior stack frames to traverse and add to the current\\n        scope. Most users will **not** need to change this parameter.\\n    target : object, optional, default None\\n        This is the target object for assignment. It is used when there is\\n        variable assignment in the expression. If so, then `target` must\\n        support item assignment with string keys, and if a copy is being\\n        returned, it must also support `.copy()`.\\n    inplace : bool, default False\\n        If `target` is provided, and the expression mutates `target`, whether\\n        to modify `target` inplace. Otherwise, return a copy of `target` with\\n        the mutation.\\n\\n    Returns\\n    -------\\n    ndarray, numeric scalar, DataFrame, Series\\n\\n    Raises\\n    ------\\n    ValueError\\n        There are many instances where such an error can be raised:\\n\\n        - `target=None`, but the expression is multiline.\\n        - The expression is multiline, but not all them have item assignment.\\n          An example of such an arrangement is this:\\n\\n          a = b + 1\\n          a + 2\\n\\n          Here, there are expressions on different lines, making it multiline,\\n          but the last line has no variable assigned to the output of `a + 2`.\\n        - `inplace=True`, but the expression is missing item assignment.\\n        - Item assignment is provided, but the `target` does not support\\n          string item assignment.\\n        - Item assignment is provided and `inplace=False`, but the `target`\\n          does not support the `.copy()` method\\n\\n    See Also\\n    --------\\n    DataFrame.query\\n    DataFrame.eval\\n\\n    Notes\\n    -----\\n    The ``dtype`` of any objects involved in an arithmetic ``%`` operation are\\n    recursively cast to ``float64``.\\n\\n    See the :ref:`enhancing performance <enhancingperf.eval>` documentation for\\n    more details.\\n    \"\"\"\\n    from pandas.core.computation.expr import Expr\\n\\n    inplace = validate_bool_kwarg(inplace, \"inplace\")\\n\\n    if isinstance(expr, str):\\n        _check_expression(expr)\\n        exprs = [e.strip() for e in expr.splitlines() if e.strip() != \\'\\']\\n    else:\\n        exprs = [expr]\\n    multi_line = len(exprs) > 1\\n\\n    if multi_line and target is None:\\n        raise ValueError(\"multi-line expressions are only valid in the \"\\n                         \"context of data, use DataFrame.eval\")\\n\\n    ret = None\\n    first_expr = True\\n    target_modified = False\\n\\n    for expr in exprs:\\n        expr = _convert_expression(expr)\\n        engine = _check_engine(engine)\\n        _check_parser(parser)\\n        _check_resolvers(resolvers)\\n        _check_for_locals(expr, level, parser)\\n\\n        # get our (possibly passed-in) scope\\n        env = _ensure_scope(level + 1, global_dict=global_dict,\\n                            local_dict=local_dict, resolvers=resolvers,\\n                            target=target)\\n\\n        parsed_expr = Expr(expr, engine=engine, parser=parser, env=env,\\n                           truediv=truediv)\\n\\n        # construct the engine and evaluate the parsed expression\\n        eng = _engines[engine]\\n        eng_inst = eng(parsed_expr)\\n        ret = eng_inst.evaluate()\\n\\n        if parsed_expr.assigner is None:\\n            if multi_line:\\n                raise ValueError(\"Multi-line expressions are only valid\"\\n                                 \" if all expressions contain an assignment\")\\n            elif inplace:\\n                raise ValueError(\"Cannot operate inplace \"\\n                                 \"if there is no assignment\")\\n\\n        # assign if needed\\n        assigner = parsed_expr.assigner\\n        if env.target is not None and assigner is not None:\\n            target_modified = True\\n\\n            # if returning a copy, copy only on the first assignment\\n            if not inplace and first_expr:\\n                try:\\n                    target = env.target.copy()\\n                except AttributeError:\\n                    raise ValueError(\"Cannot return a copy of the target\")\\n            else:\\n                target = env.target\\n\\n            # TypeError is most commonly raised (e.g. int, list), but you\\n            # get IndexError if you try to do this assignment on np.ndarray.\\n            # we will ignore numpy warnings here; e.g. if trying\\n            # to use a non-numeric indexer\\n            try:\\n                with warnings.catch_warnings(record=True):\\n                    # TODO: Filter the warnings we actually care about here.\\n                    target[assigner] = ret\\n            except (TypeError, IndexError):\\n                raise ValueError(\"Cannot assign expression output to target\")\\n\\n            if not resolvers:\\n                resolvers = ({assigner: ret},)\\n            else:\\n                # existing resolver needs updated to handle\\n                # case of mutating existing column in copy\\n                for resolver in resolvers:\\n                    if assigner in resolver:\\n                        resolver[assigner] = ret\\n                        break\\n                else:\\n                    resolvers += ({assigner: ret},)\\n\\n            ret = None\\n            first_expr = False\\n\\n    # We want to exclude `inplace=None` as being False.\\n    if inplace is False:\\n        return target if target_modified else ret',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'MultiIndex.copy',\n",
       "  'docstring': 'Make a copy of this object. Names, dtype, levels and codes can be\\n        passed and will be set on new copy.\\n\\n        Parameters\\n        ----------\\n        names : sequence, optional\\n        dtype : numpy dtype or pandas type, optional\\n        levels : sequence, optional\\n        codes : sequence, optional\\n\\n        Returns\\n        -------\\n        copy : MultiIndex\\n\\n        Notes\\n        -----\\n        In most cases, there should be no functional difference from using\\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\\n        This could be potentially expensive on large MultiIndex objects.',\n",
       "  'code': 'def copy(self, names=None, dtype=None, levels=None, codes=None,\\n             deep=False, _set_identity=False, **kwargs):\\n        \"\"\"\\n        Make a copy of this object. Names, dtype, levels and codes can be\\n        passed and will be set on new copy.\\n\\n        Parameters\\n        ----------\\n        names : sequence, optional\\n        dtype : numpy dtype or pandas type, optional\\n        levels : sequence, optional\\n        codes : sequence, optional\\n\\n        Returns\\n        -------\\n        copy : MultiIndex\\n\\n        Notes\\n        -----\\n        In most cases, there should be no functional difference from using\\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\\n        This could be potentially expensive on large MultiIndex objects.\\n        \"\"\"\\n        name = kwargs.get(\\'name\\')\\n        names = self._validate_names(name=name, names=names, deep=deep)\\n\\n        if deep:\\n            from copy import deepcopy\\n            if levels is None:\\n                levels = deepcopy(self.levels)\\n            if codes is None:\\n                codes = deepcopy(self.codes)\\n        else:\\n            if levels is None:\\n                levels = self.levels\\n            if codes is None:\\n                codes = self.codes\\n        return MultiIndex(levels=levels, codes=codes, names=names,\\n                          sortorder=self.sortorder, verify_integrity=False,\\n                          _set_identity=_set_identity)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'match',\n",
       "  'docstring': 'Compute locations of to_match into values\\n\\n    Parameters\\n    ----------\\n    to_match : array-like\\n        values to find positions of\\n    values : array-like\\n        Unique set of values\\n    na_sentinel : int, default -1\\n        Value to mark \"not found\"\\n\\n    Examples\\n    --------\\n\\n    Returns\\n    -------\\n    match : ndarray of integers',\n",
       "  'code': 'def match(to_match, values, na_sentinel=-1):\\n    \"\"\"\\n    Compute locations of to_match into values\\n\\n    Parameters\\n    ----------\\n    to_match : array-like\\n        values to find positions of\\n    values : array-like\\n        Unique set of values\\n    na_sentinel : int, default -1\\n        Value to mark \"not found\"\\n\\n    Examples\\n    --------\\n\\n    Returns\\n    -------\\n    match : ndarray of integers\\n    \"\"\"\\n    values = com.asarray_tuplesafe(values)\\n    htable, _, values, dtype, ndtype = _get_hashtable_algo(values)\\n    to_match, _, _ = _ensure_data(to_match, dtype)\\n    table = htable(min(len(to_match), 1000000))\\n    table.map_locations(values)\\n    result = table.lookup(to_match)\\n\\n    if na_sentinel != -1:\\n\\n        # replace but return a numpy array\\n        # use a Series because it handles dtype conversions properly\\n        from pandas import Series\\n        result = Series(result.ravel()).replace(-1, na_sentinel)\\n        result = result.values.reshape(result.shape)\\n\\n    return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'unique',\n",
       "  'docstring': \"Hash table-based unique. Uniques are returned in order\\n    of appearance. This does NOT sort.\\n\\n    Significantly faster than numpy.unique. Includes NA values.\\n\\n    Parameters\\n    ----------\\n    values : 1d array-like\\n\\n    Returns\\n    -------\\n    numpy.ndarray or ExtensionArray\\n\\n        The return can be:\\n\\n        * Index : when the input is an Index\\n        * Categorical : when the input is a Categorical dtype\\n        * ndarray : when the input is a Series/ndarray\\n\\n        Return numpy.ndarray or ExtensionArray.\\n\\n    See Also\\n    --------\\n    Index.unique\\n    Series.unique\\n\\n    Examples\\n    --------\\n    >>> pd.unique(pd.Series([2, 1, 3, 3]))\\n    array([2, 1, 3])\\n\\n    >>> pd.unique(pd.Series([2] + [1] * 5))\\n    array([2, 1])\\n\\n    >>> pd.unique(pd.Series([pd.Timestamp('20160101'),\\n    ...                     pd.Timestamp('20160101')]))\\n    array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\\n\\n    >>> pd.unique(pd.Series([pd.Timestamp('20160101', tz='US/Eastern'),\\n    ...                      pd.Timestamp('20160101', tz='US/Eastern')]))\\n    array([Timestamp('2016-01-01 00:00:00-0500', tz='US/Eastern')],\\n          dtype=object)\\n\\n    >>> pd.unique(pd.Index([pd.Timestamp('20160101', tz='US/Eastern'),\\n    ...                     pd.Timestamp('20160101', tz='US/Eastern')]))\\n    DatetimeIndex(['2016-01-01 00:00:00-05:00'],\\n    ...           dtype='datetime64[ns, US/Eastern]', freq=None)\\n\\n    >>> pd.unique(list('baabc'))\\n    array(['b', 'a', 'c'], dtype=object)\\n\\n    An unordered Categorical will return categories in the\\n    order of appearance.\\n\\n    >>> pd.unique(pd.Series(pd.Categorical(list('baabc'))))\\n    [b, a, c]\\n    Categories (3, object): [b, a, c]\\n\\n    >>> pd.unique(pd.Series(pd.Categorical(list('baabc'),\\n    ...                                    categories=list('abc'))))\\n    [b, a, c]\\n    Categories (3, object): [b, a, c]\\n\\n    An ordered Categorical preserves the category ordering.\\n\\n    >>> pd.unique(pd.Series(pd.Categorical(list('baabc'),\\n    ...                                    categories=list('abc'),\\n    ...                                    ordered=True)))\\n    [b, a, c]\\n    Categories (3, object): [a < b < c]\\n\\n    An array of tuples\\n\\n    >>> pd.unique([('a', 'b'), ('b', 'a'), ('a', 'c'), ('b', 'a')])\\n    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\",\n",
       "  'code': 'def unique(values):\\n    \"\"\"\\n    Hash table-based unique. Uniques are returned in order\\n    of appearance. This does NOT sort.\\n\\n    Significantly faster than numpy.unique. Includes NA values.\\n\\n    Parameters\\n    ----------\\n    values : 1d array-like\\n\\n    Returns\\n    -------\\n    numpy.ndarray or ExtensionArray\\n\\n        The return can be:\\n\\n        * Index : when the input is an Index\\n        * Categorical : when the input is a Categorical dtype\\n        * ndarray : when the input is a Series/ndarray\\n\\n        Return numpy.ndarray or ExtensionArray.\\n\\n    See Also\\n    --------\\n    Index.unique\\n    Series.unique\\n\\n    Examples\\n    --------\\n    >>> pd.unique(pd.Series([2, 1, 3, 3]))\\n    array([2, 1, 3])\\n\\n    >>> pd.unique(pd.Series([2] + [1] * 5))\\n    array([2, 1])\\n\\n    >>> pd.unique(pd.Series([pd.Timestamp(\\'20160101\\'),\\n    ...                     pd.Timestamp(\\'20160101\\')]))\\n    array([\\'2016-01-01T00:00:00.000000000\\'], dtype=\\'datetime64[ns]\\')\\n\\n    >>> pd.unique(pd.Series([pd.Timestamp(\\'20160101\\', tz=\\'US/Eastern\\'),\\n    ...                      pd.Timestamp(\\'20160101\\', tz=\\'US/Eastern\\')]))\\n    array([Timestamp(\\'2016-01-01 00:00:00-0500\\', tz=\\'US/Eastern\\')],\\n          dtype=object)\\n\\n    >>> pd.unique(pd.Index([pd.Timestamp(\\'20160101\\', tz=\\'US/Eastern\\'),\\n    ...                     pd.Timestamp(\\'20160101\\', tz=\\'US/Eastern\\')]))\\n    DatetimeIndex([\\'2016-01-01 00:00:00-05:00\\'],\\n    ...           dtype=\\'datetime64[ns, US/Eastern]\\', freq=None)\\n\\n    >>> pd.unique(list(\\'baabc\\'))\\n    array([\\'b\\', \\'a\\', \\'c\\'], dtype=object)\\n\\n    An unordered Categorical will return categories in the\\n    order of appearance.\\n\\n    >>> pd.unique(pd.Series(pd.Categorical(list(\\'baabc\\'))))\\n    [b, a, c]\\n    Categories (3, object): [b, a, c]\\n\\n    >>> pd.unique(pd.Series(pd.Categorical(list(\\'baabc\\'),\\n    ...                                    categories=list(\\'abc\\'))))\\n    [b, a, c]\\n    Categories (3, object): [b, a, c]\\n\\n    An ordered Categorical preserves the category ordering.\\n\\n    >>> pd.unique(pd.Series(pd.Categorical(list(\\'baabc\\'),\\n    ...                                    categories=list(\\'abc\\'),\\n    ...                                    ordered=True)))\\n    [b, a, c]\\n    Categories (3, object): [a < b < c]\\n\\n    An array of tuples\\n\\n    >>> pd.unique([(\\'a\\', \\'b\\'), (\\'b\\', \\'a\\'), (\\'a\\', \\'c\\'), (\\'b\\', \\'a\\')])\\n    array([(\\'a\\', \\'b\\'), (\\'b\\', \\'a\\'), (\\'a\\', \\'c\\')], dtype=object)\\n    \"\"\"\\n\\n    values = _ensure_arraylike(values)\\n\\n    if is_extension_array_dtype(values):\\n        # Dispatch to extension dtype\\'s unique.\\n        return values.unique()\\n\\n    original = values\\n    htable, _, values, dtype, ndtype = _get_hashtable_algo(values)\\n\\n    table = htable(len(values))\\n    uniques = table.unique(values)\\n    uniques = _reconstruct_data(uniques, dtype, original)\\n    return uniques',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'take',\n",
       "  'docstring': 'Take elements from an array.\\n\\n    .. versionadded:: 0.23.0\\n\\n    Parameters\\n    ----------\\n    arr : sequence\\n        Non array-likes (sequences without a dtype) are coerced\\n        to an ndarray.\\n    indices : sequence of integers\\n        Indices to be taken.\\n    axis : int, default 0\\n        The axis over which to select values.\\n    allow_fill : bool, default False\\n        How to handle negative values in `indices`.\\n\\n        * False: negative values in `indices` indicate positional indices\\n          from the right (the default). This is similar to :func:`numpy.take`.\\n\\n        * True: negative values in `indices` indicate\\n          missing values. These values are set to `fill_value`. Any other\\n          other negative values raise a ``ValueError``.\\n\\n    fill_value : any, optional\\n        Fill value to use for NA-indices when `allow_fill` is True.\\n        This may be ``None``, in which case the default NA value for\\n        the type (``self.dtype.na_value``) is used.\\n\\n        For multi-dimensional `arr`, each *element* is filled with\\n        `fill_value`.\\n\\n    Returns\\n    -------\\n    ndarray or ExtensionArray\\n        Same type as the input.\\n\\n    Raises\\n    ------\\n    IndexError\\n        When `indices` is out of bounds for the array.\\n    ValueError\\n        When the indexer contains negative values other than ``-1``\\n        and `allow_fill` is True.\\n\\n    Notes\\n    -----\\n    When `allow_fill` is False, `indices` may be whatever dimensionality\\n    is accepted by NumPy for `arr`.\\n\\n    When `allow_fill` is True, `indices` should be 1-D.\\n\\n    See Also\\n    --------\\n    numpy.take\\n\\n    Examples\\n    --------\\n    >>> from pandas.api.extensions import take\\n\\n    With the default ``allow_fill=False``, negative numbers indicate\\n    positional indices from the right.\\n\\n    >>> take(np.array([10, 20, 30]), [0, 0, -1])\\n    array([10, 10, 30])\\n\\n    Setting ``allow_fill=True`` will place `fill_value` in those positions.\\n\\n    >>> take(np.array([10, 20, 30]), [0, 0, -1], allow_fill=True)\\n    array([10., 10., nan])\\n\\n    >>> take(np.array([10, 20, 30]), [0, 0, -1], allow_fill=True,\\n    ...      fill_value=-10)\\n    array([ 10,  10, -10])',\n",
       "  'code': 'def take(arr, indices, axis=0, allow_fill=False, fill_value=None):\\n    \"\"\"\\n    Take elements from an array.\\n\\n    .. versionadded:: 0.23.0\\n\\n    Parameters\\n    ----------\\n    arr : sequence\\n        Non array-likes (sequences without a dtype) are coerced\\n        to an ndarray.\\n    indices : sequence of integers\\n        Indices to be taken.\\n    axis : int, default 0\\n        The axis over which to select values.\\n    allow_fill : bool, default False\\n        How to handle negative values in `indices`.\\n\\n        * False: negative values in `indices` indicate positional indices\\n          from the right (the default). This is similar to :func:`numpy.take`.\\n\\n        * True: negative values in `indices` indicate\\n          missing values. These values are set to `fill_value`. Any other\\n          other negative values raise a ``ValueError``.\\n\\n    fill_value : any, optional\\n        Fill value to use for NA-indices when `allow_fill` is True.\\n        This may be ``None``, in which case the default NA value for\\n        the type (``self.dtype.na_value``) is used.\\n\\n        For multi-dimensional `arr`, each *element* is filled with\\n        `fill_value`.\\n\\n    Returns\\n    -------\\n    ndarray or ExtensionArray\\n        Same type as the input.\\n\\n    Raises\\n    ------\\n    IndexError\\n        When `indices` is out of bounds for the array.\\n    ValueError\\n        When the indexer contains negative values other than ``-1``\\n        and `allow_fill` is True.\\n\\n    Notes\\n    -----\\n    When `allow_fill` is False, `indices` may be whatever dimensionality\\n    is accepted by NumPy for `arr`.\\n\\n    When `allow_fill` is True, `indices` should be 1-D.\\n\\n    See Also\\n    --------\\n    numpy.take\\n\\n    Examples\\n    --------\\n    >>> from pandas.api.extensions import take\\n\\n    With the default ``allow_fill=False``, negative numbers indicate\\n    positional indices from the right.\\n\\n    >>> take(np.array([10, 20, 30]), [0, 0, -1])\\n    array([10, 10, 30])\\n\\n    Setting ``allow_fill=True`` will place `fill_value` in those positions.\\n\\n    >>> take(np.array([10, 20, 30]), [0, 0, -1], allow_fill=True)\\n    array([10., 10., nan])\\n\\n    >>> take(np.array([10, 20, 30]), [0, 0, -1], allow_fill=True,\\n    ...      fill_value=-10)\\n    array([ 10,  10, -10])\\n    \"\"\"\\n    from pandas.core.indexing import validate_indices\\n\\n    if not is_array_like(arr):\\n        arr = np.asarray(arr)\\n\\n    indices = np.asarray(indices, dtype=np.intp)\\n\\n    if allow_fill:\\n        # Pandas style, -1 means NA\\n        validate_indices(indices, len(arr))\\n        result = take_1d(arr, indices, axis=axis, allow_fill=True,\\n                         fill_value=fill_value)\\n    else:\\n        # NumPy style\\n        result = arr.take(indices, axis=axis)\\n    return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'searchsorted',\n",
       "  'docstring': \"Find indices where elements should be inserted to maintain order.\\n\\n    .. versionadded:: 0.25.0\\n\\n    Find the indices into a sorted array `arr` (a) such that, if the\\n    corresponding elements in `value` were inserted before the indices,\\n    the order of `arr` would be preserved.\\n\\n    Assuming that `arr` is sorted:\\n\\n    ======  ================================\\n    `side`  returned index `i` satisfies\\n    ======  ================================\\n    left    ``arr[i-1] < value <= self[i]``\\n    right   ``arr[i-1] <= value < self[i]``\\n    ======  ================================\\n\\n    Parameters\\n    ----------\\n    arr: array-like\\n        Input array. If `sorter` is None, then it must be sorted in\\n        ascending order, otherwise `sorter` must be an array of indices\\n        that sort it.\\n    value : array_like\\n        Values to insert into `arr`.\\n    side : {'left', 'right'}, optional\\n        If 'left', the index of the first suitable location found is given.\\n        If 'right', return the last such index.  If there is no suitable\\n        index, return either 0 or N (where N is the length of `self`).\\n    sorter : 1-D array_like, optional\\n        Optional array of integer indices that sort array a into ascending\\n        order. They are typically the result of argsort.\\n\\n    Returns\\n    -------\\n    array of ints\\n        Array of insertion points with the same shape as `value`.\\n\\n    See Also\\n    --------\\n    numpy.searchsorted : Similar method from NumPy.\",\n",
       "  'code': 'def searchsorted(arr, value, side=\"left\", sorter=None):\\n    \"\"\"\\n    Find indices where elements should be inserted to maintain order.\\n\\n    .. versionadded:: 0.25.0\\n\\n    Find the indices into a sorted array `arr` (a) such that, if the\\n    corresponding elements in `value` were inserted before the indices,\\n    the order of `arr` would be preserved.\\n\\n    Assuming that `arr` is sorted:\\n\\n    ======  ================================\\n    `side`  returned index `i` satisfies\\n    ======  ================================\\n    left    ``arr[i-1] < value <= self[i]``\\n    right   ``arr[i-1] <= value < self[i]``\\n    ======  ================================\\n\\n    Parameters\\n    ----------\\n    arr: array-like\\n        Input array. If `sorter` is None, then it must be sorted in\\n        ascending order, otherwise `sorter` must be an array of indices\\n        that sort it.\\n    value : array_like\\n        Values to insert into `arr`.\\n    side : {\\'left\\', \\'right\\'}, optional\\n        If \\'left\\', the index of the first suitable location found is given.\\n        If \\'right\\', return the last such index.  If there is no suitable\\n        index, return either 0 or N (where N is the length of `self`).\\n    sorter : 1-D array_like, optional\\n        Optional array of integer indices that sort array a into ascending\\n        order. They are typically the result of argsort.\\n\\n    Returns\\n    -------\\n    array of ints\\n        Array of insertion points with the same shape as `value`.\\n\\n    See Also\\n    --------\\n    numpy.searchsorted : Similar method from NumPy.\\n    \"\"\"\\n    if sorter is not None:\\n        sorter = ensure_platform_int(sorter)\\n\\n    if isinstance(arr, np.ndarray) and is_integer_dtype(arr) and (\\n            is_integer(value) or is_integer_dtype(value)):\\n        from .arrays.array_ import array\\n        # if `arr` and `value` have different dtypes, `arr` would be\\n        # recast by numpy, causing a slow search.\\n        # Before searching below, we therefore try to give `value` the\\n        # same dtype as `arr`, while guarding against integer overflows.\\n        iinfo = np.iinfo(arr.dtype.type)\\n        value_arr = np.array([value]) if is_scalar(value) else np.array(value)\\n        if (value_arr >= iinfo.min).all() and (value_arr <= iinfo.max).all():\\n            # value within bounds, so no overflow, so can convert value dtype\\n            # to dtype of arr\\n            dtype = arr.dtype\\n        else:\\n            dtype = value_arr.dtype\\n\\n        if is_scalar(value):\\n            value = dtype.type(value)\\n        else:\\n            value = array(value, dtype=dtype)\\n    elif not (is_object_dtype(arr) or is_numeric_dtype(arr) or\\n              is_categorical_dtype(arr)):\\n        from pandas.core.series import Series\\n        # E.g. if `arr` is an array with dtype=\\'datetime64[ns]\\'\\n        # and `value` is a pd.Timestamp, we may need to convert value\\n        value_ser = Series(value)._values\\n        value = value_ser[0] if is_scalar(value) else value_ser\\n\\n    result = arr.searchsorted(value, side=side, sorter=sorter)\\n    return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_dt_array_cmp',\n",
       "  'docstring': 'Wrap comparison operations to convert datetime-like to datetime64',\n",
       "  'code': 'def _dt_array_cmp(cls, op):\\n    \"\"\"\\n    Wrap comparison operations to convert datetime-like to datetime64\\n    \"\"\"\\n    opname = \\'__{name}__\\'.format(name=op.__name__)\\n    nat_result = opname == \\'__ne__\\'\\n\\n    def wrapper(self, other):\\n        if isinstance(other, (ABCDataFrame, ABCSeries, ABCIndexClass)):\\n            return NotImplemented\\n\\n        other = lib.item_from_zerodim(other)\\n\\n        if isinstance(other, (datetime, np.datetime64, str)):\\n            if isinstance(other, (datetime, np.datetime64)):\\n                # GH#18435 strings get a pass from tzawareness compat\\n                self._assert_tzawareness_compat(other)\\n\\n            try:\\n                other = _to_M8(other, tz=self.tz)\\n            except ValueError:\\n                # string that cannot be parsed to Timestamp\\n                return ops.invalid_comparison(self, other, op)\\n\\n            result = op(self.asi8, other.view(\\'i8\\'))\\n            if isna(other):\\n                result.fill(nat_result)\\n        elif lib.is_scalar(other) or np.ndim(other) == 0:\\n            return ops.invalid_comparison(self, other, op)\\n        elif len(other) != len(self):\\n            raise ValueError(\"Lengths must match\")\\n        else:\\n            if isinstance(other, list):\\n                try:\\n                    other = type(self)._from_sequence(other)\\n                except ValueError:\\n                    other = np.array(other, dtype=np.object_)\\n            elif not isinstance(other, (np.ndarray, ABCIndexClass, ABCSeries,\\n                                        DatetimeArray)):\\n                # Following Timestamp convention, __eq__ is all-False\\n                # and __ne__ is all True, others raise TypeError.\\n                return ops.invalid_comparison(self, other, op)\\n\\n            if is_object_dtype(other):\\n                # We have to use _comp_method_OBJECT_ARRAY instead of numpy\\n                #  comparison otherwise it would fail to raise when\\n                #  comparing tz-aware and tz-naive\\n                with np.errstate(all=\\'ignore\\'):\\n                    result = ops._comp_method_OBJECT_ARRAY(op,\\n                                                           self.astype(object),\\n                                                           other)\\n                o_mask = isna(other)\\n            elif not (is_datetime64_dtype(other) or\\n                      is_datetime64tz_dtype(other)):\\n                # e.g. is_timedelta64_dtype(other)\\n                return ops.invalid_comparison(self, other, op)\\n            else:\\n                self._assert_tzawareness_compat(other)\\n                if isinstance(other, (ABCIndexClass, ABCSeries)):\\n                    other = other.array\\n\\n                if (is_datetime64_dtype(other) and\\n                        not is_datetime64_ns_dtype(other) or\\n                        not hasattr(other, \\'asi8\\')):\\n                    # e.g. other.dtype == \\'datetime64[s]\\'\\n                    # or an object-dtype ndarray\\n                    other = type(self)._from_sequence(other)\\n\\n                result = op(self.view(\\'i8\\'), other.view(\\'i8\\'))\\n                o_mask = other._isnan\\n\\n            result = com.values_from_object(result)\\n\\n            if o_mask.any():\\n                result[o_mask] = nat_result\\n\\n        if self._hasnans:\\n            result[self._isnan] = nat_result\\n\\n        return result\\n\\n    return compat.set_function_name(wrapper, opname, cls)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'sequence_to_dt64ns',\n",
       "  'docstring': \"Parameters\\n    ----------\\n    data : list-like\\n    dtype : dtype, str, or None, default None\\n    copy : bool, default False\\n    tz : tzinfo, str, or None, default None\\n    dayfirst : bool, default False\\n    yearfirst : bool, default False\\n    ambiguous : str, bool, or arraylike, default 'raise'\\n        See pandas._libs.tslibs.conversion.tz_localize_to_utc\\n    int_as_wall_time : bool, default False\\n        Whether to treat ints as wall time in specified timezone, or as\\n        nanosecond-precision UNIX epoch (wall time in UTC).\\n        This is used in DatetimeIndex.__init__ to deprecate the wall-time\\n        behaviour.\\n\\n        ..versionadded:: 0.24.0\\n\\n    Returns\\n    -------\\n    result : numpy.ndarray\\n        The sequence converted to a numpy array with dtype ``datetime64[ns]``.\\n    tz : tzinfo or None\\n        Either the user-provided tzinfo or one inferred from the data.\\n    inferred_freq : Tick or None\\n        The inferred frequency of the sequence.\\n\\n    Raises\\n    ------\\n    TypeError : PeriodDType data is passed\",\n",
       "  'code': 'def sequence_to_dt64ns(data, dtype=None, copy=False,\\n                       tz=None,\\n                       dayfirst=False, yearfirst=False, ambiguous=\\'raise\\',\\n                       int_as_wall_time=False):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    data : list-like\\n    dtype : dtype, str, or None, default None\\n    copy : bool, default False\\n    tz : tzinfo, str, or None, default None\\n    dayfirst : bool, default False\\n    yearfirst : bool, default False\\n    ambiguous : str, bool, or arraylike, default \\'raise\\'\\n        See pandas._libs.tslibs.conversion.tz_localize_to_utc\\n    int_as_wall_time : bool, default False\\n        Whether to treat ints as wall time in specified timezone, or as\\n        nanosecond-precision UNIX epoch (wall time in UTC).\\n        This is used in DatetimeIndex.__init__ to deprecate the wall-time\\n        behaviour.\\n\\n        ..versionadded:: 0.24.0\\n\\n    Returns\\n    -------\\n    result : numpy.ndarray\\n        The sequence converted to a numpy array with dtype ``datetime64[ns]``.\\n    tz : tzinfo or None\\n        Either the user-provided tzinfo or one inferred from the data.\\n    inferred_freq : Tick or None\\n        The inferred frequency of the sequence.\\n\\n    Raises\\n    ------\\n    TypeError : PeriodDType data is passed\\n    \"\"\"\\n\\n    inferred_freq = None\\n\\n    dtype = _validate_dt64_dtype(dtype)\\n\\n    if not hasattr(data, \"dtype\"):\\n        # e.g. list, tuple\\n        if np.ndim(data) == 0:\\n            # i.e. generator\\n            data = list(data)\\n        data = np.asarray(data)\\n        copy = False\\n    elif isinstance(data, ABCSeries):\\n        data = data._values\\n    if isinstance(data, ABCPandasArray):\\n        data = data.to_numpy()\\n\\n    if hasattr(data, \"freq\"):\\n        # i.e. DatetimeArray/Index\\n        inferred_freq = data.freq\\n\\n    # if dtype has an embedded tz, capture it\\n    tz = validate_tz_from_dtype(dtype, tz)\\n\\n    if isinstance(data, ABCIndexClass):\\n        data = data._data\\n\\n    # By this point we are assured to have either a numpy array or Index\\n    data, copy = maybe_convert_dtype(data, copy)\\n\\n    if is_object_dtype(data) or is_string_dtype(data):\\n        # TODO: We do not have tests specific to string-dtypes,\\n        #  also complex or categorical or other extension\\n        copy = False\\n        if lib.infer_dtype(data, skipna=False) == \\'integer\\':\\n            data = data.astype(np.int64)\\n        else:\\n            # data comes back here as either i8 to denote UTC timestamps\\n            #  or M8[ns] to denote wall times\\n            data, inferred_tz = objects_to_datetime64ns(\\n                data, dayfirst=dayfirst, yearfirst=yearfirst)\\n            tz = maybe_infer_tz(tz, inferred_tz)\\n            # When a sequence of timestamp objects is passed, we always\\n            # want to treat the (now i8-valued) data as UTC timestamps,\\n            # not wall times.\\n            int_as_wall_time = False\\n\\n    # `data` may have originally been a Categorical[datetime64[ns, tz]],\\n    # so we need to handle these types.\\n    if is_datetime64tz_dtype(data):\\n        # DatetimeArray -> ndarray\\n        tz = maybe_infer_tz(tz, data.tz)\\n        result = data._data\\n\\n    elif is_datetime64_dtype(data):\\n        # tz-naive DatetimeArray or ndarray[datetime64]\\n        data = getattr(data, \"_data\", data)\\n        if data.dtype != _NS_DTYPE:\\n            data = conversion.ensure_datetime64ns(data)\\n\\n        if tz is not None:\\n            # Convert tz-naive to UTC\\n            tz = timezones.maybe_get_tz(tz)\\n            data = conversion.tz_localize_to_utc(data.view(\\'i8\\'), tz,\\n                                                 ambiguous=ambiguous)\\n            data = data.view(_NS_DTYPE)\\n\\n        assert data.dtype == _NS_DTYPE, data.dtype\\n        result = data\\n\\n    else:\\n        # must be integer dtype otherwise\\n        # assume this data are epoch timestamps\\n        if tz:\\n            tz = timezones.maybe_get_tz(tz)\\n\\n        if data.dtype != _INT64_DTYPE:\\n            data = data.astype(np.int64, copy=False)\\n        if int_as_wall_time and tz is not None and not timezones.is_utc(tz):\\n            warnings.warn(_i8_message, FutureWarning, stacklevel=4)\\n            data = conversion.tz_localize_to_utc(data.view(\\'i8\\'), tz,\\n                                                 ambiguous=ambiguous)\\n            data = data.view(_NS_DTYPE)\\n        result = data.view(_NS_DTYPE)\\n\\n    if copy:\\n        # TODO: should this be deepcopy?\\n        result = result.copy()\\n\\n    assert isinstance(result, np.ndarray), type(result)\\n    assert result.dtype == \\'M8[ns]\\', result.dtype\\n\\n    # We have to call this again after possibly inferring a tz above\\n    validate_tz_from_dtype(dtype, tz)\\n\\n    return result, tz, inferred_freq',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'objects_to_datetime64ns',\n",
       "  'docstring': \"Convert data to array of timestamps.\\n\\n    Parameters\\n    ----------\\n    data : np.ndarray[object]\\n    dayfirst : bool\\n    yearfirst : bool\\n    utc : bool, default False\\n        Whether to convert timezone-aware timestamps to UTC\\n    errors : {'raise', 'ignore', 'coerce'}\\n    allow_object : bool\\n        Whether to return an object-dtype ndarray instead of raising if the\\n        data contains more than one timezone.\\n\\n    Returns\\n    -------\\n    result : ndarray\\n        np.int64 dtype if returned values represent UTC timestamps\\n        np.datetime64[ns] if returned values represent wall times\\n        object if mixed timezones\\n    inferred_tz : tzinfo or None\\n\\n    Raises\\n    ------\\n    ValueError : if data cannot be converted to datetimes\",\n",
       "  'code': 'def objects_to_datetime64ns(data, dayfirst, yearfirst,\\n                            utc=False, errors=\"raise\",\\n                            require_iso8601=False, allow_object=False):\\n    \"\"\"\\n    Convert data to array of timestamps.\\n\\n    Parameters\\n    ----------\\n    data : np.ndarray[object]\\n    dayfirst : bool\\n    yearfirst : bool\\n    utc : bool, default False\\n        Whether to convert timezone-aware timestamps to UTC\\n    errors : {\\'raise\\', \\'ignore\\', \\'coerce\\'}\\n    allow_object : bool\\n        Whether to return an object-dtype ndarray instead of raising if the\\n        data contains more than one timezone.\\n\\n    Returns\\n    -------\\n    result : ndarray\\n        np.int64 dtype if returned values represent UTC timestamps\\n        np.datetime64[ns] if returned values represent wall times\\n        object if mixed timezones\\n    inferred_tz : tzinfo or None\\n\\n    Raises\\n    ------\\n    ValueError : if data cannot be converted to datetimes\\n    \"\"\"\\n    assert errors in [\"raise\", \"ignore\", \"coerce\"]\\n\\n    # if str-dtype, convert\\n    data = np.array(data, copy=False, dtype=np.object_)\\n\\n    try:\\n        result, tz_parsed = tslib.array_to_datetime(\\n            data,\\n            errors=errors,\\n            utc=utc,\\n            dayfirst=dayfirst,\\n            yearfirst=yearfirst,\\n            require_iso8601=require_iso8601\\n        )\\n    except ValueError as e:\\n        try:\\n            values, tz_parsed = conversion.datetime_to_datetime64(data)\\n            # If tzaware, these values represent unix timestamps, so we\\n            #  return them as i8 to distinguish from wall times\\n            return values.view(\\'i8\\'), tz_parsed\\n        except (ValueError, TypeError):\\n            raise e\\n\\n    if tz_parsed is not None:\\n        # We can take a shortcut since the datetime64 numpy array\\n        #  is in UTC\\n        # Return i8 values to denote unix timestamps\\n        return result.view(\\'i8\\'), tz_parsed\\n    elif is_datetime64_dtype(result):\\n        # returning M8[ns] denotes wall-times; since tz is None\\n        #  the distinction is a thin one\\n        return result, tz_parsed\\n    elif is_object_dtype(result):\\n        # GH#23675 when called via `pd.to_datetime`, returning an object-dtype\\n        #  array is allowed.  When called via `pd.DatetimeIndex`, we can\\n        #  only accept datetime64 dtype, so raise TypeError if object-dtype\\n        #  is returned, as that indicates the values can be recognized as\\n        #  datetimes but they have conflicting timezones/awareness\\n        if allow_object:\\n            return result, tz_parsed\\n        raise TypeError(result)\\n    else:  # pragma: no cover\\n        # GH#23675 this TypeError should never be hit, whereas the TypeError\\n        #  in the object-dtype branch above is reachable.\\n        raise TypeError(result)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_validate_dt64_dtype',\n",
       "  'docstring': 'Check that a dtype, if passed, represents either a numpy datetime64[ns]\\n    dtype or a pandas DatetimeTZDtype.\\n\\n    Parameters\\n    ----------\\n    dtype : object\\n\\n    Returns\\n    -------\\n    dtype : None, numpy.dtype, or DatetimeTZDtype\\n\\n    Raises\\n    ------\\n    ValueError : invalid dtype\\n\\n    Notes\\n    -----\\n    Unlike validate_tz_from_dtype, this does _not_ allow non-existent\\n    tz errors to go through',\n",
       "  'code': 'def _validate_dt64_dtype(dtype):\\n    \"\"\"\\n    Check that a dtype, if passed, represents either a numpy datetime64[ns]\\n    dtype or a pandas DatetimeTZDtype.\\n\\n    Parameters\\n    ----------\\n    dtype : object\\n\\n    Returns\\n    -------\\n    dtype : None, numpy.dtype, or DatetimeTZDtype\\n\\n    Raises\\n    ------\\n    ValueError : invalid dtype\\n\\n    Notes\\n    -----\\n    Unlike validate_tz_from_dtype, this does _not_ allow non-existent\\n    tz errors to go through\\n    \"\"\"\\n    if dtype is not None:\\n        dtype = pandas_dtype(dtype)\\n        if is_dtype_equal(dtype, np.dtype(\"M8\")):\\n            # no precision, warn\\n            dtype = _NS_DTYPE\\n            msg = textwrap.dedent(\"\"\"\\\\\\n                Passing in \\'datetime64\\' dtype with no precision is deprecated\\n                and will raise in a future version. Please pass in\\n                \\'datetime64[ns]\\' instead.\"\"\")\\n            warnings.warn(msg, FutureWarning, stacklevel=5)\\n\\n        if ((isinstance(dtype, np.dtype) and dtype != _NS_DTYPE)\\n                or not isinstance(dtype, (np.dtype, DatetimeTZDtype))):\\n            raise ValueError(\"Unexpected value for \\'dtype\\': \\'{dtype}\\'. \"\\n                             \"Must be \\'datetime64[ns]\\' or DatetimeTZDtype\\'.\"\\n                             .format(dtype=dtype))\\n    return dtype',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatetimeArray.time',\n",
       "  'docstring': 'Returns numpy array of datetime.time. The time part of the Timestamps.',\n",
       "  'code': 'def time(self):\\n        \"\"\"\\n        Returns numpy array of datetime.time. The time part of the Timestamps.\\n        \"\"\"\\n        # If the Timestamps have a timezone that is not UTC,\\n        # convert them into their i8 representation while\\n        # keeping their timezone and not using UTC\\n        if self.tz is not None and not timezones.is_utc(self.tz):\\n            timestamps = self._local_timestamps()\\n        else:\\n            timestamps = self.asi8\\n\\n        return tslib.ints_to_pydatetime(timestamps, box=\"time\")',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'get_validation_data',\n",
       "  'docstring': 'Validate the docstring.\\n\\n    Parameters\\n    ----------\\n    doc : Docstring\\n        A Docstring object with the given function name.\\n\\n    Returns\\n    -------\\n    tuple\\n        errors : list of tuple\\n            Errors occurred during validation.\\n        warnings : list of tuple\\n            Warnings occurred during validation.\\n        examples_errs : str\\n            Examples usage displayed along the error, otherwise empty string.\\n\\n    Notes\\n    -----\\n    The errors codes are defined as:\\n    - First two characters: Section where the error happens:\\n       * GL: Global (no section, like section ordering errors)\\n       * SS: Short summary\\n       * ES: Extended summary\\n       * PR: Parameters\\n       * RT: Returns\\n       * YD: Yields\\n       * RS: Raises\\n       * WN: Warns\\n       * SA: See Also\\n       * NT: Notes\\n       * RF: References\\n       * EX: Examples\\n    - Last two characters: Numeric error code inside the section\\n\\n    For example, EX02 is the second codified error in the Examples section\\n    (which in this case is assigned to examples that do not pass the tests).\\n\\n    The error codes, their corresponding error messages, and the details on how\\n    they are validated, are not documented more than in the source code of this\\n    function.',\n",
       "  'code': 'def get_validation_data(doc):\\n    \"\"\"\\n    Validate the docstring.\\n\\n    Parameters\\n    ----------\\n    doc : Docstring\\n        A Docstring object with the given function name.\\n\\n    Returns\\n    -------\\n    tuple\\n        errors : list of tuple\\n            Errors occurred during validation.\\n        warnings : list of tuple\\n            Warnings occurred during validation.\\n        examples_errs : str\\n            Examples usage displayed along the error, otherwise empty string.\\n\\n    Notes\\n    -----\\n    The errors codes are defined as:\\n    - First two characters: Section where the error happens:\\n       * GL: Global (no section, like section ordering errors)\\n       * SS: Short summary\\n       * ES: Extended summary\\n       * PR: Parameters\\n       * RT: Returns\\n       * YD: Yields\\n       * RS: Raises\\n       * WN: Warns\\n       * SA: See Also\\n       * NT: Notes\\n       * RF: References\\n       * EX: Examples\\n    - Last two characters: Numeric error code inside the section\\n\\n    For example, EX02 is the second codified error in the Examples section\\n    (which in this case is assigned to examples that do not pass the tests).\\n\\n    The error codes, their corresponding error messages, and the details on how\\n    they are validated, are not documented more than in the source code of this\\n    function.\\n    \"\"\"\\n\\n    errs = []\\n    wrns = []\\n    if not doc.raw_doc:\\n        errs.append(error(\\'GL08\\'))\\n        return errs, wrns, \\'\\'\\n\\n    if doc.start_blank_lines != 1:\\n        errs.append(error(\\'GL01\\'))\\n    if doc.end_blank_lines != 1:\\n        errs.append(error(\\'GL02\\'))\\n    if doc.double_blank_lines:\\n        errs.append(error(\\'GL03\\'))\\n    mentioned_errs = doc.mentioned_private_classes\\n    if mentioned_errs:\\n        errs.append(error(\\'GL04\\',\\n                          mentioned_private_classes=\\', \\'.join(mentioned_errs)))\\n    for line in doc.raw_doc.splitlines():\\n        if re.match(\"^ *\\\\t\", line):\\n            errs.append(error(\\'GL05\\', line_with_tabs=line.lstrip()))\\n\\n    unexpected_sections = [section for section in doc.section_titles\\n                           if section not in ALLOWED_SECTIONS]\\n    for section in unexpected_sections:\\n        errs.append(error(\\'GL06\\',\\n                          section=section,\\n                          allowed_sections=\\', \\'.join(ALLOWED_SECTIONS)))\\n\\n    correct_order = [section for section in ALLOWED_SECTIONS\\n                     if section in doc.section_titles]\\n    if correct_order != doc.section_titles:\\n        errs.append(error(\\'GL07\\',\\n                          correct_sections=\\', \\'.join(correct_order)))\\n\\n    if (doc.deprecated_with_directive\\n            and not doc.extended_summary.startswith(\\'.. deprecated:: \\')):\\n        errs.append(error(\\'GL09\\'))\\n\\n    if not doc.summary:\\n        errs.append(error(\\'SS01\\'))\\n    else:\\n        if not doc.summary[0].isupper():\\n            errs.append(error(\\'SS02\\'))\\n        if doc.summary[-1] != \\'.\\':\\n            errs.append(error(\\'SS03\\'))\\n        if doc.summary != doc.summary.lstrip():\\n            errs.append(error(\\'SS04\\'))\\n        elif (doc.is_function_or_method\\n                and doc.summary.split(\\' \\')[0][-1] == \\'s\\'):\\n            errs.append(error(\\'SS05\\'))\\n        if doc.num_summary_lines > 1:\\n            errs.append(error(\\'SS06\\'))\\n\\n    if not doc.extended_summary:\\n        wrns.append((\\'ES01\\', \\'No extended summary found\\'))\\n\\n    # PR01: Parameters not documented\\n    # PR02: Unknown parameters\\n    # PR03: Wrong parameters order\\n    errs += doc.parameter_mismatches\\n\\n    for param in doc.doc_parameters:\\n        if not param.startswith(\"*\"):  # Check can ignore var / kwargs\\n            if not doc.parameter_type(param):\\n                if \\':\\' in param:\\n                    errs.append(error(\\'PR10\\',\\n                                      param_name=param.split(\\':\\')[0]))\\n                else:\\n                    errs.append(error(\\'PR04\\', param_name=param))\\n            else:\\n                if doc.parameter_type(param)[-1] == \\'.\\':\\n                    errs.append(error(\\'PR05\\', param_name=param))\\n                common_type_errors = [(\\'integer\\', \\'int\\'),\\n                                      (\\'boolean\\', \\'bool\\'),\\n                                      (\\'string\\', \\'str\\')]\\n                for wrong_type, right_type in common_type_errors:\\n                    if wrong_type in doc.parameter_type(param):\\n                        errs.append(error(\\'PR06\\',\\n                                          param_name=param,\\n                                          right_type=right_type,\\n                                          wrong_type=wrong_type))\\n        if not doc.parameter_desc(param):\\n            errs.append(error(\\'PR07\\', param_name=param))\\n        else:\\n            if not doc.parameter_desc(param)[0].isupper():\\n                errs.append(error(\\'PR08\\', param_name=param))\\n            if doc.parameter_desc(param)[-1] != \\'.\\':\\n                errs.append(error(\\'PR09\\', param_name=param))\\n\\n    if doc.is_function_or_method:\\n        if not doc.returns:\\n            if doc.method_returns_something:\\n                errs.append(error(\\'RT01\\'))\\n        else:\\n            if len(doc.returns) == 1 and doc.returns[0].name:\\n                errs.append(error(\\'RT02\\'))\\n            for name_or_type, type_, desc in doc.returns:\\n                if not desc:\\n                    errs.append(error(\\'RT03\\'))\\n                else:\\n                    desc = \\' \\'.join(desc)\\n                    if not desc[0].isupper():\\n                        errs.append(error(\\'RT04\\'))\\n                    if not desc.endswith(\\'.\\'):\\n                        errs.append(error(\\'RT05\\'))\\n\\n        if not doc.yields and \\'yield\\' in doc.method_source:\\n            errs.append(error(\\'YD01\\'))\\n\\n    if not doc.see_also:\\n        wrns.append(error(\\'SA01\\'))\\n    else:\\n        for rel_name, rel_desc in doc.see_also.items():\\n            if rel_desc:\\n                if not rel_desc.endswith(\\'.\\'):\\n                    errs.append(error(\\'SA02\\', reference_name=rel_name))\\n                if not rel_desc[0].isupper():\\n                    errs.append(error(\\'SA03\\', reference_name=rel_name))\\n            else:\\n                errs.append(error(\\'SA04\\', reference_name=rel_name))\\n            if rel_name.startswith(\\'pandas.\\'):\\n                errs.append(error(\\'SA05\\',\\n                                  reference_name=rel_name,\\n                                  right_reference=rel_name[len(\\'pandas.\\'):]))\\n\\n    examples_errs = \\'\\'\\n    if not doc.examples:\\n        wrns.append(error(\\'EX01\\'))\\n    else:\\n        examples_errs = doc.examples_errors\\n        if examples_errs:\\n            errs.append(error(\\'EX02\\', doctest_log=examples_errs))\\n        for err in doc.validate_pep8():\\n            errs.append(error(\\'EX03\\',\\n                              error_code=err.error_code,\\n                              error_message=err.message,\\n                              times_happening=\\' ({} times)\\'.format(err.count)\\n                                              if err.count > 1 else \\'\\'))\\n        examples_source_code = \\'\\'.join(doc.examples_source_code)\\n        for wrong_import in (\\'numpy\\', \\'pandas\\'):\\n            if \\'import {}\\'.format(wrong_import) in examples_source_code:\\n                errs.append(error(\\'EX04\\', imported_library=wrong_import))\\n    return errs, wrns, examples_errs',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ExcelWriter._value_with_fmt',\n",
       "  'docstring': 'Convert numpy types to Python types for the Excel writers.\\n\\n        Parameters\\n        ----------\\n        val : object\\n            Value to be written into cells\\n\\n        Returns\\n        -------\\n        Tuple with the first element being the converted value and the second\\n            being an optional format',\n",
       "  'code': 'def _value_with_fmt(self, val):\\n        \"\"\"Convert numpy types to Python types for the Excel writers.\\n\\n        Parameters\\n        ----------\\n        val : object\\n            Value to be written into cells\\n\\n        Returns\\n        -------\\n        Tuple with the first element being the converted value and the second\\n            being an optional format\\n        \"\"\"\\n        fmt = None\\n\\n        if is_integer(val):\\n            val = int(val)\\n        elif is_float(val):\\n            val = float(val)\\n        elif is_bool(val):\\n            val = bool(val)\\n        elif isinstance(val, datetime):\\n            fmt = self.datetime_format\\n        elif isinstance(val, date):\\n            fmt = self.date_format\\n        elif isinstance(val, timedelta):\\n            val = val.total_seconds() / float(86400)\\n            fmt = \\'0\\'\\n        else:\\n            val = compat.to_str(val)\\n\\n        return val, fmt',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_result_type_many',\n",
       "  'docstring': 'wrapper around numpy.result_type which overcomes the NPY_MAXARGS (32)\\n    argument limit',\n",
       "  'code': 'def _result_type_many(*arrays_and_dtypes):\\n    \"\"\" wrapper around numpy.result_type which overcomes the NPY_MAXARGS (32)\\n    argument limit \"\"\"\\n    try:\\n        return np.result_type(*arrays_and_dtypes)\\n    except ValueError:\\n        # we have > NPY_MAXARGS terms in our expression\\n        return reduce(np.result_type, arrays_and_dtypes)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'validate_argmin_with_skipna',\n",
       "  'docstring': \"If 'Series.argmin' is called via the 'numpy' library,\\n    the third parameter in its signature is 'out', which\\n    takes either an ndarray or 'None', so check if the\\n    'skipna' parameter is either an instance of ndarray or\\n    is None, since 'skipna' itself should be a boolean\",\n",
       "  'code': 'def validate_argmin_with_skipna(skipna, args, kwargs):\\n    \"\"\"\\n    If \\'Series.argmin\\' is called via the \\'numpy\\' library,\\n    the third parameter in its signature is \\'out\\', which\\n    takes either an ndarray or \\'None\\', so check if the\\n    \\'skipna\\' parameter is either an instance of ndarray or\\n    is None, since \\'skipna\\' itself should be a boolean\\n    \"\"\"\\n\\n    skipna, args = process_skipna(skipna, args)\\n    validate_argmin(args, kwargs)\\n    return skipna',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'validate_argmax_with_skipna',\n",
       "  'docstring': \"If 'Series.argmax' is called via the 'numpy' library,\\n    the third parameter in its signature is 'out', which\\n    takes either an ndarray or 'None', so check if the\\n    'skipna' parameter is either an instance of ndarray or\\n    is None, since 'skipna' itself should be a boolean\",\n",
       "  'code': 'def validate_argmax_with_skipna(skipna, args, kwargs):\\n    \"\"\"\\n    If \\'Series.argmax\\' is called via the \\'numpy\\' library,\\n    the third parameter in its signature is \\'out\\', which\\n    takes either an ndarray or \\'None\\', so check if the\\n    \\'skipna\\' parameter is either an instance of ndarray or\\n    is None, since \\'skipna\\' itself should be a boolean\\n    \"\"\"\\n\\n    skipna, args = process_skipna(skipna, args)\\n    validate_argmax(args, kwargs)\\n    return skipna',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'validate_argsort_with_ascending',\n",
       "  'docstring': \"If 'Categorical.argsort' is called via the 'numpy' library, the\\n    first parameter in its signature is 'axis', which takes either\\n    an integer or 'None', so check if the 'ascending' parameter has\\n    either integer type or is None, since 'ascending' itself should\\n    be a boolean\",\n",
       "  'code': 'def validate_argsort_with_ascending(ascending, args, kwargs):\\n    \"\"\"\\n    If \\'Categorical.argsort\\' is called via the \\'numpy\\' library, the\\n    first parameter in its signature is \\'axis\\', which takes either\\n    an integer or \\'None\\', so check if the \\'ascending\\' parameter has\\n    either integer type or is None, since \\'ascending\\' itself should\\n    be a boolean\\n    \"\"\"\\n\\n    if is_integer(ascending) or ascending is None:\\n        args = (ascending,) + args\\n        ascending = True\\n\\n    validate_argsort_kind(args, kwargs, max_fname_arg_count=3)\\n    return ascending',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'validate_clip_with_axis',\n",
       "  'docstring': \"If 'NDFrame.clip' is called via the numpy library, the third\\n    parameter in its signature is 'out', which can takes an ndarray,\\n    so check if the 'axis' parameter is an instance of ndarray, since\\n    'axis' itself should either be an integer or None\",\n",
       "  'code': 'def validate_clip_with_axis(axis, args, kwargs):\\n    \"\"\"\\n    If \\'NDFrame.clip\\' is called via the numpy library, the third\\n    parameter in its signature is \\'out\\', which can takes an ndarray,\\n    so check if the \\'axis\\' parameter is an instance of ndarray, since\\n    \\'axis\\' itself should either be an integer or None\\n    \"\"\"\\n\\n    if isinstance(axis, ndarray):\\n        args = (axis,) + args\\n        axis = None\\n\\n    validate_clip(args, kwargs)\\n    return axis',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'validate_cum_func_with_skipna',\n",
       "  'docstring': \"If this function is called via the 'numpy' library, the third\\n    parameter in its signature is 'dtype', which takes either a\\n    'numpy' dtype or 'None', so check if the 'skipna' parameter is\\n    a boolean or not\",\n",
       "  'code': 'def validate_cum_func_with_skipna(skipna, args, kwargs, name):\\n    \"\"\"\\n    If this function is called via the \\'numpy\\' library, the third\\n    parameter in its signature is \\'dtype\\', which takes either a\\n    \\'numpy\\' dtype or \\'None\\', so check if the \\'skipna\\' parameter is\\n    a boolean or not\\n    \"\"\"\\n    if not is_bool(skipna):\\n        args = (skipna,) + args\\n        skipna = True\\n\\n    validate_cum_func(args, kwargs, fname=name)\\n    return skipna',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'validate_take_with_convert',\n",
       "  'docstring': \"If this function is called via the 'numpy' library, the third\\n    parameter in its signature is 'axis', which takes either an\\n    ndarray or 'None', so check if the 'convert' parameter is either\\n    an instance of ndarray or is None\",\n",
       "  'code': 'def validate_take_with_convert(convert, args, kwargs):\\n    \"\"\"\\n    If this function is called via the \\'numpy\\' library, the third\\n    parameter in its signature is \\'axis\\', which takes either an\\n    ndarray or \\'None\\', so check if the \\'convert\\' parameter is either\\n    an instance of ndarray or is None\\n    \"\"\"\\n\\n    if isinstance(convert, ndarray) or convert is None:\\n        args = (convert,) + args\\n        convert = True\\n\\n    validate_take(args, kwargs, max_fname_arg_count=3, method=\\'both\\')\\n    return convert',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'validate_groupby_func',\n",
       "  'docstring': \"'args' and 'kwargs' should be empty, except for allowed\\n    kwargs because all of\\n    their necessary parameters are explicitly listed in\\n    the function signature\",\n",
       "  'code': 'def validate_groupby_func(name, args, kwargs, allowed=None):\\n    \"\"\"\\n    \\'args\\' and \\'kwargs\\' should be empty, except for allowed\\n    kwargs because all of\\n    their necessary parameters are explicitly listed in\\n    the function signature\\n    \"\"\"\\n    if allowed is None:\\n        allowed = []\\n\\n    kwargs = set(kwargs) - set(allowed)\\n\\n    if len(args) + len(kwargs) > 0:\\n        raise UnsupportedFunctionCall((\\n            \"numpy operations are not valid \"\\n            \"with groupby. Use .groupby(...).\"\\n            \"{func}() instead\".format(func=name)))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'validate_resampler_func',\n",
       "  'docstring': \"'args' and 'kwargs' should be empty because all of\\n    their necessary parameters are explicitly listed in\\n    the function signature\",\n",
       "  'code': 'def validate_resampler_func(method, args, kwargs):\\n    \"\"\"\\n    \\'args\\' and \\'kwargs\\' should be empty because all of\\n    their necessary parameters are explicitly listed in\\n    the function signature\\n    \"\"\"\\n    if len(args) + len(kwargs) > 0:\\n        if method in RESAMPLER_NUMPY_OPS:\\n            raise UnsupportedFunctionCall((\\n                \"numpy operations are not valid \"\\n                \"with resample. Use .resample(...).\"\\n                \"{func}() instead\".format(func=method)))\\n        else:\\n            raise TypeError(\"too many arguments passed in\")',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'c2f',\n",
       "  'docstring': 'Convert strings to complex number instance with specified numpy type.',\n",
       "  'code': 'def c2f(r, i, ctype_name):\\n    \"\"\"\\n    Convert strings to complex number instance with specified numpy type.\\n    \"\"\"\\n\\n    ftype = c2f_dict[ctype_name]\\n    return np.typeDict[ctype_name](ftype(r) + 1j * ftype(i))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'convert',\n",
       "  'docstring': 'convert the numpy values to a list',\n",
       "  'code': 'def convert(values):\\n    \"\"\" convert the numpy values to a list \"\"\"\\n\\n    dtype = values.dtype\\n\\n    if is_categorical_dtype(values):\\n        return values\\n\\n    elif is_object_dtype(dtype):\\n        return values.ravel().tolist()\\n\\n    if needs_i8_conversion(dtype):\\n        values = values.view(\\'i8\\')\\n    v = values.ravel()\\n\\n    if compressor == \\'zlib\\':\\n        _check_zlib()\\n\\n        # return string arrays like they are\\n        if dtype == np.object_:\\n            return v.tolist()\\n\\n        # convert to a bytes array\\n        v = v.tostring()\\n        return ExtType(0, zlib.compress(v))\\n\\n    elif compressor == \\'blosc\\':\\n        _check_blosc()\\n\\n        # return string arrays like they are\\n        if dtype == np.object_:\\n            return v.tolist()\\n\\n        # convert to a bytes array\\n        v = v.tostring()\\n        return ExtType(0, blosc.compress(v, typesize=dtype.itemsize))\\n\\n    # ndarray (on original dtype)\\n    return ExtType(0, v.tostring())',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'decode',\n",
       "  'docstring': 'Decoder for deserializing numpy data types.',\n",
       "  'code': 'def decode(obj):\\n    \"\"\"\\n    Decoder for deserializing numpy data types.\\n    \"\"\"\\n\\n    typ = obj.get(\\'typ\\')\\n    if typ is None:\\n        return obj\\n    elif typ == \\'timestamp\\':\\n        freq = obj[\\'freq\\'] if \\'freq\\' in obj else obj[\\'offset\\']\\n        return Timestamp(obj[\\'value\\'], tz=obj[\\'tz\\'], freq=freq)\\n    elif typ == \\'nat\\':\\n        return NaT\\n    elif typ == \\'period\\':\\n        return Period(ordinal=obj[\\'ordinal\\'], freq=obj[\\'freq\\'])\\n    elif typ == \\'index\\':\\n        dtype = dtype_for(obj[\\'dtype\\'])\\n        data = unconvert(obj[\\'data\\'], dtype,\\n                         obj.get(\\'compress\\'))\\n        return Index(data, dtype=dtype, name=obj[\\'name\\'])\\n    elif typ == \\'range_index\\':\\n        return RangeIndex(obj[\\'start\\'],\\n                          obj[\\'stop\\'],\\n                          obj[\\'step\\'],\\n                          name=obj[\\'name\\'])\\n    elif typ == \\'multi_index\\':\\n        dtype = dtype_for(obj[\\'dtype\\'])\\n        data = unconvert(obj[\\'data\\'], dtype,\\n                         obj.get(\\'compress\\'))\\n        data = [tuple(x) for x in data]\\n        return MultiIndex.from_tuples(data, names=obj[\\'names\\'])\\n    elif typ == \\'period_index\\':\\n        data = unconvert(obj[\\'data\\'], np.int64, obj.get(\\'compress\\'))\\n        d = dict(name=obj[\\'name\\'], freq=obj[\\'freq\\'])\\n        freq = d.pop(\\'freq\\', None)\\n        return PeriodIndex(PeriodArray(data, freq), **d)\\n\\n    elif typ == \\'datetime_index\\':\\n        data = unconvert(obj[\\'data\\'], np.int64, obj.get(\\'compress\\'))\\n        d = dict(name=obj[\\'name\\'], freq=obj[\\'freq\\'])\\n        result = DatetimeIndex(data, **d)\\n        tz = obj[\\'tz\\']\\n\\n        # reverse tz conversion\\n        if tz is not None:\\n            result = result.tz_localize(\\'UTC\\').tz_convert(tz)\\n        return result\\n\\n    elif typ in (\\'interval_index\\', \\'interval_array\\'):\\n        return globals()[obj[\\'klass\\']].from_arrays(obj[\\'left\\'],\\n                                                   obj[\\'right\\'],\\n                                                   obj[\\'closed\\'],\\n                                                   name=obj[\\'name\\'])\\n    elif typ == \\'category\\':\\n        from_codes = globals()[obj[\\'klass\\']].from_codes\\n        return from_codes(codes=obj[\\'codes\\'],\\n                          categories=obj[\\'categories\\'],\\n                          ordered=obj[\\'ordered\\'])\\n\\n    elif typ == \\'interval\\':\\n        return Interval(obj[\\'left\\'], obj[\\'right\\'], obj[\\'closed\\'])\\n    elif typ == \\'series\\':\\n        dtype = dtype_for(obj[\\'dtype\\'])\\n        pd_dtype = pandas_dtype(dtype)\\n\\n        index = obj[\\'index\\']\\n        result = Series(unconvert(obj[\\'data\\'], dtype, obj[\\'compress\\']),\\n                        index=index,\\n                        dtype=pd_dtype,\\n                        name=obj[\\'name\\'])\\n        return result\\n\\n    elif typ == \\'block_manager\\':\\n        axes = obj[\\'axes\\']\\n\\n        def create_block(b):\\n            values = _safe_reshape(unconvert(\\n                b[\\'values\\'], dtype_for(b[\\'dtype\\']),\\n                b[\\'compress\\']), b[\\'shape\\'])\\n\\n            # locs handles duplicate column names, and should be used instead\\n            # of items; see GH 9618\\n            if \\'locs\\' in b:\\n                placement = b[\\'locs\\']\\n            else:\\n                placement = axes[0].get_indexer(b[\\'items\\'])\\n\\n            if is_datetime64tz_dtype(b[\\'dtype\\']):\\n                assert isinstance(values, np.ndarray), type(values)\\n                assert values.dtype == \\'M8[ns]\\', values.dtype\\n                values = DatetimeArray(values, dtype=b[\\'dtype\\'])\\n\\n            return make_block(values=values,\\n                              klass=getattr(internals, b[\\'klass\\']),\\n                              placement=placement,\\n                              dtype=b[\\'dtype\\'])\\n\\n        blocks = [create_block(b) for b in obj[\\'blocks\\']]\\n        return globals()[obj[\\'klass\\']](BlockManager(blocks, axes))\\n    elif typ == \\'datetime\\':\\n        return parse(obj[\\'data\\'])\\n    elif typ == \\'datetime64\\':\\n        return np.datetime64(parse(obj[\\'data\\']))\\n    elif typ == \\'date\\':\\n        return parse(obj[\\'data\\']).date()\\n    elif typ == \\'timedelta\\':\\n        return timedelta(*obj[\\'data\\'])\\n    elif typ == \\'timedelta64\\':\\n        return np.timedelta64(int(obj[\\'data\\']))\\n    # elif typ == \\'sparse_series\\':\\n    #    dtype = dtype_for(obj[\\'dtype\\'])\\n    #    return SparseSeries(\\n    #        unconvert(obj[\\'sp_values\\'], dtype, obj[\\'compress\\']),\\n    #        sparse_index=obj[\\'sp_index\\'], index=obj[\\'index\\'],\\n    #        fill_value=obj[\\'fill_value\\'], kind=obj[\\'kind\\'], name=obj[\\'name\\'])\\n    # elif typ == \\'sparse_dataframe\\':\\n    #    return SparseDataFrame(\\n    #        obj[\\'data\\'], columns=obj[\\'columns\\'],\\n    #        default_fill_value=obj[\\'default_fill_value\\'],\\n    #        default_kind=obj[\\'default_kind\\']\\n    #    )\\n    # elif typ == \\'sparse_panel\\':\\n    #    return SparsePanel(\\n    #        obj[\\'data\\'], items=obj[\\'items\\'],\\n    #        default_fill_value=obj[\\'default_fill_value\\'],\\n    #        default_kind=obj[\\'default_kind\\'])\\n    elif typ == \\'block_index\\':\\n        return globals()[obj[\\'klass\\']](obj[\\'length\\'], obj[\\'blocs\\'],\\n                                       obj[\\'blengths\\'])\\n    elif typ == \\'int_index\\':\\n        return globals()[obj[\\'klass\\']](obj[\\'length\\'], obj[\\'indices\\'])\\n    elif typ == \\'ndarray\\':\\n        return unconvert(obj[\\'data\\'], np.typeDict[obj[\\'dtype\\']],\\n                         obj.get(\\'compress\\')).reshape(obj[\\'shape\\'])\\n    elif typ == \\'np_scalar\\':\\n        if obj.get(\\'sub_typ\\') == \\'np_complex\\':\\n            return c2f(obj[\\'real\\'], obj[\\'imag\\'], obj[\\'dtype\\'])\\n        else:\\n            dtype = dtype_for(obj[\\'dtype\\'])\\n            try:\\n                return dtype(obj[\\'data\\'])\\n            except (ValueError, TypeError):\\n                return dtype.type(obj[\\'data\\'])\\n    elif typ == \\'np_complex\\':\\n        return complex(obj[\\'real\\'] + \\'+\\' + obj[\\'imag\\'] + \\'j\\')\\n    elif isinstance(obj, (dict, list, set)):\\n        return obj\\n    else:\\n        return obj',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'read_json',\n",
       "  'docstring': 'Convert a JSON string to pandas object.\\n\\n    Parameters\\n    ----------\\n    path_or_buf : a valid JSON string or file-like, default: None\\n        The string could be a URL. Valid URL schemes include http, ftp, s3,\\n        gcs, and file. For file URLs, a host is expected. For instance, a local\\n        file could be ``file://localhost/path/to/table.json``\\n\\n    orient : string,\\n        Indication of expected JSON string format.\\n        Compatible JSON strings can be produced by ``to_json()`` with a\\n        corresponding orient value.\\n        The set of possible orients is:\\n\\n        - ``\\'split\\'`` : dict like\\n          ``{index -> [index], columns -> [columns], data -> [values]}``\\n        - ``\\'records\\'`` : list like\\n          ``[{column -> value}, ... , {column -> value}]``\\n        - ``\\'index\\'`` : dict like ``{index -> {column -> value}}``\\n        - ``\\'columns\\'`` : dict like ``{column -> {index -> value}}``\\n        - ``\\'values\\'`` : just the values array\\n\\n        The allowed and default values depend on the value\\n        of the `typ` parameter.\\n\\n        * when ``typ == \\'series\\'``,\\n\\n          - allowed orients are ``{\\'split\\',\\'records\\',\\'index\\'}``\\n          - default is ``\\'index\\'``\\n          - The Series index must be unique for orient ``\\'index\\'``.\\n\\n        * when ``typ == \\'frame\\'``,\\n\\n          - allowed orients are ``{\\'split\\',\\'records\\',\\'index\\',\\n            \\'columns\\',\\'values\\', \\'table\\'}``\\n          - default is ``\\'columns\\'``\\n          - The DataFrame index must be unique for orients ``\\'index\\'`` and\\n            ``\\'columns\\'``.\\n          - The DataFrame columns must be unique for orients ``\\'index\\'``,\\n            ``\\'columns\\'``, and ``\\'records\\'``.\\n\\n        .. versionadded:: 0.23.0\\n           \\'table\\' as an allowed value for the ``orient`` argument\\n\\n    typ : type of object to recover (series or frame), default \\'frame\\'\\n    dtype : boolean or dict, default None\\n        If True, infer dtypes; if a dict of column to dtype, then use those;\\n        if False, then don\\'t infer dtypes at all, applies only to the data.\\n\\n        For all ``orient`` values except ``\\'table\\'``, default is True.\\n\\n        .. versionchanged:: 0.25.0\\n\\n           Not applicable for ``orient=\\'table\\'``.\\n\\n    convert_axes : boolean, default None\\n        Try to convert the axes to the proper dtypes.\\n\\n        For all ``orient`` values except ``\\'table\\'``, default is True.\\n\\n        .. versionchanged:: 0.25.0\\n\\n           Not applicable for ``orient=\\'table\\'``.\\n\\n    convert_dates : boolean, default True\\n        List of columns to parse for dates; If True, then try to parse\\n        datelike columns default is True; a column label is datelike if\\n\\n        * it ends with ``\\'_at\\'``,\\n\\n        * it ends with ``\\'_time\\'``,\\n\\n        * it begins with ``\\'timestamp\\'``,\\n\\n        * it is ``\\'modified\\'``, or\\n\\n        * it is ``\\'date\\'``\\n\\n    keep_default_dates : boolean, default True\\n        If parsing dates, then parse the default datelike columns\\n    numpy : boolean, default False\\n        Direct decoding to numpy arrays. Supports numeric data only, but\\n        non-numeric column and index labels are supported. Note also that the\\n        JSON ordering MUST be the same for each term if numpy=True.\\n    precise_float : boolean, default False\\n        Set to enable usage of higher precision (strtod) function when\\n        decoding string to double values. Default (False) is to use fast but\\n        less precise builtin functionality\\n    date_unit : string, default None\\n        The timestamp unit to detect if converting dates. The default behaviour\\n        is to try and detect the correct precision, but if this is not desired\\n        then pass one of \\'s\\', \\'ms\\', \\'us\\' or \\'ns\\' to force parsing only seconds,\\n        milliseconds, microseconds or nanoseconds respectively.\\n    encoding : str, default is \\'utf-8\\'\\n        The encoding to use to decode py3 bytes.\\n\\n        .. versionadded:: 0.19.0\\n\\n    lines : boolean, default False\\n        Read the file as a json object per line.\\n\\n        .. versionadded:: 0.19.0\\n\\n    chunksize : integer, default None\\n        Return JsonReader object for iteration.\\n        See the `line-delimted json docs\\n        <http://pandas.pydata.org/pandas-docs/stable/io.html#io-jsonl>`_\\n        for more information on ``chunksize``.\\n        This can only be passed if `lines=True`.\\n        If this is None, the file will be read into memory all at once.\\n\\n        .. versionadded:: 0.21.0\\n\\n    compression : {\\'infer\\', \\'gzip\\', \\'bz2\\', \\'zip\\', \\'xz\\', None}, default \\'infer\\'\\n        For on-the-fly decompression of on-disk data. If \\'infer\\', then use\\n        gzip, bz2, zip or xz if path_or_buf is a string ending in\\n        \\'.gz\\', \\'.bz2\\', \\'.zip\\', or \\'xz\\', respectively, and no decompression\\n        otherwise. If using \\'zip\\', the ZIP file must contain only one data\\n        file to be read in. Set to None for no decompression.\\n\\n        .. versionadded:: 0.21.0\\n\\n    Returns\\n    -------\\n    result : Series or DataFrame, depending on the value of `typ`.\\n\\n    See Also\\n    --------\\n    DataFrame.to_json\\n\\n    Notes\\n    -----\\n    Specific to ``orient=\\'table\\'``, if a :class:`DataFrame` with a literal\\n    :class:`Index` name of `index` gets written with :func:`to_json`, the\\n    subsequent read operation will incorrectly set the :class:`Index` name to\\n    ``None``. This is because `index` is also used by :func:`DataFrame.to_json`\\n    to denote a missing :class:`Index` name, and the subsequent\\n    :func:`read_json` operation cannot distinguish between the two. The same\\n    limitation is encountered with a :class:`MultiIndex` and any names\\n    beginning with ``\\'level_\\'``.\\n\\n    Examples\\n    --------\\n\\n    >>> df = pd.DataFrame([[\\'a\\', \\'b\\'], [\\'c\\', \\'d\\']],\\n    ...                   index=[\\'row 1\\', \\'row 2\\'],\\n    ...                   columns=[\\'col 1\\', \\'col 2\\'])\\n\\n    Encoding/decoding a Dataframe using ``\\'split\\'`` formatted JSON:\\n\\n    >>> df.to_json(orient=\\'split\\')\\n    \\'{\"columns\":[\"col 1\",\"col 2\"],\\n      \"index\":[\"row 1\",\"row 2\"],\\n      \"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}\\'\\n    >>> pd.read_json(_, orient=\\'split\\')\\n          col 1 col 2\\n    row 1     a     b\\n    row 2     c     d\\n\\n    Encoding/decoding a Dataframe using ``\\'index\\'`` formatted JSON:\\n\\n    >>> df.to_json(orient=\\'index\\')\\n    \\'{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}\\'\\n    >>> pd.read_json(_, orient=\\'index\\')\\n          col 1 col 2\\n    row 1     a     b\\n    row 2     c     d\\n\\n    Encoding/decoding a Dataframe using ``\\'records\\'`` formatted JSON.\\n    Note that index labels are not preserved with this encoding.\\n\\n    >>> df.to_json(orient=\\'records\\')\\n    \\'[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]\\'\\n    >>> pd.read_json(_, orient=\\'records\\')\\n      col 1 col 2\\n    0     a     b\\n    1     c     d\\n\\n    Encoding with Table Schema\\n\\n    >>> df.to_json(orient=\\'table\\')\\n    \\'{\"schema\": {\"fields\": [{\"name\": \"index\", \"type\": \"string\"},\\n                            {\"name\": \"col 1\", \"type\": \"string\"},\\n                            {\"name\": \"col 2\", \"type\": \"string\"}],\\n                    \"primaryKey\": \"index\",\\n                    \"pandas_version\": \"0.20.0\"},\\n        \"data\": [{\"index\": \"row 1\", \"col 1\": \"a\", \"col 2\": \"b\"},\\n                {\"index\": \"row 2\", \"col 1\": \"c\", \"col 2\": \"d\"}]}\\'',\n",
       "  'code': 'def read_json(path_or_buf=None, orient=None, typ=\\'frame\\', dtype=None,\\n              convert_axes=None, convert_dates=True, keep_default_dates=True,\\n              numpy=False, precise_float=False, date_unit=None, encoding=None,\\n              lines=False, chunksize=None, compression=\\'infer\\'):\\n    \"\"\"\\n    Convert a JSON string to pandas object.\\n\\n    Parameters\\n    ----------\\n    path_or_buf : a valid JSON string or file-like, default: None\\n        The string could be a URL. Valid URL schemes include http, ftp, s3,\\n        gcs, and file. For file URLs, a host is expected. For instance, a local\\n        file could be ``file://localhost/path/to/table.json``\\n\\n    orient : string,\\n        Indication of expected JSON string format.\\n        Compatible JSON strings can be produced by ``to_json()`` with a\\n        corresponding orient value.\\n        The set of possible orients is:\\n\\n        - ``\\'split\\'`` : dict like\\n          ``{index -> [index], columns -> [columns], data -> [values]}``\\n        - ``\\'records\\'`` : list like\\n          ``[{column -> value}, ... , {column -> value}]``\\n        - ``\\'index\\'`` : dict like ``{index -> {column -> value}}``\\n        - ``\\'columns\\'`` : dict like ``{column -> {index -> value}}``\\n        - ``\\'values\\'`` : just the values array\\n\\n        The allowed and default values depend on the value\\n        of the `typ` parameter.\\n\\n        * when ``typ == \\'series\\'``,\\n\\n          - allowed orients are ``{\\'split\\',\\'records\\',\\'index\\'}``\\n          - default is ``\\'index\\'``\\n          - The Series index must be unique for orient ``\\'index\\'``.\\n\\n        * when ``typ == \\'frame\\'``,\\n\\n          - allowed orients are ``{\\'split\\',\\'records\\',\\'index\\',\\n            \\'columns\\',\\'values\\', \\'table\\'}``\\n          - default is ``\\'columns\\'``\\n          - The DataFrame index must be unique for orients ``\\'index\\'`` and\\n            ``\\'columns\\'``.\\n          - The DataFrame columns must be unique for orients ``\\'index\\'``,\\n            ``\\'columns\\'``, and ``\\'records\\'``.\\n\\n        .. versionadded:: 0.23.0\\n           \\'table\\' as an allowed value for the ``orient`` argument\\n\\n    typ : type of object to recover (series or frame), default \\'frame\\'\\n    dtype : boolean or dict, default None\\n        If True, infer dtypes; if a dict of column to dtype, then use those;\\n        if False, then don\\'t infer dtypes at all, applies only to the data.\\n\\n        For all ``orient`` values except ``\\'table\\'``, default is True.\\n\\n        .. versionchanged:: 0.25.0\\n\\n           Not applicable for ``orient=\\'table\\'``.\\n\\n    convert_axes : boolean, default None\\n        Try to convert the axes to the proper dtypes.\\n\\n        For all ``orient`` values except ``\\'table\\'``, default is True.\\n\\n        .. versionchanged:: 0.25.0\\n\\n           Not applicable for ``orient=\\'table\\'``.\\n\\n    convert_dates : boolean, default True\\n        List of columns to parse for dates; If True, then try to parse\\n        datelike columns default is True; a column label is datelike if\\n\\n        * it ends with ``\\'_at\\'``,\\n\\n        * it ends with ``\\'_time\\'``,\\n\\n        * it begins with ``\\'timestamp\\'``,\\n\\n        * it is ``\\'modified\\'``, or\\n\\n        * it is ``\\'date\\'``\\n\\n    keep_default_dates : boolean, default True\\n        If parsing dates, then parse the default datelike columns\\n    numpy : boolean, default False\\n        Direct decoding to numpy arrays. Supports numeric data only, but\\n        non-numeric column and index labels are supported. Note also that the\\n        JSON ordering MUST be the same for each term if numpy=True.\\n    precise_float : boolean, default False\\n        Set to enable usage of higher precision (strtod) function when\\n        decoding string to double values. Default (False) is to use fast but\\n        less precise builtin functionality\\n    date_unit : string, default None\\n        The timestamp unit to detect if converting dates. The default behaviour\\n        is to try and detect the correct precision, but if this is not desired\\n        then pass one of \\'s\\', \\'ms\\', \\'us\\' or \\'ns\\' to force parsing only seconds,\\n        milliseconds, microseconds or nanoseconds respectively.\\n    encoding : str, default is \\'utf-8\\'\\n        The encoding to use to decode py3 bytes.\\n\\n        .. versionadded:: 0.19.0\\n\\n    lines : boolean, default False\\n        Read the file as a json object per line.\\n\\n        .. versionadded:: 0.19.0\\n\\n    chunksize : integer, default None\\n        Return JsonReader object for iteration.\\n        See the `line-delimted json docs\\n        <http://pandas.pydata.org/pandas-docs/stable/io.html#io-jsonl>`_\\n        for more information on ``chunksize``.\\n        This can only be passed if `lines=True`.\\n        If this is None, the file will be read into memory all at once.\\n\\n        .. versionadded:: 0.21.0\\n\\n    compression : {\\'infer\\', \\'gzip\\', \\'bz2\\', \\'zip\\', \\'xz\\', None}, default \\'infer\\'\\n        For on-the-fly decompression of on-disk data. If \\'infer\\', then use\\n        gzip, bz2, zip or xz if path_or_buf is a string ending in\\n        \\'.gz\\', \\'.bz2\\', \\'.zip\\', or \\'xz\\', respectively, and no decompression\\n        otherwise. If using \\'zip\\', the ZIP file must contain only one data\\n        file to be read in. Set to None for no decompression.\\n\\n        .. versionadded:: 0.21.0\\n\\n    Returns\\n    -------\\n    result : Series or DataFrame, depending on the value of `typ`.\\n\\n    See Also\\n    --------\\n    DataFrame.to_json\\n\\n    Notes\\n    -----\\n    Specific to ``orient=\\'table\\'``, if a :class:`DataFrame` with a literal\\n    :class:`Index` name of `index` gets written with :func:`to_json`, the\\n    subsequent read operation will incorrectly set the :class:`Index` name to\\n    ``None``. This is because `index` is also used by :func:`DataFrame.to_json`\\n    to denote a missing :class:`Index` name, and the subsequent\\n    :func:`read_json` operation cannot distinguish between the two. The same\\n    limitation is encountered with a :class:`MultiIndex` and any names\\n    beginning with ``\\'level_\\'``.\\n\\n    Examples\\n    --------\\n\\n    >>> df = pd.DataFrame([[\\'a\\', \\'b\\'], [\\'c\\', \\'d\\']],\\n    ...                   index=[\\'row 1\\', \\'row 2\\'],\\n    ...                   columns=[\\'col 1\\', \\'col 2\\'])\\n\\n    Encoding/decoding a Dataframe using ``\\'split\\'`` formatted JSON:\\n\\n    >>> df.to_json(orient=\\'split\\')\\n    \\'{\"columns\":[\"col 1\",\"col 2\"],\\n      \"index\":[\"row 1\",\"row 2\"],\\n      \"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}\\'\\n    >>> pd.read_json(_, orient=\\'split\\')\\n          col 1 col 2\\n    row 1     a     b\\n    row 2     c     d\\n\\n    Encoding/decoding a Dataframe using ``\\'index\\'`` formatted JSON:\\n\\n    >>> df.to_json(orient=\\'index\\')\\n    \\'{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}\\'\\n    >>> pd.read_json(_, orient=\\'index\\')\\n          col 1 col 2\\n    row 1     a     b\\n    row 2     c     d\\n\\n    Encoding/decoding a Dataframe using ``\\'records\\'`` formatted JSON.\\n    Note that index labels are not preserved with this encoding.\\n\\n    >>> df.to_json(orient=\\'records\\')\\n    \\'[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]\\'\\n    >>> pd.read_json(_, orient=\\'records\\')\\n      col 1 col 2\\n    0     a     b\\n    1     c     d\\n\\n    Encoding with Table Schema\\n\\n    >>> df.to_json(orient=\\'table\\')\\n    \\'{\"schema\": {\"fields\": [{\"name\": \"index\", \"type\": \"string\"},\\n                            {\"name\": \"col 1\", \"type\": \"string\"},\\n                            {\"name\": \"col 2\", \"type\": \"string\"}],\\n                    \"primaryKey\": \"index\",\\n                    \"pandas_version\": \"0.20.0\"},\\n        \"data\": [{\"index\": \"row 1\", \"col 1\": \"a\", \"col 2\": \"b\"},\\n                {\"index\": \"row 2\", \"col 1\": \"c\", \"col 2\": \"d\"}]}\\'\\n    \"\"\"\\n\\n    if orient == \\'table\\' and dtype:\\n        raise ValueError(\"cannot pass both dtype and orient=\\'table\\'\")\\n    if orient == \\'table\\' and convert_axes:\\n        raise ValueError(\"cannot pass both convert_axes and orient=\\'table\\'\")\\n\\n    if dtype is None and orient != \\'table\\':\\n        dtype = True\\n    if convert_axes is None and orient != \\'table\\':\\n        convert_axes = True\\n\\n    compression = _infer_compression(path_or_buf, compression)\\n    filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(\\n        path_or_buf, encoding=encoding, compression=compression,\\n    )\\n\\n    json_reader = JsonReader(\\n        filepath_or_buffer, orient=orient, typ=typ, dtype=dtype,\\n        convert_axes=convert_axes, convert_dates=convert_dates,\\n        keep_default_dates=keep_default_dates, numpy=numpy,\\n        precise_float=precise_float, date_unit=date_unit, encoding=encoding,\\n        lines=lines, chunksize=chunksize, compression=compression,\\n    )\\n\\n    if chunksize:\\n        return json_reader\\n\\n    result = json_reader.read()\\n    if should_close:\\n        try:\\n            filepath_or_buffer.close()\\n        except:  # noqa: flake8\\n            pass\\n    return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'JsonReader._get_object_parser',\n",
       "  'docstring': 'Parses a json document into a pandas object.',\n",
       "  'code': 'def _get_object_parser(self, json):\\n        \"\"\"\\n        Parses a json document into a pandas object.\\n        \"\"\"\\n        typ = self.typ\\n        dtype = self.dtype\\n        kwargs = {\\n            \"orient\": self.orient, \"dtype\": self.dtype,\\n            \"convert_axes\": self.convert_axes,\\n            \"convert_dates\": self.convert_dates,\\n            \"keep_default_dates\": self.keep_default_dates, \"numpy\": self.numpy,\\n            \"precise_float\": self.precise_float, \"date_unit\": self.date_unit\\n        }\\n        obj = None\\n        if typ == \\'frame\\':\\n            obj = FrameParser(json, **kwargs).parse()\\n\\n        if typ == \\'series\\' or obj is None:\\n            if not isinstance(dtype, bool):\\n                kwargs[\\'dtype\\'] = dtype\\n            obj = SeriesParser(json, **kwargs).parse()\\n\\n        return obj',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'FloatArrayFormatter.get_result_as_array',\n",
       "  'docstring': 'Returns the float values converted into strings using\\n        the parameters given at initialisation, as a numpy array',\n",
       "  'code': 'def get_result_as_array(self):\\n        \"\"\"\\n        Returns the float values converted into strings using\\n        the parameters given at initialisation, as a numpy array\\n        \"\"\"\\n\\n        if self.formatter is not None:\\n            return np.array([self.formatter(x) for x in self.values])\\n\\n        if self.fixed_width:\\n            threshold = get_option(\"display.chop_threshold\")\\n        else:\\n            threshold = None\\n\\n        # if we have a fixed_width, we\\'ll need to try different float_format\\n        def format_values_with(float_format):\\n            formatter = self._value_formatter(float_format, threshold)\\n\\n            # default formatter leaves a space to the left when formatting\\n            # floats, must be consistent for left-justifying NaNs (GH #25061)\\n            if self.justify == \\'left\\':\\n                na_rep = \\' \\' + self.na_rep\\n            else:\\n                na_rep = self.na_rep\\n\\n            # separate the wheat from the chaff\\n            values = self.values\\n            is_complex = is_complex_dtype(values)\\n            mask = isna(values)\\n            if hasattr(values, \\'to_dense\\'):  # sparse numpy ndarray\\n                values = values.to_dense()\\n            values = np.array(values, dtype=\\'object\\')\\n            values[mask] = na_rep\\n            imask = (~mask).ravel()\\n            values.flat[imask] = np.array([formatter(val)\\n                                           for val in values.ravel()[imask]])\\n\\n            if self.fixed_width:\\n                if is_complex:\\n                    return _trim_zeros_complex(values, na_rep)\\n                else:\\n                    return _trim_zeros_float(values, na_rep)\\n\\n            return values\\n\\n        # There is a special default string when we are fixed-width\\n        # The default is otherwise to use str instead of a formatting string\\n        if self.float_format is None:\\n            if self.fixed_width:\\n                float_format = partial(\\'{value: .{digits:d}f}\\'.format,\\n                                       digits=self.digits)\\n            else:\\n                float_format = self.float_format\\n        else:\\n            float_format = lambda value: self.float_format % value\\n\\n        formatted_values = format_values_with(float_format)\\n\\n        if not self.fixed_width:\\n            return formatted_values\\n\\n        # we need do convert to engineering format if some values are too small\\n        # and would appear as 0, or if some values are too big and take too\\n        # much space\\n\\n        if len(formatted_values) > 0:\\n            maxlen = max(len(x) for x in formatted_values)\\n            too_long = maxlen > self.digits + 6\\n        else:\\n            too_long = False\\n\\n        with np.errstate(invalid=\\'ignore\\'):\\n            abs_vals = np.abs(self.values)\\n            # this is pretty arbitrary for now\\n            # large values: more that 8 characters including decimal symbol\\n            # and first digit, hence > 1e6\\n            has_large_values = (abs_vals > 1e6).any()\\n            has_small_values = ((abs_vals < 10**(-self.digits)) &\\n                                (abs_vals > 0)).any()\\n\\n        if has_small_values or (too_long and has_large_values):\\n            float_format = partial(\\'{value: .{digits:d}e}\\'.format,\\n                                   digits=self.digits)\\n            formatted_values = format_values_with(float_format)\\n\\n        return formatted_values',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'is_dtype_union_equal',\n",
       "  'docstring': 'Check whether two arrays have compatible dtypes to do a union.\\n    numpy types are checked with ``is_dtype_equal``. Extension types are\\n    checked separately.\\n\\n    Parameters\\n    ----------\\n    source : The first dtype to compare\\n    target : The second dtype to compare\\n\\n    Returns\\n    ----------\\n    boolean\\n        Whether or not the two dtypes are equal.\\n\\n    >>> is_dtype_equal(\"int\", int)\\n    True\\n\\n    >>> is_dtype_equal(CategoricalDtype([\\'a\\', \\'b\\'],\\n    ...                CategoricalDtype([\\'b\\', \\'c\\']))\\n    True\\n\\n    >>> is_dtype_equal(CategoricalDtype([\\'a\\', \\'b\\'],\\n    ...                CategoricalDtype([\\'b\\', \\'c\\'], ordered=True))\\n    False',\n",
       "  'code': 'def is_dtype_union_equal(source, target):\\n    \"\"\"\\n    Check whether two arrays have compatible dtypes to do a union.\\n    numpy types are checked with ``is_dtype_equal``. Extension types are\\n    checked separately.\\n\\n    Parameters\\n    ----------\\n    source : The first dtype to compare\\n    target : The second dtype to compare\\n\\n    Returns\\n    ----------\\n    boolean\\n        Whether or not the two dtypes are equal.\\n\\n    >>> is_dtype_equal(\"int\", int)\\n    True\\n\\n    >>> is_dtype_equal(CategoricalDtype([\\'a\\', \\'b\\'],\\n    ...                CategoricalDtype([\\'b\\', \\'c\\']))\\n    True\\n\\n    >>> is_dtype_equal(CategoricalDtype([\\'a\\', \\'b\\'],\\n    ...                CategoricalDtype([\\'b\\', \\'c\\'], ordered=True))\\n    False\\n    \"\"\"\\n    source = _get_dtype(source)\\n    target = _get_dtype(target)\\n    if is_categorical_dtype(source) and is_categorical_dtype(target):\\n        # ordered False for both\\n        return source.ordered is target.ordered\\n    return is_dtype_equal(source, target)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'is_numeric_v_string_like',\n",
       "  'docstring': 'Check if we are comparing a string-like object to a numeric ndarray.\\n\\n    NumPy doesn\\'t like to compare such objects, especially numeric arrays\\n    and scalar string-likes.\\n\\n    Parameters\\n    ----------\\n    a : array-like, scalar\\n        The first object to check.\\n    b : array-like, scalar\\n        The second object to check.\\n\\n    Returns\\n    -------\\n    boolean\\n        Whether we return a comparing a string-like object to a numeric array.\\n\\n    Examples\\n    --------\\n    >>> is_numeric_v_string_like(1, 1)\\n    False\\n    >>> is_numeric_v_string_like(\"foo\", \"foo\")\\n    False\\n    >>> is_numeric_v_string_like(1, \"foo\")  # non-array numeric\\n    False\\n    >>> is_numeric_v_string_like(np.array([1]), \"foo\")\\n    True\\n    >>> is_numeric_v_string_like(\"foo\", np.array([1]))  # symmetric check\\n    True\\n    >>> is_numeric_v_string_like(np.array([1, 2]), np.array([\"foo\"]))\\n    True\\n    >>> is_numeric_v_string_like(np.array([\"foo\"]), np.array([1, 2]))\\n    True\\n    >>> is_numeric_v_string_like(np.array([1]), np.array([2]))\\n    False\\n    >>> is_numeric_v_string_like(np.array([\"foo\"]), np.array([\"foo\"]))\\n    False',\n",
       "  'code': 'def is_numeric_v_string_like(a, b):\\n    \"\"\"\\n    Check if we are comparing a string-like object to a numeric ndarray.\\n\\n    NumPy doesn\\'t like to compare such objects, especially numeric arrays\\n    and scalar string-likes.\\n\\n    Parameters\\n    ----------\\n    a : array-like, scalar\\n        The first object to check.\\n    b : array-like, scalar\\n        The second object to check.\\n\\n    Returns\\n    -------\\n    boolean\\n        Whether we return a comparing a string-like object to a numeric array.\\n\\n    Examples\\n    --------\\n    >>> is_numeric_v_string_like(1, 1)\\n    False\\n    >>> is_numeric_v_string_like(\"foo\", \"foo\")\\n    False\\n    >>> is_numeric_v_string_like(1, \"foo\")  # non-array numeric\\n    False\\n    >>> is_numeric_v_string_like(np.array([1]), \"foo\")\\n    True\\n    >>> is_numeric_v_string_like(\"foo\", np.array([1]))  # symmetric check\\n    True\\n    >>> is_numeric_v_string_like(np.array([1, 2]), np.array([\"foo\"]))\\n    True\\n    >>> is_numeric_v_string_like(np.array([\"foo\"]), np.array([1, 2]))\\n    True\\n    >>> is_numeric_v_string_like(np.array([1]), np.array([2]))\\n    False\\n    >>> is_numeric_v_string_like(np.array([\"foo\"]), np.array([\"foo\"]))\\n    False\\n    \"\"\"\\n\\n    is_a_array = isinstance(a, np.ndarray)\\n    is_b_array = isinstance(b, np.ndarray)\\n\\n    is_a_numeric_array = is_a_array and is_numeric_dtype(a)\\n    is_b_numeric_array = is_b_array and is_numeric_dtype(b)\\n    is_a_string_array = is_a_array and is_string_like_dtype(a)\\n    is_b_string_array = is_b_array and is_string_like_dtype(b)\\n\\n    is_a_scalar_string_like = not is_a_array and is_string_like(a)\\n    is_b_scalar_string_like = not is_b_array and is_string_like(b)\\n\\n    return ((is_a_numeric_array and is_b_scalar_string_like) or\\n            (is_b_numeric_array and is_a_scalar_string_like) or\\n            (is_a_numeric_array and is_b_string_array) or\\n            (is_b_numeric_array and is_a_string_array))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'infer_dtype_from_object',\n",
       "  'docstring': 'Get a numpy dtype.type-style object for a dtype object.\\n\\n    This methods also includes handling of the datetime64[ns] and\\n    datetime64[ns, TZ] objects.\\n\\n    If no dtype can be found, we return ``object``.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype, type\\n        The dtype object whose numpy dtype.type-style\\n        object we want to extract.\\n\\n    Returns\\n    -------\\n    dtype_object : The extracted numpy dtype.type-style object.',\n",
       "  'code': 'def infer_dtype_from_object(dtype):\\n    \"\"\"\\n    Get a numpy dtype.type-style object for a dtype object.\\n\\n    This methods also includes handling of the datetime64[ns] and\\n    datetime64[ns, TZ] objects.\\n\\n    If no dtype can be found, we return ``object``.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype, type\\n        The dtype object whose numpy dtype.type-style\\n        object we want to extract.\\n\\n    Returns\\n    -------\\n    dtype_object : The extracted numpy dtype.type-style object.\\n    \"\"\"\\n\\n    if isinstance(dtype, type) and issubclass(dtype, np.generic):\\n        # Type object from a dtype\\n        return dtype\\n    elif isinstance(dtype, (np.dtype, PandasExtensionDtype, ExtensionDtype)):\\n        # dtype object\\n        try:\\n            _validate_date_like_dtype(dtype)\\n        except TypeError:\\n            # Should still pass if we don\\'t have a date-like\\n            pass\\n        return dtype.type\\n\\n    try:\\n        dtype = pandas_dtype(dtype)\\n    except TypeError:\\n        pass\\n\\n    if is_extension_array_dtype(dtype):\\n        return dtype.type\\n    elif isinstance(dtype, str):\\n\\n        # TODO(jreback)\\n        # should deprecate these\\n        if dtype in [\\'datetimetz\\', \\'datetime64tz\\']:\\n            return DatetimeTZDtype.type\\n        elif dtype in [\\'period\\']:\\n            raise NotImplementedError\\n\\n        if dtype == \\'datetime\\' or dtype == \\'timedelta\\':\\n            dtype += \\'64\\'\\n        try:\\n            return infer_dtype_from_object(getattr(np, dtype))\\n        except (AttributeError, TypeError):\\n            # Handles cases like _get_dtype(int) i.e.,\\n            # Python objects that are valid dtypes\\n            # (unlike user-defined types, in general)\\n            #\\n            # TypeError handles the float16 type code of \\'e\\'\\n            # further handle internal types\\n            pass\\n\\n    return infer_dtype_from_object(np.dtype(dtype))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'pandas_dtype',\n",
       "  'docstring': 'Convert input into a pandas only dtype object or a numpy dtype object.\\n\\n    Parameters\\n    ----------\\n    dtype : object to be converted\\n\\n    Returns\\n    -------\\n    np.dtype or a pandas dtype\\n\\n    Raises\\n    ------\\n    TypeError if not a dtype',\n",
       "  'code': 'def pandas_dtype(dtype):\\n    \"\"\"\\n    Convert input into a pandas only dtype object or a numpy dtype object.\\n\\n    Parameters\\n    ----------\\n    dtype : object to be converted\\n\\n    Returns\\n    -------\\n    np.dtype or a pandas dtype\\n\\n    Raises\\n    ------\\n    TypeError if not a dtype\\n    \"\"\"\\n    # short-circuit\\n    if isinstance(dtype, np.ndarray):\\n        return dtype.dtype\\n    elif isinstance(dtype, (np.dtype, PandasExtensionDtype, ExtensionDtype)):\\n        return dtype\\n\\n    # registered extension types\\n    result = registry.find(dtype)\\n    if result is not None:\\n        return result\\n\\n    # try a numpy dtype\\n    # raise a consistent TypeError if failed\\n    try:\\n        npdtype = np.dtype(dtype)\\n    except Exception:\\n        # we don\\'t want to force a repr of the non-string\\n        if not isinstance(dtype, str):\\n            raise TypeError(\"data type not understood\")\\n        raise TypeError(\"data type \\'{}\\' not understood\".format(\\n            dtype))\\n\\n    # Any invalid dtype (such as pd.Timestamp) should raise an error.\\n    # np.dtype(invalid_type).kind = 0 for such objects. However, this will\\n    # also catch some valid dtypes such as object, np.object_ and \\'object\\'\\n    # which we safeguard against by catching them earlier and returning\\n    # np.dtype(valid_dtype) before this condition is evaluated.\\n    if is_hashable(dtype) and dtype in [object, np.object_, \\'object\\', \\'O\\']:\\n        # check hashability to avoid errors/DeprecationWarning when we get\\n        # here and `dtype` is an array\\n        return npdtype\\n    elif npdtype.kind == \\'O\\':\\n        raise TypeError(\"dtype \\'{}\\' not understood\".format(dtype))\\n\\n    return npdtype',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_MergeOperation._create_join_index',\n",
       "  'docstring': 'Create a join index by rearranging one index to match another\\n\\n        Parameters\\n        ----------\\n        index: Index being rearranged\\n        other_index: Index used to supply values not found in index\\n        indexer: how to rearrange index\\n        how: replacement is only necessary if indexer based on other_index\\n\\n        Returns\\n        -------\\n        join_index',\n",
       "  'code': 'def _create_join_index(self, index, other_index, indexer,\\n                           other_indexer, how=\\'left\\'):\\n        \"\"\"\\n        Create a join index by rearranging one index to match another\\n\\n        Parameters\\n        ----------\\n        index: Index being rearranged\\n        other_index: Index used to supply values not found in index\\n        indexer: how to rearrange index\\n        how: replacement is only necessary if indexer based on other_index\\n\\n        Returns\\n        -------\\n        join_index\\n        \"\"\"\\n        join_index = index.take(indexer)\\n        if (self.how in (how, \\'outer\\') and\\n                not isinstance(other_index, MultiIndex)):\\n            # if final index requires values in other_index but not target\\n            # index, indexer may hold missing (-1) values, causing Index.take\\n            # to take the final value in target index\\n            mask = indexer == -1\\n            if np.any(mask):\\n                # if values missing (-1) from target index,\\n                # take from other_index instead\\n                join_list = join_index.to_numpy()\\n                other_list = other_index.take(other_indexer).to_numpy()\\n                join_list[mask] = other_list[mask]\\n                join_index = Index(join_list, dtype=join_index.dtype,\\n                                   name=join_index.name)\\n        return join_index',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_DtypeOpsMixin.is_dtype',\n",
       "  'docstring': \"Check if we match 'dtype'.\\n\\n        Parameters\\n        ----------\\n        dtype : object\\n            The object to check.\\n\\n        Returns\\n        -------\\n        is_dtype : bool\\n\\n        Notes\\n        -----\\n        The default implementation is True if\\n\\n        1. ``cls.construct_from_string(dtype)`` is an instance\\n           of ``cls``.\\n        2. ``dtype`` is an object and is an instance of ``cls``\\n        3. ``dtype`` has a ``dtype`` attribute, and any of the above\\n           conditions is true for ``dtype.dtype``.\",\n",
       "  'code': 'def is_dtype(cls, dtype):\\n        \"\"\"Check if we match \\'dtype\\'.\\n\\n        Parameters\\n        ----------\\n        dtype : object\\n            The object to check.\\n\\n        Returns\\n        -------\\n        is_dtype : bool\\n\\n        Notes\\n        -----\\n        The default implementation is True if\\n\\n        1. ``cls.construct_from_string(dtype)`` is an instance\\n           of ``cls``.\\n        2. ``dtype`` is an object and is an instance of ``cls``\\n        3. ``dtype`` has a ``dtype`` attribute, and any of the above\\n           conditions is true for ``dtype.dtype``.\\n        \"\"\"\\n        dtype = getattr(dtype, \\'dtype\\', dtype)\\n\\n        if isinstance(dtype, (ABCSeries, ABCIndexClass,\\n                              ABCDataFrame, np.dtype)):\\n            # https://github.com/pandas-dev/pandas/issues/22960\\n            # avoid passing data to `construct_from_string`. This could\\n            # cause a FutureWarning from numpy about failing elementwise\\n            # comparison from, e.g., comparing DataFrame == \\'category\\'.\\n            return False\\n        elif dtype is None:\\n            return False\\n        elif isinstance(dtype, cls):\\n            return True\\n        try:\\n            return cls.construct_from_string(dtype) is not None\\n        except TypeError:\\n            return False',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'cat_core',\n",
       "  'docstring': 'Auxiliary function for :meth:`str.cat`\\n\\n    Parameters\\n    ----------\\n    list_of_columns : list of numpy arrays\\n        List of arrays to be concatenated with sep;\\n        these arrays may not contain NaNs!\\n    sep : string\\n        The separator string for concatenating the columns\\n\\n    Returns\\n    -------\\n    nd.array\\n        The concatenation of list_of_columns with sep',\n",
       "  'code': 'def cat_core(list_of_columns, sep):\\n    \"\"\"\\n    Auxiliary function for :meth:`str.cat`\\n\\n    Parameters\\n    ----------\\n    list_of_columns : list of numpy arrays\\n        List of arrays to be concatenated with sep;\\n        these arrays may not contain NaNs!\\n    sep : string\\n        The separator string for concatenating the columns\\n\\n    Returns\\n    -------\\n    nd.array\\n        The concatenation of list_of_columns with sep\\n    \"\"\"\\n    list_with_sep = [sep] * (2 * len(list_of_columns) - 1)\\n    list_with_sep[::2] = list_of_columns\\n    return np.sum(list_with_sep, axis=0)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'FrameApply.apply_raw',\n",
       "  'docstring': 'apply to the values as a numpy array',\n",
       "  'code': 'def apply_raw(self):\\n        \"\"\" apply to the values as a numpy array \"\"\"\\n\\n        try:\\n            result = reduction.reduce(self.values, self.f, axis=self.axis)\\n        except Exception:\\n            result = np.apply_along_axis(self.f, self.axis, self.values)\\n\\n        # TODO: mixed type case\\n        if result.ndim == 2:\\n            return self.obj._constructor(result,\\n                                         index=self.index,\\n                                         columns=self.columns)\\n        else:\\n            return self.obj._constructor_sliced(result,\\n                                                index=self.agg_axis)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'cartesian_product',\n",
       "  'docstring': \"Numpy version of itertools.product.\\n    Sometimes faster (for large inputs)...\\n\\n    Parameters\\n    ----------\\n    X : list-like of list-likes\\n\\n    Returns\\n    -------\\n    product : list of ndarrays\\n\\n    Examples\\n    --------\\n    >>> cartesian_product([list('ABC'), [1, 2]])\\n    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='|S1'),\\n    array([1, 2, 1, 2, 1, 2])]\\n\\n    See Also\\n    --------\\n    itertools.product : Cartesian product of input iterables.  Equivalent to\\n        nested for-loops.\",\n",
       "  'code': 'def cartesian_product(X):\\n    \"\"\"\\n    Numpy version of itertools.product.\\n    Sometimes faster (for large inputs)...\\n\\n    Parameters\\n    ----------\\n    X : list-like of list-likes\\n\\n    Returns\\n    -------\\n    product : list of ndarrays\\n\\n    Examples\\n    --------\\n    >>> cartesian_product([list(\\'ABC\\'), [1, 2]])\\n    [array([\\'A\\', \\'A\\', \\'B\\', \\'B\\', \\'C\\', \\'C\\'], dtype=\\'|S1\\'),\\n    array([1, 2, 1, 2, 1, 2])]\\n\\n    See Also\\n    --------\\n    itertools.product : Cartesian product of input iterables.  Equivalent to\\n        nested for-loops.\\n    \"\"\"\\n    msg = \"Input must be a list-like of list-likes\"\\n    if not is_list_like(X):\\n        raise TypeError(msg)\\n    for x in X:\\n        if not is_list_like(x):\\n            raise TypeError(msg)\\n\\n    if len(X) == 0:\\n        return []\\n\\n    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)\\n    cumprodX = np.cumproduct(lenX)\\n\\n    a = np.roll(cumprodX, 1)\\n    a[0] = 1\\n\\n    if cumprodX[-1] != 0:\\n        b = cumprodX[-1] / cumprodX\\n    else:\\n        # if any factor is empty, the cartesian product is empty\\n        b = np.zeros_like(cumprodX)\\n\\n    return [np.tile(np.repeat(np.asarray(com.values_from_object(x)), b[i]),\\n                    np.product(a[i]))\\n            for i, x in enumerate(X)]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DatasetFormatter.largest_indices',\n",
       "  'docstring': 'Returns the `n` largest indices from a numpy array `arr`.',\n",
       "  'code': 'def largest_indices(arr, n):\\n        \"Returns the `n` largest indices from a numpy array `arr`.\"\\n        #https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\\n        flat = arr.flatten()\\n        indices = np.argpartition(flat, -n)[-n:]\\n        indices = indices[np.argsort(-flat[indices])]\\n        return np.unravel_index(indices, arr.shape)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'np2model_tensor',\n",
       "  'docstring': 'Tranform numpy array `a` to a tensor of the same type.',\n",
       "  'code': 'def np2model_tensor(a):\\n    \"Tranform numpy array `a` to a tensor of the same type.\"\\n    dtype = model_type(a.dtype)\\n    res = as_tensor(a)\\n    if not dtype: return res\\n    return res.type(dtype)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'TrackerCallback.get_monitor_value',\n",
       "  'docstring': 'Pick the monitored value.',\n",
       "  'code': 'def get_monitor_value(self):\\n        \"Pick the monitored value.\"\\n        if self.monitor==\\'trn_loss\\' and len(self.learn.recorder.losses) == 0: return None\\n        elif len(self.learn.recorder.val_losses) == 0: return None\\n        values = {\\'train_loss\\':self.learn.recorder.losses[-1].cpu().numpy(),\\n                  \\'valid_loss\\':self.learn.recorder.val_losses[-1]}\\n        if values[\\'valid_loss\\'] is None: return\\n        if self.learn.recorder.metrics:\\n            for m, n in zip(self.learn.recorder.metrics[-1],self.learn.recorder.names[3:-1]):\\n                values[n] = m\\n        if values.get(self.monitor) is None:\\n            warn(f\\'{self.__class__} conditioned on metric `{self.monitor}` which is not available. Available metrics are: {\", \".join(map(str, self.learn.recorder.names[1:-1]))}\\')\\n        return values.get(self.monitor)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ConvLearner.predict_array',\n",
       "  'docstring': 'This over-ride is necessary because otherwise the learner method accesses the wrong model when it is called\\n        with precompute set to true\\n\\n        Args:\\n            arr: a numpy array to be used as input to the model for prediction purposes\\n        Returns:\\n            a numpy array containing the predictions from the model',\n",
       "  'code': 'def predict_array(self, arr):\\n        \"\"\"\\n        This over-ride is necessary because otherwise the learner method accesses the wrong model when it is called\\n        with precompute set to true\\n\\n        Args:\\n            arr: a numpy array to be used as input to the model for prediction purposes\\n        Returns:\\n            a numpy array containing the predictions from the model\\n        \"\"\"\\n        precompute = self.precompute\\n        self.precompute = False\\n        pred = super().predict_array(arr)\\n        self.precompute = precompute\\n        return pred',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ImageModelResults.plot_val_with_title',\n",
       "  'docstring': 'Displays the images and their probabilities of belonging to a certain class\\n\\n            Arguments:\\n                idxs (numpy.ndarray): indexes of the image samples from the dataset\\n                y (int): the selected class\\n\\n            Returns:\\n                Plots the images in n rows [rows = n]',\n",
       "  'code': 'def plot_val_with_title(self, idxs, y):\\n        \"\"\" Displays the images and their probabilities of belonging to a certain class\\n\\n            Arguments:\\n                idxs (numpy.ndarray): indexes of the image samples from the dataset\\n                y (int): the selected class\\n\\n            Returns:\\n                Plots the images in n rows [rows = n]\\n        \"\"\"\\n        # if there are any samples to be displayed\\n        if len(idxs) > 0:\\n            imgs = np.stack([self.ds[x][0] for x in idxs])\\n            title_probs = [self.probs[x,y] for x in idxs]\\n\\n            return plots(self.ds.denorm(imgs), rows=1, titles=title_probs)\\n        # if idxs is empty return false\\n        else:\\n            return False;',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ImageModelResults.most_by_mask',\n",
       "  'docstring': 'Extracts the first 4 most correct/incorrect indexes from the ordered list of probabilities\\n\\n            Arguments:\\n                mask (numpy.ndarray): the mask of probabilities specific to the selected class; a boolean array with shape (num_of_samples,) which contains True where class==selected_class, and False everywhere else\\n                y (int): the selected class\\n                mult (int): sets the ordering; -1 descending, 1 ascending\\n\\n            Returns:\\n                idxs (ndarray): An array of indexes of length 4',\n",
       "  'code': 'def most_by_mask(self, mask, y, mult):\\n        \"\"\" Extracts the first 4 most correct/incorrect indexes from the ordered list of probabilities\\n\\n            Arguments:\\n                mask (numpy.ndarray): the mask of probabilities specific to the selected class; a boolean array with shape (num_of_samples,) which contains True where class==selected_class, and False everywhere else\\n                y (int): the selected class\\n                mult (int): sets the ordering; -1 descending, 1 ascending\\n\\n            Returns:\\n                idxs (ndarray): An array of indexes of length 4\\n        \"\"\"\\n        idxs = np.where(mask)[0]\\n        cnt = min(4, len(idxs))\\n        return idxs[np.argsort(mult * self.probs[idxs,y])[:cnt]]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ImageModelResults.most_uncertain_by_mask',\n",
       "  'docstring': 'Extracts the first 4 most uncertain indexes from the ordered list of probabilities\\n\\n            Arguments:\\n                mask (numpy.ndarray): the mask of probabilities specific to the selected class; a boolean array with shape (num_of_samples,) which contains True where class==selected_class, and False everywhere else\\n                y (int): the selected class\\n\\n            Returns:\\n                idxs (ndarray): An array of indexes of length 4',\n",
       "  'code': 'def most_uncertain_by_mask(self, mask, y):\\n        \"\"\" Extracts the first 4 most uncertain indexes from the ordered list of probabilities\\n\\n            Arguments:\\n                mask (numpy.ndarray): the mask of probabilities specific to the selected class; a boolean array with shape (num_of_samples,) which contains True where class==selected_class, and False everywhere else\\n                y (int): the selected class\\n\\n            Returns:\\n                idxs (ndarray): An array of indexes of length 4\\n        \"\"\"\\n        idxs = np.where(mask)[0]\\n        # the most uncertain samples will have abs(probs-1/num_classes) close to 0;\\n        return idxs[np.argsort(np.abs(self.probs[idxs,y]-(1/self.num_classes)))[:4]]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ImageModelResults.most_by_correct',\n",
       "  'docstring': 'Extracts the predicted classes which correspond to the selected class (y) and to the specific case (prediction is correct - is_true=True, prediction is wrong - is_true=False)\\n\\n            Arguments:\\n                y (int): the selected class\\n                is_correct (boolean): a boolean flag (True, False) which specify the what to look for. Ex: True - most correct samples, False - most incorrect samples\\n\\n            Returns:\\n                idxs (numpy.ndarray): An array of indexes (numpy.ndarray)',\n",
       "  'code': 'def most_by_correct(self, y, is_correct):\\n        \"\"\" Extracts the predicted classes which correspond to the selected class (y) and to the specific case (prediction is correct - is_true=True, prediction is wrong - is_true=False)\\n\\n            Arguments:\\n                y (int): the selected class\\n                is_correct (boolean): a boolean flag (True, False) which specify the what to look for. Ex: True - most correct samples, False - most incorrect samples\\n\\n            Returns:\\n                idxs (numpy.ndarray): An array of indexes (numpy.ndarray)\\n        \"\"\"\\n        # mult=-1 when the is_correct flag is true -> when we want to display the most correct classes we will make a descending sorting (argsort) because we want that the biggest probabilities to be displayed first.\\n        # When is_correct is false, we want to display the most incorrect classes, so we want an ascending sorting since our interest is in the smallest probabilities.\\n        mult = -1 if is_correct==True else 1\\n        return self.most_by_mask(((self.preds == self.ds.y)==is_correct)\\n                                 & (self.ds.y == y), y, mult)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ImageModelResults.most_by_uncertain',\n",
       "  'docstring': 'Extracts the predicted classes which correspond to the selected class (y) and have probabilities nearest to 1/number_of_classes (eg. 0.5 for 2 classes, 0.33 for 3 classes) for the selected class.\\n\\n            Arguments:\\n                y (int): the selected class\\n\\n            Returns:\\n                idxs (numpy.ndarray): An array of indexes (numpy.ndarray)',\n",
       "  'code': 'def most_by_uncertain(self, y):\\n        \"\"\" Extracts the predicted classes which correspond to the selected class (y) and have probabilities nearest to 1/number_of_classes (eg. 0.5 for 2 classes, 0.33 for 3 classes) for the selected class.\\n\\n            Arguments:\\n                y (int): the selected class\\n\\n            Returns:\\n                idxs (numpy.ndarray): An array of indexes (numpy.ndarray)\\n        \"\"\"\\n        return self.most_uncertain_by_mask((self.ds.y == y), y)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'tfms_from_model',\n",
       "  'docstring': 'Returns separate transformers of images for training and validation.\\n    Transformers are constructed according to the image statistics given by the model. (See tfms_from_stats)\\n\\n    Arguments:\\n        f_model: model, pretrained or not pretrained',\n",
       "  'code': 'def tfms_from_model(f_model, sz, aug_tfms=None, max_zoom=None, pad=0, crop_type=CropType.RANDOM,\\n                    tfm_y=None, sz_y=None, pad_mode=cv2.BORDER_REFLECT, norm_y=True, scale=None):\\n    \"\"\" Returns separate transformers of images for training and validation.\\n    Transformers are constructed according to the image statistics given by the model. (See tfms_from_stats)\\n\\n    Arguments:\\n        f_model: model, pretrained or not pretrained\\n    \"\"\"\\n    stats = inception_stats if f_model in inception_models else imagenet_stats\\n    return tfms_from_stats(stats, sz, aug_tfms, max_zoom=max_zoom, pad=pad, crop_type=crop_type,\\n                           tfm_y=tfm_y, sz_y=sz_y, pad_mode=pad_mode, norm_y=norm_y, scale=scale)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'folder_source',\n",
       "  'docstring': 'Returns the filenames and labels for a folder within a path\\n    \\n    Returns:\\n    -------\\n    fnames: a list of the filenames within `folder`\\n    all_lbls: a list of all of the labels in `folder`, where the # of labels is determined by the # of directories within `folder`\\n    lbl_arr: a numpy array of the label indices in `all_lbls`',\n",
       "  'code': 'def folder_source(path, folder):\\n    \"\"\"\\n    Returns the filenames and labels for a folder within a path\\n    \\n    Returns:\\n    -------\\n    fnames: a list of the filenames within `folder`\\n    all_lbls: a list of all of the labels in `folder`, where the # of labels is determined by the # of directories within `folder`\\n    lbl_arr: a numpy array of the label indices in `all_lbls`\\n    \"\"\"\\n    fnames, lbls, all_lbls = read_dirs(path, folder)\\n    lbl2idx = {lbl:idx for idx,lbl in enumerate(all_lbls)}\\n    idxs = [lbl2idx[lbl] for lbl in lbls]\\n    lbl_arr = np.array(idxs, dtype=int)\\n    return fnames, lbl_arr, all_lbls',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'open_image',\n",
       "  'docstring': 'Opens an image using OpenCV given the file path.\\n\\n    Arguments:\\n        fn: the file path of the image\\n\\n    Returns:\\n        The image in RGB format as numpy array of floats normalized to range between 0.0 - 1.0',\n",
       "  'code': 'def open_image(fn):\\n    \"\"\" Opens an image using OpenCV given the file path.\\n\\n    Arguments:\\n        fn: the file path of the image\\n\\n    Returns:\\n        The image in RGB format as numpy array of floats normalized to range between 0.0 - 1.0\\n    \"\"\"\\n    flags = cv2.IMREAD_UNCHANGED+cv2.IMREAD_ANYDEPTH+cv2.IMREAD_ANYCOLOR\\n    if not os.path.exists(fn) and not str(fn).startswith(\"http\"):\\n        raise OSError(\\'No such file or directory: {}\\'.format(fn))\\n    elif os.path.isdir(fn) and not str(fn).startswith(\"http\"):\\n        raise OSError(\\'Is a directory: {}\\'.format(fn))\\n    elif isdicom(fn):\\n        slice = pydicom.read_file(fn)\\n        if slice.PhotometricInterpretation.startswith(\\'MONOCHROME\\'):\\n            # Make a fake RGB image\\n            im = np.stack([slice.pixel_array]*3,-1)\\n            return im / ((1 << slice.BitsStored)-1)\\n        else:\\n            # No support for RGB yet, as it involves various color spaces.\\n            # It shouldn\\'t be too difficult to add though, if needed.\\n            raise OSError(\\'Unsupported DICOM image with PhotometricInterpretation=={}\\'.format(slice.PhotometricInterpretation))\\n    else:\\n        #res = np.array(Image.open(fn), dtype=np.float32)/255\\n        #if len(res.shape)==2: res = np.repeat(res[...,None],3,2)\\n        #return res\\n        try:\\n            if str(fn).startswith(\"http\"):\\n                req = urllib.urlopen(str(fn))\\n                image = np.asarray(bytearray(req.read()), dtype=\"uint8\")\\n                im = cv2.imdecode(image, flags).astype(np.float32)/255\\n            else:\\n                im = cv2.imread(str(fn), flags).astype(np.float32)/255\\n            if im is None: raise OSError(f\\'File not recognized by opencv: {fn}\\')\\n            return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\\n        except Exception as e:\\n            raise OSError(\\'Error handling image at: {}\\'.format(fn)) from e',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ImageClassifierData.from_arrays',\n",
       "  'docstring': 'Read in images and their labels given as numpy arrays\\n\\n        Arguments:\\n            path: a root path of the data (used for storing trained models, precomputed values, etc)\\n            trn: a tuple of training data matrix and target label/classification array (e.g. `trn=(x,y)` where `x` has the\\n                shape of `(5000, 784)` and `y` has the shape of `(5000,)`)\\n            val: a tuple of validation data matrix and target label/classification array.\\n            bs: batch size\\n            tfms: transformations (for data augmentations). e.g. output of `tfms_from_model`\\n            classes: a list of all labels/classifications\\n            num_workers: a number of workers\\n            test: a matrix of test data (the shape should match `trn[0]`)\\n\\n        Returns:\\n            ImageClassifierData',\n",
       "  'code': 'def from_arrays(cls, path, trn, val, bs=64, tfms=(None,None), classes=None, num_workers=4, test=None, continuous=False):\\n        \"\"\" Read in images and their labels given as numpy arrays\\n\\n        Arguments:\\n            path: a root path of the data (used for storing trained models, precomputed values, etc)\\n            trn: a tuple of training data matrix and target label/classification array (e.g. `trn=(x,y)` where `x` has the\\n                shape of `(5000, 784)` and `y` has the shape of `(5000,)`)\\n            val: a tuple of validation data matrix and target label/classification array.\\n            bs: batch size\\n            tfms: transformations (for data augmentations). e.g. output of `tfms_from_model`\\n            classes: a list of all labels/classifications\\n            num_workers: a number of workers\\n            test: a matrix of test data (the shape should match `trn[0]`)\\n\\n        Returns:\\n            ImageClassifierData\\n        \"\"\"\\n        f = ArraysIndexRegressionDataset if continuous else ArraysIndexDataset\\n        datasets = cls.get_ds(f, trn, val, tfms, test=test)\\n        return cls(path, datasets, bs, num_workers, classes=classes)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ImageClassifierData.from_path_and_array',\n",
       "  'docstring': 'Read in images given a sub-folder and their labels given a numpy array\\n\\n        Arguments:\\n            path: a root path of the data (used for storing trained models, precomputed values, etc)\\n            folder: a name of the folder in which training images are contained.\\n            y: numpy array which contains target labels ordered by filenames.\\n            bs: batch size\\n            tfms: transformations (for data augmentations). e.g. output of `tfms_from_model`\\n            val_idxs: index of images to be used for validation. e.g. output of `get_cv_idxs`.\\n                If None, default arguments to get_cv_idxs are used.\\n            test_name: a name of the folder which contains test images.\\n            num_workers: number of workers\\n\\n        Returns:\\n            ImageClassifierData',\n",
       "  'code': 'def from_path_and_array(cls, path, folder, y, classes=None, val_idxs=None, test_name=None,\\n            num_workers=8, tfms=(None,None), bs=64):\\n        \"\"\" Read in images given a sub-folder and their labels given a numpy array\\n\\n        Arguments:\\n            path: a root path of the data (used for storing trained models, precomputed values, etc)\\n            folder: a name of the folder in which training images are contained.\\n            y: numpy array which contains target labels ordered by filenames.\\n            bs: batch size\\n            tfms: transformations (for data augmentations). e.g. output of `tfms_from_model`\\n            val_idxs: index of images to be used for validation. e.g. output of `get_cv_idxs`.\\n                If None, default arguments to get_cv_idxs are used.\\n            test_name: a name of the folder which contains test images.\\n            num_workers: number of workers\\n\\n        Returns:\\n            ImageClassifierData\\n        \"\"\"\\n        assert not (tfms[0] is None or tfms[1] is None), \"please provide transformations for your train and validation sets\"\\n        assert not (os.path.isabs(folder)), \"folder needs to be a relative path\"\\n        fnames = np.core.defchararray.add(f\\'{folder}/\\', sorted(os.listdir(f\\'{path}{folder}\\')))\\n        return cls.from_names_and_array(path, fnames, y, classes, val_idxs, test_name,\\n                num_workers=num_workers, tfms=tfms, bs=bs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'A',\n",
       "  'docstring': 'convert iterable object into numpy array',\n",
       "  'code': 'def A(*a):\\n    \"\"\"convert iterable object into numpy array\"\"\"\\n    return np.array(a[0]) if len(a)==1 else [np.array(o) for o in a]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'T',\n",
       "  'docstring': 'Convert numpy array into a pytorch tensor. \\n    if Cuda is available and USE_GPU=True, store resulting tensor in GPU.',\n",
       "  'code': 'def T(a, half=False, cuda=True):\\n    \"\"\"\\n    Convert numpy array into a pytorch tensor. \\n    if Cuda is available and USE_GPU=True, store resulting tensor in GPU.\\n    \"\"\"\\n    if not torch.is_tensor(a):\\n        a = np.array(np.ascontiguousarray(a))\\n        if a.dtype in (np.int8, np.int16, np.int32, np.int64):\\n            a = torch.LongTensor(a.astype(np.int64))\\n        elif a.dtype in (np.float32, np.float64):\\n            a = to_half(a) if half else torch.FloatTensor(a)\\n        else: raise NotImplementedError(a.dtype)\\n    if cuda: a = to_gpu(a)\\n    return a',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'to_np',\n",
       "  'docstring': 'returns an np.array object given an input of np.array, list, tuple, torch variable or tensor.',\n",
       "  'code': \"def to_np(v):\\n    '''returns an np.array object given an input of np.array, list, tuple, torch variable or tensor.'''\\n    if isinstance(v, float): return np.array(v)\\n    if isinstance(v, (np.ndarray, np.generic)): return v\\n    if isinstance(v, (list,tuple)): return [to_np(o) for o in v]\\n    if isinstance(v, Variable): v=v.data\\n    if torch.cuda.is_available():\\n        if is_half_tensor(v): v=v.float()\\n    if isinstance(v, torch.FloatTensor): v=v.float()\\n    return v.cpu().numpy()\",\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Learner.lr_find',\n",
       "  'docstring': \"Helps you find an optimal learning rate for a model.\\n\\n         It uses the technique developed in the 2015 paper\\n         `Cyclical Learning Rates for Training Neural Networks`, where\\n         we simply keep increasing the learning rate from a very small value,\\n         until the loss starts decreasing.\\n\\n        Args:\\n            start_lr (float/numpy array) : Passing in a numpy array allows you\\n                to specify learning rates for a learner's layer_groups\\n            end_lr (float) : The maximum learning rate to try.\\n            wds (iterable/float)\\n\\n        Examples:\\n            As training moves us closer to the optimal weights for a model,\\n            the optimal learning rate will be smaller. We can take advantage of\\n            that knowledge and provide lr_find() with a starting learning rate\\n            1000x smaller than the model's current learning rate as such:\\n\\n            >> learn.lr_find(lr/1000)\\n\\n            >> lrs = np.array([ 1e-4, 1e-3, 1e-2 ])\\n            >> learn.lr_find(lrs / 1000)\\n\\n        Notes:\\n            lr_find() may finish before going through each batch of examples if\\n            the loss decreases enough.\\n\\n        .. _Cyclical Learning Rates for Training Neural Networks:\\n            http://arxiv.org/abs/1506.01186\",\n",
       "  'code': 'def lr_find(self, start_lr=1e-5, end_lr=10, wds=None, linear=False, **kwargs):\\n        \"\"\"Helps you find an optimal learning rate for a model.\\n\\n         It uses the technique developed in the 2015 paper\\n         `Cyclical Learning Rates for Training Neural Networks`, where\\n         we simply keep increasing the learning rate from a very small value,\\n         until the loss starts decreasing.\\n\\n        Args:\\n            start_lr (float/numpy array) : Passing in a numpy array allows you\\n                to specify learning rates for a learner\\'s layer_groups\\n            end_lr (float) : The maximum learning rate to try.\\n            wds (iterable/float)\\n\\n        Examples:\\n            As training moves us closer to the optimal weights for a model,\\n            the optimal learning rate will be smaller. We can take advantage of\\n            that knowledge and provide lr_find() with a starting learning rate\\n            1000x smaller than the model\\'s current learning rate as such:\\n\\n            >> learn.lr_find(lr/1000)\\n\\n            >> lrs = np.array([ 1e-4, 1e-3, 1e-2 ])\\n            >> learn.lr_find(lrs / 1000)\\n\\n        Notes:\\n            lr_find() may finish before going through each batch of examples if\\n            the loss decreases enough.\\n\\n        .. _Cyclical Learning Rates for Training Neural Networks:\\n            http://arxiv.org/abs/1506.01186\\n\\n        \"\"\"\\n        self.save(\\'tmp\\')\\n        layer_opt = self.get_layer_opt(start_lr, wds)\\n        self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), end_lr, linear=linear)\\n        self.fit_gen(self.model, self.data, layer_opt, 1, **kwargs)\\n        self.load(\\'tmp\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Learner.lr_find2',\n",
       "  'docstring': \"A variant of lr_find() that helps find the best learning rate. It doesn't do\\n        an epoch but a fixed num of iterations (which may be more or less than an epoch\\n        depending on your data).\\n        At each step, it computes the validation loss and the metrics on the next\\n        batch of the validation data, so it's slower than lr_find().\\n\\n        Args:\\n            start_lr (float/numpy array) : Passing in a numpy array allows you\\n                to specify learning rates for a learner's layer_groups\\n            end_lr (float) : The maximum learning rate to try.\\n            num_it : the number of iterations you want it to run\\n            wds (iterable/float)\\n            stop_dv : stops (or not) when the losses starts to explode.\",\n",
       "  'code': 'def lr_find2(self, start_lr=1e-5, end_lr=10, num_it = 100, wds=None, linear=False, stop_dv=True, **kwargs):\\n        \"\"\"A variant of lr_find() that helps find the best learning rate. It doesn\\'t do\\n        an epoch but a fixed num of iterations (which may be more or less than an epoch\\n        depending on your data).\\n        At each step, it computes the validation loss and the metrics on the next\\n        batch of the validation data, so it\\'s slower than lr_find().\\n\\n        Args:\\n            start_lr (float/numpy array) : Passing in a numpy array allows you\\n                to specify learning rates for a learner\\'s layer_groups\\n            end_lr (float) : The maximum learning rate to try.\\n            num_it : the number of iterations you want it to run\\n            wds (iterable/float)\\n            stop_dv : stops (or not) when the losses starts to explode.\\n        \"\"\"\\n        self.save(\\'tmp\\')\\n        layer_opt = self.get_layer_opt(start_lr, wds)\\n        self.sched = LR_Finder2(layer_opt, num_it, end_lr, linear=linear, metrics=self.metrics, stop_dv=stop_dv)\\n        self.fit_gen(self.model, self.data, layer_opt, num_it//len(self.data.trn_dl) + 1, all_val=True, **kwargs)\\n        self.load(\\'tmp\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Learner.predict_array',\n",
       "  'docstring': 'Args:\\n            arr: a numpy array to be used as input to the model for prediction purposes\\n        Returns:\\n            a numpy array containing the predictions from the model',\n",
       "  'code': 'def predict_array(self, arr):\\n        \"\"\"\\n        Args:\\n            arr: a numpy array to be used as input to the model for prediction purposes\\n        Returns:\\n            a numpy array containing the predictions from the model\\n        \"\"\"\\n        if not isinstance(arr, np.ndarray): raise OSError(f\\'Not valid numpy array\\')\\n        self.model.eval()\\n        return to_np(self.model(to_gpu(V(T(arr)))))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Learner.TTA',\n",
       "  'docstring': 'Predict with Test Time Augmentation (TTA)\\n\\n        Additional to the original test/validation images, apply image augmentation to them\\n        (just like for training images) and calculate the mean of predictions. The intent\\n        is to increase the accuracy of predictions by examining the images using multiple\\n        perspectives.\\n\\n\\n            n_aug: a number of augmentation images to use per original image\\n            is_test: indicate to use test images; otherwise use validation images\\n\\n        Returns:\\n            (tuple): a tuple containing:\\n\\n                log predictions (numpy.ndarray): log predictions (i.e. `np.exp(log_preds)` will return probabilities)\\n                targs (numpy.ndarray): target values when `is_test==False`; zeros otherwise.',\n",
       "  'code': 'def TTA(self, n_aug=4, is_test=False):\\n        \"\"\" Predict with Test Time Augmentation (TTA)\\n\\n        Additional to the original test/validation images, apply image augmentation to them\\n        (just like for training images) and calculate the mean of predictions. The intent\\n        is to increase the accuracy of predictions by examining the images using multiple\\n        perspectives.\\n\\n\\n            n_aug: a number of augmentation images to use per original image\\n            is_test: indicate to use test images; otherwise use validation images\\n\\n        Returns:\\n            (tuple): a tuple containing:\\n\\n                log predictions (numpy.ndarray): log predictions (i.e. `np.exp(log_preds)` will return probabilities)\\n                targs (numpy.ndarray): target values when `is_test==False`; zeros otherwise.\\n        \"\"\"\\n        dl1 = self.data.test_dl     if is_test else self.data.val_dl\\n        dl2 = self.data.test_aug_dl if is_test else self.data.aug_dl\\n        preds1,targs = predict_with_targs(self.model, dl1)\\n        preds1 = [preds1]*math.ceil(n_aug/4)\\n        preds2 = [predict_with_targs(self.model, dl2)[0] for i in tqdm(range(n_aug), leave=False)]\\n        return np.stack(preds1+preds2), targs',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'pil2tensor',\n",
       "  'docstring': 'Convert PIL style `image` array to torch style image tensor.',\n",
       "  'code': 'def pil2tensor(image:Union[NPImage,NPArray],dtype:np.dtype)->TensorImage:\\n    \"Convert PIL style `image` array to torch style image tensor.\"\\n    a = np.asarray(image)\\n    if a.ndim==2 : a = np.expand_dims(a,2)\\n    a = np.transpose(a, (1, 0, 2))\\n    a = np.transpose(a, (2, 1, 0))\\n    return torch.from_numpy(a.astype(dtype, copy=False) )',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'image2np',\n",
       "  'docstring': 'Convert from torch style `image` to numpy/matplotlib style.',\n",
       "  'code': 'def image2np(image:Tensor)->np.ndarray:\\n    \"Convert from torch style `image` to numpy/matplotlib style.\"\\n    res = image.cpu().permute(1,2,0).numpy()\\n    return res[...,0] if res.shape[2]==1 else res',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'softmax',\n",
       "  'docstring': 'Numpy Softmax, via comments on https://gist.github.com/stober/1946926\\n\\n    >>> res = softmax(np.array([0, 200, 10]))\\n    >>> np.sum(res)\\n    1.0\\n    >>> np.all(np.abs(res - np.array([0, 1, 0])) < 0.0001)\\n    True\\n    >>> res = softmax(np.array([[0, 200, 10], [0, 10, 200], [200, 0, 10]]))\\n    >>> np.sum(res, axis=1)\\n    array([ 1.,  1.,  1.])\\n    >>> res = softmax(np.array([[0, 200, 10], [0, 10, 200]]))\\n    >>> np.sum(res, axis=1)\\n    array([ 1.,  1.])',\n",
       "  'code': \"def softmax(x):\\n    '''\\n    Numpy Softmax, via comments on https://gist.github.com/stober/1946926\\n\\n    >>> res = softmax(np.array([0, 200, 10]))\\n    >>> np.sum(res)\\n    1.0\\n    >>> np.all(np.abs(res - np.array([0, 1, 0])) < 0.0001)\\n    True\\n    >>> res = softmax(np.array([[0, 200, 10], [0, 10, 200], [200, 0, 10]]))\\n    >>> np.sum(res, axis=1)\\n    array([ 1.,  1.,  1.])\\n    >>> res = softmax(np.array([[0, 200, 10], [0, 10, 200]]))\\n    >>> np.sum(res, axis=1)\\n    array([ 1.,  1.])\\n    '''\\n    if x.ndim == 1:\\n        x = x.reshape((1, -1))\\n    max_x = np.max(x, axis=1).reshape((-1, 1))\\n    exp_x = np.exp(x - max_x)\\n    return exp_x / np.sum(exp_x, axis=1).reshape((-1, 1))\",\n",
       "  'language': 'python'},\n",
       " {'function_name': 'predict_text',\n",
       "  'docstring': 'Do the actual prediction on the text using the\\n        model and mapping files passed',\n",
       "  'code': 'def predict_text(stoi, model, text):\\n    \"\"\"Do the actual prediction on the text using the\\n        model and mapping files passed\\n    \"\"\"\\n\\n    # prefix text with tokens:\\n    #   xbos: beginning of sentence\\n    #   xfld 1: we are using a single field here\\n    input_str = \\'xbos xfld 1 \\' + text\\n\\n    # predictions are done on arrays of input.\\n    # We only have a single input, so turn it into a 1x1 array\\n    texts = [input_str]\\n\\n    # tokenize using the fastai wrapper around spacy\\n    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\\n\\n    # turn into integers for each word\\n    encoded = [stoi[p] for p in tok[0]]\\n\\n    # we want a [x,1] array where x is the number\\n    #  of words inputted (including the prefix tokens)\\n    ary = np.reshape(np.array(encoded),(-1,1))\\n\\n    # turn this array into a tensor\\n    tensor = torch.from_numpy(ary)\\n\\n    # wrap in a torch Variable\\n    variable = Variable(tensor)\\n\\n    # do the predictions\\n    predictions = model(variable)\\n\\n    # convert back to numpy\\n    numpy_preds = predictions[0].data.numpy()\\n\\n    return softmax(numpy_preds[0])[0]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Binder.add',\n",
       "  'docstring': \"Add a doc's annotations to the binder for serialization.\",\n",
       "  'code': 'def add(self, doc):\\n        \"\"\"Add a doc\\'s annotations to the binder for serialization.\"\"\"\\n        array = doc.to_array(self.attrs)\\n        if len(array.shape) == 1:\\n            array = array.reshape((array.shape[0], 1))\\n        self.tokens.append(array)\\n        spaces = doc.to_array(SPACY)\\n        assert array.shape[0] == spaces.shape[0]\\n        spaces = spaces.reshape((spaces.shape[0], 1))\\n        self.spaces.append(numpy.asarray(spaces, dtype=bool))\\n        self.strings.update(w.text for w in doc)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Binder.to_bytes',\n",
       "  'docstring': \"Serialize the binder's annotations into a byte string.\",\n",
       "  'code': 'def to_bytes(self):\\n        \"\"\"Serialize the binder\\'s annotations into a byte string.\"\"\"\\n        for tokens in self.tokens:\\n            assert len(tokens.shape) == 2, tokens.shape\\n        lengths = [len(tokens) for tokens in self.tokens]\\n        msg = {\\n            \"attrs\": self.attrs,\\n            \"tokens\": numpy.vstack(self.tokens).tobytes(\"C\"),\\n            \"spaces\": numpy.vstack(self.spaces).tobytes(\"C\"),\\n            \"lengths\": numpy.asarray(lengths, dtype=\"int32\").tobytes(\"C\"),\\n            \"strings\": list(self.strings),\\n        }\\n        return gzip.compress(srsly.msgpack_dumps(msg))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Binder.from_bytes',\n",
       "  'docstring': \"Deserialize the binder's annotations from a byte string.\",\n",
       "  'code': 'def from_bytes(self, string):\\n        \"\"\"Deserialize the binder\\'s annotations from a byte string.\"\"\"\\n        msg = srsly.msgpack_loads(gzip.decompress(string))\\n        self.attrs = msg[\"attrs\"]\\n        self.strings = set(msg[\"strings\"])\\n        lengths = numpy.fromstring(msg[\"lengths\"], dtype=\"int32\")\\n        flat_spaces = numpy.fromstring(msg[\"spaces\"], dtype=bool)\\n        flat_tokens = numpy.fromstring(msg[\"tokens\"], dtype=\"uint64\")\\n        shape = (flat_tokens.size // len(self.attrs), len(self.attrs))\\n        flat_tokens = flat_tokens.reshape(shape)\\n        flat_spaces = flat_spaces.reshape((flat_spaces.size, 1))\\n        self.tokens = NumpyOps().unflatten(flat_tokens, lengths)\\n        self.spaces = NumpyOps().unflatten(flat_spaces, lengths)\\n        for tokens in self.tokens:\\n            assert len(tokens.shape) == 2, tokens.shape\\n        return self',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Trainer.write_tensorboard_text',\n",
       "  'docstring': 'Saves text to Tensorboard.\\n        Note: Only works on tensorflow r1.2 or above.\\n        :param key: The name of the text.\\n        :param input_dict: A dictionary that will be displayed in a table on Tensorboard.',\n",
       "  'code': 'def write_tensorboard_text(self, key, input_dict):\\n        \"\"\"\\n        Saves text to Tensorboard.\\n        Note: Only works on tensorflow r1.2 or above.\\n        :param key: The name of the text.\\n        :param input_dict: A dictionary that will be displayed in a table on Tensorboard.\\n        \"\"\"\\n        try:\\n            with tf.Session() as sess:\\n                s_op = tf.summary.text(key, tf.convert_to_tensor(\\n                    ([[str(x), str(input_dict[x])] for x in input_dict])))\\n                s = sess.run(s_op)\\n                self.summary_writer.add_summary(s, self.get_step)\\n        except:\\n            LOGGER.info(\\n                \"Cannot write text summary for Tensorboard. Tensorflow version must be r1.2 or above.\")\\n            pass',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'UnityEnvironment._flatten',\n",
       "  'docstring': 'Converts arrays to list.\\n        :param arr: numpy vector.\\n        :return: flattened list.',\n",
       "  'code': 'def _flatten(cls, arr) -> List[float]:\\n        \"\"\"\\n        Converts arrays to list.\\n        :param arr: numpy vector.\\n        :return: flattened list.\\n        \"\"\"\\n        if isinstance(arr, cls.SCALAR_ACTION_TYPES):\\n            arr = [float(arr)]\\n        if isinstance(arr, np.ndarray):\\n            arr = arr.tolist()\\n        if len(arr) == 0:\\n            return arr\\n        if isinstance(arr[0], np.ndarray):\\n            arr = [item for sublist in arr for item in sublist.tolist()]\\n        if isinstance(arr[0], list):\\n            arr = [item for sublist in arr for item in sublist]\\n        arr = [float(x) for x in arr]\\n        return arr',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'PPOModel.create_inverse_model',\n",
       "  'docstring': 'Creates inverse model TensorFlow ops for Curiosity module.\\n        Predicts action taken given current and future encoded states.\\n        :param encoded_state: Tensor corresponding to encoded current state.\\n        :param encoded_next_state: Tensor corresponding to encoded next state.',\n",
       "  'code': 'def create_inverse_model(self, encoded_state, encoded_next_state):\\n        \"\"\"\\n        Creates inverse model TensorFlow ops for Curiosity module.\\n        Predicts action taken given current and future encoded states.\\n        :param encoded_state: Tensor corresponding to encoded current state.\\n        :param encoded_next_state: Tensor corresponding to encoded next state.\\n        \"\"\"\\n        combined_input = tf.concat([encoded_state, encoded_next_state], axis=1)\\n        hidden = tf.layers.dense(combined_input, 256, activation=self.swish)\\n        if self.brain.vector_action_space_type == \"continuous\":\\n            pred_action = tf.layers.dense(hidden, self.act_size[0], activation=None)\\n            squared_difference = tf.reduce_sum(tf.squared_difference(pred_action, self.selected_actions), axis=1)\\n            self.inverse_loss = tf.reduce_mean(tf.dynamic_partition(squared_difference, self.mask, 2)[1])\\n        else:\\n            pred_action = tf.concat(\\n                [tf.layers.dense(hidden, self.act_size[i], activation=tf.nn.softmax)\\n                 for i in range(len(self.act_size))], axis=1)\\n            cross_entropy = tf.reduce_sum(-tf.log(pred_action + 1e-10) * self.selected_actions, axis=1)\\n            self.inverse_loss = tf.reduce_mean(tf.dynamic_partition(cross_entropy, self.mask, 2)[1])',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'PPOModel.create_forward_model',\n",
       "  'docstring': 'Creates forward model TensorFlow ops for Curiosity module.\\n        Predicts encoded future state based on encoded current state and given action.\\n        :param encoded_state: Tensor corresponding to encoded current state.\\n        :param encoded_next_state: Tensor corresponding to encoded next state.',\n",
       "  'code': 'def create_forward_model(self, encoded_state, encoded_next_state):\\n        \"\"\"\\n        Creates forward model TensorFlow ops for Curiosity module.\\n        Predicts encoded future state based on encoded current state and given action.\\n        :param encoded_state: Tensor corresponding to encoded current state.\\n        :param encoded_next_state: Tensor corresponding to encoded next state.\\n        \"\"\"\\n        combined_input = tf.concat([encoded_state, self.selected_actions], axis=1)\\n        hidden = tf.layers.dense(combined_input, 256, activation=self.swish)\\n        # We compare against the concatenation of all observation streams, hence `self.vis_obs_size + int(self.vec_obs_size > 0)`.\\n        pred_next_state = tf.layers.dense(hidden, self.curiosity_enc_size * (self.vis_obs_size + int(self.vec_obs_size > 0)),\\n                                          activation=None)\\n\\n        squared_difference = 0.5 * tf.reduce_sum(tf.squared_difference(pred_next_state, encoded_next_state), axis=1)\\n        self.intrinsic_reward = tf.clip_by_value(self.curiosity_strength * squared_difference, 0, 1)\\n        self.forward_loss = tf.reduce_mean(tf.dynamic_partition(squared_difference, self.mask, 2)[1])',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'PPOModel.create_ppo_optimizer',\n",
       "  'docstring': 'Creates training-specific Tensorflow ops for PPO models.\\n        :param probs: Current policy probabilities\\n        :param old_probs: Past policy probabilities\\n        :param value: Current value estimate\\n        :param beta: Entropy regularization strength\\n        :param entropy: Current policy entropy\\n        :param epsilon: Value for policy-divergence threshold\\n        :param lr: Learning rate\\n        :param max_step: Total number of training steps.',\n",
       "  'code': 'def create_ppo_optimizer(self, probs, old_probs, value, entropy, beta, epsilon, lr, max_step):\\n        \"\"\"\\n        Creates training-specific Tensorflow ops for PPO models.\\n        :param probs: Current policy probabilities\\n        :param old_probs: Past policy probabilities\\n        :param value: Current value estimate\\n        :param beta: Entropy regularization strength\\n        :param entropy: Current policy entropy\\n        :param epsilon: Value for policy-divergence threshold\\n        :param lr: Learning rate\\n        :param max_step: Total number of training steps.\\n        \"\"\"\\n        self.returns_holder = tf.placeholder(shape=[None], dtype=tf.float32, name=\\'discounted_rewards\\')\\n        self.advantage = tf.placeholder(shape=[None, 1], dtype=tf.float32, name=\\'advantages\\')\\n        self.learning_rate = tf.train.polynomial_decay(lr, self.global_step, max_step, 1e-10, power=1.0)\\n\\n        self.old_value = tf.placeholder(shape=[None], dtype=tf.float32, name=\\'old_value_estimates\\')\\n\\n        decay_epsilon = tf.train.polynomial_decay(epsilon, self.global_step, max_step, 0.1, power=1.0)\\n        decay_beta = tf.train.polynomial_decay(beta, self.global_step, max_step, 1e-5, power=1.0)\\n        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\\n\\n        clipped_value_estimate = self.old_value + tf.clip_by_value(tf.reduce_sum(value, axis=1) - self.old_value,\\n                                                                   - decay_epsilon, decay_epsilon)\\n\\n        v_opt_a = tf.squared_difference(self.returns_holder, tf.reduce_sum(value, axis=1))\\n        v_opt_b = tf.squared_difference(self.returns_holder, clipped_value_estimate)\\n        self.value_loss = tf.reduce_mean(tf.dynamic_partition(tf.maximum(v_opt_a, v_opt_b), self.mask, 2)[1])\\n\\n        # Here we calculate PPO policy loss. In continuous control this is done independently for each action gaussian\\n        # and then averaged together. This provides significantly better performance than treating the probability\\n        # as an average of probabilities, or as a joint probability.\\n        r_theta = tf.exp(probs - old_probs)\\n        p_opt_a = r_theta * self.advantage\\n        p_opt_b = tf.clip_by_value(r_theta, 1.0 - decay_epsilon, 1.0 + decay_epsilon) * self.advantage\\n        self.policy_loss = -tf.reduce_mean(tf.dynamic_partition(tf.minimum(p_opt_a, p_opt_b), self.mask, 2)[1])\\n\\n        self.loss = self.policy_loss + 0.5 * self.value_loss - decay_beta * tf.reduce_mean(\\n            tf.dynamic_partition(entropy, self.mask, 2)[1])\\n\\n        if self.use_curiosity:\\n            self.loss += 10 * (0.2 * self.forward_loss + 0.8 * self.inverse_loss)\\n        self.update_batch = optimizer.minimize(self.loss)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'convert',\n",
       "  'docstring': 'Converts a TensorFlow model into a Barracuda model.\\n    :param source_file: The TensorFlow Model\\n    :param target_file: The name of the file the converted model will be saved to\\n    :param trim_unused_by_output: The regexp to match output nodes to remain in the model. All other uconnected nodes will be removed.\\n    :param verbose: If True, will display debug messages\\n    :param compress_f16: If true, the float values will be converted to f16\\n    :return:',\n",
       "  'code': 'def convert(source_file, target_file, trim_unused_by_output=\"\", verbose=False, compress_f16=False):\\n    \"\"\"\\n    Converts a TensorFlow model into a Barracuda model.\\n    :param source_file: The TensorFlow Model\\n    :param target_file: The name of the file the converted model will be saved to\\n    :param trim_unused_by_output: The regexp to match output nodes to remain in the model. All other uconnected nodes will be removed.\\n    :param verbose: If True, will display debug messages\\n    :param compress_f16: If true, the float values will be converted to f16\\n    :return:\\n    \"\"\"\\n    if (type(verbose)==bool):\\n        args = Struct()\\n        args.verbose = verbose\\n        args.print_layers = verbose\\n        args.print_source_json = verbose\\n        args.print_barracuda_json = verbose\\n        args.print_layer_links = verbose\\n        args.print_patterns = verbose\\n        args.print_tensors = verbose\\n    else:\\n        args = verbose\\n\\n    # Load Tensorflow model\\n    print(\"Converting %s to %s\" % (source_file, target_file))\\n    f = open(source_file, \\'rb\\')\\n    i_model = tf.GraphDef()\\n    i_model.ParseFromString(f.read())\\n\\n    if args.verbose:\\n        print(\\'OP_TYPES:\\', {layer.op for layer in i_model.node})\\n\\n    if args.print_source_json or args.verbose:\\n        for layer in i_model.node:\\n            if not layer.op == \\'Const\\':\\n                print(\\'MODEL:\\', MessageToJson(layer) + \",\")\\n\\n    # Convert\\n    o_model = barracuda.Model()\\n    o_model.layers, o_input_shapes, o_model.tensors, o_model.memories = \\\\\\n        process_model(i_model, args)\\n\\n    # Cleanup unconnected Identities (they might linger after processing complex node patterns like LSTM)\\n    def cleanup_layers(layers):\\n        all_layers = {l.name for l in layers}\\n        all_inputs = {i for l in layers for i in l.inputs}\\n\\n        def is_unconnected_identity(layer):\\n            if layer.class_name == \\'Activation\\' and layer.activation == 0: # Identity\\n                assert(len(layer.inputs) == 1)\\n                if layer.inputs[0] not in all_layers and layer.name not in all_inputs:\\n                    return True;\\n            return False;\\n\\n        return [l for l in layers if not is_unconnected_identity(l)]\\n    o_model.layers = cleanup_layers(o_model.layers)\\n\\n    all_inputs = {i for l in o_model.layers for i in l.inputs}\\n    embedded_tensors = {t.name for l in o_model.layers for t in l.tensors}\\n\\n    # Find global tensors\\n    def dims_to_barracuda_shape(dims):\\n        shape = list(dims)\\n        while len(shape) < 4:\\n            shape = [1] + shape\\n        return shape\\n    o_model.globals = [t for t in o_model.tensors if t not in all_inputs and t not in embedded_tensors]\\n    #for x in global_tensors:\\n    #    shape = dims_to_barracuda_shape(get_tensor_dims(o_model.tensors[x]))    \\n    #    o_globals += [Struct(\\n    #        name = x,\\n    #        shape = shape,\\n    #        data = np.reshape(get_tensor_data(o_model.tensors[x]), shape).astype(np.float32))]\\n\\n    # Trim\\n    if trim_unused_by_output:\\n        o_model.layers = barracuda.trim(o_model.layers, trim_unused_by_output, args.verbose)\\n\\n    # Create load layers for constants\\n    const_tensors = [i for i in all_inputs if i in o_model.tensors]\\n    const_tensors += o_model.globals\\n    for x in const_tensors:\\n        shape = dims_to_barracuda_shape(get_tensor_dims(o_model.tensors[x]))\\n\\n        o_l = Struct(\\n            type        = 255,  # Load\\n            class_name  = \"Const\",\\n            name        = x,\\n            pads        = [0,0,0,0],\\n            strides     = [],\\n            pool_size   = [],\\n            axis        = -1,\\n            alpha       = 1,\\n            beta        = 0,\\n            activation  = 0,\\n            inputs      = [],\\n            tensors     = [Struct(\\n                name = x,\\n                shape = shape,\\n                data = np.reshape(get_tensor_data(o_model.tensors[x]), shape).astype(np.float32))]\\n        )\\n        o_model.layers.insert(0, o_l)\\n\\n    # Find model inputs & outputs\\n    all_layers = {l.name for l in o_model.layers}\\n    # global inputs => are inputs that are NOT connected to any layer in the network\\n    # global outputs => are outputs that are NOT feeding any layer in the network OR are coming from Identity layers\\n    o_model.inputs = {i:o_input_shapes[i] for l in o_model.layers for i in l.inputs if i not in all_layers and i not in o_model.memories}\\n\\n    def is_output_layer(layer):\\n        if layer.class_name == \\'Const\\': # Constants never count as global output even when unconnected\\n            return False;\\n        if layer.name not in all_inputs: # this layer is not inputing to any other layer\\n            return True\\n        if layer.class_name == \\'Activation\\' and layer.activation == 0: # Identity marks global output\\n            return True\\n        return False\\n    o_model.outputs = [l.name for l in o_model.layers if is_output_layer(l)]\\n\\n    # Compress\\n    if compress_f16:\\n        o_model = barracuda.compress(o_model)\\n\\n    # Sort model so that layer inputs are always ready upfront\\n    o_model.layers = barracuda.sort(o_model.layers, o_model.inputs, o_model.memories, args.verbose)\\n\\n    # Summary\\n    barracuda.summary(o_model,\\n        print_layer_links = args.print_layer_links or args.verbose,\\n        print_barracuda_json = args.print_barracuda_json or args.verbose,\\n        print_tensors = args.print_tensors or args.verbose)\\n\\n    # Write to file\\n    barracuda.write(o_model, target_file)\\n    print(\\'DONE: wrote\\', target_file, \\'file.\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'TrainerController._save_model',\n",
       "  'docstring': 'Saves current model to checkpoint folder.\\n        :param steps: Current number of steps in training process.\\n        :param saver: Tensorflow saver for session.',\n",
       "  'code': 'def _save_model(self, steps=0):\\n        \"\"\"\\n        Saves current model to checkpoint folder.\\n        :param steps: Current number of steps in training process.\\n        :param saver: Tensorflow saver for session.\\n        \"\"\"\\n        for brain_name in self.trainers.keys():\\n            self.trainers[brain_name].save_model()\\n        self.logger.info(\\'Saved Model\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'BrainInfo.process_pixels',\n",
       "  'docstring': 'Converts byte array observation image into numpy array, re-sizes it,\\n        and optionally converts it to grey scale\\n        :param gray_scale: Whether to convert the image to grayscale.\\n        :param image_bytes: input byte array corresponding to image\\n        :return: processed numpy array of observation from environment',\n",
       "  'code': 'def process_pixels(image_bytes, gray_scale):\\n        \"\"\"\\n        Converts byte array observation image into numpy array, re-sizes it,\\n        and optionally converts it to grey scale\\n        :param gray_scale: Whether to convert the image to grayscale.\\n        :param image_bytes: input byte array corresponding to image\\n        :return: processed numpy array of observation from environment\\n        \"\"\"\\n        s = bytearray(image_bytes)\\n        image = Image.open(io.BytesIO(s))\\n        s = np.array(image) / 255.0\\n        if gray_scale:\\n            s = np.mean(s, axis=2)\\n            s = np.reshape(s, [s.shape[0], s.shape[1], 1])\\n        return s',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SupersetDataFrame.db_type',\n",
       "  'docstring': 'Given a numpy dtype, Returns a generic database type',\n",
       "  'code': 'def db_type(cls, dtype):\\n        \"\"\"Given a numpy dtype, Returns a generic database type\"\"\"\\n        if isinstance(dtype, ExtensionDtype):\\n            return cls.type_map.get(dtype.kind)\\n        elif hasattr(dtype, \\'char\\'):\\n            return cls.type_map.get(dtype.char)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'cv',\n",
       "  'docstring': 'Cross-validation with given parameters.\\n\\n    Parameters\\n    ----------\\n    params : dict\\n        Booster params.\\n    dtrain : DMatrix\\n        Data to be trained.\\n    num_boost_round : int\\n        Number of boosting iterations.\\n    nfold : int\\n        Number of folds in CV.\\n    stratified : bool\\n        Perform stratified sampling.\\n    folds : a KFold or StratifiedKFold instance or list of fold indices\\n        Sklearn KFolds or StratifiedKFolds object.\\n        Alternatively may explicitly pass sample indices for each fold.\\n        For ``n`` folds, **folds** should be a length ``n`` list of tuples.\\n        Each tuple is ``(in,out)`` where ``in`` is a list of indices to be used\\n        as the training samples for the ``n`` th fold and ``out`` is a list of\\n        indices to be used as the testing samples for the ``n`` th fold.\\n    metrics : string or list of strings\\n        Evaluation metrics to be watched in CV.\\n    obj : function\\n        Custom objective function.\\n    feval : function\\n        Custom evaluation function.\\n    maximize : bool\\n        Whether to maximize feval.\\n    early_stopping_rounds: int\\n        Activates early stopping. CV error needs to decrease at least\\n        every <early_stopping_rounds> round(s) to continue.\\n        Last entry in evaluation history is the one from best iteration.\\n    fpreproc : function\\n        Preprocessing function that takes (dtrain, dtest, param) and returns\\n        transformed versions of those.\\n    as_pandas : bool, default True\\n        Return pd.DataFrame when pandas is installed.\\n        If False or pandas is not installed, return np.ndarray\\n    verbose_eval : bool, int, or None, default None\\n        Whether to display the progress. If None, progress will be displayed\\n        when np.ndarray is returned. If True, progress will be displayed at\\n        boosting stage. If an integer is given, progress will be displayed\\n        at every given `verbose_eval` boosting stage.\\n    show_stdv : bool, default True\\n        Whether to display the standard deviation in progress.\\n        Results are not affected, and always contains std.\\n    seed : int\\n        Seed used to generate the folds (passed to numpy.random.seed).\\n    callbacks : list of callback functions\\n        List of callback functions that are applied at end of each iteration.\\n        It is possible to use predefined callbacks by using\\n        :ref:`Callback API <callback_api>`.\\n        Example:\\n\\n        .. code-block:: python\\n\\n            [xgb.callback.reset_learning_rate(custom_rates)]\\n    shuffle : bool\\n        Shuffle data before creating folds.\\n\\n    Returns\\n    -------\\n    evaluation history : list(string)',\n",
       "  'code': 'def cv(params, dtrain, num_boost_round=10, nfold=3, stratified=False, folds=None,\\n       metrics=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None,\\n       fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True,\\n       seed=0, callbacks=None, shuffle=True):\\n    # pylint: disable = invalid-name\\n    \"\"\"Cross-validation with given parameters.\\n\\n    Parameters\\n    ----------\\n    params : dict\\n        Booster params.\\n    dtrain : DMatrix\\n        Data to be trained.\\n    num_boost_round : int\\n        Number of boosting iterations.\\n    nfold : int\\n        Number of folds in CV.\\n    stratified : bool\\n        Perform stratified sampling.\\n    folds : a KFold or StratifiedKFold instance or list of fold indices\\n        Sklearn KFolds or StratifiedKFolds object.\\n        Alternatively may explicitly pass sample indices for each fold.\\n        For ``n`` folds, **folds** should be a length ``n`` list of tuples.\\n        Each tuple is ``(in,out)`` where ``in`` is a list of indices to be used\\n        as the training samples for the ``n`` th fold and ``out`` is a list of\\n        indices to be used as the testing samples for the ``n`` th fold.\\n    metrics : string or list of strings\\n        Evaluation metrics to be watched in CV.\\n    obj : function\\n        Custom objective function.\\n    feval : function\\n        Custom evaluation function.\\n    maximize : bool\\n        Whether to maximize feval.\\n    early_stopping_rounds: int\\n        Activates early stopping. CV error needs to decrease at least\\n        every <early_stopping_rounds> round(s) to continue.\\n        Last entry in evaluation history is the one from best iteration.\\n    fpreproc : function\\n        Preprocessing function that takes (dtrain, dtest, param) and returns\\n        transformed versions of those.\\n    as_pandas : bool, default True\\n        Return pd.DataFrame when pandas is installed.\\n        If False or pandas is not installed, return np.ndarray\\n    verbose_eval : bool, int, or None, default None\\n        Whether to display the progress. If None, progress will be displayed\\n        when np.ndarray is returned. If True, progress will be displayed at\\n        boosting stage. If an integer is given, progress will be displayed\\n        at every given `verbose_eval` boosting stage.\\n    show_stdv : bool, default True\\n        Whether to display the standard deviation in progress.\\n        Results are not affected, and always contains std.\\n    seed : int\\n        Seed used to generate the folds (passed to numpy.random.seed).\\n    callbacks : list of callback functions\\n        List of callback functions that are applied at end of each iteration.\\n        It is possible to use predefined callbacks by using\\n        :ref:`Callback API <callback_api>`.\\n        Example:\\n\\n        .. code-block:: python\\n\\n            [xgb.callback.reset_learning_rate(custom_rates)]\\n    shuffle : bool\\n        Shuffle data before creating folds.\\n\\n    Returns\\n    -------\\n    evaluation history : list(string)\\n    \"\"\"\\n    if stratified is True and not SKLEARN_INSTALLED:\\n        raise XGBoostError(\\'sklearn needs to be installed in order to use stratified cv\\')\\n\\n    if isinstance(metrics, str):\\n        metrics = [metrics]\\n\\n    if isinstance(params, list):\\n        _metrics = [x[1] for x in params if x[0] == \\'eval_metric\\']\\n        params = dict(params)\\n        if \\'eval_metric\\' in params:\\n            params[\\'eval_metric\\'] = _metrics\\n    else:\\n        params = dict((k, v) for k, v in params.items())\\n\\n    if (not metrics) and \\'eval_metric\\' in params:\\n        if isinstance(params[\\'eval_metric\\'], list):\\n            metrics = params[\\'eval_metric\\']\\n        else:\\n            metrics = [params[\\'eval_metric\\']]\\n\\n    params.pop(\"eval_metric\", None)\\n\\n    results = {}\\n    cvfolds = mknfold(dtrain, nfold, params, seed, metrics, fpreproc,\\n                      stratified, folds, shuffle)\\n\\n    # setup callbacks\\n    callbacks = [] if callbacks is None else callbacks\\n    if early_stopping_rounds is not None:\\n        callbacks.append(callback.early_stop(early_stopping_rounds,\\n                                             maximize=maximize,\\n                                             verbose=False))\\n\\n    if isinstance(verbose_eval, bool) and verbose_eval:\\n        callbacks.append(callback.print_evaluation(show_stdv=show_stdv))\\n    else:\\n        if isinstance(verbose_eval, int):\\n            callbacks.append(callback.print_evaluation(verbose_eval, show_stdv=show_stdv))\\n\\n    callbacks_before_iter = [\\n        cb for cb in callbacks if cb.__dict__.get(\\'before_iteration\\', False)]\\n    callbacks_after_iter = [\\n        cb for cb in callbacks if not cb.__dict__.get(\\'before_iteration\\', False)]\\n\\n    for i in range(num_boost_round):\\n        for cb in callbacks_before_iter:\\n            cb(CallbackEnv(model=None,\\n                           cvfolds=cvfolds,\\n                           iteration=i,\\n                           begin_iteration=0,\\n                           end_iteration=num_boost_round,\\n                           rank=0,\\n                           evaluation_result_list=None))\\n        for fold in cvfolds:\\n            fold.update(i, obj)\\n        res = aggcv([f.eval(i, feval) for f in cvfolds])\\n\\n        for key, mean, std in res:\\n            if key + \\'-mean\\' not in results:\\n                results[key + \\'-mean\\'] = []\\n            if key + \\'-std\\' not in results:\\n                results[key + \\'-std\\'] = []\\n            results[key + \\'-mean\\'].append(mean)\\n            results[key + \\'-std\\'].append(std)\\n        try:\\n            for cb in callbacks_after_iter:\\n                cb(CallbackEnv(model=None,\\n                               cvfolds=cvfolds,\\n                               iteration=i,\\n                               begin_iteration=0,\\n                               end_iteration=num_boost_round,\\n                               rank=0,\\n                               evaluation_result_list=res))\\n        except EarlyStopException as e:\\n            for k in results:\\n                results[k] = results[k][:(e.best_iteration + 1)]\\n            break\\n    if as_pandas:\\n        try:\\n            import pandas as pd\\n            results = pd.DataFrame.from_dict(results)\\n        except ImportError:\\n            pass\\n    return results',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'XGBModel.predict',\n",
       "  'docstring': \"Predict with `data`.\\n\\n        .. note:: This function is not thread safe.\\n\\n          For each booster object, predict can only be called from one thread.\\n          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\\n          of model object and then call ``predict()``.\\n\\n        .. note:: Using ``predict()`` with DART booster\\n\\n          If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\\n          some of the trees will be evaluated. This will produce incorrect results if ``data`` is\\n          not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\\n          a nonzero value, e.g.\\n\\n          .. code-block:: python\\n\\n            preds = bst.predict(dtest, ntree_limit=num_round)\\n\\n        Parameters\\n        ----------\\n        data : DMatrix\\n            The dmatrix storing the input.\\n        output_margin : bool\\n            Whether to output the raw untransformed margin value.\\n        ntree_limit : int\\n            Limit number of trees in the prediction; defaults to best_ntree_limit if defined\\n            (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\\n        validate_features : bool\\n            When this is True, validate that the Booster's and data's feature_names are identical.\\n            Otherwise, it is assumed that the feature_names are the same.\\n        Returns\\n        -------\\n        prediction : numpy array\",\n",
       "  'code': 'def predict(self, data, output_margin=False, ntree_limit=None, validate_features=True):\\n        \"\"\"\\n        Predict with `data`.\\n\\n        .. note:: This function is not thread safe.\\n\\n          For each booster object, predict can only be called from one thread.\\n          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\\n          of model object and then call ``predict()``.\\n\\n        .. note:: Using ``predict()`` with DART booster\\n\\n          If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\\n          some of the trees will be evaluated. This will produce incorrect results if ``data`` is\\n          not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\\n          a nonzero value, e.g.\\n\\n          .. code-block:: python\\n\\n            preds = bst.predict(dtest, ntree_limit=num_round)\\n\\n        Parameters\\n        ----------\\n        data : DMatrix\\n            The dmatrix storing the input.\\n        output_margin : bool\\n            Whether to output the raw untransformed margin value.\\n        ntree_limit : int\\n            Limit number of trees in the prediction; defaults to best_ntree_limit if defined\\n            (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\\n        validate_features : bool\\n            When this is True, validate that the Booster\\'s and data\\'s feature_names are identical.\\n            Otherwise, it is assumed that the feature_names are the same.\\n        Returns\\n        -------\\n        prediction : numpy array\\n        \"\"\"\\n        # pylint: disable=missing-docstring,invalid-name\\n        test_dmatrix = DMatrix(data, missing=self.missing, nthread=self.n_jobs)\\n        # get ntree_limit to use - if none specified, default to\\n        # best_ntree_limit if defined, otherwise 0.\\n        if ntree_limit is None:\\n            ntree_limit = getattr(self, \"best_ntree_limit\", 0)\\n        return self.get_booster().predict(test_dmatrix,\\n                                          output_margin=output_margin,\\n                                          ntree_limit=ntree_limit,\\n                                          validate_features=validate_features)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'XGBClassifier.predict',\n",
       "  'docstring': \"Predict with `data`.\\n\\n        .. note:: This function is not thread safe.\\n\\n          For each booster object, predict can only be called from one thread.\\n          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\\n          of model object and then call ``predict()``.\\n\\n        .. note:: Using ``predict()`` with DART booster\\n\\n          If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\\n          some of the trees will be evaluated. This will produce incorrect results if ``data`` is\\n          not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\\n          a nonzero value, e.g.\\n\\n          .. code-block:: python\\n\\n            preds = bst.predict(dtest, ntree_limit=num_round)\\n\\n        Parameters\\n        ----------\\n        data : DMatrix\\n            The dmatrix storing the input.\\n        output_margin : bool\\n            Whether to output the raw untransformed margin value.\\n        ntree_limit : int\\n            Limit number of trees in the prediction; defaults to best_ntree_limit if defined\\n            (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\\n        validate_features : bool\\n            When this is True, validate that the Booster's and data's feature_names are identical.\\n            Otherwise, it is assumed that the feature_names are the same.\\n        Returns\\n        -------\\n        prediction : numpy array\",\n",
       "  'code': 'def predict(self, data, output_margin=False, ntree_limit=None, validate_features=True):\\n        \"\"\"\\n        Predict with `data`.\\n\\n        .. note:: This function is not thread safe.\\n\\n          For each booster object, predict can only be called from one thread.\\n          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\\n          of model object and then call ``predict()``.\\n\\n        .. note:: Using ``predict()`` with DART booster\\n\\n          If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\\n          some of the trees will be evaluated. This will produce incorrect results if ``data`` is\\n          not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\\n          a nonzero value, e.g.\\n\\n          .. code-block:: python\\n\\n            preds = bst.predict(dtest, ntree_limit=num_round)\\n\\n        Parameters\\n        ----------\\n        data : DMatrix\\n            The dmatrix storing the input.\\n        output_margin : bool\\n            Whether to output the raw untransformed margin value.\\n        ntree_limit : int\\n            Limit number of trees in the prediction; defaults to best_ntree_limit if defined\\n            (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\\n        validate_features : bool\\n            When this is True, validate that the Booster\\'s and data\\'s feature_names are identical.\\n            Otherwise, it is assumed that the feature_names are the same.\\n        Returns\\n        -------\\n        prediction : numpy array\\n        \"\"\"\\n        test_dmatrix = DMatrix(data, missing=self.missing, nthread=self.n_jobs)\\n        if ntree_limit is None:\\n            ntree_limit = getattr(self, \"best_ntree_limit\", 0)\\n        class_probs = self.get_booster().predict(test_dmatrix,\\n                                                 output_margin=output_margin,\\n                                                 ntree_limit=ntree_limit,\\n                                                 validate_features=validate_features)\\n        if output_margin:\\n            # If output_margin is active, simply return the scores\\n            return class_probs\\n\\n        if len(class_probs.shape) > 1:\\n            column_indexes = np.argmax(class_probs, axis=1)\\n        else:\\n            column_indexes = np.repeat(0, class_probs.shape[0])\\n            column_indexes[class_probs > 0.5] = 1\\n        return self._le.inverse_transform(column_indexes)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'XGBClassifier.predict_proba',\n",
       "  'docstring': \"Predict the probability of each `data` example being of a given class.\\n\\n        .. note:: This function is not thread safe\\n\\n            For each booster object, predict can only be called from one thread.\\n            If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\\n            of model object and then call predict\\n\\n        Parameters\\n        ----------\\n        data : DMatrix\\n            The dmatrix storing the input.\\n        ntree_limit : int\\n            Limit number of trees in the prediction; defaults to best_ntree_limit if defined\\n            (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\\n        validate_features : bool\\n            When this is True, validate that the Booster's and data's feature_names are identical.\\n            Otherwise, it is assumed that the feature_names are the same.\\n\\n        Returns\\n        -------\\n        prediction : numpy array\\n            a numpy array with the probability of each data example being of a given class.\",\n",
       "  'code': 'def predict_proba(self, data, ntree_limit=None, validate_features=True):\\n        \"\"\"\\n        Predict the probability of each `data` example being of a given class.\\n\\n        .. note:: This function is not thread safe\\n\\n            For each booster object, predict can only be called from one thread.\\n            If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\\n            of model object and then call predict\\n\\n        Parameters\\n        ----------\\n        data : DMatrix\\n            The dmatrix storing the input.\\n        ntree_limit : int\\n            Limit number of trees in the prediction; defaults to best_ntree_limit if defined\\n            (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\\n        validate_features : bool\\n            When this is True, validate that the Booster\\'s and data\\'s feature_names are identical.\\n            Otherwise, it is assumed that the feature_names are the same.\\n\\n        Returns\\n        -------\\n        prediction : numpy array\\n            a numpy array with the probability of each data example being of a given class.\\n        \"\"\"\\n        test_dmatrix = DMatrix(data, missing=self.missing, nthread=self.n_jobs)\\n        if ntree_limit is None:\\n            ntree_limit = getattr(self, \"best_ntree_limit\", 0)\\n        class_probs = self.get_booster().predict(test_dmatrix,\\n                                                 ntree_limit=ntree_limit,\\n                                                 validate_features=validate_features)\\n        if self.objective == \"multi:softprob\":\\n            return class_probs\\n        classone_probs = class_probs\\n        classzero_probs = 1.0 - classone_probs\\n        return np.vstack((classzero_probs, classone_probs)).transpose()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ctypes2numpy',\n",
       "  'docstring': 'Convert a ctypes pointer array to a numpy array.',\n",
       "  'code': 'def ctypes2numpy(cptr, length, dtype):\\n    \"\"\"Convert a ctypes pointer array to a numpy array.\\n    \"\"\"\\n    NUMPY_TO_CTYPES_MAPPING = {\\n        np.float32: ctypes.c_float,\\n        np.uint32: ctypes.c_uint,\\n    }\\n    if dtype not in NUMPY_TO_CTYPES_MAPPING:\\n        raise RuntimeError(\\'Supported types: {}\\'.format(NUMPY_TO_CTYPES_MAPPING.keys()))\\n    ctype = NUMPY_TO_CTYPES_MAPPING[dtype]\\n    if not isinstance(cptr, ctypes.POINTER(ctype)):\\n        raise RuntimeError(\\'expected {} pointer\\'.format(ctype))\\n    res = np.zeros(length, dtype=dtype)\\n    if not ctypes.memmove(res.ctypes.data, cptr, length * res.strides[0]):\\n        raise RuntimeError(\\'memmove failed\\')\\n    return res',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_maybe_dt_array',\n",
       "  'docstring': 'Extract numpy array from single column data table',\n",
       "  'code': 'def _maybe_dt_array(array):\\n    \"\"\" Extract numpy array from single column data table \"\"\"\\n    if not isinstance(array, DataTable) or array is None:\\n        return array\\n\\n    if array.shape[1] > 1:\\n        raise ValueError(\\'DataTable for label or weight cannot have multiple columns\\')\\n\\n    # below requires new dt version\\n    # extract first column\\n    array = array.to_numpy()[:, 0].astype(\\'float\\')\\n\\n    return array',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DMatrix._init_from_npy2d',\n",
       "  'docstring': \"Initialize data from a 2-D numpy matrix.\\n\\n        If ``mat`` does not have ``order='C'`` (aka row-major) or is not contiguous,\\n        a temporary copy will be made.\\n\\n        If ``mat`` does not have ``dtype=numpy.float32``, a temporary copy will be made.\\n\\n        So there could be as many as two temporary data copies; be mindful of input layout\\n        and type if memory use is a concern.\",\n",
       "  'code': 'def _init_from_npy2d(self, mat, missing, nthread):\\n        \"\"\"\\n        Initialize data from a 2-D numpy matrix.\\n\\n        If ``mat`` does not have ``order=\\'C\\'`` (aka row-major) or is not contiguous,\\n        a temporary copy will be made.\\n\\n        If ``mat`` does not have ``dtype=numpy.float32``, a temporary copy will be made.\\n\\n        So there could be as many as two temporary data copies; be mindful of input layout\\n        and type if memory use is a concern.\\n        \"\"\"\\n        if len(mat.shape) != 2:\\n            raise ValueError(\\'Input numpy.ndarray must be 2 dimensional\\')\\n        # flatten the array by rows and ensure it is float32.\\n        # we try to avoid data copies if possible (reshape returns a view when possible\\n        # and we explicitly tell np.array to try and avoid copying)\\n        data = np.array(mat.reshape(mat.size), copy=False, dtype=np.float32)\\n        handle = ctypes.c_void_p()\\n        missing = missing if missing is not None else np.nan\\n        if nthread is None:\\n            _check_call(_LIB.XGDMatrixCreateFromMat(\\n                data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\\n                c_bst_ulong(mat.shape[0]),\\n                c_bst_ulong(mat.shape[1]),\\n                ctypes.c_float(missing),\\n                ctypes.byref(handle)))\\n        else:\\n            _check_call(_LIB.XGDMatrixCreateFromMat_omp(\\n                data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\\n                c_bst_ulong(mat.shape[0]),\\n                c_bst_ulong(mat.shape[1]),\\n                ctypes.c_float(missing),\\n                ctypes.byref(handle),\\n                nthread))\\n        self.handle = handle',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DMatrix.set_float_info',\n",
       "  'docstring': 'Set float type property into the DMatrix.\\n\\n        Parameters\\n        ----------\\n        field: str\\n            The field name of the information\\n\\n        data: numpy array\\n            The array of data to be set',\n",
       "  'code': 'def set_float_info(self, field, data):\\n        \"\"\"Set float type property into the DMatrix.\\n\\n        Parameters\\n        ----------\\n        field: str\\n            The field name of the information\\n\\n        data: numpy array\\n            The array of data to be set\\n        \"\"\"\\n        if getattr(data, \\'base\\', None) is not None and \\\\\\n           data.base is not None and isinstance(data, np.ndarray) \\\\\\n           and isinstance(data.base, np.ndarray) and (not data.flags.c_contiguous):\\n            self.set_float_info_npy2d(field, data)\\n            return\\n        c_data = c_array(ctypes.c_float, data)\\n        _check_call(_LIB.XGDMatrixSetFloatInfo(self.handle,\\n                                               c_str(field),\\n                                               c_data,\\n                                               c_bst_ulong(len(data))))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DMatrix.set_float_info_npy2d',\n",
       "  'docstring': 'Set float type property into the DMatrix\\n           for numpy 2d array input\\n\\n        Parameters\\n        ----------\\n        field: str\\n            The field name of the information\\n\\n        data: numpy array\\n            The array of data to be set',\n",
       "  'code': 'def set_float_info_npy2d(self, field, data):\\n        \"\"\"Set float type property into the DMatrix\\n           for numpy 2d array input\\n\\n        Parameters\\n        ----------\\n        field: str\\n            The field name of the information\\n\\n        data: numpy array\\n            The array of data to be set\\n        \"\"\"\\n        if getattr(data, \\'base\\', None) is not None and \\\\\\n           data.base is not None and isinstance(data, np.ndarray) \\\\\\n           and isinstance(data.base, np.ndarray) and (not data.flags.c_contiguous):\\n            warnings.warn(\"Use subset (sliced data) of np.ndarray is not recommended \" +\\n                          \"because it will generate extra copies and increase memory consumption\")\\n            data = np.array(data, copy=True, dtype=np.float32)\\n        else:\\n            data = np.array(data, copy=False, dtype=np.float32)\\n        c_data = data.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\\n        _check_call(_LIB.XGDMatrixSetFloatInfo(self.handle,\\n                                               c_str(field),\\n                                               c_data,\\n                                               c_bst_ulong(len(data))))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'DMatrix.set_uint_info',\n",
       "  'docstring': 'Set uint type property into the DMatrix.\\n\\n        Parameters\\n        ----------\\n        field: str\\n            The field name of the information\\n\\n        data: numpy array\\n            The array of data to be set',\n",
       "  'code': 'def set_uint_info(self, field, data):\\n        \"\"\"Set uint type property into the DMatrix.\\n\\n        Parameters\\n        ----------\\n        field: str\\n            The field name of the information\\n\\n        data: numpy array\\n            The array of data to be set\\n        \"\"\"\\n        if getattr(data, \\'base\\', None) is not None and \\\\\\n           data.base is not None and isinstance(data, np.ndarray) \\\\\\n           and isinstance(data.base, np.ndarray) and (not data.flags.c_contiguous):\\n            warnings.warn(\"Use subset (sliced data) of np.ndarray is not recommended \" +\\n                          \"because it will generate extra copies and increase memory consumption\")\\n            data = np.array(data, copy=True, dtype=ctypes.c_uint)\\n        else:\\n            data = np.array(data, copy=False, dtype=ctypes.c_uint)\\n        _check_call(_LIB.XGDMatrixSetUIntInfo(self.handle,\\n                                              c_str(field),\\n                                              c_array(ctypes.c_uint, data),\\n                                              c_bst_ulong(len(data))))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Booster.predict',\n",
       "  'docstring': \"Predict with data.\\n\\n        .. note:: This function is not thread safe.\\n\\n          For each booster object, predict can only be called from one thread.\\n          If you want to run prediction using multiple thread, call ``bst.copy()`` to make copies\\n          of model object and then call ``predict()``.\\n\\n        .. note:: Using ``predict()`` with DART booster\\n\\n          If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\\n          some of the trees will be evaluated. This will produce incorrect results if ``data`` is\\n          not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\\n          a nonzero value, e.g.\\n\\n          .. code-block:: python\\n\\n            preds = bst.predict(dtest, ntree_limit=num_round)\\n\\n        Parameters\\n        ----------\\n        data : DMatrix\\n            The dmatrix storing the input.\\n\\n        output_margin : bool\\n            Whether to output the raw untransformed margin value.\\n\\n        ntree_limit : int\\n            Limit number of trees in the prediction; defaults to 0 (use all trees).\\n\\n        pred_leaf : bool\\n            When this option is on, the output will be a matrix of (nsample, ntrees)\\n            with each record indicating the predicted leaf index of each sample in each tree.\\n            Note that the leaf index of a tree is unique per tree, so you may find leaf 1\\n            in both tree 1 and tree 0.\\n\\n        pred_contribs : bool\\n            When this is True the output will be a matrix of size (nsample, nfeats + 1)\\n            with each record indicating the feature contributions (SHAP values) for that\\n            prediction. The sum of all feature contributions is equal to the raw untransformed\\n            margin value of the prediction. Note the final column is the bias term.\\n\\n        approx_contribs : bool\\n            Approximate the contributions of each feature\\n\\n        pred_interactions : bool\\n            When this is True the output will be a matrix of size (nsample, nfeats + 1, nfeats + 1)\\n            indicating the SHAP interaction values for each pair of features. The sum of each\\n            row (or column) of the interaction values equals the corresponding SHAP value (from\\n            pred_contribs), and the sum of the entire matrix equals the raw untransformed margin\\n            value of the prediction. Note the last row and column correspond to the bias term.\\n\\n        validate_features : bool\\n            When this is True, validate that the Booster's and data's feature_names are identical.\\n            Otherwise, it is assumed that the feature_names are the same.\\n\\n        Returns\\n        -------\\n        prediction : numpy array\",\n",
       "  'code': 'def predict(self, data, output_margin=False, ntree_limit=0, pred_leaf=False,\\n                pred_contribs=False, approx_contribs=False, pred_interactions=False,\\n                validate_features=True):\\n        \"\"\"\\n        Predict with data.\\n\\n        .. note:: This function is not thread safe.\\n\\n          For each booster object, predict can only be called from one thread.\\n          If you want to run prediction using multiple thread, call ``bst.copy()`` to make copies\\n          of model object and then call ``predict()``.\\n\\n        .. note:: Using ``predict()`` with DART booster\\n\\n          If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\\n          some of the trees will be evaluated. This will produce incorrect results if ``data`` is\\n          not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\\n          a nonzero value, e.g.\\n\\n          .. code-block:: python\\n\\n            preds = bst.predict(dtest, ntree_limit=num_round)\\n\\n        Parameters\\n        ----------\\n        data : DMatrix\\n            The dmatrix storing the input.\\n\\n        output_margin : bool\\n            Whether to output the raw untransformed margin value.\\n\\n        ntree_limit : int\\n            Limit number of trees in the prediction; defaults to 0 (use all trees).\\n\\n        pred_leaf : bool\\n            When this option is on, the output will be a matrix of (nsample, ntrees)\\n            with each record indicating the predicted leaf index of each sample in each tree.\\n            Note that the leaf index of a tree is unique per tree, so you may find leaf 1\\n            in both tree 1 and tree 0.\\n\\n        pred_contribs : bool\\n            When this is True the output will be a matrix of size (nsample, nfeats + 1)\\n            with each record indicating the feature contributions (SHAP values) for that\\n            prediction. The sum of all feature contributions is equal to the raw untransformed\\n            margin value of the prediction. Note the final column is the bias term.\\n\\n        approx_contribs : bool\\n            Approximate the contributions of each feature\\n\\n        pred_interactions : bool\\n            When this is True the output will be a matrix of size (nsample, nfeats + 1, nfeats + 1)\\n            indicating the SHAP interaction values for each pair of features. The sum of each\\n            row (or column) of the interaction values equals the corresponding SHAP value (from\\n            pred_contribs), and the sum of the entire matrix equals the raw untransformed margin\\n            value of the prediction. Note the last row and column correspond to the bias term.\\n\\n        validate_features : bool\\n            When this is True, validate that the Booster\\'s and data\\'s feature_names are identical.\\n            Otherwise, it is assumed that the feature_names are the same.\\n\\n        Returns\\n        -------\\n        prediction : numpy array\\n        \"\"\"\\n        option_mask = 0x00\\n        if output_margin:\\n            option_mask |= 0x01\\n        if pred_leaf:\\n            option_mask |= 0x02\\n        if pred_contribs:\\n            option_mask |= 0x04\\n        if approx_contribs:\\n            option_mask |= 0x08\\n        if pred_interactions:\\n            option_mask |= 0x10\\n\\n        if validate_features:\\n            self._validate_features(data)\\n\\n        length = c_bst_ulong()\\n        preds = ctypes.POINTER(ctypes.c_float)()\\n        _check_call(_LIB.XGBoosterPredict(self.handle, data.handle,\\n                                          ctypes.c_int(option_mask),\\n                                          ctypes.c_uint(ntree_limit),\\n                                          ctypes.byref(length),\\n                                          ctypes.byref(preds)))\\n        preds = ctypes2numpy(preds, length.value, np.float32)\\n        if pred_leaf:\\n            preds = preds.astype(np.int32)\\n        nrow = data.num_row()\\n        if preds.size != nrow and preds.size % nrow == 0:\\n            chunk_size = int(preds.size / nrow)\\n\\n            if pred_interactions:\\n                ngroup = int(chunk_size / ((data.num_col() + 1) * (data.num_col() + 1)))\\n                if ngroup == 1:\\n                    preds = preds.reshape(nrow, data.num_col() + 1, data.num_col() + 1)\\n                else:\\n                    preds = preds.reshape(nrow, ngroup, data.num_col() + 1, data.num_col() + 1)\\n            elif pred_contribs:\\n                ngroup = int(chunk_size / (data.num_col() + 1))\\n                if ngroup == 1:\\n                    preds = preds.reshape(nrow, data.num_col() + 1)\\n                else:\\n                    preds = preds.reshape(nrow, ngroup, data.num_col() + 1)\\n            else:\\n                preds = preds.reshape(nrow, chunk_size)\\n        return preds',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Booster.get_split_value_histogram',\n",
       "  'docstring': 'Get split value histogram of a feature\\n\\n        Parameters\\n        ----------\\n        feature: str\\n            The name of the feature.\\n        fmap: str (optional)\\n            The name of feature map file.\\n        bin: int, default None\\n            The maximum number of bins.\\n            Number of bins equals number of unique split values n_unique,\\n            if bins == None or bins > n_unique.\\n        as_pandas: bool, default True\\n            Return pd.DataFrame when pandas is installed.\\n            If False or pandas is not installed, return numpy ndarray.\\n\\n        Returns\\n        -------\\n        a histogram of used splitting values for the specified feature\\n        either as numpy array or pandas DataFrame.',\n",
       "  'code': 'def get_split_value_histogram(self, feature, fmap=\\'\\', bins=None, as_pandas=True):\\n        \"\"\"Get split value histogram of a feature\\n\\n        Parameters\\n        ----------\\n        feature: str\\n            The name of the feature.\\n        fmap: str (optional)\\n            The name of feature map file.\\n        bin: int, default None\\n            The maximum number of bins.\\n            Number of bins equals number of unique split values n_unique,\\n            if bins == None or bins > n_unique.\\n        as_pandas: bool, default True\\n            Return pd.DataFrame when pandas is installed.\\n            If False or pandas is not installed, return numpy ndarray.\\n\\n        Returns\\n        -------\\n        a histogram of used splitting values for the specified feature\\n        either as numpy array or pandas DataFrame.\\n        \"\"\"\\n        xgdump = self.get_dump(fmap=fmap)\\n        values = []\\n        regexp = re.compile(r\"\\\\[{0}<([\\\\d.Ee+-]+)\\\\]\".format(feature))\\n        for i, _ in enumerate(xgdump):\\n            m = re.findall(regexp, xgdump[i])\\n            values.extend([float(x) for x in m])\\n\\n        n_unique = len(np.unique(values))\\n        bins = max(min(n_unique, bins) if bins is not None else n_unique, 1)\\n\\n        nph = np.histogram(values, bins=bins)\\n        nph = np.column_stack((nph[1][1:], nph[0]))\\n        nph = nph[nph[:, 1] > 0]\\n\\n        if as_pandas and PANDAS_INSTALLED:\\n            return DataFrame(nph, columns=[\\'SplitValue\\', \\'Count\\'])\\n        if as_pandas and not PANDAS_INSTALLED:\\n            sys.stderr.write(\\n                \"Returning histogram as ndarray (as_pandas == True, but pandas is not installed).\")\\n        return nph',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'run_benchmarks',\n",
       "  'docstring': \"r'''\\n    Core of the running of the benchmarks. We will run on all of models, against\\n    the WAV file provided as wav, and the provided alphabet.\",\n",
       "  'code': 'def run_benchmarks(dir, models, wav, alphabet, lm_binary=None, trie=None, iters=-1):\\n    r\\'\\'\\'\\n    Core of the running of the benchmarks. We will run on all of models, against\\n    the WAV file provided as wav, and the provided alphabet.\\n    \\'\\'\\'\\n\\n    assert_valid_dir(dir)\\n\\n    inference_times = [ ]\\n\\n    for model in models:\\n        model_filename = model\\n\\n        current_model = {\\n          \\'name\\':   model,\\n          \\'iters\\':  [ ],\\n          \\'mean\\':   numpy.infty,\\n          \\'stddev\\': numpy.infty\\n        }\\n\\n        if lm_binary and trie:\\n            cmdline = \\'./deepspeech --model \"%s\" --alphabet \"%s\" --lm \"%s\" --trie \"%s\" --audio \"%s\" -t\\' % (model_filename, alphabet, lm_binary, trie, wav)\\n        else:\\n            cmdline = \\'./deepspeech --model \"%s\" --alphabet \"%s\" --audio \"%s\" -t\\' % (model_filename, alphabet, wav)\\n\\n        for it in range(iters):\\n            sys.stdout.write(\\'\\\\rRunning %s: %d/%d\\' % (os.path.basename(model), (it+1), iters))\\n            sys.stdout.flush()\\n            rc, stdout, stderr = exec_command(cmdline, cwd=dir)\\n            if rc == 0:\\n                inference_time = float(stdout.split(\\'\\\\n\\')[1].split(\\'=\\')[-1])\\n                # print(\"[%d] model=%s inference=%f\" % (it, model, inference_time))\\n                current_model[\\'iters\\'].append(inference_time)\\n            else:\\n                print(\\'exec_command(\"%s\") failed with rc=%d\\' % (cmdline, rc))\\n                print(\\'stdout: %s\\' % stdout)\\n                print(\\'stderr: %s\\' % stderr)\\n                raise AssertionError(\\'Execution failure: rc=%d\\' % (rc))\\n\\n        sys.stdout.write(\\'\\\\n\\')\\n        sys.stdout.flush()\\n        current_model[\\'mean\\']   = numpy.mean(current_model[\\'iters\\'])\\n        current_model[\\'stddev\\'] = numpy.std(current_model[\\'iters\\'])\\n        inference_times.append(current_model)\\n\\n    return inference_times',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'text_to_char_array',\n",
       "  'docstring': 'r\"\"\"\\n    Given a Python string ``original``, remove unsupported characters, map characters\\n    to integers and return a numpy array representing the processed string.',\n",
       "  'code': 'def text_to_char_array(original, alphabet):\\n    r\"\"\"\\n    Given a Python string ``original``, remove unsupported characters, map characters\\n    to integers and return a numpy array representing the processed string.\\n    \"\"\"\\n    return np.asarray([alphabet.label_from_string(c) for c in original])',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'calculate_mean_edit_distance_and_loss',\n",
       "  'docstring': \"r'''\\n    This routine beam search decodes a mini-batch and calculates the loss and mean edit distance.\\n    Next to total and average loss it returns the mean edit distance,\\n    the decoded result and the batch's original Y.\",\n",
       "  'code': \"def calculate_mean_edit_distance_and_loss(iterator, dropout, reuse):\\n    r'''\\n    This routine beam search decodes a mini-batch and calculates the loss and mean edit distance.\\n    Next to total and average loss it returns the mean edit distance,\\n    the decoded result and the batch's original Y.\\n    '''\\n    # Obtain the next batch of data\\n    (batch_x, batch_seq_len), batch_y = iterator.get_next()\\n\\n    # Calculate the logits of the batch\\n    logits, _ = create_model(batch_x, batch_seq_len, dropout, reuse=reuse)\\n\\n    # Compute the CTC loss using TensorFlow's `ctc_loss`\\n    total_loss = tf.nn.ctc_loss(labels=batch_y, inputs=logits, sequence_length=batch_seq_len)\\n\\n    # Calculate the average loss across the batch\\n    avg_loss = tf.reduce_mean(total_loss)\\n\\n    # Finally we return the average loss\\n    return avg_loss\",\n",
       "  'language': 'python'},\n",
       " {'function_name': 'export',\n",
       "  'docstring': \"r'''\\n    Restores the trained variables into a simpler graph that will be exported for serving.\",\n",
       "  'code': 'def export():\\n    r\\'\\'\\'\\n    Restores the trained variables into a simpler graph that will be exported for serving.\\n    \\'\\'\\'\\n    log_info(\\'Exporting the model...\\')\\n    from tensorflow.python.framework.ops import Tensor, Operation\\n\\n    inputs, outputs, _ = create_inference_graph(batch_size=FLAGS.export_batch_size, n_steps=FLAGS.n_steps, tflite=FLAGS.export_tflite)\\n    output_names_tensors = [tensor.op.name for tensor in outputs.values() if isinstance(tensor, Tensor)]\\n    output_names_ops = [op.name for op in outputs.values() if isinstance(op, Operation)]\\n    output_names = \",\".join(output_names_tensors + output_names_ops)\\n\\n    if not FLAGS.export_tflite:\\n        mapping = {v.op.name: v for v in tf.global_variables() if not v.op.name.startswith(\\'previous_state_\\')}\\n    else:\\n        # Create a saver using variables from the above newly created graph\\n        def fixup(name):\\n            if name.startswith(\\'rnn/lstm_cell/\\'):\\n                return name.replace(\\'rnn/lstm_cell/\\', \\'lstm_fused_cell/\\')\\n            return name\\n\\n        mapping = {fixup(v.op.name): v for v in tf.global_variables()}\\n\\n    saver = tf.train.Saver(mapping)\\n\\n    # Restore variables from training checkpoint\\n    checkpoint = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\\n    checkpoint_path = checkpoint.model_checkpoint_path\\n\\n    output_filename = \\'output_graph.pb\\'\\n    if FLAGS.remove_export:\\n        if os.path.isdir(FLAGS.export_dir):\\n            log_info(\\'Removing old export\\')\\n            shutil.rmtree(FLAGS.export_dir)\\n    try:\\n        output_graph_path = os.path.join(FLAGS.export_dir, output_filename)\\n\\n        if not os.path.isdir(FLAGS.export_dir):\\n            os.makedirs(FLAGS.export_dir)\\n\\n        def do_graph_freeze(output_file=None, output_node_names=None, variables_blacklist=None):\\n            return freeze_graph.freeze_graph_with_def_protos(\\n                input_graph_def=tf.get_default_graph().as_graph_def(),\\n                input_saver_def=saver.as_saver_def(),\\n                input_checkpoint=checkpoint_path,\\n                output_node_names=output_node_names,\\n                restore_op_name=None,\\n                filename_tensor_name=None,\\n                output_graph=output_file,\\n                clear_devices=False,\\n                variable_names_blacklist=variables_blacklist,\\n                initializer_nodes=\\'\\')\\n\\n        if not FLAGS.export_tflite:\\n            frozen_graph = do_graph_freeze(output_node_names=output_names, variables_blacklist=\\'previous_state_c,previous_state_h\\')\\n            frozen_graph.version = int(file_relative_read(\\'GRAPH_VERSION\\').strip())\\n\\n            # Add a no-op node to the graph with metadata information to be loaded by the native client\\n            metadata = frozen_graph.node.add()\\n            metadata.name = \\'model_metadata\\'\\n            metadata.op = \\'NoOp\\'\\n            metadata.attr[\\'sample_rate\\'].i = FLAGS.audio_sample_rate\\n            metadata.attr[\\'feature_win_len\\'].i = FLAGS.feature_win_len\\n            metadata.attr[\\'feature_win_step\\'].i = FLAGS.feature_win_step\\n            if FLAGS.export_language:\\n                metadata.attr[\\'language\\'].s = FLAGS.export_language.encode(\\'ascii\\')\\n\\n            with open(output_graph_path, \\'wb\\') as fout:\\n                fout.write(frozen_graph.SerializeToString())\\n        else:\\n            frozen_graph = do_graph_freeze(output_node_names=output_names, variables_blacklist=\\'\\')\\n            output_tflite_path = os.path.join(FLAGS.export_dir, output_filename.replace(\\'.pb\\', \\'.tflite\\'))\\n\\n            converter = tf.lite.TFLiteConverter(frozen_graph, input_tensors=inputs.values(), output_tensors=outputs.values())\\n            converter.post_training_quantize = True\\n            # AudioSpectrogram and Mfcc ops are custom but have built-in kernels in TFLite\\n            converter.allow_custom_ops = True\\n            tflite_model = converter.convert()\\n\\n            with open(output_tflite_path, \\'wb\\') as fout:\\n                fout.write(tflite_model)\\n\\n            log_info(\\'Exported model for TF Lite engine as {}\\'.format(os.path.basename(output_tflite_path)))\\n\\n        log_info(\\'Models exported at %s\\' % (FLAGS.export_dir))\\n    except RuntimeError as e:\\n        log_error(str(e))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'random_square_mask',\n",
       "  'docstring': 'Create a numpy array with specified shape and masked fraction.\\n\\n  Args:\\n    shape: tuple, shape of the mask to create.\\n    fraction: float, fraction of the mask area to populate with `mask_scalar`.\\n\\n  Returns:\\n    numpy.array: A numpy array storing the mask.',\n",
       "  'code': 'def random_square_mask(shape, fraction):\\n  \"\"\"Create a numpy array with specified shape and masked fraction.\\n\\n  Args:\\n    shape: tuple, shape of the mask to create.\\n    fraction: float, fraction of the mask area to populate with `mask_scalar`.\\n\\n  Returns:\\n    numpy.array: A numpy array storing the mask.\\n  \"\"\"\\n\\n  mask = np.ones(shape)\\n\\n  patch_area = shape[0]*shape[1]*fraction\\n  patch_dim = np.int(math.floor(math.sqrt(patch_area)))\\n  if patch_area == 0 or patch_dim == 0:\\n    return mask\\n\\n  x = np.random.randint(shape[0] - patch_dim)\\n  y = np.random.randint(shape[1] - patch_dim)\\n\\n  mask[x:(x + patch_dim), y:(y + patch_dim), :] = 0\\n\\n  return mask',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'transformer_moe_2k',\n",
       "  'docstring': 'Base transformers model with moe.\\n\\n  Will have the following architecture:\\n  * No encoder.\\n    * Layer 0: a - sep  (self-attention - unmasked separable convolutions)\\n    * Layer 1: a - sep\\n    * Layer 2: a - sep\\n    * Layer 3: a - sep\\n    * Layer 4: a - sep\\n  * Decoder architecture:\\n    * Layer 0: a - a - sepm  (self-attention - enco/deco-attention - masked sep)\\n    * Layer 1: a - a - sepm\\n    * Layer 2: a - a - moe  (mixture of expert layers in the middle)\\n    * Layer 3: a - a - sepm\\n    * Layer 4: a - a - sepm\\n\\n  Returns:\\n    hparams',\n",
       "  'code': 'def transformer_moe_2k():\\n  \"\"\"Base transformers model with moe.\\n\\n  Will have the following architecture:\\n  * No encoder.\\n    * Layer 0: a - sep  (self-attention - unmasked separable convolutions)\\n    * Layer 1: a - sep\\n    * Layer 2: a - sep\\n    * Layer 3: a - sep\\n    * Layer 4: a - sep\\n  * Decoder architecture:\\n    * Layer 0: a - a - sepm  (self-attention - enco/deco-attention - masked sep)\\n    * Layer 1: a - a - sepm\\n    * Layer 2: a - a - moe  (mixture of expert layers in the middle)\\n    * Layer 3: a - a - sepm\\n    * Layer 4: a - a - sepm\\n\\n  Returns:\\n    hparams\\n  \"\"\"\\n  hparams = transformer_moe_8k()\\n  hparams.batch_size = 2048\\n\\n  hparams.default_ff = \"sep\"\\n\\n  # hparams.layer_types contains the network architecture:\\n  encoder_archi = \"a/a/a/a/a\"\\n  decoder_archi = \"a-sepm/a-sepm/a-moe/a-sepm/a-sepm\"\\n  hparams.layer_types = \"{}#{}\".format(encoder_archi, decoder_archi)\\n\\n  return hparams',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Problem.decode_example',\n",
       "  'docstring': 'Return a dict of Tensors from a serialized tensorflow.Example.',\n",
       "  'code': 'def decode_example(self, serialized_example):\\n    \"\"\"Return a dict of Tensors from a serialized tensorflow.Example.\"\"\"\\n    data_fields, data_items_to_decoders = self.example_reading_spec()\\n    # Necessary to rejoin examples in the correct order with the Cloud ML Engine\\n    # batch prediction API.\\n    data_fields[\"batch_prediction_key\"] = tf.FixedLenFeature([1], tf.int64, 0)\\n    if data_items_to_decoders is None:\\n      data_items_to_decoders = {\\n          field: tf.contrib.slim.tfexample_decoder.Tensor(field)\\n          for field in data_fields\\n      }\\n\\n    decoder = tf.contrib.slim.tfexample_decoder.TFExampleDecoder(\\n        data_fields, data_items_to_decoders)\\n\\n    decode_items = list(sorted(data_items_to_decoders))\\n    decoded = decoder.decode(serialized_example, items=decode_items)\\n    return dict(zip(decode_items, decoded))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Problem._dataset_partition',\n",
       "  'docstring': 'Which part of the training data to read.\\n\\n    If there are multiple parallel calls to input_fn (multiple TPU hosts),\\n    then we want each one to read from a separate partition of the training\\n    data.\\n\\n    Args:\\n      mode: tf.estimator.ModeKeys\\n      config: RunConfig\\n      params: A dict that contains parameters.\\n    Returns:\\n      partition_id: an integer\\n      num_partitions: an integer',\n",
       "  'code': 'def _dataset_partition(self, mode, config, params):\\n    \"\"\"Which part of the training data to read.\\n\\n    If there are multiple parallel calls to input_fn (multiple TPU hosts),\\n    then we want each one to read from a separate partition of the training\\n    data.\\n\\n    Args:\\n      mode: tf.estimator.ModeKeys\\n      config: RunConfig\\n      params: A dict that contains parameters.\\n    Returns:\\n      partition_id: an integer\\n      num_partitions: an integer\\n    \"\"\"\\n    if mode != tf.estimator.ModeKeys.TRAIN or not hasattr(config, \"tpu_config\"):\\n      # Reset in the case when using TPU but alternating TRAIN and EVAL.\\n      self._next_partition_id = 0\\n      return 0, 1\\n    phift = config.tpu_config.per_host_input_for_training\\n    # This is the mesh-tensorflow case.\\n    if (hasattr(tpu_config.InputPipelineConfig, \"BROADCAST\") and\\n        phift == tpu_config.InputPipelineConfig.BROADCAST):\\n      return 0, 1\\n    if phift:\\n      num_hosts = (params[\"context\"].num_hosts if \"context\" in params\\n                   else config.tpu_config.num_shards // 8)\\n      num_partitions = max(num_hosts, 1)\\n    else:\\n      num_partitions = config.tpu_config.num_shards\\n    partition_id = getattr(self, \"_next_partition_id\", 0)\\n    self._next_partition_id = partition_id + 1\\n    tf.logging.info(\"num_partitions = %d partition_id = %d\" %\\n                    (num_partitions, partition_id))\\n    assert partition_id < num_partitions\\n    return partition_id, num_partitions',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'get_att_mats',\n",
       "  'docstring': \"Get's the tensors representing the attentions from a build model.\\n\\n  The attentions are stored in a dict on the Transformer object while building\\n  the graph.\\n\\n  Args:\\n    translate_model: Transformer object to fetch the attention weights from.\\n\\n  Returns:\\n  Tuple of attention matrices; (\\n      enc_atts: Encoder self attention weights.\\n        A list of `num_layers` numpy arrays of size\\n        (batch_size, num_heads, inp_len, inp_len)\\n      dec_atts: Decoder self attetnion weights.\\n        A list of `num_layers` numpy arrays of size\\n        (batch_size, num_heads, out_len, out_len)\\n      encdec_atts: Encoder-Decoder attention weights.\\n        A list of `num_layers` numpy arrays of size\\n        (batch_size, num_heads, out_len, inp_len)\\n  )\",\n",
       "  'code': 'def get_att_mats(translate_model):\\n  \"\"\"Get\\'s the tensors representing the attentions from a build model.\\n\\n  The attentions are stored in a dict on the Transformer object while building\\n  the graph.\\n\\n  Args:\\n    translate_model: Transformer object to fetch the attention weights from.\\n\\n  Returns:\\n  Tuple of attention matrices; (\\n      enc_atts: Encoder self attention weights.\\n        A list of `num_layers` numpy arrays of size\\n        (batch_size, num_heads, inp_len, inp_len)\\n      dec_atts: Decoder self attetnion weights.\\n        A list of `num_layers` numpy arrays of size\\n        (batch_size, num_heads, out_len, out_len)\\n      encdec_atts: Encoder-Decoder attention weights.\\n        A list of `num_layers` numpy arrays of size\\n        (batch_size, num_heads, out_len, inp_len)\\n  )\\n  \"\"\"\\n  enc_atts = []\\n  dec_atts = []\\n  encdec_atts = []\\n\\n  prefix = \"transformer/body/\"\\n  postfix_self_attention = \"/multihead_attention/dot_product_attention\"\\n  if translate_model.hparams.self_attention_type == \"dot_product_relative\":\\n    postfix_self_attention = (\"/multihead_attention/\"\\n                              \"dot_product_attention_relative\")\\n  postfix_encdec = \"/multihead_attention/dot_product_attention\"\\n\\n  for i in range(translate_model.hparams.num_hidden_layers):\\n    enc_att = translate_model.attention_weights[\\n        \"%sencoder/layer_%i/self_attention%s\"\\n        % (prefix, i, postfix_self_attention)]\\n    dec_att = translate_model.attention_weights[\\n        \"%sdecoder/layer_%i/self_attention%s\"\\n        % (prefix, i, postfix_self_attention)]\\n    encdec_att = translate_model.attention_weights[\\n        \"%sdecoder/layer_%i/encdec_attention%s\" % (prefix, i, postfix_encdec)]\\n    enc_atts.append(enc_att)\\n    dec_atts.append(dec_att)\\n    encdec_atts.append(encdec_att)\\n\\n  return enc_atts, dec_atts, encdec_atts',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'AttentionVisualizer.get_vis_data_from_string',\n",
       "  'docstring': 'Constructs the data needed for visualizing attentions.\\n\\n    Args:\\n      sess: A tf.Session object.\\n      input_string: The input sentence to be translated and visualized.\\n\\n    Returns:\\n      Tuple of (\\n          output_string: The translated sentence.\\n          input_list: Tokenized input sentence.\\n          output_list: Tokenized translation.\\n          att_mats: Tuple of attention matrices; (\\n              enc_atts: Encoder self attention weights.\\n                A list of `num_layers` numpy arrays of size\\n                (batch_size, num_heads, inp_len, inp_len)\\n              dec_atts: Decoder self attention weights.\\n                A list of `num_layers` numpy arrays of size\\n                (batch_size, num_heads, out_len, out_len)\\n              encdec_atts: Encoder-Decoder attention weights.\\n                A list of `num_layers` numpy arrays of size\\n                (batch_size, num_heads, out_len, inp_len)\\n          )',\n",
       "  'code': 'def get_vis_data_from_string(self, sess, input_string):\\n    \"\"\"Constructs the data needed for visualizing attentions.\\n\\n    Args:\\n      sess: A tf.Session object.\\n      input_string: The input sentence to be translated and visualized.\\n\\n    Returns:\\n      Tuple of (\\n          output_string: The translated sentence.\\n          input_list: Tokenized input sentence.\\n          output_list: Tokenized translation.\\n          att_mats: Tuple of attention matrices; (\\n              enc_atts: Encoder self attention weights.\\n                A list of `num_layers` numpy arrays of size\\n                (batch_size, num_heads, inp_len, inp_len)\\n              dec_atts: Decoder self attention weights.\\n                A list of `num_layers` numpy arrays of size\\n                (batch_size, num_heads, out_len, out_len)\\n              encdec_atts: Encoder-Decoder attention weights.\\n                A list of `num_layers` numpy arrays of size\\n                (batch_size, num_heads, out_len, inp_len)\\n          )\\n    \"\"\"\\n    encoded_inputs = self.encode(input_string)\\n\\n    # Run inference graph to get the translation.\\n    out = sess.run(self.samples, {\\n        self.inputs: encoded_inputs,\\n    })\\n\\n    # Run the decoded translation through the training graph to get the\\n    # attention tensors.\\n    att_mats = sess.run(self.att_mats, {\\n        self.inputs: encoded_inputs,\\n        self.targets: np.reshape(out, [1, -1, 1, 1]),\\n    })\\n\\n    output_string = self.decode(out)\\n    input_list = self.decode_list(encoded_inputs)\\n    output_list = self.decode_list(out)\\n\\n    return output_string, input_list, output_list, att_mats',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'get_sari',\n",
       "  'docstring': 'Computes the SARI scores from the given source, prediction and targets.\\n\\n  Args:\\n    source_ids: A 2D tf.Tensor of size (batch_size , sequence_length)\\n    prediction_ids: A 2D tf.Tensor of size (batch_size, sequence_length)\\n    target_ids: A 3D tf.Tensor of size (batch_size, number_of_targets,\\n        sequence_length)\\n    max_gram_size: int. largest n-gram size we care about (e.g. 3 for unigrams,\\n        bigrams, and trigrams)\\n\\n  Returns:\\n    A 4-tuple of 1D float Tensors of size (batch_size) for the SARI score and\\n        the keep, addition and deletion scores.',\n",
       "  'code': 'def get_sari(source_ids, prediction_ids, target_ids, max_gram_size=4):\\n  \"\"\"Computes the SARI scores from the given source, prediction and targets.\\n\\n  Args:\\n    source_ids: A 2D tf.Tensor of size (batch_size , sequence_length)\\n    prediction_ids: A 2D tf.Tensor of size (batch_size, sequence_length)\\n    target_ids: A 3D tf.Tensor of size (batch_size, number_of_targets,\\n        sequence_length)\\n    max_gram_size: int. largest n-gram size we care about (e.g. 3 for unigrams,\\n        bigrams, and trigrams)\\n\\n  Returns:\\n    A 4-tuple of 1D float Tensors of size (batch_size) for the SARI score and\\n        the keep, addition and deletion scores.\\n  \"\"\"\\n\\n  def get_sari_numpy(source_ids, prediction_ids, target_ids):\\n    \"\"\"Iterate over elements in the batch and call the SARI function.\"\"\"\\n    sari_scores = []\\n    keep_scores = []\\n    add_scores = []\\n    deletion_scores = []\\n    # Iterate over elements in the batch.\\n    for source_ids_i, prediction_ids_i, target_ids_i in zip(\\n        source_ids, prediction_ids, target_ids):\\n      sari, keep, add, deletion = get_sari_score(\\n          source_ids_i, prediction_ids_i, target_ids_i, max_gram_size,\\n          BETA_FOR_SARI_DELETION_F_MEASURE)\\n      sari_scores.append(sari)\\n      keep_scores.append(keep)\\n      add_scores.append(add)\\n      deletion_scores.append(deletion)\\n    return (np.asarray(sari_scores), np.asarray(keep_scores),\\n            np.asarray(add_scores), np.asarray(deletion_scores))\\n\\n  sari, keep, add, deletion = tf.py_func(\\n      get_sari_numpy,\\n      [source_ids, prediction_ids, target_ids],\\n      [tf.float64, tf.float64, tf.float64, tf.float64])\\n  return sari, keep, add, deletion',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_extract_mnist_images',\n",
       "  'docstring': 'Extract images from an MNIST file into a numpy array.\\n\\n  Args:\\n    filename: The path to an MNIST images file.\\n    num_images: The number of images in the file.\\n\\n  Returns:\\n    A numpy array of shape [number_of_images, height, width, channels].',\n",
       "  'code': 'def _extract_mnist_images(filename, num_images):\\n  \"\"\"Extract images from an MNIST file into a numpy array.\\n\\n  Args:\\n    filename: The path to an MNIST images file.\\n    num_images: The number of images in the file.\\n\\n  Returns:\\n    A numpy array of shape [number_of_images, height, width, channels].\\n  \"\"\"\\n  with gzip.open(filename) as bytestream:\\n    bytestream.read(16)\\n    buf = bytestream.read(_MNIST_IMAGE_SIZE * _MNIST_IMAGE_SIZE * num_images)\\n    data = np.frombuffer(buf, dtype=np.uint8)\\n    data = data.reshape(num_images, _MNIST_IMAGE_SIZE, _MNIST_IMAGE_SIZE, 1)\\n  return data',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_extract_mnist_labels',\n",
       "  'docstring': 'Extract labels from an MNIST file into integers.\\n\\n  Args:\\n    filename: The path to an MNIST labels file.\\n    num_labels: The number of labels in the file.\\n\\n  Returns:\\n    A int64 numpy array of shape [num_labels]',\n",
       "  'code': 'def _extract_mnist_labels(filename, num_labels):\\n  \"\"\"Extract labels from an MNIST file into integers.\\n\\n  Args:\\n    filename: The path to an MNIST labels file.\\n    num_labels: The number of labels in the file.\\n\\n  Returns:\\n    A int64 numpy array of shape [num_labels]\\n  \"\"\"\\n  with gzip.open(filename) as bytestream:\\n    bytestream.read(8)\\n    buf = bytestream.read(num_labels)\\n    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\\n  return labels',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'produce_examples',\n",
       "  'docstring': 'Produce examples from shard_ids to out_filepaths.',\n",
       "  'code': 'def produce_examples(shard_ids, wikis_dir, refs_dir, urls_dir, vocab_path,\\n                     out_filepaths):\\n  \"\"\"Produce examples from shard_ids to out_filepaths.\"\"\"\\n  # * Join the Wikipedia articles with their references\\n  # * Run Tf-idf to sort reference paragraphs\\n  # * Encode the Wikipedia and reference text with the vocabulary\\n  # * Write out TFRecords of tensorflow.Example\\n  tf.logging.info(\"Processing %d input shards into %d output files.\",\\n                  len(shard_ids), len(out_filepaths))\\n\\n  vocab = text_encoder.SubwordTextEncoder(vocab_path)\\n  eot_ids = vocab.encode(EOT)\\n\\n  def example_generator():\\n    \"\"\"Generate Example dicts.\"\"\"\\n    stats = dict(total_original_wikis=0, total_original_refs=0,\\n                 total_found_refs=0, ref_lengths=[], wiki_original_refs=[],\\n                 wiki_found_refs=[], wikis_skipped_no_refs=0,\\n                 wikis_skipped_short_lead=0, num_wikis_written=0)\\n    ref_files_by_shard = _references_files_by_shard(refs_dir)\\n    for shard_id in shard_ids:\\n      tf.logging.info(\"Processing shard %d\", shard_id)\\n      wiki_urls = _wiki_urls_for_shard(shard_id, urls_dir)\\n      tf.logging.info(\"Loaded wiki URLs for shard\")\\n      refs_content = _references_content(ref_files_by_shard[shard_id])\\n      tf.logging.info(\"Loaded reference content for shard\")\\n      for i, wiki in enumerate(_wiki_articles(shard_id, wikis_dir)):\\n        if not i % 1000:\\n          tf.logging.info(\"Processing wiki index %d for shard %d\", i, shard_id)\\n        stats[\"total_original_wikis\"] += 1\\n\\n        # Get reference content\\n        wiki_ref_content = []\\n        ref_urls = wiki_urls[wiki.url][\"refs\"]\\n        stats[\"total_original_refs\"] += len(ref_urls)\\n        stats_wiki_original_refs = len(ref_urls)\\n        stats_wiki_found_refs = 0\\n        for ref_url in ref_urls:\\n          ref_content = refs_content.get(ref_url)\\n          if not ref_content:\\n            continue\\n          stats[\"total_found_refs\"] += 1\\n          stats[\"ref_lengths\"].append(len(ref_content))\\n          stats_wiki_found_refs += 1\\n          wiki_ref_content.append(ref_content)\\n\\n        stats[\"wiki_original_refs\"].append(stats_wiki_original_refs)\\n        stats[\"wiki_found_refs\"].append(stats_wiki_found_refs)\\n        if not wiki_ref_content or len(wiki_ref_content) < _MIN_REFS:\\n          # No/few refs were found\\n          stats[\"wikis_skipped_no_refs\"] += 1\\n          continue\\n\\n        # Rank reference paragraphs with TFIDF\\n        wiki_title = _normalize_text(wiki.title)\\n        ranked_paragraphs = rank_reference_paragraphs(wiki_title,\\n                                                      wiki_ref_content)\\n\\n        # Construct inputs from Wiki title and references\\n        inputs = []\\n        inputs.extend(vocab.encode(wiki_title))\\n        inputs.extend(eot_ids)\\n        for paragraph in ranked_paragraphs:\\n          if len(inputs) >= 1e6:\\n            break\\n          paragraph += \" \"\\n          inputs.extend(vocab.encode(paragraph))\\n\\n        # Construct targets from article sections\\n        targets, section_boundaries = _encode_wiki_sections(\\n            wiki.sections, vocab)\\n\\n        # Skip if lead section is too short\\n        if (not section_boundaries or\\n            section_boundaries[0] < _MIN_LEADSECTION_TOKENS):\\n          stats[\"wikis_skipped_short_lead\"] += 1\\n          continue\\n\\n        inputs.append(text_encoder.EOS_ID)\\n        targets.append(text_encoder.EOS_ID)\\n\\n        stats[\"num_wikis_written\"] += 1\\n        yield {\\n            \"inputs\": inputs,\\n            \"targets\": targets,\\n            \"section_boundaries\": section_boundaries,\\n        }\\n\\n    tf.logging.info(\"Total: %d, Skipped: %d\",\\n                    stats[\"num_wikis_written\"],\\n                    stats[\"total_original_wikis\"] - stats[\"num_wikis_written\"])\\n    tf.logging.info(\"Total refs: %d, Skipped refs: %d\",\\n                    stats[\"total_found_refs\"],\\n                    stats[\"total_original_refs\"] - stats[\"total_found_refs\"])\\n    stats_fname = os.path.join(os.path.split(out_filepaths[0])[0],\\n                               \"stats.%d.json\" % shard_ids[0])\\n    with tf.gfile.Open(stats_fname, \"w\") as f:\\n      f.write(json.dumps(stats))\\n\\n  generator_utils.generate_files(example_generator(), out_filepaths)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'float16_activations_var_getter',\n",
       "  'docstring': 'A custom getter function for float32 parameters and float16 activations.\\n\\n  This function ensures the following:\\n    1. All variables requested with type fp16 are stored as type fp32.\\n    2. All variables requested with type fp32 are returned as type fp16.\\n  See https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/\\n  #training_tensorflow for more information on this strategy.\\n\\n  Args:\\n    getter: custom getter\\n    *args: arguments\\n    **kwargs: keyword arguments\\n\\n  Returns:\\n    variables with the correct dtype.\\n\\n  Raises:\\n    KeyError: if \"dtype\" is not provided as a kwarg.',\n",
       "  'code': 'def float16_activations_var_getter(getter, *args, **kwargs):\\n  \"\"\"A custom getter function for float32 parameters and float16 activations.\\n\\n  This function ensures the following:\\n    1. All variables requested with type fp16 are stored as type fp32.\\n    2. All variables requested with type fp32 are returned as type fp16.\\n  See https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/\\n  #training_tensorflow for more information on this strategy.\\n\\n  Args:\\n    getter: custom getter\\n    *args: arguments\\n    **kwargs: keyword arguments\\n\\n  Returns:\\n    variables with the correct dtype.\\n\\n  Raises:\\n    KeyError: if \"dtype\" is not provided as a kwarg.\\n  \"\"\"\\n  requested_dtype = kwargs[\"dtype\"]\\n\\n  if requested_dtype == tf.float16:\\n    kwargs[\"dtype\"] = tf.float32\\n\\n  if requested_dtype == tf.float32:\\n    requested_dtype = tf.float16\\n  var = getter(*args, **kwargs)\\n  # This if statement is needed to guard the cast, because batch norm\\n  # assigns directly to the return value of this custom getter. The cast\\n  # makes the return value not a variable so it cannot be assigned. Batch\\n  # norm variables are always in fp32 so this if statement is never\\n  # triggered for them.\\n  if var.dtype.base_dtype != requested_dtype:\\n    var = tf.cast(var, requested_dtype)\\n  return var',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'compute_one_decoding_video_metrics',\n",
       "  'docstring': 'Computes the average of all the metric for one decoding.\\n\\n  Args:\\n    iterator: dataset iterator.\\n    feed_dict: feed dict to initialize iterator.\\n    num_videos: number of videos.\\n\\n  Returns:\\n    all_psnr: 2-D Numpy array, shape=(num_samples, num_frames)\\n    all_ssim: 2-D Numpy array, shape=(num_samples, num_frames)',\n",
       "  'code': 'def compute_one_decoding_video_metrics(iterator, feed_dict, num_videos):\\n  \"\"\"Computes the average of all the metric for one decoding.\\n\\n  Args:\\n    iterator: dataset iterator.\\n    feed_dict: feed dict to initialize iterator.\\n    num_videos: number of videos.\\n\\n  Returns:\\n    all_psnr: 2-D Numpy array, shape=(num_samples, num_frames)\\n    all_ssim: 2-D Numpy array, shape=(num_samples, num_frames)\\n  \"\"\"\\n  output, target = iterator.get_next()\\n  metrics = psnr_and_ssim(output, target)\\n\\n  with tf.Session() as sess:\\n    sess.run(tf.local_variables_initializer())\\n    initalizer = iterator._initializer  # pylint: disable=protected-access\\n    if initalizer is not None:\\n      sess.run(initalizer, feed_dict=feed_dict)\\n\\n    all_psnr, all_ssim = [], []\\n    for i in range(num_videos):\\n      print(\"Computing video: %d\" % i)\\n      psnr_np, ssim_np = sess.run(metrics)\\n      all_psnr.append(psnr_np)\\n      all_ssim.append(ssim_np)\\n    all_psnr = np.array(all_psnr)\\n    all_ssim = np.array(all_ssim)\\n    return all_psnr, all_ssim',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'reduce_to_best_decode',\n",
       "  'docstring': 'Extracts the best-decode from the metrics according to reduce_func.\\n\\n  Args:\\n    metrics: 3-D numpy array, shape=(num_decodes, num_samples, num_frames)\\n    reduce_func: callable, np.argmax or np.argmin.\\n  Returns:\\n    best_metrics: 2-D numpy array, shape=(num_samples, num_frames).\\n    best_decode_ind: 1-D numpy array, shape=(num_samples,)',\n",
       "  'code': 'def reduce_to_best_decode(metrics, reduce_func):\\n  \"\"\"Extracts the best-decode from the metrics according to reduce_func.\\n\\n  Args:\\n    metrics: 3-D numpy array, shape=(num_decodes, num_samples, num_frames)\\n    reduce_func: callable, np.argmax or np.argmin.\\n  Returns:\\n    best_metrics: 2-D numpy array, shape=(num_samples, num_frames).\\n    best_decode_ind: 1-D numpy array, shape=(num_samples,)\\n  \"\"\"\\n  num_videos = metrics.shape[1]\\n  # Take mean of the metric across the frames to approximate the video\\n  # closest to the ground truth.\\n  mean_across_frames = np.mean(metrics, axis=-1)\\n\\n  # For every sample, use the decode that has a maximum mean-metric.\\n  best_decode_ind = reduce_func(mean_across_frames, axis=0)\\n  best_metrics = metrics[best_decode_ind, np.arange(num_videos), :]\\n  return best_metrics, best_decode_ind',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'compute_all_metrics_statistics',\n",
       "  'docstring': 'Computes statistics of metrics across multiple decodings.\\n\\n  Args:\\n    all_results: dict of 3-D numpy arrays.\\n                 Each array has shape=(num_decodes, num_samples, num_frames).\\n  Returns:\\n    statistics: dict of 1-D numpy arrays, shape=(num_frames).\\n                First the statistic (max/mean/std) is computed across the\\n                decodes, then the mean is taken across num_samples.\\n    decode_inds: dict of 1-D numpy arrays, shape=(num_samples,)\\n                 Each element represents the index of the decode corresponding\\n                 to the best statistic.',\n",
       "  'code': 'def compute_all_metrics_statistics(all_results):\\n  \"\"\"Computes statistics of metrics across multiple decodings.\\n\\n  Args:\\n    all_results: dict of 3-D numpy arrays.\\n                 Each array has shape=(num_decodes, num_samples, num_frames).\\n  Returns:\\n    statistics: dict of 1-D numpy arrays, shape=(num_frames).\\n                First the statistic (max/mean/std) is computed across the\\n                decodes, then the mean is taken across num_samples.\\n    decode_inds: dict of 1-D numpy arrays, shape=(num_samples,)\\n                 Each element represents the index of the decode corresponding\\n                 to the best statistic.\\n  \"\"\"\\n  statistics = {}\\n  decode_inds = {}\\n  all_metrics = all_results.keys()\\n\\n  for key in all_metrics:\\n    values = all_results[key]\\n    statistics[key + \"_MEAN\"] = np.mean(values, axis=0)\\n    statistics[key + \"_STD\"] = np.std(values, axis=0)\\n    min_stats, min_decode_ind = reduce_to_best_decode(values, np.argmin)\\n    statistics[key + \"_MIN\"] = min_stats\\n    decode_inds[key + \"_MIN_DECODE\"] = min_decode_ind\\n    max_stats, max_decode_ind = reduce_to_best_decode(values, np.argmax)\\n    statistics[key + \"_MAX\"] = max_stats\\n    decode_inds[key + \"_MAX_DECODE\"] = max_decode_ind\\n\\n  # Computes mean of each statistic across the dataset.\\n  for key in statistics:\\n    statistics[key] = np.mean(statistics[key], axis=0)\\n  return statistics, decode_inds',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_encode_gif',\n",
       "  'docstring': 'Encodes numpy images into gif string.\\n\\n  Args:\\n    images: A 4-D `uint8` `np.array` (or a list of 3-D images) of shape\\n      `[time, height, width, channels]` where `channels` is 1 or 3.\\n    fps: frames per second of the animation\\n\\n  Returns:\\n    The encoded gif string.\\n\\n  Raises:\\n    IOError: If the ffmpeg command returns an error.',\n",
       "  'code': 'def _encode_gif(images, fps):\\n  \"\"\"Encodes numpy images into gif string.\\n\\n  Args:\\n    images: A 4-D `uint8` `np.array` (or a list of 3-D images) of shape\\n      `[time, height, width, channels]` where `channels` is 1 or 3.\\n    fps: frames per second of the animation\\n\\n  Returns:\\n    The encoded gif string.\\n\\n  Raises:\\n    IOError: If the ffmpeg command returns an error.\\n  \"\"\"\\n  writer = WholeVideoWriter(fps)\\n  writer.write_multi(images)\\n  return writer.finish()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'YellowFinOptimizer.minimize',\n",
       "  'docstring': 'Adapted from TensorFlow Optimizer base class member function.\\n\\n    Add operations to minimize `loss` by updating `var_list`.\\n    This method simply combines calls `compute_gradients()` and\\n    `apply_gradients()`. If you want to process the gradient before applying\\n    them call `tf.gradients()` and `self.apply_gradients()` explicitly instead\\n    of using this function.\\n\\n    Args:\\n      loss: A Tensor containing the value to minimize.\\n      global_step: Optional Variable to increment by one after the variables\\n        have been updated.\\n      var_list: Optional list or tuple of Variable objects to update to\\n        minimize loss. Defaults to the list of variables collected in\\n        the graph under the key GraphKeys.TRAINABLE_VARIABLES.\\n      gate_gradients: How to gate the computation of gradients.\\n        Can be GATE_NONE, GATE_OP, or GATE_GRAPH.\\n      aggregation_method: Specifies the method used to combine gradient terms.\\n        Valid values are defined in the class AggregationMethod.\\n      colocate_gradients_with_ops: If True, try collocating gradients with\\n        the corresponding op.\\n      name: Optional name for the returned operation.\\n      grad_loss: Optional. A Tensor holding the gradient computed for loss.\\n\\n    Returns:\\n      An Operation that updates the variables in var_list.\\n        If global_step was not None, that operation also increments global_step.\\n\\n    Raises:\\n      ValueError: if no gradients are provided for any variable.',\n",
       "  'code': 'def minimize(self,\\n               loss,\\n               global_step=None,\\n               var_list=None,\\n               gate_gradients=GATE_OP,\\n               aggregation_method=None,\\n               colocate_gradients_with_ops=False,\\n               name=None,\\n               grad_loss=None):\\n    \"\"\"Adapted from TensorFlow Optimizer base class member function.\\n\\n    Add operations to minimize `loss` by updating `var_list`.\\n    This method simply combines calls `compute_gradients()` and\\n    `apply_gradients()`. If you want to process the gradient before applying\\n    them call `tf.gradients()` and `self.apply_gradients()` explicitly instead\\n    of using this function.\\n\\n    Args:\\n      loss: A Tensor containing the value to minimize.\\n      global_step: Optional Variable to increment by one after the variables\\n        have been updated.\\n      var_list: Optional list or tuple of Variable objects to update to\\n        minimize loss. Defaults to the list of variables collected in\\n        the graph under the key GraphKeys.TRAINABLE_VARIABLES.\\n      gate_gradients: How to gate the computation of gradients.\\n        Can be GATE_NONE, GATE_OP, or GATE_GRAPH.\\n      aggregation_method: Specifies the method used to combine gradient terms.\\n        Valid values are defined in the class AggregationMethod.\\n      colocate_gradients_with_ops: If True, try collocating gradients with\\n        the corresponding op.\\n      name: Optional name for the returned operation.\\n      grad_loss: Optional. A Tensor holding the gradient computed for loss.\\n\\n    Returns:\\n      An Operation that updates the variables in var_list.\\n        If global_step was not None, that operation also increments global_step.\\n\\n    Raises:\\n      ValueError: if no gradients are provided for any variable.\\n    \"\"\"\\n    grads_and_vars = self._momentum_optimizer.compute_gradients(\\n        loss,\\n        var_list=var_list,\\n        gate_gradients=gate_gradients,\\n        aggregation_method=aggregation_method,\\n        colocate_gradients_with_ops=colocate_gradients_with_ops,\\n        grad_loss=grad_loss)\\n\\n    vars_with_grad = [v for g, v in grads_and_vars if g is not None]\\n    if not vars_with_grad:\\n      raise ValueError(\\n          \"No gradients provided for any variable, check your graph for ops\"\\n          \" that do not support gradients, between variables %s and loss %s.\" %\\n          ([str(v) for _, v in grads_and_vars], loss))\\n    for g, v in grads_and_vars:\\n      print(\"g \", g)\\n      print(\"v \", v)\\n\\n    return self.apply_gradients(grads_and_vars,\\n                                global_step=global_step,\\n                                name=name)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'BatchNorm',\n",
       "  'docstring': 'Layer construction function for a batch normalization layer.',\n",
       "  'code': 'def BatchNorm(x, params, axis=(0, 1, 2), epsilon=1e-5,\\n              center=True, scale=True, **unused_kwargs):\\n  \"\"\"Layer construction function for a batch normalization layer.\"\"\"\\n  mean = np.mean(x, axis, keepdims=True)\\n  # Fast but less numerically-stable variance calculation than np.var.\\n  m1 = np.mean(x**2, axis, keepdims=True)\\n  var = m1 - mean**2\\n  z = (x - mean) / np.sqrt(var + epsilon)\\n\\n  # Expand the parameters to have the right axes.\\n  beta, gamma = params\\n  # TODO(phawkins): np.expand_dims should accept an axis tuple.\\n  # (https://github.com/numpy/numpy/issues/12290)\\n  ed = tuple(None if i in axis else slice(None) for i in range(np.ndim(x)))\\n  beta = beta[ed]\\n  gamma = gamma[ed]\\n\\n  # Return the z rescaled by the parameters if requested.\\n  if center and scale:\\n    return gamma * z + beta\\n  if center:\\n    return z + beta\\n  if scale:\\n    return gamma * z\\n  return z',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_original_vocab',\n",
       "  'docstring': 'Returns a set containing the original vocabulary.\\n\\n  This is important for comparing with published results.\\n\\n  Args:\\n    tmp_dir: directory containing dataset.\\n\\n  Returns:\\n    a set of strings',\n",
       "  'code': 'def _original_vocab(tmp_dir):\\n  \"\"\"Returns a set containing the original vocabulary.\\n\\n  This is important for comparing with published results.\\n\\n  Args:\\n    tmp_dir: directory containing dataset.\\n\\n  Returns:\\n    a set of strings\\n  \"\"\"\\n  vocab_url = (\"http://download.tensorflow.org/models/LM_LSTM_CNN/\"\\n               \"vocab-2016-09-10.txt\")\\n  vocab_filename = os.path.basename(vocab_url + \".en\")\\n  vocab_filepath = os.path.join(tmp_dir, vocab_filename)\\n  if not os.path.exists(vocab_filepath):\\n    generator_utils.maybe_download(tmp_dir, vocab_filename, vocab_url)\\n  return set([\\n      text_encoder.native_to_unicode(l.strip())\\n      for l in tf.gfile.Open(vocab_filepath)\\n  ])',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'interpolations_to_summary',\n",
       "  'docstring': 'Converts interpolated frames into tf summaries.\\n\\n  The summaries consists of:\\n    1. Image summary corresponding to the first frame.\\n    2. Image summary corresponding to the last frame.\\n    3. The interpolated frames as a gif summary.\\n\\n  Args:\\n    sample_ind: int\\n    interpolations: Numpy array, shape=(num_interp, H, W, 3)\\n    first_frame: Numpy array, shape=(HWC)\\n    last_frame: Numpy array, shape=(HWC)\\n    hparams: HParams, train hparams\\n    decode_hp: HParams, decode hparams\\n  Returns:\\n    summaries: list of tf Summary Values.',\n",
       "  'code': 'def interpolations_to_summary(sample_ind, interpolations, first_frame,\\n                              last_frame, hparams, decode_hp):\\n  \"\"\"Converts interpolated frames into tf summaries.\\n\\n  The summaries consists of:\\n    1. Image summary corresponding to the first frame.\\n    2. Image summary corresponding to the last frame.\\n    3. The interpolated frames as a gif summary.\\n\\n  Args:\\n    sample_ind: int\\n    interpolations: Numpy array, shape=(num_interp, H, W, 3)\\n    first_frame: Numpy array, shape=(HWC)\\n    last_frame: Numpy array, shape=(HWC)\\n    hparams: HParams, train hparams\\n    decode_hp: HParams, decode hparams\\n  Returns:\\n    summaries: list of tf Summary Values.\\n  \"\"\"\\n  parent_tag = \"sample_%d\" % sample_ind\\n  frame_shape = hparams.problem.frame_shape\\n  interp_shape = [hparams.batch_size, decode_hp.num_interp] + frame_shape\\n  interpolations = np.reshape(interpolations, interp_shape)\\n  interp_tag = \"%s/interp/%s\" % (parent_tag, decode_hp.channel_interp)\\n  if decode_hp.channel_interp == \"ranked\":\\n    interp_tag = \"%s/rank_%d\" % (interp_tag, decode_hp.rank_interp)\\n  summaries, _ = common_video.py_gif_summary(\\n      interp_tag, interpolations, return_summary_value=True,\\n      max_outputs=decode_hp.max_display_outputs,\\n      fps=decode_hp.frames_per_second)\\n\\n  if decode_hp.save_frames:\\n    first_frame_summ = image_utils.image_to_tf_summary_value(\\n        first_frame, \"%s/first\" % parent_tag)\\n    last_frame_summ = image_utils.image_to_tf_summary_value(\\n        last_frame, \"%s/last\" % parent_tag)\\n    summaries.append(first_frame_summ)\\n    summaries.append(last_frame_summ)\\n  return summaries',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_build_vocab',\n",
       "  'docstring': 'Reads a file to build a vocabulary of `vocab_size` most common words.\\n\\n   The vocabulary is sorted by occurrence count and has one word per line.\\n   Originally from:\\n   https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py\\n\\n  Args:\\n    filename: file to read list of words from.\\n    vocab_path: path where to save the vocabulary.\\n    vocab_size: size of the vocabulary to generate.',\n",
       "  'code': 'def _build_vocab(filename, vocab_path, vocab_size):\\n  \"\"\"Reads a file to build a vocabulary of `vocab_size` most common words.\\n\\n   The vocabulary is sorted by occurrence count and has one word per line.\\n   Originally from:\\n   https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py\\n\\n  Args:\\n    filename: file to read list of words from.\\n    vocab_path: path where to save the vocabulary.\\n    vocab_size: size of the vocabulary to generate.\\n  \"\"\"\\n  data = _read_words(filename)\\n  counter = collections.Counter(data)\\n  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\\n  words, _ = list(zip(*count_pairs))\\n  words = words[:vocab_size]\\n  with open(vocab_path, \"w\") as f:\\n    f.write(\"\\\\n\".join(words))',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_get_attention',\n",
       "  'docstring': \"Compute representation of the attention ready for the d3 visualization.\\n\\n  Args:\\n    inp_text: list of strings, words to be displayed on the left of the vis\\n    out_text: list of strings, words to be displayed on the right of the vis\\n    enc_atts: numpy array, encoder self-attentions\\n        [num_layers, batch_size, num_heads, enc_length, enc_length]\\n    dec_atts: numpy array, decoder self-attentions\\n        [num_layers, batch_size, num_heads, dec_length, dec_length]\\n    encdec_atts: numpy array, encoder-decoder attentions\\n        [num_layers, batch_size, num_heads, dec_length, enc_length]\\n\\n  Returns:\\n    Dictionary of attention representations with the structure:\\n    {\\n      'all': Representations for showing all attentions at the same time.\\n      'inp_inp': Representations for showing encoder self-attentions\\n      'inp_out': Representations for showing encoder-decoder attentions\\n      'out_out': Representations for showing decoder self-attentions\\n    }\\n    and each sub-dictionary has structure:\\n    {\\n      'att': list of inter attentions matrices, one for each attention head\\n      'top_text': list of strings, words to be displayed on the left of the vis\\n      'bot_text': list of strings, words to be displayed on the right of the vis\\n    }\",\n",
       "  'code': 'def _get_attention(inp_text, out_text, enc_atts, dec_atts, encdec_atts):\\n  \"\"\"Compute representation of the attention ready for the d3 visualization.\\n\\n  Args:\\n    inp_text: list of strings, words to be displayed on the left of the vis\\n    out_text: list of strings, words to be displayed on the right of the vis\\n    enc_atts: numpy array, encoder self-attentions\\n        [num_layers, batch_size, num_heads, enc_length, enc_length]\\n    dec_atts: numpy array, decoder self-attentions\\n        [num_layers, batch_size, num_heads, dec_length, dec_length]\\n    encdec_atts: numpy array, encoder-decoder attentions\\n        [num_layers, batch_size, num_heads, dec_length, enc_length]\\n\\n  Returns:\\n    Dictionary of attention representations with the structure:\\n    {\\n      \\'all\\': Representations for showing all attentions at the same time.\\n      \\'inp_inp\\': Representations for showing encoder self-attentions\\n      \\'inp_out\\': Representations for showing encoder-decoder attentions\\n      \\'out_out\\': Representations for showing decoder self-attentions\\n    }\\n    and each sub-dictionary has structure:\\n    {\\n      \\'att\\': list of inter attentions matrices, one for each attention head\\n      \\'top_text\\': list of strings, words to be displayed on the left of the vis\\n      \\'bot_text\\': list of strings, words to be displayed on the right of the vis\\n    }\\n  \"\"\"\\n  def get_full_attention(layer):\\n    \"\"\"Get the full input+output - input+output attentions.\"\"\"\\n    enc_att = enc_atts[layer][0]\\n    dec_att = dec_atts[layer][0]\\n    encdec_att = encdec_atts[layer][0]\\n    enc_att = np.transpose(enc_att, [0, 2, 1])\\n    dec_att = np.transpose(dec_att, [0, 2, 1])\\n    encdec_att = np.transpose(encdec_att, [0, 2, 1])\\n    # [heads, query_length, memory_length]\\n    enc_length = enc_att.shape[1]\\n    dec_length = dec_att.shape[1]\\n    num_heads = enc_att.shape[0]\\n    first = np.concatenate([enc_att, encdec_att], axis=2)\\n    second = np.concatenate(\\n        [np.zeros((num_heads, dec_length, enc_length)), dec_att], axis=2)\\n    full_att = np.concatenate([first, second], axis=1)\\n    return [ha.T.tolist() for ha in full_att]\\n\\n  def get_inp_inp_attention(layer):\\n    att = np.transpose(enc_atts[layer][0], (0, 2, 1))\\n    return [ha.T.tolist() for ha in att]\\n\\n  def get_out_inp_attention(layer):\\n    att = np.transpose(encdec_atts[layer][0], (0, 2, 1))\\n    return [ha.T.tolist() for ha in att]\\n\\n  def get_out_out_attention(layer):\\n    att = np.transpose(dec_atts[layer][0], (0, 2, 1))\\n    return [ha.T.tolist() for ha in att]\\n\\n  def get_attentions(get_attention_fn):\\n    num_layers = len(enc_atts)\\n    return [get_attention_fn(i) for i in range(num_layers)]\\n\\n  attentions = {\\n      \\'all\\': {\\n          \\'att\\': get_attentions(get_full_attention),\\n          \\'top_text\\': inp_text + out_text,\\n          \\'bot_text\\': inp_text + out_text,\\n      },\\n      \\'inp_inp\\': {\\n          \\'att\\': get_attentions(get_inp_inp_attention),\\n          \\'top_text\\': inp_text,\\n          \\'bot_text\\': inp_text,\\n      },\\n      \\'inp_out\\': {\\n          \\'att\\': get_attentions(get_out_inp_attention),\\n          \\'top_text\\': inp_text,\\n          \\'bot_text\\': out_text,\\n      },\\n      \\'out_out\\': {\\n          \\'att\\': get_attentions(get_out_out_attention),\\n          \\'top_text\\': out_text,\\n          \\'bot_text\\': out_text,\\n      },\\n  }\\n\\n  return attentions',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'make_grpc_request_fn',\n",
       "  'docstring': 'Wraps function to make grpc requests with runtime args.',\n",
       "  'code': 'def make_grpc_request_fn(servable_name, server, timeout_secs):\\n  \"\"\"Wraps function to make grpc requests with runtime args.\"\"\"\\n  stub = _create_stub(server)\\n\\n  def _make_grpc_request(examples):\\n    \"\"\"Builds and sends request to TensorFlow model server.\"\"\"\\n    request = predict_pb2.PredictRequest()\\n    request.model_spec.name = servable_name\\n    request.inputs[\"input\"].CopyFrom(\\n        tf.make_tensor_proto(\\n            [ex.SerializeToString() for ex in examples], shape=[len(examples)]))\\n    response = stub.Predict(request, timeout_secs)\\n    outputs = tf.make_ndarray(response.outputs[\"outputs\"])\\n    scores = tf.make_ndarray(response.outputs[\"scores\"])\\n    assert len(outputs) == len(scores)\\n    return [{  # pylint: disable=g-complex-comprehension\\n        \"outputs\": output,\\n        \"scores\": score\\n    } for output, score in zip(outputs, scores)]\\n\\n  return _make_grpc_request',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'get_random_number_generator_and_set_seed',\n",
       "  'docstring': 'Get a JAX random number generator and set random seed everywhere.',\n",
       "  'code': 'def get_random_number_generator_and_set_seed(seed=None):\\n  \"\"\"Get a JAX random number generator and set random seed everywhere.\"\"\"\\n  random.seed(seed)\\n  # While python random accepts None as seed and uses time/os seed then,\\n  # some other functions expect integers so we create one here.\\n  if seed is None:\\n    seed = random.randint(0, 2**31 - 1)\\n  tf.set_random_seed(seed)\\n  numpy.random.seed(seed)\\n  return jax_random.get_prng(seed)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'universal_transformer_act',\n",
       "  'docstring': \"ACT based models.\\n\\n  Implementations of all act models are based on craffel@'s cl/160711592.\\n\\n  (1) Basic AUT based on remainder-distribution ACT (position-wise).\\n  (2) AUT with global halting probability (not position-wise).\\n  (3) AUT with random halting probability (not position-wise).\\n  (4) AUT with final state as accumulation of all states.\\n\\n  Args:\\n    x: input\\n    hparams: model hyper-parameters\\n    ffn_unit: feed-forward unit\\n    attention_unit: multi-head attention unit\\n\\n  Returns:\\n    the output tensor,  (ponder_times, remainders)\\n\\n  Raises:\\n    ValueError: Unknown act type\",\n",
       "  'code': 'def universal_transformer_act(x, hparams, ffn_unit, attention_unit):\\n  \"\"\"ACT based models.\\n\\n  Implementations of all act models are based on craffel@\\'s cl/160711592.\\n\\n  (1) Basic AUT based on remainder-distribution ACT (position-wise).\\n  (2) AUT with global halting probability (not position-wise).\\n  (3) AUT with random halting probability (not position-wise).\\n  (4) AUT with final state as accumulation of all states.\\n\\n  Args:\\n    x: input\\n    hparams: model hyper-parameters\\n    ffn_unit: feed-forward unit\\n    attention_unit: multi-head attention unit\\n\\n  Returns:\\n    the output tensor,  (ponder_times, remainders)\\n\\n  Raises:\\n    ValueError: Unknown act type\\n  \"\"\"\\n  if hparams.act_type not in [\"basic\", \"global\", \"random\", \"accumulated\"]:\\n    raise ValueError(\"Unknown act type: %s\" % hparams.act_type)\\n\\n  state = x\\n  act_max_steps = hparams.act_max_steps\\n  threshold = 1.0 - hparams.act_epsilon\\n  state_shape_static = state.get_shape()\\n\\n  state_slice = slice(0, 2)\\n  if hparams.act_type == \"global\":\\n    state_slice = slice(0, 1)\\n\\n  # Dynamic shape for update tensors below\\n  update_shape = tf.shape(state)[state_slice]\\n\\n  # Halting probabilities (p_t^n in the paper)\\n  halting_probability = tf.zeros(update_shape, name=\"halting_probability\")\\n\\n  # Remainders (R(t) in the paper)\\n  remainders = tf.zeros(update_shape, name=\"remainder\")\\n\\n  # Number of updates performed (N(t) in the paper)\\n  n_updates = tf.zeros(update_shape, name=\"n_updates\")\\n\\n  # Previous cell states (s_t in the paper)\\n  previous_state = tf.zeros_like(state, name=\"previous_state\")\\n  step = tf.constant(0, dtype=tf.int32)\\n\\n  def ut_function(state, step, halting_probability, remainders, n_updates,\\n                  previous_state):\\n    \"\"\"implements act (position-wise halting).\\n\\n    Args:\\n      state: 3-D Tensor: [batch_size, length, channel]\\n      step: indicates number of steps taken so far\\n      halting_probability: halting probability\\n      remainders: act remainders\\n      n_updates: act n_updates\\n      previous_state: previous state\\n\\n    Returns:\\n      transformed_state: transformed state\\n      step: step+1\\n      halting_probability: halting probability\\n      remainders: act remainders\\n      n_updates: act n_updates\\n      new_state: new state\\n    \"\"\"\\n    state = step_preprocess(state, step, hparams)\\n\\n    if hparams.act_type == \"random\":\\n      # random as halting probability\\n      p = tf.random_uniform(\\n          shape=common_layers.shape_list(halting_probability))\\n    else:\\n      with tf.variable_scope(\"sigmoid_activation_for_pondering\"):\\n        p = common_layers.dense(\\n            state,\\n            1,\\n            activation=tf.nn.sigmoid,\\n            use_bias=True,\\n            bias_initializer=tf.constant_initializer(\\n                hparams.act_halting_bias_init))\\n\\n        if hparams.act_type == \"global\":\\n          # average over all positions (as a global halting prob)\\n          p = tf.reduce_mean(p, axis=1)\\n          p = tf.squeeze(p)\\n        else:\\n          # maintain position-wise probabilities\\n          p = tf.squeeze(p, axis=-1)\\n\\n    # Mask for inputs which have not halted yet\\n    still_running = tf.cast(tf.less(halting_probability, 1.0), tf.float32)\\n\\n    # Mask of inputs which halted at this step\\n    new_halted = tf.cast(\\n        tf.greater(halting_probability + p * still_running, threshold),\\n        tf.float32) * still_running\\n\\n    # Mask of inputs which haven\\'t halted, and didn\\'t halt this step\\n    still_running = tf.cast(\\n        tf.less_equal(halting_probability + p * still_running, threshold),\\n        tf.float32) * still_running\\n\\n    # Add the halting probability for this step to the halting\\n    # probabilities for those input which haven\\'t halted yet\\n    halting_probability += p * still_running\\n\\n    # Compute remainders for the inputs which halted at this step\\n    remainders += new_halted * (1 - halting_probability)\\n\\n    # Add the remainders to those inputs which halted at this step\\n    halting_probability += new_halted * remainders\\n\\n    # Increment n_updates for all inputs which are still running\\n    n_updates += still_running + new_halted\\n\\n    # Compute the weight to be applied to the new state and output\\n    # 0 when the input has already halted\\n    # p when the input hasn\\'t halted yet\\n    # the remainders when it halted this step\\n    update_weights = tf.expand_dims(\\n        p * still_running + new_halted * remainders, -1)\\n    if hparams.act_type == \"global\":\\n      update_weights = tf.expand_dims(update_weights, -1)\\n\\n    # apply transformation on the state\\n    transformed_state = state\\n    for i in range(hparams.num_inrecurrence_layers):\\n      with tf.variable_scope(\"rec_layer_%d\" % i):\\n        transformed_state = ffn_unit(attention_unit(transformed_state))\\n\\n    # update running part in the weighted state and keep the rest\\n    new_state = ((transformed_state * update_weights) +\\n                 (previous_state * (1 - update_weights)))\\n\\n    if hparams.act_type == \"accumulated\":\\n      # Add in the weighted state\\n      new_state = (transformed_state * update_weights) + previous_state\\n\\n    # remind TensorFlow of everything\\'s shape\\n    transformed_state.set_shape(state_shape_static)\\n    for x in [halting_probability, remainders, n_updates]:\\n      x.set_shape(state_shape_static[state_slice])\\n    new_state.set_shape(state_shape_static)\\n    step += 1\\n    return (transformed_state, step, halting_probability, remainders, n_updates,\\n            new_state)\\n\\n  # While loop stops when this predicate is FALSE.\\n  # Ie all (probability < 1-eps AND counter < N) are false.\\n  def should_continue(u0, u1, halting_probability, u2, n_updates, u3):\\n    del u0, u1, u2, u3\\n    return tf.reduce_any(\\n        tf.logical_and(\\n            tf.less(halting_probability, threshold),\\n            tf.less(n_updates, act_max_steps)))\\n\\n  # Do while loop iterations until predicate above is false.\\n  (_, _, _, remainder, n_updates, new_state) = tf.while_loop(\\n      should_continue, ut_function,\\n      (state, step, halting_probability, remainders, n_updates, previous_state),\\n      maximum_iterations=act_max_steps + 1)\\n\\n  ponder_times = n_updates\\n  remainders = remainder\\n\\n  tf.contrib.summary.scalar(\"ponder_times\", tf.reduce_mean(ponder_times))\\n\\n  return new_state, (ponder_times, remainders)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'EnvProblem.process_rewards',\n",
       "  'docstring': 'Clips, rounds, and changes to integer type.\\n\\n    Args:\\n      rewards: numpy array of raw (float) rewards.\\n\\n    Returns:\\n      processed_rewards: numpy array of np.int64',\n",
       "  'code': 'def process_rewards(self, rewards):\\n    \"\"\"Clips, rounds, and changes to integer type.\\n\\n    Args:\\n      rewards: numpy array of raw (float) rewards.\\n\\n    Returns:\\n      processed_rewards: numpy array of np.int64\\n    \"\"\"\\n\\n    min_reward, max_reward = self.reward_range\\n\\n    # Clips at min and max reward.\\n    rewards = np.clip(rewards, min_reward, max_reward)\\n    # Round to (nearest) int and convert to integral type.\\n    rewards = np.around(rewards, decimals=0).astype(np.int64)\\n    return rewards',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'EnvProblem._reset',\n",
       "  'docstring': \"Resets environments at indices shouldn't pre-process or record.\\n\\n    Subclasses should override this to do the actual reset if something other\\n    than the default implementation is desired.\\n\\n    Args:\\n      indices: list of indices of underlying envs to call reset on.\\n\\n    Returns:\\n      np.ndarray of stacked observations from the reset-ed envs.\",\n",
       "  'code': 'def _reset(self, indices):\\n    \"\"\"Resets environments at indices shouldn\\'t pre-process or record.\\n\\n    Subclasses should override this to do the actual reset if something other\\n    than the default implementation is desired.\\n\\n    Args:\\n      indices: list of indices of underlying envs to call reset on.\\n\\n    Returns:\\n      np.ndarray of stacked observations from the reset-ed envs.\\n    \"\"\"\\n\\n    # Pre-conditions: common_preconditions, see `assert_common_preconditions`.\\n    self.assert_common_preconditions()\\n\\n    # This returns a numpy array with first dimension `len(indices)` and the\\n    # rest being the dimensionality of the observation.\\n    return np.stack([self._envs[index].reset() for index in indices])',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'collect_trajectories',\n",
       "  'docstring': 'Collect trajectories with the given policy net and behaviour.\\n\\n  Args:\\n    env: A gym env interface, for now this is not-batched.\\n    policy_fun: observations(B,T+1) -> log-probabs(B,T+1, A) callable.\\n    num_trajectories: int, number of trajectories.\\n    policy: string, \"greedy\", \"epsilon-greedy\", or \"categorical-sampling\" i.e.\\n        how to use the policy_fun to return an action.\\n    max_timestep: int or None, the index of the maximum time-step at which we\\n        return the trajectory, None for ending a trajectory only when env\\n        returns done.\\n    epsilon: float, the epsilon for `epsilon-greedy` policy.\\n\\n  Returns:\\n    trajectory: list of (observation, action, reward) tuples, where each element\\n    `i` is a tuple of numpy arrays with shapes as follows:\\n    observation[i] = (B, T_i + 1)\\n    action[i] = (B, T_i)\\n    reward[i] = (B, T_i)',\n",
       "  'code': 'def collect_trajectories(env,\\n                         policy_fun,\\n                         num_trajectories=1,\\n                         policy=\"greedy\",\\n                         max_timestep=None,\\n                         epsilon=0.1):\\n  \"\"\"Collect trajectories with the given policy net and behaviour.\\n\\n  Args:\\n    env: A gym env interface, for now this is not-batched.\\n    policy_fun: observations(B,T+1) -> log-probabs(B,T+1, A) callable.\\n    num_trajectories: int, number of trajectories.\\n    policy: string, \"greedy\", \"epsilon-greedy\", or \"categorical-sampling\" i.e.\\n        how to use the policy_fun to return an action.\\n    max_timestep: int or None, the index of the maximum time-step at which we\\n        return the trajectory, None for ending a trajectory only when env\\n        returns done.\\n    epsilon: float, the epsilon for `epsilon-greedy` policy.\\n\\n  Returns:\\n    trajectory: list of (observation, action, reward) tuples, where each element\\n    `i` is a tuple of numpy arrays with shapes as follows:\\n    observation[i] = (B, T_i + 1)\\n    action[i] = (B, T_i)\\n    reward[i] = (B, T_i)\\n  \"\"\"\\n  trajectories = []\\n\\n  for t in range(num_trajectories):\\n    t_start = time.time()\\n    rewards = []\\n    actions = []\\n    done = False\\n\\n    observation = env.reset()\\n\\n    # This is currently shaped (1, 1) + OBS, but new observations will keep\\n    # getting added to it, making it eventually (1, T+1) + OBS\\n    observation_history = observation[np.newaxis, np.newaxis, :]\\n\\n    # Run either till we\\'re done OR if max_timestep is defined only till that\\n    # timestep.\\n    ts = 0\\n    while ((not done) and\\n           (not max_timestep or observation_history.shape[1] < max_timestep)):\\n      ts_start = time.time()\\n      # Run the policy, to pick an action, shape is (1, t, A) because\\n      # observation_history is shaped (1, t) + OBS\\n      predictions = policy_fun(observation_history)\\n\\n      # We need the predictions for the last time-step, so squeeze the batch\\n      # dimension and take the last time-step.\\n      predictions = np.squeeze(predictions, axis=0)[-1]\\n\\n      # Policy can be run in one of the following ways:\\n      #  - Greedy\\n      #  - Epsilon-Greedy\\n      #  - Categorical-Sampling\\n      action = None\\n      if policy == \"greedy\":\\n        action = np.argmax(predictions)\\n      elif policy == \"epsilon-greedy\":\\n        # A schedule for epsilon is 1/k where k is the episode number sampled.\\n        if onp.random.random() < epsilon:\\n          # Choose an action at random.\\n          action = onp.random.randint(0, high=len(predictions))\\n        else:\\n          # Return the best action.\\n          action = np.argmax(predictions)\\n      elif policy == \"categorical-sampling\":\\n        # NOTE: The predictions aren\\'t probabilities but log-probabilities\\n        # instead, since they were computed with LogSoftmax.\\n        # So just np.exp them to make them probabilities.\\n        predictions = np.exp(predictions)\\n        action = onp.argwhere(onp.random.multinomial(1, predictions) == 1)\\n      else:\\n        raise ValueError(\"Unknown policy: %s\" % policy)\\n\\n      # NOTE: Assumption, single batch.\\n      try:\\n        action = int(action)\\n      except TypeError as err:\\n        # Let\\'s dump some information before we die off.\\n        logging.error(\"Cannot convert action into an integer: [%s]\", err)\\n        logging.error(\"action.shape: [%s]\", action.shape)\\n        logging.error(\"action: [%s]\", action)\\n        logging.error(\"predictions.shape: [%s]\", predictions.shape)\\n        logging.error(\"predictions: [%s]\", predictions)\\n        logging.error(\"observation_history: [%s]\", observation_history)\\n        raise err\\n\\n      observation, reward, done, _ = env.step(action)\\n\\n      # observation is of shape OBS, so add extra dims and concatenate on the\\n      # time dimension.\\n      observation_history = np.concatenate(\\n          [observation_history, observation[np.newaxis, np.newaxis, :]], axis=1)\\n\\n      rewards.append(reward)\\n      actions.append(action)\\n\\n      ts += 1\\n      logging.vlog(\\n          2, \"  Collected time-step[ %5d] of trajectory[ %5d] in [%0.2f] msec.\",\\n          ts, t, get_time(ts_start))\\n    logging.vlog(\\n        2, \" Collected trajectory[ %5d] in [%0.2f] msec.\", t, get_time(t_start))\\n\\n    # This means we are done we\\'re been terminated early.\\n    assert done or (\\n        max_timestep and max_timestep >= observation_history.shape[1])\\n    # observation_history is (1, T+1) + OBS, lets squeeze out the batch dim.\\n    observation_history = np.squeeze(observation_history, axis=0)\\n    trajectories.append(\\n        (observation_history, np.stack(actions), np.stack(rewards)))\\n\\n  return trajectories',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'layers',\n",
       "  'docstring': 'Get the layers module good for TF 1 and TF 2 work for now.',\n",
       "  'code': 'def layers():\\n  \"\"\"Get the layers module good for TF 1 and TF 2 work for now.\"\"\"\\n  global _cached_layers\\n  if _cached_layers is not None:\\n    return _cached_layers\\n  layers_module = tf.layers\\n  try:\\n    from tensorflow.python import tf2  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\\n    if tf2.enabled():\\n      tf.logging.info(\"Running in V2 mode, using Keras layers.\")\\n      layers_module = tf.keras.layers\\n  except ImportError:\\n    pass\\n  _cached_layers = layers_module\\n  return layers_module',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'layer_prepostprocess',\n",
       "  'docstring': 'Apply a sequence of functions to the input or output of a layer.\\n\\n  The sequence is specified as a string which may contain the following\\n  characters:\\n    a: add previous_value\\n    n: apply normalization\\n    d: apply dropout\\n    z: zero add\\n\\n  For example, if sequence==\"dna\", then the output is\\n    previous_value + normalize(dropout(x))\\n\\n  Args:\\n    previous_value: A Tensor, to be added as a residual connection (\\'a\\')\\n    x: A Tensor to be transformed.\\n    sequence: a string.\\n    dropout_rate: a float\\n    norm_type: a string (see apply_norm())\\n    depth: an integer (size of last dimension of x).\\n    epsilon: a float (parameter for normalization)\\n    default_name: a string\\n    name: a string\\n    dropout_broadcast_dims:  an optional list of integers less than 3\\n      specifying in which dimensions to broadcast the dropout decisions.\\n      saves memory.\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n\\n  Returns:\\n    a Tensor',\n",
       "  'code': 'def layer_prepostprocess(previous_value,\\n                         x,\\n                         sequence,\\n                         dropout_rate,\\n                         norm_type,\\n                         depth,\\n                         epsilon,\\n                         default_name,\\n                         name=None,\\n                         dropout_broadcast_dims=None,\\n                         layer_collection=None):\\n  \"\"\"Apply a sequence of functions to the input or output of a layer.\\n\\n  The sequence is specified as a string which may contain the following\\n  characters:\\n    a: add previous_value\\n    n: apply normalization\\n    d: apply dropout\\n    z: zero add\\n\\n  For example, if sequence==\"dna\", then the output is\\n    previous_value + normalize(dropout(x))\\n\\n  Args:\\n    previous_value: A Tensor, to be added as a residual connection (\\'a\\')\\n    x: A Tensor to be transformed.\\n    sequence: a string.\\n    dropout_rate: a float\\n    norm_type: a string (see apply_norm())\\n    depth: an integer (size of last dimension of x).\\n    epsilon: a float (parameter for normalization)\\n    default_name: a string\\n    name: a string\\n    dropout_broadcast_dims:  an optional list of integers less than 3\\n      specifying in which dimensions to broadcast the dropout decisions.\\n      saves memory.\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n\\n  Returns:\\n    a Tensor\\n  \"\"\"\\n  with tf.variable_scope(name, default_name=default_name):\\n    if sequence == \"none\":\\n      return x\\n    for c in sequence:\\n      if c == \"a\":\\n        x += previous_value\\n      elif c == \"z\":\\n        x = zero_add(previous_value, x)\\n      elif c == \"n\":\\n        x = apply_norm(\\n            x, norm_type, depth, epsilon, layer_collection=layer_collection)\\n      else:\\n        assert c == \"d\", (\"Unknown sequence step %s\" % c)\\n        x = dropout_with_broadcast_dims(\\n            x, 1.0 - dropout_rate, broadcast_dims=dropout_broadcast_dims)\\n    return x',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'layer_preprocess',\n",
       "  'docstring': 'Apply layer preprocessing.\\n\\n  See layer_prepostprocess() for details.\\n\\n  A hyperparameters object is passed for convenience.  The hyperparameters\\n  that may be used are:\\n\\n    layer_preprocess_sequence\\n    layer_prepostprocess_dropout\\n    norm_type\\n    hidden_size\\n    norm_epsilon\\n\\n  Args:\\n    layer_input: a Tensor\\n    hparams: a hyperparameters object.\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n\\n  Returns:\\n    a Tensor',\n",
       "  'code': 'def layer_preprocess(layer_input, hparams, layer_collection=None):\\n  \"\"\"Apply layer preprocessing.\\n\\n  See layer_prepostprocess() for details.\\n\\n  A hyperparameters object is passed for convenience.  The hyperparameters\\n  that may be used are:\\n\\n    layer_preprocess_sequence\\n    layer_prepostprocess_dropout\\n    norm_type\\n    hidden_size\\n    norm_epsilon\\n\\n  Args:\\n    layer_input: a Tensor\\n    hparams: a hyperparameters object.\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n\\n  Returns:\\n    a Tensor\\n  \"\"\"\\n  assert \"a\" not in hparams.layer_preprocess_sequence, (\\n      \"No residual connections allowed in hparams.layer_preprocess_sequence\")\\n  assert \"z\" not in hparams.layer_preprocess_sequence, (\\n      \"No residual connections allowed in hparams.layer_preprocess_sequence\")\\n  return layer_prepostprocess(\\n      None,\\n      layer_input,\\n      sequence=hparams.layer_preprocess_sequence,\\n      dropout_rate=hparams.layer_prepostprocess_dropout,\\n      norm_type=hparams.norm_type,\\n      depth=None,\\n      epsilon=hparams.norm_epsilon,\\n      dropout_broadcast_dims=comma_separated_string_to_integer_list(\\n          getattr(hparams, \"layer_prepostprocess_dropout_broadcast_dims\", \"\")),\\n      default_name=\"layer_prepostprocess\",\\n      layer_collection=layer_collection)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'sru',\n",
       "  'docstring': 'SRU cell as in https://arxiv.org/abs/1709.02755.\\n\\n  As defined in the paper:\\n  (1) x\\'_t = W x_t\\n  (2) f_t = sigmoid(Wf x_t + bf)\\n  (3) r_t = sigmoid(Wr x_t + br)\\n  (4) c_t = f_t * c_{t-1} + (1 - f_t) * x\\'_t\\n  (5) h_t = r_t * activation(c_t) + (1 - r_t) * x_t\\n\\n  This version uses functional ops to be faster on GPUs with TF-1.9+.\\n\\n  Args:\\n    x: A tensor of shape [batch, ..., channels] ; ... is treated as time.\\n    num_layers: How many SRU layers; default is 2 as results for 1 disappoint.\\n    activation: Optional activation function, try tf.nn.tanh or tf.nn.relu.\\n    initial_state: Optional initial c-state, set to zeros if None.\\n    name: Optional name, \"sru\" by default.\\n    reuse: Optional reuse.\\n\\n  Returns:\\n    A tensor of the same shape as x.\\n\\n  Raises:\\n    ValueError: if num_layers is not positive.',\n",
       "  'code': 'def sru(x,\\n        num_layers=2,\\n        activation=None,\\n        initial_state=None,\\n        name=None,\\n        reuse=None):\\n  \"\"\"SRU cell as in https://arxiv.org/abs/1709.02755.\\n\\n  As defined in the paper:\\n  (1) x\\'_t = W x_t\\n  (2) f_t = sigmoid(Wf x_t + bf)\\n  (3) r_t = sigmoid(Wr x_t + br)\\n  (4) c_t = f_t * c_{t-1} + (1 - f_t) * x\\'_t\\n  (5) h_t = r_t * activation(c_t) + (1 - r_t) * x_t\\n\\n  This version uses functional ops to be faster on GPUs with TF-1.9+.\\n\\n  Args:\\n    x: A tensor of shape [batch, ..., channels] ; ... is treated as time.\\n    num_layers: How many SRU layers; default is 2 as results for 1 disappoint.\\n    activation: Optional activation function, try tf.nn.tanh or tf.nn.relu.\\n    initial_state: Optional initial c-state, set to zeros if None.\\n    name: Optional name, \"sru\" by default.\\n    reuse: Optional reuse.\\n\\n  Returns:\\n    A tensor of the same shape as x.\\n\\n  Raises:\\n    ValueError: if num_layers is not positive.\\n  \"\"\"\\n  if num_layers < 1:\\n    raise ValueError(\"Number of layers must be positive: %d\" % num_layers)\\n  if is_xla_compiled():  # On TPU the XLA does a good job with while.\\n    return sru_with_scan(x, num_layers, activation, initial_state, name, reuse)\\n  try:\\n    from tensorflow.contrib.recurrent.python.ops import functional_rnn  # pylint: disable=g-import-not-at-top\\n  except ImportError:\\n    tf.logging.info(\"functional_rnn not found, using sru_with_scan instead\")\\n    return sru_with_scan(x, num_layers, activation, initial_state, name, reuse)\\n\\n  with tf.variable_scope(name, default_name=\"sru\", values=[x], reuse=reuse):\\n    # We assume x is [batch, ..., channels] and treat all ... as time.\\n    x_shape = shape_list(x)\\n    x = tf.reshape(x, [x_shape[0], -1, x_shape[-1]])\\n    initial_state = initial_state or tf.zeros([x_shape[0], x_shape[-1]])\\n    cell = CumsumprodCell(initial_state)\\n    # Calculate SRU on each layer.\\n    for i in range(num_layers):\\n      # The parallel part of the SRU.\\n      x_orig = x\\n      x, f, r = tf.split(\\n          layers().Dense(3 * x_shape[-1], name=\"kernel_%d\" % i)(x), 3, axis=-1)\\n      f, r = tf.sigmoid(f), tf.sigmoid(r)\\n      x_times_one_minus_f = x * (1.0 - f)  # Compute in parallel for speed.\\n      # Calculate states.\\n      concat = tf.concat([x_times_one_minus_f, f], axis=-1)\\n      c_states, _ = functional_rnn.functional_rnn(\\n          cell, concat, time_major=False)\\n      # Final output.\\n      if activation is not None:\\n        c_states = activation(c_states)\\n      h = c_states * r + (1.0 - r) * x_orig\\n      x = h  # Next layer.\\n    return tf.reshape(x, x_shape)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ones_matrix_band_part',\n",
       "  'docstring': 'Matrix band part of ones.\\n\\n  Args:\\n    rows: int determining number of rows in output\\n    cols: int\\n    num_lower: int, maximum distance backward. Negative values indicate\\n      unlimited.\\n    num_upper: int, maximum distance forward. Negative values indicate\\n      unlimited.\\n    out_shape: shape to reshape output by.\\n\\n  Returns:\\n    Tensor of size rows * cols reshaped into shape out_shape.',\n",
       "  'code': 'def ones_matrix_band_part(rows, cols, num_lower, num_upper, out_shape=None):\\n  \"\"\"Matrix band part of ones.\\n\\n  Args:\\n    rows: int determining number of rows in output\\n    cols: int\\n    num_lower: int, maximum distance backward. Negative values indicate\\n      unlimited.\\n    num_upper: int, maximum distance forward. Negative values indicate\\n      unlimited.\\n    out_shape: shape to reshape output by.\\n\\n  Returns:\\n    Tensor of size rows * cols reshaped into shape out_shape.\\n  \"\"\"\\n  if all([isinstance(el, int) for el in [rows, cols, num_lower, num_upper]]):\\n    # Needed info is constant, so we construct in numpy\\n    if num_lower < 0:\\n      num_lower = rows - 1\\n    if num_upper < 0:\\n      num_upper = cols - 1\\n    lower_mask = np.tri(cols, rows, num_lower).T\\n    upper_mask = np.tri(rows, cols, num_upper)\\n    band = np.ones((rows, cols)) * lower_mask * upper_mask\\n    if out_shape:\\n      band = band.reshape(out_shape)\\n    band = tf.constant(band, tf.float32)\\n  else:\\n    band = tf.matrix_band_part(\\n        tf.ones([rows, cols]), tf.cast(num_lower, tf.int64),\\n        tf.cast(num_upper, tf.int64))\\n    if out_shape:\\n      band = tf.reshape(band, out_shape)\\n\\n  return band',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'create_session_config',\n",
       "  'docstring': 'The TensorFlow Session config to use.',\n",
       "  'code': 'def create_session_config(log_device_placement=False,\\n                          enable_graph_rewriter=False,\\n                          gpu_mem_fraction=0.95,\\n                          use_tpu=False,\\n                          xla_jit_level=tf.OptimizerOptions.OFF,\\n                          inter_op_parallelism_threads=0,\\n                          intra_op_parallelism_threads=0):\\n  \"\"\"The TensorFlow Session config to use.\"\"\"\\n  if use_tpu:\\n    graph_options = tf.GraphOptions()\\n  else:\\n    if enable_graph_rewriter:\\n      rewrite_options = rewriter_config_pb2.RewriterConfig()\\n      rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.ON\\n      graph_options = tf.GraphOptions(rewrite_options=rewrite_options)\\n    else:\\n      graph_options = tf.GraphOptions(\\n          optimizer_options=tf.OptimizerOptions(\\n              opt_level=tf.OptimizerOptions.L1,\\n              do_function_inlining=False,\\n              global_jit_level=xla_jit_level))\\n\\n  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_mem_fraction)\\n\\n  config = tf.ConfigProto(\\n      allow_soft_placement=True,\\n      graph_options=graph_options,\\n      gpu_options=gpu_options,\\n      log_device_placement=log_device_placement,\\n      inter_op_parallelism_threads=inter_op_parallelism_threads,\\n      intra_op_parallelism_threads=intra_op_parallelism_threads,\\n      isolate_session_state=True)\\n  return config',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'T2TExperiment.run_std_server',\n",
       "  'docstring': \"Starts a TensorFlow server and joins the serving thread.\\n\\n    Typically used for parameter servers.\\n\\n    Raises:\\n      ValueError: if not enough information is available in the estimator's\\n        config to create a server.\",\n",
       "  'code': 'def run_std_server(self):\\n    \"\"\"Starts a TensorFlow server and joins the serving thread.\\n\\n    Typically used for parameter servers.\\n\\n    Raises:\\n      ValueError: if not enough information is available in the estimator\\'s\\n        config to create a server.\\n    \"\"\"\\n    config = tf.estimator.RunConfig()\\n    server = tf.train.Server(\\n        config.cluster_spec,\\n        job_name=config.task_type,\\n        task_index=config.task_id,\\n        protocol=config.protocol)\\n    server.join()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'maybe_download',\n",
       "  'docstring': \"Download filename from uri unless it's already in directory.\\n\\n  Copies a remote file to local if that local file does not already exist.  If\\n  the local file pre-exists this function call, it does not check that the local\\n  file is a copy of the remote.\\n\\n  Remote filenames can be filepaths, any URI readable by tensorflow.gfile, or a\\n  URL.\\n\\n  Args:\\n    directory: path to the directory that will be used.\\n    filename: name of the file to download to (do nothing if it already exists).\\n    uri: URI to copy (or download) from.\\n\\n  Returns:\\n    The path to the downloaded file.\",\n",
       "  'code': 'def maybe_download(directory, filename, uri):\\n  \"\"\"Download filename from uri unless it\\'s already in directory.\\n\\n  Copies a remote file to local if that local file does not already exist.  If\\n  the local file pre-exists this function call, it does not check that the local\\n  file is a copy of the remote.\\n\\n  Remote filenames can be filepaths, any URI readable by tensorflow.gfile, or a\\n  URL.\\n\\n  Args:\\n    directory: path to the directory that will be used.\\n    filename: name of the file to download to (do nothing if it already exists).\\n    uri: URI to copy (or download) from.\\n\\n  Returns:\\n    The path to the downloaded file.\\n  \"\"\"\\n  tf.gfile.MakeDirs(directory)\\n  filepath = os.path.join(directory, filename)\\n  if tf.gfile.Exists(filepath):\\n    tf.logging.info(\"Not downloading, file already found: %s\" % filepath)\\n    return filepath\\n\\n  tf.logging.info(\"Downloading %s to %s\" % (uri, filepath))\\n  try:\\n    tf.gfile.Copy(uri, filepath)\\n  except tf.errors.UnimplementedError:\\n    if uri.startswith(\"http\"):\\n      inprogress_filepath = filepath + \".incomplete\"\\n      inprogress_filepath, _ = urllib.urlretrieve(\\n          uri, inprogress_filepath, reporthook=download_report_hook)\\n      # Print newline to clear the carriage return from the download progress\\n      print()\\n      tf.gfile.Rename(inprogress_filepath, filepath)\\n    else:\\n      raise ValueError(\"Unrecognized URI: \" + filepath)\\n  statinfo = os.stat(filepath)\\n  tf.logging.info(\"Successfully downloaded %s, %s bytes.\" %\\n                  (filename, statinfo.st_size))\\n  return filepath',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'tfrecord_iterator',\n",
       "  'docstring': 'Yields records from TFRecord files.\\n\\n  Args:\\n    filenames: list<str>, list of TFRecord filenames to read from.\\n    gzipped: bool, whether the TFRecord files are gzip-encoded.\\n    example_spec: dict<str feature name, tf.VarLenFeature/tf.FixedLenFeature>,\\n      if provided, will parse each record as a tensorflow.Example proto.\\n\\n  Yields:\\n    Records (or parsed Examples, if example_spec is provided) from files.',\n",
       "  'code': 'def tfrecord_iterator(filenames, gzipped=False, example_spec=None):\\n  \"\"\"Yields records from TFRecord files.\\n\\n  Args:\\n    filenames: list<str>, list of TFRecord filenames to read from.\\n    gzipped: bool, whether the TFRecord files are gzip-encoded.\\n    example_spec: dict<str feature name, tf.VarLenFeature/tf.FixedLenFeature>,\\n      if provided, will parse each record as a tensorflow.Example proto.\\n\\n  Yields:\\n    Records (or parsed Examples, if example_spec is provided) from files.\\n  \"\"\"\\n  with tf.Graph().as_default():\\n    dataset = tf.data.Dataset.from_tensor_slices(filenames)\\n\\n    def _load_records(filename):\\n      return tf.data.TFRecordDataset(\\n          filename,\\n          compression_type=tf.constant(\"GZIP\") if gzipped else None,\\n          buffer_size=16 * 1000 * 1000)\\n\\n    dataset = dataset.flat_map(_load_records)\\n\\n    def _parse_example(ex_ser):\\n      return tf.parse_single_example(ex_ser, example_spec)\\n\\n    if example_spec:\\n      dataset = dataset.map(_parse_example, num_parallel_calls=32)\\n    dataset = dataset.prefetch(100)\\n    record_it = dataset.make_one_shot_iterator().get_next()\\n\\n    with tf.Session() as sess:\\n      while True:\\n        try:\\n          ex = sess.run(record_it)\\n          yield ex\\n        except tf.errors.OutOfRangeError:\\n          break',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'update_hparams_for_universal_transformer',\n",
       "  'docstring': 'Adds default hparams for all of the variants of the Universal Transformer.\\n\\n  Args:\\n    hparams: default hparams (usually one of the standard hparams from\\n      transformer model (like \"transformer_base\")\\n\\n  Returns:\\n    hparams with default values for Universal Transformers hyper-parameters',\n",
       "  'code': 'def update_hparams_for_universal_transformer(hparams):\\n  \"\"\"Adds default hparams for all of the variants of the Universal Transformer.\\n\\n  Args:\\n    hparams: default hparams (usually one of the standard hparams from\\n      transformer model (like \"transformer_base\")\\n\\n  Returns:\\n    hparams with default values for Universal Transformers hyper-parameters\\n\\n  \"\"\"\\n  hparams.daisy_chain_variables = False  # Breaks multi-gpu in while loops.\\n\\n  # If not None, mixes vanilla transformer with Universal Transformer.\\n  # Options: None, \"before_ut\", and \"after_ut\".\\n  hparams.add_hparam(\"mix_with_transformer\", None)\\n\\n  # Number of vanilla transformer layers used to be mixed with u-transofmer.\\n  hparams.add_hparam(\"num_mixedin_layers\", 2)\\n  # Number of transformer layers within the recurrent block (default is 1).\\n  hparams.add_hparam(\"num_inrecurrence_layers\", 1)\\n\\n  # Type of recurrency:\\n  # basic, highway, skip, dwa, act, rnn, gru, lstm.\\n  hparams.add_hparam(\"recurrence_type\", \"basic\")\\n\\n  # Number of steps (which is equivalent to num layer in transformer).\\n  hparams.add_hparam(\"num_rec_steps\", hparams.num_hidden_layers)\\n\\n  # Add the positional mebedding at each step(horisontal timing)\\n  hparams.add_hparam(\"add_position_timing_signal\", True)\\n  if hparams.add_position_timing_signal:\\n    hparams.pos = None\\n  # Logic of position shifting when using timing signal:\\n  # None, \"random\", \"step\"\\n  hparams.add_hparam(\"position_start_index\", None)\\n\\n  # Add an step embedding at each step (vertical timing)\\n  hparams.add_hparam(\"add_step_timing_signal\", True)\\n  # Either \"learned\" or \"sinusoid\"\\n  hparams.add_hparam(\"step_timing_signal_type\", \"learned\")\\n\\n  # Add or concat the timing signal (applied both on position and step timing).\\n  # Options: \"add\" and \"concat\".\\n  hparams.add_hparam(\"add_or_concat_timing_signal\", \"add\")\\n\\n  # Add SRU at the beginning of each Universal Transformer step.\\n  # This can be considered as a position timing signal\\n  hparams.add_hparam(\"add_sru\", False)\\n\\n  # Default ffn layer is separable convolution.\\n  # Options: \"fc\" and \"sepconv\".\\n  hparams.add_hparam(\"transformer_ffn_type\", \"fc\")\\n\\n  # Transform bias (in models with highway or skip connection).\\n  hparams.add_hparam(\"transform_bias_init\", -1.0)\\n  hparams.add_hparam(\"couple_carry_transform_gates\", True)\\n\\n  # Depth-wise attention (grid-transformer!) hparams:\\n  # Adds depth embedding, if true.\\n  hparams.add_hparam(\"depth_embedding\", True)\\n  # Learns attention weights for elements (instead of positions), if true.\\n  hparams.add_hparam(\"dwa_elements\", True)\\n\\n  # Type of ffn_layer used for gate in skip, highway, etc.\\n  # \"dense\" or \"dense_dropconnect\".\\n  # With dense_relu_dense, the bias/kernel initializations will not be applied.\\n  hparams.add_hparam(\"gate_ffn_layer\", \"dense\")\\n\\n  # LSTM forget bias for lstm style recurrence.\\n  hparams.add_hparam(\"lstm_forget_bias\", 1.0)\\n  # Uses the memory at the last step as the final output, if true.\\n  hparams.add_hparam(\"use_memory_as_final_state\", False)\\n  # if also add a ffn unit to the transition function when using gru/lstm\\n  hparams.add_hparam(\"add_ffn_unit_to_the_transition_function\", False)\\n\\n  # Type of act: basic/accumulated/global (instead of position-wise!)/random.\\n  hparams.add_hparam(\"act_type\", \"basic\")\\n  # Max number of steps (forces halting at this step).\\n  hparams.add_hparam(\"act_max_steps\", 2 * hparams.num_hidden_layers)\\n  hparams.add_hparam(\"act_halting_bias_init\", 1.0)\\n  hparams.add_hparam(\"act_epsilon\", 0.01)\\n  hparams.add_hparam(\"act_loss_weight\", 0.01)\\n\\n  return hparams',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_pack_images',\n",
       "  'docstring': 'Helper utility to make a tiled field of images from numpy arrays.\\n\\n  Args:\\n    images: Image tensor in shape [N, W, H, C].\\n    rows: Number of images per row in tiled image.\\n    cols: Number of images per column in tiled image.\\n\\n  Returns:\\n    A tiled image of shape [W * rows, H * cols, C].\\n    Truncates incomplete rows.',\n",
       "  'code': 'def _pack_images(images, rows, cols):\\n  \"\"\"Helper utility to make a tiled field of images from numpy arrays.\\n\\n  Args:\\n    images: Image tensor in shape [N, W, H, C].\\n    rows: Number of images per row in tiled image.\\n    cols: Number of images per column in tiled image.\\n\\n  Returns:\\n    A tiled image of shape [W * rows, H * cols, C].\\n    Truncates incomplete rows.\\n  \"\"\"\\n  shape = onp.shape(images)\\n  width, height, depth = shape[-3:]\\n  images = onp.reshape(images, (-1, width, height, depth))\\n  batch = onp.shape(images)[0]\\n  rows = onp.minimum(rows, batch)\\n  cols = onp.minimum(batch // rows, cols)\\n  images = images[:rows * cols]\\n  images = onp.reshape(images, (rows, cols, width, height, depth))\\n  images = onp.transpose(images, [0, 2, 1, 3, 4])\\n  images = onp.reshape(images, [rows * width, cols * height, depth])\\n  return images',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SummaryWriter.text',\n",
       "  'docstring': 'Saves a text summary.\\n\\n    Args:\\n      tag: str: label for this data\\n      textdata: string, or 1D/2D list/numpy array of strings\\n      step: int: training step\\n    Note: markdown formatting is rendered by tensorboard.',\n",
       "  'code': 'def text(self, tag, textdata, step=None):\\n    \"\"\"Saves a text summary.\\n\\n    Args:\\n      tag: str: label for this data\\n      textdata: string, or 1D/2D list/numpy array of strings\\n      step: int: training step\\n    Note: markdown formatting is rendered by tensorboard.\\n    \"\"\"\\n    if step is None:\\n      step = self._step\\n    else:\\n      self._step = step\\n    smd = SummaryMetadata(\\n        plugin_data=SummaryMetadata.PluginData(plugin_name=\\'text\\'))\\n    if isinstance(textdata, (str, bytes)):\\n      tensor = tf.make_tensor_proto(\\n          values=[textdata.encode(encoding=\\'utf_8\\')], shape=(1,))\\n    else:\\n      textdata = onp.array(textdata)  # convert lists, jax arrays, etc.\\n      datashape = onp.shape(textdata)\\n      if len(datashape) == 1:\\n        tensor = tf.make_tensor_proto(\\n            values=[td.encode(encoding=\\'utf_8\\') for td in textdata],\\n            shape=(datashape[0],))\\n      elif len(datashape) == 2:\\n        tensor = tf.make_tensor_proto(\\n            values=[\\n                td.encode(encoding=\\'utf_8\\') for td in onp.reshape(textdata, -1)\\n            ],\\n            shape=(datashape[0], datashape[1]))\\n    summary = Summary(\\n        value=[Summary.Value(tag=tag, metadata=smd, tensor=tensor)])\\n    self.add_summary(summary, step)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'basic_params1',\n",
       "  'docstring': 'A set of basic hyperparameters.',\n",
       "  'code': 'def basic_params1():\\n  \"\"\"A set of basic hyperparameters.\"\"\"\\n  return hparam.HParams(\\n      # If the problem consists of variable-length sequences\\n      # (see problem.batch_size_means_tokens()), then this is the number\\n      # of tokens per batch per GPU or per TPU core.  Otherwise, this is\\n      # the number of examples per GPU or per TPU core.\\n      batch_size=4096,\\n      batch_shuffle_size=512,\\n      # If True, then if the features are of variable length, the batch_size is\\n      # used as the actual batch size (and not tokens per batch).\\n      use_fixed_batch_size=False,\\n      num_hidden_layers=4,\\n      kernel_height=3,\\n      kernel_width=1,\\n      hidden_size=64,\\n      compress_steps=0,\\n      # All hyperparameters ending in \"dropout\" are automatically set to 0.0\\n      # when not in training mode.\\n      dropout=0.2,\\n      clip_grad_norm=2.0,\\n      grad_noise_scale=0.0,\\n      summarize_grads=False,\\n      # Flag for whether mlperf mode is on\\n      mlperf_mode=False,\\n      # Whether to log the name and size of every variable\\n      summarize_vars=False,\\n      initializer=\"orthogonal\",\\n      initializer_gain=1.5,\\n      label_smoothing=0.1,\\n      optimizer=\"adam\",\\n      optimizer_adam_epsilon=1e-6,\\n      optimizer_adam_beta1=0.85,\\n      optimizer_adam_beta2=0.997,\\n      optimizer_momentum_momentum=0.9,\\n      optimizer_momentum_nesterov=False,\\n      optimizer_adafactor_beta1=0.0,\\n      optimizer_adafactor_beta2=0.999,\\n      optimizer_adafactor_factored=True,\\n      optimizer_adafactor_decay_type=\"pow\",\\n      optimizer_adafactor_memory_exponent=0.8,\\n      optimizer_adafactor_clipping_threshold=1.0,\\n      optimizer_adafactor_multiply_by_parameter_scale=True,\\n      # Number of accumulating steps for multi step optimizers.\\n      optimizer_multistep_accumulate_steps=0,\\n      # Loss scaling used.\\n      # Generally only necessary with mixed precision training.\\n      # Mixed precision training only supports exponential scaling currently\\n      # To disable the scaler, see to 0/False\\n      mixed_precision_optimizer_loss_scaler=\"exponential\",\\n      # Determines the initial loss scaling value for mixed precision\\n      mixed_precision_optimizer_init_loss_scale=2**15,\\n      # Whether to zero gradients that were not computed, so that the\\n      # appropriate slots are created. Useful for sharing checkpoints between\\n      # models with different sets of heads.\\n      optimizer_zero_grads=False,\\n      weight_decay=1e-6,\\n      weight_noise=0.0,\\n      # Defines the learning rate as a product of named functions.\\n      # Available functions are listed in learning_rate._LEARNING_RATE_FUNCTIONS\\n      # e.g. \"constant*linear_warmup*rsqrt_decay*rsqrt_hidden_size\"\\n      learning_rate_schedule=\"legacy\",\\n      learning_rate_constant=1.0,\\n      # If learning_rate_schedule==\"legacy\",\\n      # then we specify decay scheme here.  Warmup is always exponential,\\n      # except with \"noam\" learning rate decay scheme.\\n      # see optimize.legacy_learning_rate_schedule()\\n      # TODO(noam): migrate everyone away from this.\\n      learning_rate_decay_scheme=\"none\",\\n      # decay_steps and decay_staircase for learning_rate_decay_scheme==\"exp\"\\n      learning_rate_decay_steps=5000,\\n      learning_rate_decay_staircase=False,\\n      learning_rate_minimum=None,\\n      learning_rate_decay_rate=1.0,\\n      learning_rate_warmup_steps=100,\\n      learning_rate_cosine_cycle_steps=250000,\\n      learning_rate=0.1,\\n      sampling_method=\"argmax\",  # \"argmax\" or \"random\"\\n      sampling_temp=1.0,  # temperature for sampling\\n      sampling_keep_top_k=-1,  # If >0, ignore all but the top k logits\\n      # expand the logits a piece at a time - saves memory.\\n      factored_logits=False,\\n      multiply_embedding_mode=\"sqrt_depth\",\\n      # Parameters related to mixtures of experts.\\n      moe_hidden_sizes=\"2048\",  # hidden layer sizes (comma-separated)\\n      moe_num_experts=64,  # number of experts per layer\\n      moe_k=2,  # how many experts to use for each batch element\\n      moe_loss_coef=1e-2,\\n      # Sequences of operations to perform on layer input and layer output.\\n      # Used by common_layers.layer_preprocess, common_layers.layer_postprocess\\n      # Each character represents an operation:\\n      # none: no preprocessing\\n      #    d: apply dropout\\n      #    n: apply normalization (see norm_type and norm_epsilon)\\n      #    a: add layer input (residual connection - only during postprocess)\\n      # The special string \"none\" is used instead of the empty string\\n      # to indicate no pre/postprocessing, since the empty string causes\\n      # trouble for hyperparameter tuning.\\n      # TODO(noam): The current settings (\"\", \"dan\") are the published version\\n      # of the transformer.  (\"n\", \"da\") seems better for harder-to-learn\\n      # models, so it should probably be the default.\\n      layer_preprocess_sequence=\"none\",\\n      layer_postprocess_sequence=\"dan\",\\n      # dropout rate to use during layer_preprocess and layer_postprocess\\n      layer_prepostprocess_dropout=0.1,\\n      # broadcast dimensions for layer_prepostprocess_dropout\\n      # a comma-separated list of integers.\\n      # see common_layers.dropout_with_broadcast_dims()\\n      # Change this to \"1\" to save memory.\\n      layer_prepostprocess_dropout_broadcast_dims=\"\",\\n      # dropout some symbols (set them to 0) before embedding.\\n      symbol_dropout=0.0,\\n      # What type of normalization to use\\n      norm_type=\"layer\",  # \"batch\", layer\", \"noam\", \"none\".\\n      # epsilon parameter to normalization function\\n      norm_epsilon=1e-6,\\n      # pad vocabularies so that this value divides the vocabulary size.\\n      vocab_divisor=1,\\n      # During training, we drop sequences whose inputs and targets are shorter\\n      # than min_length\\n      min_length=0,\\n      # During training, we drop sequences whose inputs or targets are longer\\n      # than max_length.\\n      # If max_length==0, we use hparams.batch_size instead.\\n      max_length=0,\\n      # Pack examples on the fly.\\n      pack_dataset=False,\\n      # Use custom ops not included in standard tensorflow.\\n      use_custom_ops=True,\\n      # Split targets on the first axis into chunks of this length.\\n      split_targets_chunk_length=0,\\n      split_targets_max_chunks=100,\\n      split_targets_strided_training=False,\\n      # Maximum length in the smallest length bucket.  Setting this\\n      # flag too high will result in wasteful padding of short\\n      # sequences.  Due to some (hopefully) temporary hacks in the\\n      # data reading and batching code, setting this flag too low\\n      # results in a very long batch-shuffling queue.\\n      # TODO(noam): change this once the Datasets API changes.\\n      min_length_bucket=8,\\n      # This flag controls the number of length buckets in the data\\n      # reader.  The buckets have maximum lengths from\\n      # min_bucket_length to (max_length or batch_size), increasing\\n      # (approximately) by factors of length_bucket_step.\\n      length_bucket_step=1.1,\\n      # If set to True, drop sequences longer than max_length during eval.\\n      # This affects the validity of the evaluation metrics.\\n      eval_drop_long_sequences=False,\\n      # If True, run the model autoregressively instead of teacher-forcing\\n      # during eval\\n      eval_run_autoregressive=False,\\n      # (For features with symbol modality) If True, share all of the\\n      # input embeddings, target embeddings, and softmax weights.\\n      shared_embedding_and_softmax_weights=False,\\n      # (For features with symbol modality) If True, share the input embeddings\\n      # and target embeddings.\\n      shared_embedding=False,\\n      # (For features with symbol modality) Number to shard embeddings by.\\n      symbol_modality_num_shards=1,\\n      # Feature transformations are optional dictionaries comprising key-value\\n      # pairs of a feature name (str) and its transformation (function). If not\\n      # specified, T2TModel applies a default transformation according to the\\n      # feature\\'s modality. Bottom is applicable to all features; loss, top, and\\n      # weights_fn are only applicable to target features.\\n      # TODO(trandustin): `name` is an optional hparam for legacy reasons,\\n      # defining variable scope names. Remove this hparam in the future.\\n      bottom={},\\n      loss={},\\n      name={},\\n      top={},\\n      weights_fn={},\\n      # The maximum length of \"input\" sequence.\\n      # Sequences longer than this value will be truncated. 0 or negative values\\n      # mean there is no maximum or truncation.\\n      # You can change this behavior by overriding preprocess_example() method\\n      # in your problem class.\\n      max_input_seq_length=0,\\n      # The maximum length of \"target\" sequence.\\n      # Sequences longer than this value will be truncated. 0 or negative values\\n      # mean there is no maximum or truncation.\\n      # You can change this behavior by overriding preprocess_example() method\\n      # in your problem class.\\n      max_target_seq_length=0,\\n      # if nonzero, we split the target sequences on example read.\\n      # This is for use with language modeling problems with fixed length\\n      # examples.  e.g.  The examples may be written with length 65536, but we\\n      # want to split each example into 64 examples of length 1024.\\n      split_to_length=0,\\n      # Video settings: how many frames to batch on input and targets.\\n      video_num_input_frames=1,\\n      video_num_target_frames=1,\\n      # This flag allows us to optionally treat a seq-to-seq problem\\n      # as a language model.  Legal values are:\\n      #\\n      # \"none\" - Do not prepend the inputs to the targets.\\n      # \"prepend_inputs_masked_attention\"\\n      #     replace \"targets\" in preprocessing with\\n      #     tf.concat([inputs, [0], targets], axis=1)\\n      #     i.e. we prepend the inputs to the targets with a single\\n      #     padding token in between.  Use masked self-attention on the\\n      #     entire resulting sequence.  During training, we compute losses on\\n      #     the combined sequence.  During eval, we compute the metrics\\n      #     on only the targets portion.\\n      # \"prepend_inputs_full_attention\"\\n      #     similar to the previous option except that each\\n      #     position in the inputs portion can see the\\n      #     entire inputs portion.  This removes the challenge of\\n      #     autoregressively predicting the inputs portion.\\n      prepend_mode=\"none\",\\n      # Scheduled sampling is interesting for auto-regressive models.\\n      # It runs an additional step using the generated output as autoregressive\\n      # targets, which can improve the models inference results later. The\\n      # parameter scheduled_sampling_prob determines with what probability\\n      # will such additional step be run. It\\'s turned off (0.0) by default.\\n      # This probability will exponentially warm up for the number of\\n      # steps determined by scheduled_sampling_warmup_steps.\\n      # The tensor used for the n-th pass will consist of outputs from\\n      # the (n-1)-th pass mixed with gold truth, with the proportion of gold\\n      # determined by scheduled_sampling_gold_mixin_prob. Control the number\\n      # of passes with scheduled_sampling_num_passes.\\n      scheduled_sampling_prob=0.0,\\n      scheduled_sampling_warmup_steps=50000,\\n      scheduled_sampling_gold_mixin_prob=0.5,\\n      # TODO(duckworthd): Uncomment when we can ascertain why adding an\\n      # extra field to HParam causes test failures.\\n      # scheduled_sampling_num_passes=1,\\n\\n      # This setting controls whether to copy variables around in a daisy chain\\n      # (if true) or leave their placement to TensorFlow. It only affects multi\\n      # device training and mostly should be turned on for performance. One\\n      # exception are recurrent models: with dynamic loops it must be off.\\n      daisy_chain_variables=True,\\n      # If True in PREDICT mode, then last-position-only optimizations are not\\n      # used.\\n      force_full_predict=False,\\n      # Set this for pure model parallelism.  There is only one data shard.\\n      no_data_parallelism=False,\\n      # dtype used for activations. - \"float32\" or \"bfloat16\"\\n      # activation_dtype=\"bfloat16\" currently only works on TPU.\\n      #    It lowers activation-memory usage\\n      #    and does not appear to affect quality.\\n      #    You can train on TPU with activation_dtype=\"bfloat16\" and evaluate\\n      #    on CPU/GPU with activation_dtype=\"float32\"\\n      activation_dtype=\"float32\",\\n      # dtype used for parameters: \"float32\" or \"bfloat16\"\\n      # bfloat16 currently only works with optimizer=\"adafactor\".\\n      #   The savings in memory allow for training larger models.\\n      #   Weights are encoded as (w*128)^8, using pseudostochastic\\n      #   roundoff.  Initial experiments show that model quality is similar\\n      #   to baseline for about 3M training steps, but worse thereafter.\\n      weight_dtype=\"float32\",\\n      # Directory containing a checkpoint for a pretrained model. This will only\\n      # be used if a new run is being started. Parameters not found in the\\n      # pretrained model will be randomly initialized. Superfluous parameters in\\n      # the pretrained model will be ignored.\\n      pretrained_model_dir=\"\",\\n      # Threshold used for two cases: the primary task probability for the\\n      # constant mixing schedule, and the exponential schedule limit for when\\n      # mixing should stop (eg: 0.5 means stop at 50-50 mixing, 0.8 means stop\\n      # at 20-80 mixing for the primary-others mixing case.)\\n      multiproblem_schedule_threshold=0.5,\\n      # For more than 2 tasks, we may want to specify per-task thresholds here.\\n      # In that case, this needs to be a string with as many floating point\\n      # numbers as the number of tasks in the multi-problem. These numbers\\n      # are later normalized to add up to 1 and taken as probabilities for\\n      # each task. This enforces a constant mixing schedule and if this is\\n      # empty then the threshold from above is used for the first task and\\n      # the other tasks get the remaining probability split uniformly.\\n      multiproblem_per_task_threshold=\"\",\\n      # The number of examples at which the proportion of the mixed in datasets\\n      # is multiproblem_schedule_threshold\\n      multiproblem_schedule_max_examples=1e7,\\n      # When training multiproblems, we can mix the data according to different\\n      # schedules. Example: a constant schedule mixing 20-80 between the primary\\n      # and other tasks.\\n      # A list of supported schedules can be found in\\n      # `data_generators.multi_problem.py`.\\n      multiproblem_mixing_schedule=\"constant\",\\n      # A boolean that decides whether input sequence losses and target label\\n      # losses in classification problems should be reweighted.\\n      multiproblem_reweight_label_loss=False,\\n      # How much weight the targets in classification problems receive. Inputs\\n      # receive 1 minus this weight.\\n      multiproblem_label_weight=0.5,\\n      # Hyperparameters for relative attention.\\n      # The maximum relative positional distance to learn an embedding for.\\n      max_relative_position=0,\\n      # If heads share the same relative embedding.\\n      heads_share_relative_embedding=False,\\n      # If relative embedding terms are added to values too.\\n      add_relative_to_values=False,\\n      # If enable the host_call which is executed every training step.\\n      # There could be a performance drop if host_call function is slow and\\n      # cannot keep up with the TPU-side computation.\\n      tpu_enable_host_call=False,\\n      # Pad batch dim of inputs to nearest multiple of batch multiple.\\n      pad_batch=False,\\n      # When true, do not evaluate on the language model data when running the\\n      # multiproblem since it can take a while. If False, set eval_steps to\\n      # something large like 6000 or 10000.\\n      multiproblem_target_eval_only=False,\\n      # Max out the vocab size to a power of 2 for efficiency and to reserve\\n      # extra space in the vocabulary for new task ids and label classes.\\n      multiproblem_vocab_size=-1,\\n      # When using multiproblem with generation tasks, need to truncate the\\n      # inputs and targets manually before concatenating them.\\n      multiproblem_max_input_length=-1,\\n      multiproblem_max_target_length=-1,\\n      # If positive, makes training targets fixed-length in MultiProblem.\\n      multiproblem_fixed_train_length=-1,\\n      # Load weights from a second model. For instance, when using\\n      # pre-trained weights, you might want to initialize the encoder\\n      # and decoder by loading different models.\\n      warm_start_from_second=\"\",\\n      # Area attention hyper parameters\\n      area_value_mode=\"none\",\\n      area_key_mode=\"none\",\\n      # Using area attention for the number of layers from the bottom\\n      num_area_layers=0,\\n      max_area_width=1,\\n      max_area_height=1,\\n      memory_height=1\\n  )',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'image_to_tf_summary_value',\n",
       "  'docstring': 'Converts a NumPy image to a tf.Summary.Value object.\\n\\n  Args:\\n    image: 3-D NumPy array.\\n    tag: name for tf.Summary.Value for display in tensorboard.\\n  Returns:\\n    image_summary: A tf.Summary.Value object.',\n",
       "  'code': 'def image_to_tf_summary_value(image, tag):\\n  \"\"\"Converts a NumPy image to a tf.Summary.Value object.\\n\\n  Args:\\n    image: 3-D NumPy array.\\n    tag: name for tf.Summary.Value for display in tensorboard.\\n  Returns:\\n    image_summary: A tf.Summary.Value object.\\n  \"\"\"\\n  curr_image = np.asarray(image, dtype=np.uint8)\\n  height, width, n_channels = curr_image.shape\\n  # If monochrome image, then reshape to [height, width]\\n  if n_channels == 1:\\n    curr_image = np.reshape(curr_image, [height, width])\\n  s = io.BytesIO()\\n  matplotlib_pyplot().imsave(s, curr_image, format=\"png\")\\n  img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\\n                             height=height, width=width,\\n                             colorspace=n_channels)\\n  return tf.Summary.Value(tag=tag, image=img_sum)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'encode_images_as_png',\n",
       "  'docstring': 'Yield images encoded as pngs.',\n",
       "  'code': 'def encode_images_as_png(images):\\n  \"\"\"Yield images encoded as pngs.\"\"\"\\n  if tf.executing_eagerly():\\n    for image in images:\\n      yield tf.image.encode_png(image).numpy()\\n  else:\\n    (height, width, channels) = images[0].shape\\n    with tf.Graph().as_default():\\n      image_t = tf.placeholder(dtype=tf.uint8, shape=(height, width, channels))\\n      encoded_image_t = tf.image.encode_png(image_t)\\n      with tf.Session() as sess:\\n        for image in images:\\n          enc_string = sess.run(encoded_image_t, feed_dict={image_t: image})\\n          yield enc_string',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'image_generator',\n",
       "  'docstring': 'Generator for images that takes image and labels lists and creates pngs.\\n\\n  Args:\\n    images: list of images given as [width x height x channels] numpy arrays.\\n    labels: list of ints, same length as images.\\n\\n  Yields:\\n    A dictionary representing the images with the following fields:\\n    * image/encoded: the string encoding the image as PNG,\\n    * image/format: the string \"png\" representing image format,\\n    * image/class/label: an integer representing the label,\\n    * image/height: an integer representing the height,\\n    * image/width: an integer representing the width.\\n    Every field is actually a singleton list of the corresponding type.\\n\\n  Raises:\\n    ValueError: if images is an empty list.',\n",
       "  'code': 'def image_generator(images, labels):\\n  \"\"\"Generator for images that takes image and labels lists and creates pngs.\\n\\n  Args:\\n    images: list of images given as [width x height x channels] numpy arrays.\\n    labels: list of ints, same length as images.\\n\\n  Yields:\\n    A dictionary representing the images with the following fields:\\n    * image/encoded: the string encoding the image as PNG,\\n    * image/format: the string \"png\" representing image format,\\n    * image/class/label: an integer representing the label,\\n    * image/height: an integer representing the height,\\n    * image/width: an integer representing the width.\\n    Every field is actually a singleton list of the corresponding type.\\n\\n  Raises:\\n    ValueError: if images is an empty list.\\n  \"\"\"\\n  if not images:\\n    raise ValueError(\"Must provide some images for the generator.\")\\n  width, height, _ = images[0].shape\\n  for (enc_image, label) in zip(encode_images_as_png(images), labels):\\n    yield {\\n        \"image/encoded\": [enc_image],\\n        \"image/format\": [\"png\"],\\n        \"image/class/label\": [int(label)],\\n        \"image/height\": [height],\\n        \"image/width\": [width]\\n    }',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'compute_attention_component',\n",
       "  'docstring': 'Computes attention compoenent (query, key or value).\\n\\n  Args:\\n    antecedent: a Tensor with shape [batch, length, channels]\\n    total_depth: an integer\\n    filter_width: An integer specifying how wide you want the attention\\n      component to be.\\n    padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\\n    name: a string specifying scope name.\\n    vars_3d_num_heads: an optional integer (if we want to use 3d variables)\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n\\n  Returns:\\n    c : [batch, length, depth] tensor',\n",
       "  'code': 'def compute_attention_component(antecedent,\\n                                total_depth,\\n                                filter_width=1,\\n                                padding=\"VALID\",\\n                                name=\"c\",\\n                                vars_3d_num_heads=0,\\n                                layer_collection=None):\\n  \"\"\"Computes attention compoenent (query, key or value).\\n\\n  Args:\\n    antecedent: a Tensor with shape [batch, length, channels]\\n    total_depth: an integer\\n    filter_width: An integer specifying how wide you want the attention\\n      component to be.\\n    padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\\n    name: a string specifying scope name.\\n    vars_3d_num_heads: an optional integer (if we want to use 3d variables)\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n\\n  Returns:\\n    c : [batch, length, depth] tensor\\n  \"\"\"\\n  if layer_collection is not None:\\n    if filter_width != 1 or vars_3d_num_heads != 0:\\n      raise ValueError(\\n          \"KFAC implementation only supports filter_width=1 (actual: {}) and \"\\n          \"vars_3d_num_heads=0 (actual: {}).\".format(\\n              filter_width, vars_3d_num_heads))\\n  if vars_3d_num_heads > 0:\\n    assert filter_width == 1\\n    input_depth = antecedent.get_shape().as_list()[-1]\\n    depth_per_head = total_depth // vars_3d_num_heads\\n    initializer_stddev = input_depth ** -0.5\\n    if \"q\" in name:\\n      initializer_stddev *= depth_per_head ** -0.5\\n    var = tf.get_variable(\\n        name, [input_depth,\\n               vars_3d_num_heads,\\n               total_depth // vars_3d_num_heads],\\n        initializer=tf.random_normal_initializer(stddev=initializer_stddev))\\n    var = tf.cast(var, antecedent.dtype)\\n    var = tf.reshape(var, [input_depth, total_depth])\\n    return tf.tensordot(antecedent, var, axes=1)\\n  if filter_width == 1:\\n    return common_layers.dense(\\n        antecedent, total_depth, use_bias=False, name=name,\\n        layer_collection=layer_collection)\\n  else:\\n    return common_layers.conv1d(\\n        antecedent, total_depth, filter_width, padding=padding, name=name)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'compute_qkv',\n",
       "  'docstring': 'Computes query, key and value.\\n\\n  Args:\\n    query_antecedent: a Tensor with shape [batch, length_q, channels]\\n    memory_antecedent: a Tensor with shape [batch, length_m, channels]\\n    total_key_depth: an integer\\n    total_value_depth: an integer\\n    q_filter_width: An integer specifying how wide you want the query to be.\\n    kv_filter_width: An integer specifying how wide you want the keys and values\\n    to be.\\n    q_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\\n    kv_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\\n    vars_3d_num_heads: an optional (if we want to use 3d variables)\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n\\n  Returns:\\n    q, k, v : [batch, length, depth] tensors',\n",
       "  'code': 'def compute_qkv(query_antecedent,\\n                memory_antecedent,\\n                total_key_depth,\\n                total_value_depth,\\n                q_filter_width=1,\\n                kv_filter_width=1,\\n                q_padding=\"VALID\",\\n                kv_padding=\"VALID\",\\n                vars_3d_num_heads=0,\\n                layer_collection=None):\\n  \"\"\"Computes query, key and value.\\n\\n  Args:\\n    query_antecedent: a Tensor with shape [batch, length_q, channels]\\n    memory_antecedent: a Tensor with shape [batch, length_m, channels]\\n    total_key_depth: an integer\\n    total_value_depth: an integer\\n    q_filter_width: An integer specifying how wide you want the query to be.\\n    kv_filter_width: An integer specifying how wide you want the keys and values\\n    to be.\\n    q_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\\n    kv_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\\n    vars_3d_num_heads: an optional (if we want to use 3d variables)\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n\\n  Returns:\\n    q, k, v : [batch, length, depth] tensors\\n  \"\"\"\\n  if memory_antecedent is None:\\n    memory_antecedent = query_antecedent\\n  q = compute_attention_component(\\n      query_antecedent,\\n      total_key_depth,\\n      q_filter_width,\\n      q_padding,\\n      \"q\",\\n      vars_3d_num_heads=vars_3d_num_heads,\\n      layer_collection=layer_collection)\\n  k = compute_attention_component(\\n      memory_antecedent,\\n      total_key_depth,\\n      kv_filter_width,\\n      kv_padding,\\n      \"k\",\\n      vars_3d_num_heads=vars_3d_num_heads,\\n      layer_collection=layer_collection)\\n  v = compute_attention_component(\\n      memory_antecedent,\\n      total_value_depth,\\n      kv_filter_width,\\n      kv_padding,\\n      \"v\",\\n      vars_3d_num_heads=vars_3d_num_heads,\\n      layer_collection=layer_collection)\\n  return q, k, v',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'multihead_attention',\n",
       "  'docstring': 'Multihead scaled-dot-product attention with input/output transformations.\\n\\n  Args:\\n    query_antecedent: a Tensor with shape [batch, length_q, channels]\\n    memory_antecedent: a Tensor with shape [batch, length_m, channels] or None\\n    bias: bias Tensor (see attention_bias())\\n    total_key_depth: an integer\\n    total_value_depth: an integer\\n    output_depth: an integer\\n    num_heads: an integer dividing total_key_depth and total_value_depth\\n    dropout_rate: a floating point number\\n    attention_type: a string, either \"dot_product\", \"dot_product_relative\",\\n                    \"local_mask_right\", \"local_unmasked\", \"masked_dilated_1d\",\\n                    \"unmasked_dilated_1d\", graph, or any attention function\\n                    with the signature (query, key, value, **kwargs)\\n    max_relative_position: Maximum distance between inputs to generate\\n                           unique relation embeddings for. Only relevant\\n                           when using \"dot_product_relative\" attention.\\n    heads_share_relative_embedding: boolean to share relative embeddings\\n    add_relative_to_values: a boolean for whether to add relative component to\\n                            values.\\n    image_shapes: optional tuple of integer scalars.\\n                  see comments for attention_image_summary()\\n    block_length: an integer - relevant for \"local_mask_right\"\\n    block_width: an integer - relevant for \"local_unmasked\"\\n    q_filter_width: An integer specifying how wide you want the query to be.\\n    kv_filter_width: An integer specifying how wide you want the keys and values\\n                     to be.\\n    q_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\\n               kv_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is \"VALID\":\\n               no padding.\\n    cache: dict containing Tensors which are the results of previous\\n           attentions, used for fast decoding. Expects the dict to contrain two\\n           keys (\\'k\\' and \\'v\\'), for the initial call the values for these keys\\n           should be empty Tensors of the appropriate shape.\\n               \\'k\\' [batch_size, 0, key_channels]\\n               \\'v\\' [batch_size, 0, value_channels]\\n    gap_size: Integer option for dilated attention to indicate spacing between\\n              memory blocks.\\n    num_memory_blocks: Integer option to indicate how many memory blocks to look\\n                       at.\\n    name: an optional string.\\n    save_weights_to: an optional dictionary to capture attention weights\\n      for vizualization; the weights tensor will be appended there under\\n      a string key created from the variable scope (including name).\\n    make_image_summary: Whether to make an attention image summary.\\n    dropout_broadcast_dims:  an optional list of integers less than 4\\n      specifying in which dimensions to broadcast the dropout decisions.\\n      saves memory.\\n    vars_3d: use 3-dimensional variables for input/output transformations\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n    recurrent_memory: An optional transformer_memory.RecurrentMemory, which\\n      retains state across chunks. Default is None.\\n    chunk_number: an optional integer Tensor with shape [batch] used to operate\\n      the recurrent_memory.\\n    hard_attention_k: integer, if > 0 triggers hard attention (picking top-k).\\n    max_area_width: the max width allowed for an area.\\n    max_area_height: the max height allowed for an area.\\n    memory_height: the height of the memory.\\n    area_key_mode: the mode for computing area keys, which can be \"mean\",\\n      \"concat\", \"sum\", \"sample_concat\", and \"sample_sum\".\\n    area_value_mode: the mode for computing area values, which can be either\\n      \"mean\", or \"sum\".\\n    training: indicating if it is in the training mode.\\n    **kwargs (dict): Parameters for the attention function.\\n\\n  Caching:\\n    WARNING: For decoder self-attention, i.e. when memory_antecedent == None,\\n    the caching assumes that the bias contains future masking.\\n\\n    The caching works by saving all the previous key and value values so that\\n    you are able to send just the last query location to this attention\\n    function. I.e. if the cache dict is provided it assumes the query is of the\\n    shape [batch_size, 1, hidden_dim] rather than the full memory.\\n\\n  Returns:\\n    The result of the attention transformation. The output shape is\\n        [batch_size, length_q, hidden_dim]\\n    unless the cache dict is provided in which case only the last memory\\n    position is calculated and the output shape is [batch_size, 1, hidden_dim]\\n    Optionally returns an additional loss parameters (ex: load balance loss for\\n    the experts) returned by the attention_type function.\\n\\n  Raises:\\n    ValueError: if the key depth or value depth are not divisible by the\\n      number of attention heads.',\n",
       "  'code': 'def multihead_attention(query_antecedent,\\n                        memory_antecedent,\\n                        bias,\\n                        total_key_depth,\\n                        total_value_depth,\\n                        output_depth,\\n                        num_heads,\\n                        dropout_rate,\\n                        attention_type=\"dot_product\",\\n                        max_relative_position=None,\\n                        heads_share_relative_embedding=False,\\n                        add_relative_to_values=False,\\n                        image_shapes=None,\\n                        block_length=128,\\n                        block_width=128,\\n                        q_filter_width=1,\\n                        kv_filter_width=1,\\n                        q_padding=\"VALID\",\\n                        kv_padding=\"VALID\",\\n                        cache=None,\\n                        gap_size=0,\\n                        num_memory_blocks=2,\\n                        name=\"multihead_attention\",\\n                        save_weights_to=None,\\n                        make_image_summary=True,\\n                        dropout_broadcast_dims=None,\\n                        vars_3d=False,\\n                        layer_collection=None,\\n                        recurrent_memory=None,\\n                        chunk_number=None,\\n                        hard_attention_k=0,\\n                        max_area_width=1,\\n                        max_area_height=1,\\n                        memory_height=1,\\n                        area_key_mode=\"mean\",\\n                        area_value_mode=\"sum\",\\n                        training=True,\\n                        **kwargs):\\n  \"\"\"Multihead scaled-dot-product attention with input/output transformations.\\n\\n  Args:\\n    query_antecedent: a Tensor with shape [batch, length_q, channels]\\n    memory_antecedent: a Tensor with shape [batch, length_m, channels] or None\\n    bias: bias Tensor (see attention_bias())\\n    total_key_depth: an integer\\n    total_value_depth: an integer\\n    output_depth: an integer\\n    num_heads: an integer dividing total_key_depth and total_value_depth\\n    dropout_rate: a floating point number\\n    attention_type: a string, either \"dot_product\", \"dot_product_relative\",\\n                    \"local_mask_right\", \"local_unmasked\", \"masked_dilated_1d\",\\n                    \"unmasked_dilated_1d\", graph, or any attention function\\n                    with the signature (query, key, value, **kwargs)\\n    max_relative_position: Maximum distance between inputs to generate\\n                           unique relation embeddings for. Only relevant\\n                           when using \"dot_product_relative\" attention.\\n    heads_share_relative_embedding: boolean to share relative embeddings\\n    add_relative_to_values: a boolean for whether to add relative component to\\n                            values.\\n    image_shapes: optional tuple of integer scalars.\\n                  see comments for attention_image_summary()\\n    block_length: an integer - relevant for \"local_mask_right\"\\n    block_width: an integer - relevant for \"local_unmasked\"\\n    q_filter_width: An integer specifying how wide you want the query to be.\\n    kv_filter_width: An integer specifying how wide you want the keys and values\\n                     to be.\\n    q_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\\n               kv_padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is \"VALID\":\\n               no padding.\\n    cache: dict containing Tensors which are the results of previous\\n           attentions, used for fast decoding. Expects the dict to contrain two\\n           keys (\\'k\\' and \\'v\\'), for the initial call the values for these keys\\n           should be empty Tensors of the appropriate shape.\\n               \\'k\\' [batch_size, 0, key_channels]\\n               \\'v\\' [batch_size, 0, value_channels]\\n    gap_size: Integer option for dilated attention to indicate spacing between\\n              memory blocks.\\n    num_memory_blocks: Integer option to indicate how many memory blocks to look\\n                       at.\\n    name: an optional string.\\n    save_weights_to: an optional dictionary to capture attention weights\\n      for vizualization; the weights tensor will be appended there under\\n      a string key created from the variable scope (including name).\\n    make_image_summary: Whether to make an attention image summary.\\n    dropout_broadcast_dims:  an optional list of integers less than 4\\n      specifying in which dimensions to broadcast the dropout decisions.\\n      saves memory.\\n    vars_3d: use 3-dimensional variables for input/output transformations\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n    recurrent_memory: An optional transformer_memory.RecurrentMemory, which\\n      retains state across chunks. Default is None.\\n    chunk_number: an optional integer Tensor with shape [batch] used to operate\\n      the recurrent_memory.\\n    hard_attention_k: integer, if > 0 triggers hard attention (picking top-k).\\n    max_area_width: the max width allowed for an area.\\n    max_area_height: the max height allowed for an area.\\n    memory_height: the height of the memory.\\n    area_key_mode: the mode for computing area keys, which can be \"mean\",\\n      \"concat\", \"sum\", \"sample_concat\", and \"sample_sum\".\\n    area_value_mode: the mode for computing area values, which can be either\\n      \"mean\", or \"sum\".\\n    training: indicating if it is in the training mode.\\n    **kwargs (dict): Parameters for the attention function.\\n\\n  Caching:\\n    WARNING: For decoder self-attention, i.e. when memory_antecedent == None,\\n    the caching assumes that the bias contains future masking.\\n\\n    The caching works by saving all the previous key and value values so that\\n    you are able to send just the last query location to this attention\\n    function. I.e. if the cache dict is provided it assumes the query is of the\\n    shape [batch_size, 1, hidden_dim] rather than the full memory.\\n\\n  Returns:\\n    The result of the attention transformation. The output shape is\\n        [batch_size, length_q, hidden_dim]\\n    unless the cache dict is provided in which case only the last memory\\n    position is calculated and the output shape is [batch_size, 1, hidden_dim]\\n    Optionally returns an additional loss parameters (ex: load balance loss for\\n    the experts) returned by the attention_type function.\\n\\n  Raises:\\n    ValueError: if the key depth or value depth are not divisible by the\\n      number of attention heads.\\n  \"\"\"\\n  if total_key_depth % num_heads != 0:\\n    raise ValueError(\"Key depth (%d) must be divisible by the number of \"\\n                     \"attention heads (%d).\" % (total_key_depth, num_heads))\\n  if total_value_depth % num_heads != 0:\\n    raise ValueError(\"Value depth (%d) must be divisible by the number of \"\\n                     \"attention heads (%d).\" % (total_value_depth, num_heads))\\n  vars_3d_num_heads = num_heads if vars_3d else 0\\n\\n  if layer_collection is not None:\\n    if cache is not None:\\n      raise ValueError(\"KFAC implementation only supports cache is None.\")\\n    if vars_3d:\\n      raise ValueError(\"KFAC implementation does not support 3d vars.\")\\n\\n  if recurrent_memory is not None:\\n    if memory_antecedent is not None:\\n      raise ValueError(\"Recurrent memory requires memory_antecedent is None.\")\\n    if cache is not None:\\n      raise ValueError(\"Cache is not supported when using recurrent memory.\")\\n    if vars_3d:\\n      raise ValueError(\"3d vars are not supported when using recurrent memory.\")\\n    if layer_collection is not None:\\n      raise ValueError(\"KFAC is not supported when using recurrent memory.\")\\n    if chunk_number is None:\\n      raise ValueError(\"chunk_number is required when using recurrent memory.\")\\n\\n  with tf.variable_scope(name, default_name=\"multihead_attention\",\\n                         values=[query_antecedent, memory_antecedent]):\\n\\n    if recurrent_memory is not None:\\n      (\\n          recurrent_memory_transaction,\\n          query_antecedent, memory_antecedent, bias,\\n      ) = recurrent_memory.pre_attention(\\n          chunk_number,\\n          query_antecedent, memory_antecedent, bias,\\n      )\\n\\n    if cache is None or memory_antecedent is None:\\n      q, k, v = compute_qkv(query_antecedent, memory_antecedent,\\n                            total_key_depth, total_value_depth, q_filter_width,\\n                            kv_filter_width, q_padding, kv_padding,\\n                            vars_3d_num_heads=vars_3d_num_heads,\\n                            layer_collection=layer_collection)\\n    if cache is not None:\\n      if attention_type not in [\"dot_product\", \"dot_product_relative\"]:\\n        # TODO(petershaw): Support caching when using relative position\\n        # representations, i.e. \"dot_product_relative\" attention.\\n        raise NotImplementedError(\\n            \"Caching is not guaranteed to work with attention types other than\"\\n            \" dot_product.\")\\n      if bias is None:\\n        raise ValueError(\"Bias required for caching. See function docstring \"\\n                         \"for details.\")\\n\\n      if memory_antecedent is not None:\\n        # Encoder-Decoder Attention Cache\\n        q = compute_attention_component(query_antecedent, total_key_depth,\\n                                        q_filter_width, q_padding, \"q\",\\n                                        vars_3d_num_heads=vars_3d_num_heads)\\n        k = cache[\"k_encdec\"]\\n        v = cache[\"v_encdec\"]\\n      else:\\n        k = split_heads(k, num_heads)\\n        v = split_heads(v, num_heads)\\n        decode_loop_step = kwargs.get(\"decode_loop_step\")\\n        if decode_loop_step is None:\\n          k = cache[\"k\"] = tf.concat([cache[\"k\"], k], axis=2)\\n          v = cache[\"v\"] = tf.concat([cache[\"v\"], v], axis=2)\\n        else:\\n          # Inplace update is required for inference on TPU.\\n          # Inplace_ops only supports inplace_update on the first dimension.\\n          # The performance of current implementation is better than updating\\n          # the tensor by adding the result of matmul(one_hot,\\n          # update_in_current_step)\\n          tmp_k = tf.transpose(cache[\"k\"], perm=[2, 0, 1, 3])\\n          tmp_k = inplace_ops.alias_inplace_update(\\n              tmp_k, decode_loop_step, tf.squeeze(k, axis=2))\\n          k = cache[\"k\"] = tf.transpose(tmp_k, perm=[1, 2, 0, 3])\\n          tmp_v = tf.transpose(cache[\"v\"], perm=[2, 0, 1, 3])\\n          tmp_v = inplace_ops.alias_inplace_update(\\n              tmp_v, decode_loop_step, tf.squeeze(v, axis=2))\\n          v = cache[\"v\"] = tf.transpose(tmp_v, perm=[1, 2, 0, 3])\\n\\n    q = split_heads(q, num_heads)\\n    if cache is None:\\n      k = split_heads(k, num_heads)\\n      v = split_heads(v, num_heads)\\n\\n    key_depth_per_head = total_key_depth // num_heads\\n    if not vars_3d:\\n      q *= key_depth_per_head**-0.5\\n\\n    additional_returned_value = None\\n    if callable(attention_type):  # Generic way to extend multihead_attention\\n      x = attention_type(q, k, v, **kwargs)\\n      if isinstance(x, tuple):\\n        x, additional_returned_value = x  # Unpack\\n    elif attention_type == \"dot_product\":\\n      if max_area_width > 1 or max_area_height > 1:\\n        x = area_attention.dot_product_area_attention(\\n            q, k, v, bias, dropout_rate, image_shapes,\\n            save_weights_to=save_weights_to,\\n            dropout_broadcast_dims=dropout_broadcast_dims,\\n            max_area_width=max_area_width,\\n            max_area_height=max_area_height,\\n            memory_height=memory_height,\\n            area_key_mode=area_key_mode,\\n            area_value_mode=area_value_mode,\\n            training=training)\\n      else:\\n        x = dot_product_attention(q, k, v, bias, dropout_rate, image_shapes,\\n                                  save_weights_to=save_weights_to,\\n                                  make_image_summary=make_image_summary,\\n                                  dropout_broadcast_dims=dropout_broadcast_dims,\\n                                  activation_dtype=kwargs.get(\\n                                      \"activation_dtype\"),\\n                                  hard_attention_k=hard_attention_k)\\n    elif attention_type == \"dot_product_relative\":\\n      x = dot_product_attention_relative(\\n          q,\\n          k,\\n          v,\\n          bias,\\n          max_relative_position,\\n          dropout_rate,\\n          image_shapes,\\n          save_weights_to=save_weights_to,\\n          make_image_summary=make_image_summary,\\n          cache=cache is not None,\\n          allow_memory=recurrent_memory is not None,\\n          hard_attention_k=hard_attention_k)\\n    elif attention_type == \"dot_product_unmasked_relative_v2\":\\n      x = dot_product_unmasked_self_attention_relative_v2(\\n          q,\\n          k,\\n          v,\\n          bias,\\n          max_relative_position,\\n          dropout_rate,\\n          image_shapes,\\n          make_image_summary=make_image_summary,\\n          dropout_broadcast_dims=dropout_broadcast_dims,\\n          heads_share_relative_embedding=heads_share_relative_embedding,\\n          add_relative_to_values=add_relative_to_values)\\n    elif attention_type == \"dot_product_relative_v2\":\\n      x = dot_product_self_attention_relative_v2(\\n          q,\\n          k,\\n          v,\\n          bias,\\n          max_relative_position,\\n          dropout_rate,\\n          image_shapes,\\n          make_image_summary=make_image_summary,\\n          dropout_broadcast_dims=dropout_broadcast_dims,\\n          heads_share_relative_embedding=heads_share_relative_embedding,\\n          add_relative_to_values=add_relative_to_values)\\n    elif attention_type == \"local_within_block_mask_right\":\\n      x = masked_within_block_local_attention_1d(\\n          q, k, v, block_length=block_length)\\n    elif attention_type == \"local_relative_mask_right\":\\n      x = masked_relative_local_attention_1d(\\n          q,\\n          k,\\n          v,\\n          block_length=block_length,\\n          make_image_summary=make_image_summary,\\n          dropout_rate=dropout_rate,\\n          heads_share_relative_embedding=heads_share_relative_embedding,\\n          add_relative_to_values=add_relative_to_values,\\n          name=\"masked_relative_local_attention_1d\")\\n    elif attention_type == \"local_mask_right\":\\n      x = masked_local_attention_1d(\\n          q,\\n          k,\\n          v,\\n          block_length=block_length,\\n          make_image_summary=make_image_summary)\\n    elif attention_type == \"local_unmasked\":\\n      x = local_attention_1d(\\n          q, k, v, block_length=block_length, filter_width=block_width)\\n    elif attention_type == \"masked_dilated_1d\":\\n      x = masked_dilated_self_attention_1d(q, k, v, block_length, block_width,\\n                                           gap_size, num_memory_blocks)\\n    else:\\n      assert attention_type == \"unmasked_dilated_1d\"\\n      x = dilated_self_attention_1d(q, k, v, block_length, block_width,\\n                                    gap_size, num_memory_blocks)\\n    x = combine_heads(x)\\n\\n    # Set last dim specifically.\\n    x.set_shape(x.shape.as_list()[:-1] + [total_value_depth])\\n\\n    if vars_3d:\\n      o_var = tf.get_variable(\\n          \"o\", [num_heads, total_value_depth // num_heads, output_depth])\\n      o_var = tf.cast(o_var, x.dtype)\\n      o_var = tf.reshape(o_var, [total_value_depth, output_depth])\\n      x = tf.tensordot(x, o_var, axes=1)\\n    else:\\n      x = common_layers.dense(\\n          x, output_depth, use_bias=False, name=\"output_transform\",\\n          layer_collection=layer_collection)\\n\\n    if recurrent_memory is not None:\\n      x = recurrent_memory.post_attention(recurrent_memory_transaction, x)\\n    if additional_returned_value is not None:\\n      return x, additional_returned_value\\n    return x',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'construct_model',\n",
       "  'docstring': 'Constructs the tensorflow graph of the hierarchical model.',\n",
       "  'code': 'def construct_model(images,\\n                    actions=None,\\n                    context_frames=2,\\n                    hparams=None,\\n                    is_training=True):\\n  \"\"\"Constructs the tensorflow graph of the hierarchical model.\"\"\"\\n\\n  pred_depth = 20\\n\\n  enc_out_all, pred_out_all, van_out_all, van_on_enc_all = [], [], [], []\\n\\n  lstm_states = [None] * (pred_depth + 2)\\n\\n  enc_out = encoder_vgg(\\n      images[0], hparams.enc_size, False, scope_prefix=\\'timestep/\\',\\n      hparams=hparams, is_training=is_training)\\n  enc_out = tf.identity(enc_out, \\'enc_out\\')\\n  enc_out_all.append(enc_out)\\n\\n  num_timesteps = len(actions) - 1\\n  sum_freq = int(num_timesteps / 4 + 1)\\n\\n  reuse = False\\n  for timestep, action in zip(range(len(actions) - 1), actions[:-1]):\\n    done_warm_start = timestep > context_frames - 1\\n\\n    with tf.variable_scope(\\'timestep\\', reuse=reuse):\\n      if done_warm_start:\\n        pred_input = pred_out_all[-1]\\n      else:\\n        pred_input = enc_out_all[-1]\\n      pred_out = predictor(\\n          pred_input, action, lstm_states, pred_depth, False, hparams=hparams)\\n      pred_out = tf.identity(pred_out, \\'pred_out\\')\\n      if timestep % sum_freq == 0:  # and not hparams.use_tpu:\\n        tf.summary.histogram(\\'pred_out\\', pred_out)\\n      pred_out_all.append(pred_out)\\n\\n      if timestep % sum_freq == 0:  # and not hparams.use_tpu:\\n        tf.summary.histogram(\\'lstm_state\\', lstm_states[0])\\n      van_out, _, _ = van(\\n          enc_out_all[0],\\n          images[0],\\n          pred_out,\\n          images[timestep + 1],\\n          tf.AUTO_REUSE,\\n          hparams=hparams)\\n      van_out = tf.identity(van_out, \\'van_out\\')\\n      van_out_all.append(van_out)\\n\\n      enc_out = encoder_vgg(\\n          images[timestep + 1], hparams.enc_size, True, hparams=hparams,\\n          is_training=is_training)\\n      enc_out = tf.identity(enc_out, \\'enc_out\\')\\n      if timestep % sum_freq == 0:  # and not hparams.use_tpu:\\n        tf.summary.histogram(\\'enc_out\\', enc_out)\\n      enc_out_all.append(enc_out)\\n\\n      van_input = images[0]\\n      enc_noise = tf.zeros_like(enc_out)\\n      if timestep % sum_freq == 0:  # and not hparams.use_tpu:\\n        tf.summary.histogram(\\'enc_noise\\', enc_noise)\\n      van_on_enc, _, _ = van(\\n          enc_out_all[0],\\n          van_input,\\n          enc_out + enc_noise,\\n          images[timestep + 1],\\n          tf.AUTO_REUSE,\\n          hparams=hparams)\\n      van_on_enc = tf.identity(van_on_enc, \\'van_on_enc\\')\\n      van_on_enc_all.append(van_on_enc)\\n\\n      reuse = True\\n\\n  return enc_out_all, pred_out_all, van_out_all, van_on_enc_all',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'transformer_decoder',\n",
       "  'docstring': 'A stack of transformer layers.\\n\\n  Args:\\n    decoder_input: a Tensor\\n    encoder_output: a Tensor\\n    decoder_self_attention_bias: bias Tensor for self-attention (see\\n      common_attention.attention_bias())\\n    encoder_decoder_attention_bias: bias Tensor for encoder-decoder attention\\n      (see common_attention.attention_bias())\\n    hparams: hyperparameters for model\\n    cache: dict, containing tensors which are the results of previous\\n      attentions, used for fast decoding.\\n    decode_loop_step: An integer, step number of the decoding loop. Only used\\n      for inference on TPU.\\n    name: a string\\n    nonpadding: optional Tensor with shape [batch_size, encoder_length]\\n      indicating what positions are not padding.  This is used to mask out\\n      padding in convolutional layers.  We generally only need this mask for\\n      \"packed\" datasets, because for ordinary datasets, no padding is ever\\n      followed by nonpadding.\\n    save_weights_to: an optional dictionary to capture attention weights for\\n      visualization; the weights tensor will be appended there under a string\\n      key created from the variable scope (including name).\\n    make_image_summary: Whether to make an attention image summary.\\n    losses: optional list onto which to append extra training losses\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n    recurrent_memory_by_layer: Optional dict, mapping layer names to instances\\n      of transformer_memory.RecurrentMemory. Default is None.\\n    chunk_number: an optional integer Tensor with shape [batch] used to operate\\n      the recurrent_memory.\\n\\n  Returns:\\n    y: a Tensors',\n",
       "  'code': 'def transformer_decoder(decoder_input,\\n                        encoder_output,\\n                        decoder_self_attention_bias,\\n                        encoder_decoder_attention_bias,\\n                        hparams,\\n                        cache=None,\\n                        decode_loop_step=None,\\n                        name=\"decoder\",\\n                        nonpadding=None,\\n                        save_weights_to=None,\\n                        make_image_summary=True,\\n                        losses=None,\\n                        layer_collection=None,\\n                        recurrent_memory_by_layer=None,\\n                        chunk_number=None,\\n                        ):\\n  \"\"\"A stack of transformer layers.\\n\\n  Args:\\n    decoder_input: a Tensor\\n    encoder_output: a Tensor\\n    decoder_self_attention_bias: bias Tensor for self-attention (see\\n      common_attention.attention_bias())\\n    encoder_decoder_attention_bias: bias Tensor for encoder-decoder attention\\n      (see common_attention.attention_bias())\\n    hparams: hyperparameters for model\\n    cache: dict, containing tensors which are the results of previous\\n      attentions, used for fast decoding.\\n    decode_loop_step: An integer, step number of the decoding loop. Only used\\n      for inference on TPU.\\n    name: a string\\n    nonpadding: optional Tensor with shape [batch_size, encoder_length]\\n      indicating what positions are not padding.  This is used to mask out\\n      padding in convolutional layers.  We generally only need this mask for\\n      \"packed\" datasets, because for ordinary datasets, no padding is ever\\n      followed by nonpadding.\\n    save_weights_to: an optional dictionary to capture attention weights for\\n      visualization; the weights tensor will be appended there under a string\\n      key created from the variable scope (including name).\\n    make_image_summary: Whether to make an attention image summary.\\n    losses: optional list onto which to append extra training losses\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n    recurrent_memory_by_layer: Optional dict, mapping layer names to instances\\n      of transformer_memory.RecurrentMemory. Default is None.\\n    chunk_number: an optional integer Tensor with shape [batch] used to operate\\n      the recurrent_memory.\\n\\n  Returns:\\n    y: a Tensors\\n  \"\"\"\\n  x = decoder_input\\n  attention_dropout_broadcast_dims = (\\n      common_layers.comma_separated_string_to_integer_list(\\n          getattr(hparams, \"attention_dropout_broadcast_dims\", \"\")))\\n\\n  mlperf_log.transformer_print(\\n      key=mlperf_log.MODEL_HP_NUM_HIDDEN_LAYERS,\\n      value=hparams.num_decoder_layers or hparams.num_hidden_layers,\\n      hparams=hparams)\\n  mlperf_log.transformer_print(\\n      key=mlperf_log.MODEL_HP_ATTENTION_DROPOUT,\\n      value=hparams.attention_dropout,\\n      hparams=hparams)\\n  mlperf_log.transformer_print(\\n      key=mlperf_log.MODEL_HP_ATTENTION_DENSE,\\n      value={\\n          \"use_bias\": \"false\",\\n          \"num_heads\": hparams.num_heads,\\n          \"hidden_size\": hparams.hidden_size\\n      },\\n      hparams=hparams)\\n\\n  with tf.variable_scope(name):\\n    for layer in range(hparams.num_decoder_layers or hparams.num_hidden_layers):\\n      layer_name = \"layer_%d\" % layer\\n      layer_cache = cache[layer_name] if cache is not None else None\\n      if recurrent_memory_by_layer is not None:\\n        recurrent_memory = recurrent_memory_by_layer[layer_name]\\n      else:\\n        recurrent_memory = None\\n\\n      if layer < hparams.get(\"num_area_layers\", 0):\\n        max_area_width = hparams.get(\"max_area_width\", 1)\\n        max_area_height = hparams.get(\"max_area_height\", 1)\\n        memory_height = hparams.get(\"max_area_height\", 1)\\n      else:\\n        max_area_width = 1\\n        max_area_height = 1\\n        memory_height = 1\\n      with tf.variable_scope(layer_name):\\n        with tf.variable_scope(\"self_attention\"):\\n          y = common_attention.multihead_attention(\\n              common_layers.layer_preprocess(\\n                  x, hparams, layer_collection=layer_collection),\\n              None,\\n              decoder_self_attention_bias,\\n              hparams.attention_key_channels or hparams.hidden_size,\\n              hparams.attention_value_channels or hparams.hidden_size,\\n              hparams.hidden_size,\\n              hparams.num_heads,\\n              hparams.attention_dropout,\\n              attention_type=hparams.self_attention_type,\\n              max_relative_position=hparams.max_relative_position,\\n              heads_share_relative_embedding=(\\n                  hparams.heads_share_relative_embedding),\\n              add_relative_to_values=hparams.add_relative_to_values,\\n              save_weights_to=save_weights_to,\\n              cache=layer_cache,\\n              make_image_summary=make_image_summary,\\n              dropout_broadcast_dims=attention_dropout_broadcast_dims,\\n              max_length=hparams.get(\"max_length\"),\\n              decode_loop_step=decode_loop_step,\\n              vars_3d=hparams.get(\"attention_variables_3d\"),\\n              activation_dtype=hparams.get(\"activation_dtype\", \"float32\"),\\n              weight_dtype=hparams.get(\"weight_dtype\", \"float32\"),\\n              layer_collection=layer_collection,\\n              recurrent_memory=recurrent_memory,\\n              chunk_number=chunk_number,\\n              hard_attention_k=hparams.get(\"hard_attention_k\", 0),\\n              max_area_width=max_area_width,\\n              max_area_height=max_area_height,\\n              memory_height=memory_height,\\n              area_key_mode=hparams.get(\"area_key_mode\", \"none\"),\\n              area_value_mode=hparams.get(\"area_value_mode\", \"none\"),\\n              training=(hparams.get(\"mode\", tf.estimator.ModeKeys.TRAIN)\\n                        == tf.estimator.ModeKeys.TRAIN))\\n          x = common_layers.layer_postprocess(x, y, hparams)\\n        if encoder_output is not None:\\n          with tf.variable_scope(\"encdec_attention\"):\\n            y = common_attention.multihead_attention(\\n                common_layers.layer_preprocess(\\n                    x, hparams, layer_collection=layer_collection),\\n                encoder_output,\\n                encoder_decoder_attention_bias,\\n                hparams.attention_key_channels or hparams.hidden_size,\\n                hparams.attention_value_channels or hparams.hidden_size,\\n                hparams.hidden_size,\\n                hparams.num_heads,\\n                hparams.attention_dropout,\\n                max_relative_position=hparams.max_relative_position,\\n                heads_share_relative_embedding=(\\n                    hparams.heads_share_relative_embedding),\\n                add_relative_to_values=hparams.add_relative_to_values,\\n                save_weights_to=save_weights_to,\\n                cache=layer_cache,\\n                make_image_summary=make_image_summary,\\n                dropout_broadcast_dims=attention_dropout_broadcast_dims,\\n                max_length=hparams.get(\"max_length\"),\\n                vars_3d=hparams.get(\"attention_variables_3d\"),\\n                activation_dtype=hparams.get(\"activation_dtype\", \"float32\"),\\n                weight_dtype=hparams.get(\"weight_dtype\", \"float32\"),\\n                layer_collection=layer_collection,\\n                hard_attention_k=hparams.get(\"hard_attention_k\", 0),\\n                max_area_width=max_area_width,\\n                max_area_height=max_area_height,\\n                memory_height=memory_height,\\n                area_key_mode=hparams.get(\"area_key_mode\", \"none\"),\\n                area_value_mode=hparams.get(\"area_value_mode\", \"none\"),\\n                training=(hparams.get(\"mode\", tf.estimator.ModeKeys.TRAIN)\\n                          == tf.estimator.ModeKeys.TRAIN))\\n            x = common_layers.layer_postprocess(x, y, hparams)\\n        with tf.variable_scope(\"ffn\"):\\n          y = transformer_ffn_layer(\\n              common_layers.layer_preprocess(\\n                  x, hparams, layer_collection=layer_collection),\\n              hparams,\\n              conv_padding=\"LEFT\",\\n              nonpadding_mask=nonpadding,\\n              losses=losses,\\n              cache=layer_cache,\\n              decode_loop_step=decode_loop_step,\\n              layer_collection=layer_collection)\\n          x = common_layers.layer_postprocess(x, y, hparams)\\n    # if normalization is done in layer_preprocess, then it should also be done\\n    # on the output, since the output can grow very large, being the sum of\\n    # a whole stack of unnormalized layer outputs.\\n    mlperf_log.transformer_print(\\n        key=mlperf_log.MODEL_HP_NORM,\\n        value={\"hidden_size\": hparams.hidden_size})\\n    return common_layers.layer_preprocess(\\n        x, hparams, layer_collection=layer_collection)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'transformer_ffn_layer',\n",
       "  'docstring': 'Feed-forward layer in the transformer.\\n\\n  Args:\\n    x: a Tensor of shape [batch_size, length, hparams.hidden_size]\\n    hparams: hyperparameters for model\\n    pad_remover: an expert_utils.PadRemover object tracking the padding\\n      positions. If provided, when using convolutional settings, the padding\\n      is removed before applying the convolution, and restored afterward. This\\n      can give a significant speedup.\\n    conv_padding: a string - either \"LEFT\" or \"SAME\".\\n    nonpadding_mask: an optional Tensor with shape [batch_size, length].\\n      needed for convolutional layers with \"SAME\" padding.\\n      Contains 1.0 in positions corresponding to nonpadding.\\n    losses: optional list onto which to append extra training losses\\n    cache: dict, containing tensors which are the results of previous\\n        attentions, used for fast decoding.\\n    decode_loop_step: An integer, step number of the decoding loop.\\n        Only used for inference on TPU.\\n    readout_filter_size: if it\\'s greater than 0, then it will be used instead of\\n      filter_size\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n\\n\\n  Returns:\\n    a Tensor of shape [batch_size, length, hparams.hidden_size]\\n\\n  Raises:\\n    ValueError: If losses arg is None, but layer generates extra losses.',\n",
       "  'code': 'def transformer_ffn_layer(x,\\n                          hparams,\\n                          pad_remover=None,\\n                          conv_padding=\"LEFT\",\\n                          nonpadding_mask=None,\\n                          losses=None,\\n                          cache=None,\\n                          decode_loop_step=None,\\n                          readout_filter_size=0,\\n                          layer_collection=None):\\n  \"\"\"Feed-forward layer in the transformer.\\n\\n  Args:\\n    x: a Tensor of shape [batch_size, length, hparams.hidden_size]\\n    hparams: hyperparameters for model\\n    pad_remover: an expert_utils.PadRemover object tracking the padding\\n      positions. If provided, when using convolutional settings, the padding\\n      is removed before applying the convolution, and restored afterward. This\\n      can give a significant speedup.\\n    conv_padding: a string - either \"LEFT\" or \"SAME\".\\n    nonpadding_mask: an optional Tensor with shape [batch_size, length].\\n      needed for convolutional layers with \"SAME\" padding.\\n      Contains 1.0 in positions corresponding to nonpadding.\\n    losses: optional list onto which to append extra training losses\\n    cache: dict, containing tensors which are the results of previous\\n        attentions, used for fast decoding.\\n    decode_loop_step: An integer, step number of the decoding loop.\\n        Only used for inference on TPU.\\n    readout_filter_size: if it\\'s greater than 0, then it will be used instead of\\n      filter_size\\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\\n      KFAC optimizer. Default is None.\\n\\n\\n  Returns:\\n    a Tensor of shape [batch_size, length, hparams.hidden_size]\\n\\n  Raises:\\n    ValueError: If losses arg is None, but layer generates extra losses.\\n  \"\"\"\\n  ffn_layer = hparams.ffn_layer\\n  relu_dropout_broadcast_dims = (\\n      common_layers.comma_separated_string_to_integer_list(\\n          getattr(hparams, \"relu_dropout_broadcast_dims\", \"\")))\\n  if ffn_layer == \"conv_hidden_relu\":\\n    # Backwards compatibility\\n    ffn_layer = \"dense_relu_dense\"\\n  if ffn_layer == \"dense_relu_dense\":\\n    # In simple convolution mode, use `pad_remover` to speed up processing.\\n    mlperf_log.transformer_print(\\n        key=mlperf_log.MODEL_HP_FFN_FILTER_DENSE,\\n        value={\\n            \"filter_size\": hparams.filter_size,\\n            \"use_bias\": \"True\",\\n            \"activation\": mlperf_log.RELU\\n        })\\n    mlperf_log.transformer_print(\\n        key=mlperf_log.MODEL_HP_FFN_OUTPUT_DENSE,\\n        value={\\n            \"hidden_size\": hparams.hidden_size,\\n            \"use_bias\": \"True\",\\n        })\\n    mlperf_log.transformer_print(\\n        key=mlperf_log.MODEL_HP_RELU_DROPOUT, value=hparams.relu_dropout)\\n    if pad_remover:\\n      original_shape = common_layers.shape_list(x)\\n      # Collapse `x` across examples, and remove padding positions.\\n      x = tf.reshape(x, tf.concat([[-1], original_shape[2:]], axis=0))\\n      x = tf.expand_dims(pad_remover.remove(x), axis=0)\\n    conv_output = common_layers.dense_relu_dense(\\n        x,\\n        hparams.filter_size,\\n        hparams.hidden_size,\\n        dropout=hparams.relu_dropout,\\n        dropout_broadcast_dims=relu_dropout_broadcast_dims,\\n        layer_collection=layer_collection)\\n    if pad_remover:\\n      # Restore `conv_output` to the original shape of `x`, including padding.\\n      conv_output = tf.reshape(\\n          pad_remover.restore(tf.squeeze(conv_output, axis=0)), original_shape)\\n    return conv_output\\n  elif ffn_layer == \"conv_relu_conv\":\\n    return common_layers.conv_relu_conv(\\n        x,\\n        readout_filter_size or hparams.filter_size,\\n        hparams.hidden_size,\\n        first_kernel_size=hparams.conv_first_kernel,\\n        second_kernel_size=1,\\n        padding=conv_padding,\\n        nonpadding_mask=nonpadding_mask,\\n        dropout=hparams.relu_dropout,\\n        cache=cache,\\n        decode_loop_step=decode_loop_step)\\n  elif ffn_layer == \"parameter_attention\":\\n    return common_attention.parameter_attention(\\n        x, hparams.parameter_attention_key_channels or hparams.hidden_size,\\n        hparams.parameter_attention_value_channels or hparams.hidden_size,\\n        hparams.hidden_size, readout_filter_size or hparams.filter_size,\\n        hparams.num_heads,\\n        hparams.attention_dropout)\\n  elif ffn_layer == \"conv_hidden_relu_with_sepconv\":\\n    return common_layers.conv_hidden_relu(\\n        x,\\n        readout_filter_size or hparams.filter_size,\\n        hparams.hidden_size,\\n        kernel_size=(3, 1),\\n        second_kernel_size=(31, 1),\\n        padding=\"LEFT\",\\n        dropout=hparams.relu_dropout)\\n  elif ffn_layer == \"sru\":\\n    return common_layers.sru(x)\\n  elif ffn_layer == \"local_moe_tpu\":\\n    overhead = hparams.moe_overhead_eval\\n    if hparams.mode == tf.estimator.ModeKeys.TRAIN:\\n      overhead = hparams.moe_overhead_train\\n    ret, loss = expert_utils.local_moe_tpu(\\n        x,\\n        hparams.filter_size // 2,\\n        hparams.hidden_size,\\n        hparams.moe_num_experts,\\n        overhead=overhead,\\n        loss_coef=hparams.moe_loss_coef)\\n  elif ffn_layer == \"local_moe\":\\n    overhead = hparams.moe_overhead_eval\\n    if hparams.mode == tf.estimator.ModeKeys.TRAIN:\\n      overhead = hparams.moe_overhead_train\\n    ret, loss = expert_utils.local_moe(\\n        x,\\n        True,\\n        expert_utils.ffn_expert_fn(hparams.hidden_size, [hparams.filter_size],\\n                                   hparams.hidden_size),\\n        hparams.moe_num_experts,\\n        k=hparams.moe_k,\\n        hparams=hparams)\\n    losses.append(loss)\\n    return ret\\n  else:\\n    assert ffn_layer == \"none\"\\n    return x',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'create_eager_metrics_internal',\n",
       "  'docstring': 'Create metrics accumulators and averager for Eager mode.\\n\\n  Args:\\n    metric_fns: dict<metric name, metric function>\\n    weights_fn: function that takes labels and returns a weights mask. Defaults\\n      to weights of all 1, i.e. common_layers.weights_all. Use\\n      common_layers.weights_nonzero if labels have 0-padding.\\n\\n  Returns:\\n    (accum_fn(predictions, targets) => None,\\n     result_fn() => dict<str metric_name, float avg_val>',\n",
       "  'code': 'def create_eager_metrics_internal(metric_fns,\\n                                  weights_fn=common_layers.weights_all):\\n  \"\"\"Create metrics accumulators and averager for Eager mode.\\n\\n  Args:\\n    metric_fns: dict<metric name, metric function>\\n    weights_fn: function that takes labels and returns a weights mask. Defaults\\n      to weights of all 1, i.e. common_layers.weights_all. Use\\n      common_layers.weights_nonzero if labels have 0-padding.\\n\\n  Returns:\\n    (accum_fn(predictions, targets) => None,\\n     result_fn() => dict<str metric_name, float avg_val>\\n  \"\"\"\\n  tfe_metrics = {}\\n\\n  for name in metric_fns:\\n    tfe_metrics[name] = tfe.metrics.Mean(name=name)\\n\\n  def metric_accum(predictions, targets):\\n    for name, metric_fn in metric_fns.items():\\n      val, weight = metric_fn(predictions, targets,\\n                              weights_fn=weights_fn)\\n      tfe_metrics[name](np.squeeze(val), np.squeeze(weight))\\n\\n  def metric_means():\\n    avgs = {}\\n    for name in metric_fns:\\n      avgs[name] = tfe_metrics[name].result().numpy()\\n    return avgs\\n\\n  return metric_accum, metric_means',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_try_twice_tf_glob',\n",
       "  'docstring': 'Glob twice, first time possibly catching `NotFoundError`.\\n\\n  tf.gfile.Glob may crash with\\n\\n  ```\\n  tensorflow.python.framework.errors_impl.NotFoundError:\\n  xy/model.ckpt-1130761_temp_9cb4cb0b0f5f4382b5ea947aadfb7a40;\\n  No such file or directory\\n  ```\\n\\n  Standard glob.glob does not have this bug, but does not handle multiple\\n  filesystems (e.g. `gs://`), so we call tf.gfile.Glob, the first time possibly\\n  catching the `NotFoundError`.\\n\\n  Args:\\n    pattern: str, glob pattern.\\n\\n  Returns:\\n    list<str> matching filepaths.',\n",
       "  'code': 'def _try_twice_tf_glob(pattern):\\n  \"\"\"Glob twice, first time possibly catching `NotFoundError`.\\n\\n  tf.gfile.Glob may crash with\\n\\n  ```\\n  tensorflow.python.framework.errors_impl.NotFoundError:\\n  xy/model.ckpt-1130761_temp_9cb4cb0b0f5f4382b5ea947aadfb7a40;\\n  No such file or directory\\n  ```\\n\\n  Standard glob.glob does not have this bug, but does not handle multiple\\n  filesystems (e.g. `gs://`), so we call tf.gfile.Glob, the first time possibly\\n  catching the `NotFoundError`.\\n\\n  Args:\\n    pattern: str, glob pattern.\\n\\n  Returns:\\n    list<str> matching filepaths.\\n  \"\"\"\\n  try:\\n    return tf.gfile.Glob(pattern)\\n  except tf.errors.NotFoundError:\\n    return tf.gfile.Glob(pattern)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'stepfiles_iterator',\n",
       "  'docstring': 'Continuously yield new files with steps in filename as they appear.\\n\\n  This is useful for checkpoint files or other files whose names differ just in\\n  an integer marking the number of steps and match the wildcard path_prefix +\\n  \"*-[0-9]*\" + path_suffix.\\n\\n  Unlike `tf.contrib.training.checkpoints_iterator`, this implementation always\\n  starts from the oldest files (and it cannot miss any file). Note that the\\n  oldest checkpoint may be deleted anytime by Tensorflow (if set up so). It is\\n  up to the user to check that the files returned by this generator actually\\n  exist.\\n\\n  Args:\\n    path_prefix: The directory + possible common filename prefix to the files.\\n    wait_minutes: The maximum amount of minutes to wait between files.\\n    min_steps: Skip files with lower global step.\\n    path_suffix: Common filename suffix (after steps), including possible\\n      extension dot.\\n    sleep_sec: How often to check for new files.\\n\\n  Yields:\\n    named tuples (filename, mtime, ctime, steps) of the files as they arrive.',\n",
       "  'code': 'def stepfiles_iterator(path_prefix, wait_minutes=0, min_steps=0,\\n                       path_suffix=\".index\", sleep_sec=10):\\n  \"\"\"Continuously yield new files with steps in filename as they appear.\\n\\n  This is useful for checkpoint files or other files whose names differ just in\\n  an integer marking the number of steps and match the wildcard path_prefix +\\n  \"*-[0-9]*\" + path_suffix.\\n\\n  Unlike `tf.contrib.training.checkpoints_iterator`, this implementation always\\n  starts from the oldest files (and it cannot miss any file). Note that the\\n  oldest checkpoint may be deleted anytime by Tensorflow (if set up so). It is\\n  up to the user to check that the files returned by this generator actually\\n  exist.\\n\\n  Args:\\n    path_prefix: The directory + possible common filename prefix to the files.\\n    wait_minutes: The maximum amount of minutes to wait between files.\\n    min_steps: Skip files with lower global step.\\n    path_suffix: Common filename suffix (after steps), including possible\\n      extension dot.\\n    sleep_sec: How often to check for new files.\\n\\n  Yields:\\n    named tuples (filename, mtime, ctime, steps) of the files as they arrive.\\n  \"\"\"\\n  # Wildcard D*-[0-9]* does not match D/x-1, so if D is a directory let\\n  # path_prefix=\"D/\".\\n  if not path_prefix.endswith(os.sep) and os.path.isdir(path_prefix):\\n    path_prefix += os.sep\\n  stepfiles = _read_stepfiles_list(path_prefix, path_suffix, min_steps)\\n  tf.logging.info(\"Found %d files with steps: %s\",\\n                  len(stepfiles),\\n                  \", \".join(str(x.steps) for x in reversed(stepfiles)))\\n  exit_time = time.time() + wait_minutes * 60\\n  while True:\\n    if not stepfiles and wait_minutes:\\n      tf.logging.info(\\n          \"Waiting till %s if a new file matching %s*-[0-9]*%s appears\",\\n          time.asctime(time.localtime(exit_time)), path_prefix, path_suffix)\\n      while True:\\n        stepfiles = _read_stepfiles_list(path_prefix, path_suffix, min_steps)\\n        if stepfiles or time.time() > exit_time:\\n          break\\n        time.sleep(sleep_sec)\\n    if not stepfiles:\\n      return\\n\\n    stepfile = stepfiles.pop()\\n    exit_time, min_steps = (stepfile.ctime + wait_minutes * 60,\\n                            stepfile.steps + 1)\\n    yield stepfile',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'TransformerModel.process',\n",
       "  'docstring': 'Returns the visualizations for query.\\n\\n    Args:\\n      query: The query to process.\\n\\n    Returns:\\n      A dictionary of results with processing and graph visualizations.',\n",
       "  'code': 'def process(self, query):\\n    \"\"\"Returns the visualizations for query.\\n\\n    Args:\\n      query: The query to process.\\n\\n    Returns:\\n      A dictionary of results with processing and graph visualizations.\\n    \"\"\"\\n    tf.logging.info(\"Processing new query [%s]\" %query)\\n\\n    # Create the new TFDBG hook directory.\\n    hook_dir = \"/tmp/t2t_server_dump/request_%d\" %int(time.time())\\n    os.makedirs(hook_dir)\\n    hooks = [tfdbg.DumpingDebugHook(hook_dir, watch_fn=topk_watch_fn)]\\n\\n    # TODO(kstevens): This is extremely hacky and slow for responding to\\n    # queries.  Figure out a reasonable way to pre-load the model weights before\\n    # forking and run queries through the estimator quickly.\\n    def server_input_fn():\\n      \"\"\"Generator that returns just the current query.\"\"\"\\n      for _ in range(1):\\n        input_ids = self.source_vocab.encode(query)\\n        input_ids.append(text_encoder.EOS_ID)\\n        x = [1, 100, len(input_ids)] + input_ids\\n        x += [0] * (self.const_array_size - len(x))\\n        d = {\\n            \"inputs\": np.array(x).astype(np.int32),\\n        }\\n        yield d\\n\\n    def input_fn():\\n      \"\"\"Generator that returns just the current query.\"\"\"\\n      gen_fn = decoding.make_input_fn_from_generator(server_input_fn())\\n      example = gen_fn()\\n      # TODO(kstevens): Make this method public\\n      # pylint: disable=protected-access\\n      return decoding._interactive_input_tensor_to_features_dict(\\n          example, self.hparams)\\n\\n    # Make the prediction for the current query.\\n    result_iter = self.estimator.predict(input_fn, hooks=hooks)\\n    result = None\\n    for result in result_iter:\\n      break\\n\\n    # Extract the beam search information by reading the dumped TFDBG event\\n    # tensors.  We first read and record the per step beam sequences then record\\n    # the beam scores.  Afterwards we align the two sets of values to create the\\n    # full graph vertices and edges.\\n    decoding_graph = graph.Graph()\\n    run_dirs = sorted(glob.glob(os.path.join(hook_dir, \"run_*\")))\\n    for run_dir in run_dirs:\\n      # Record the different completed and active beam sequence ids.\\n      alive_sequences = deque()\\n      finished_sequences = deque()\\n\\n      # Make the root vertex since it always needs to exist.\\n      decoding_graph.get_vertex(sequence_key([0]))\\n\\n      # Create the initial vertices and edges for the active and finished\\n      # sequences.  We uniquely define each vertex using it\\'s full sequence path\\n      # as a string to ensure there\\'s no collisions when the same step has two\\n      # instances of an output id.\\n      dump_dir = tfdbg.DebugDumpDir(run_dir, validate=False)\\n      seq_datums = dump_dir.find(predicate=seq_filter)\\n      for seq_datum in seq_datums:\\n        sequences = np.array(seq_datum.get_tensor()).astype(int)[0]\\n        if \"alive\" in seq_datum.node_name:\\n          alive_sequences.append(sequences)\\n        if \"finished\" in seq_datum.node_name:\\n          finished_sequences.append(sequences)\\n\\n        for sequence in sequences:\\n          pieces = self.targets_vocab.decode_list(sequence)\\n          index = sequence[-1]\\n          if index == 0:\\n            continue\\n\\n          parent = decoding_graph.get_vertex(sequence_key(sequence[:-1]))\\n          current = decoding_graph.get_vertex(sequence_key(sequence))\\n\\n          edge = decoding_graph.add_edge(parent, current)\\n          edge.data[\"label\"] = pieces[-1]\\n          edge.data[\"label_id\"] = index\\n          # Coerce the type to be a python bool.  Numpy bools can\\'t be easily\\n          # converted to JSON.\\n          edge.data[\"completed\"] = bool(index == 1)\\n\\n      # Examine the score results and store the scores with the associated edges\\n      # in the graph.  We fetch the vertices (and relevant edges) by looking\\n      # into the saved beam sequences stored above.\\n      score_datums = dump_dir.find(predicate=scores_filter)\\n      for score_datum in score_datums:\\n        if \"alive\" in score_datum.node_name:\\n          sequences = alive_sequences.popleft()\\n\\n        if \"finished\" in score_datum.node_name:\\n          sequences = finished_sequences.popleft()\\n\\n        scores = np.array(score_datum.get_tensor()).astype(float)[0]\\n        for i, score in enumerate(scores):\\n          sequence = sequences[i]\\n          if sequence[-1] == 0:\\n            continue\\n\\n          vertex = decoding_graph.get_vertex(sequence_key(sequence))\\n          edge = decoding_graph.edges[vertex.in_edges[0]]\\n          edge.data[\"score\"] = score\\n          edge.data[\"log_probability\"] = score\\n          edge.data[\"total_log_probability\"] = score\\n\\n    # Delete the hook dir to save disk space\\n    shutil.rmtree(hook_dir)\\n\\n    # Create the graph visualization data structure.\\n    graph_vis = {\\n        \"visualization_name\": \"graph\",\\n        \"title\": \"Graph\",\\n        \"name\": \"graph\",\\n        \"search_graph\": decoding_graph.to_dict(),\\n    }\\n\\n    # Create the processing visualization data structure.\\n    # TODO(kstevens): Make this method public\\n    # pylint: disable=protected-access\\n    output_ids = decoding._save_until_eos(result[\"outputs\"].flatten(), False)\\n    output_pieces = self.targets_vocab.decode_list(output_ids)\\n    output_token = [{\"text\": piece} for piece in output_pieces]\\n    output = self.targets_vocab.decode(output_ids)\\n\\n    source_steps = [{\\n        \"step_name\": \"Initial\",\\n        \"segment\": [{\\n            \"text\": query\\n        }],\\n    }]\\n\\n    target_steps = [{\\n        \"step_name\": \"Initial\",\\n        \"segment\": output_token,\\n    }, {\\n        \"step_name\": \"Final\",\\n        \"segment\": [{\\n            \"text\": output\\n        }],\\n    }]\\n\\n    processing_vis = {\\n        \"visualization_name\": \"processing\",\\n        \"title\": \"Processing\",\\n        \"name\": \"processing\",\\n        \"query_processing\": {\\n            \"source_processing\": source_steps,\\n            \"target_processing\": target_steps,\\n        },\\n    }\\n\\n    return {\\n        \"result\": [processing_vis, graph_vis],\\n    }',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'score_file',\n",
       "  'docstring': 'Score each line in a file and return the scores.',\n",
       "  'code': 'def score_file(filename):\\n  \"\"\"Score each line in a file and return the scores.\"\"\"\\n  # Prepare model.\\n  hparams = create_hparams()\\n  encoders = registry.problem(FLAGS.problem).feature_encoders(FLAGS.data_dir)\\n  has_inputs = \"inputs\" in encoders\\n\\n  # Prepare features for feeding into the model.\\n  if has_inputs:\\n    inputs_ph = tf.placeholder(dtype=tf.int32)  # Just length dimension.\\n    batch_inputs = tf.reshape(inputs_ph, [1, -1, 1, 1])  # Make it 4D.\\n  targets_ph = tf.placeholder(dtype=tf.int32)  # Just length dimension.\\n  batch_targets = tf.reshape(targets_ph, [1, -1, 1, 1])  # Make it 4D.\\n  if has_inputs:\\n    features = {\"inputs\": batch_inputs, \"targets\": batch_targets}\\n  else:\\n    features = {\"targets\": batch_targets}\\n\\n  # Prepare the model and the graph when model runs on features.\\n  model = registry.model(FLAGS.model)(hparams, tf.estimator.ModeKeys.EVAL)\\n  _, losses = model(features)\\n  saver = tf.train.Saver()\\n\\n  with tf.Session() as sess:\\n    # Load weights from checkpoint.\\n    if FLAGS.checkpoint_path is None:\\n      ckpts = tf.train.get_checkpoint_state(FLAGS.output_dir)\\n      ckpt = ckpts.model_checkpoint_path\\n    else:\\n      ckpt = FLAGS.checkpoint_path\\n    saver.restore(sess, ckpt)\\n    # Run on each line.\\n    with tf.gfile.Open(filename) as f:\\n      lines = f.readlines()\\n    results = []\\n    for line in lines:\\n      tab_split = line.split(\"\\\\t\")\\n      if len(tab_split) > 2:\\n        raise ValueError(\"Each line must have at most one tab separator.\")\\n      if len(tab_split) == 1:\\n        targets = tab_split[0].strip()\\n      else:\\n        targets = tab_split[1].strip()\\n        inputs = tab_split[0].strip()\\n      # Run encoders and append EOS symbol.\\n      targets_numpy = encoders[\"targets\"].encode(\\n          targets) + [text_encoder.EOS_ID]\\n      if has_inputs:\\n        inputs_numpy = encoders[\"inputs\"].encode(inputs) + [text_encoder.EOS_ID]\\n      # Prepare the feed.\\n      if has_inputs:\\n        feed = {inputs_ph: inputs_numpy, targets_ph: targets_numpy}\\n      else:\\n        feed = {targets_ph: targets_numpy}\\n      # Get the score.\\n      np_loss = sess.run(losses[\"training\"], feed)\\n      results.append(np_loss)\\n  return results',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_define_collect',\n",
       "  'docstring': 'Collect trajectories.\\n\\n  Args:\\n    batch_env: Batch environment.\\n    ppo_hparams: PPO hparams, defined in tensor2tensor.models.research.rl.\\n    scope: var scope.\\n    frame_stack_size: Number of last observations to feed into the policy.\\n    eval_phase: TODO(koz4k): Write docstring.\\n    sampling_temp: Sampling temperature for the policy.\\n    force_beginning_resets: Whether to reset at the beginning of each episode.\\n\\n  Returns:\\n    Returns memory (observations, rewards, dones, actions,\\n    pdfs, values_functions)\\n    containing a rollout of environment from nested wrapped structure.',\n",
       "  'code': 'def _define_collect(batch_env, ppo_hparams, scope, frame_stack_size, eval_phase,\\n                    sampling_temp, force_beginning_resets):\\n  \"\"\"Collect trajectories.\\n\\n  Args:\\n    batch_env: Batch environment.\\n    ppo_hparams: PPO hparams, defined in tensor2tensor.models.research.rl.\\n    scope: var scope.\\n    frame_stack_size: Number of last observations to feed into the policy.\\n    eval_phase: TODO(koz4k): Write docstring.\\n    sampling_temp: Sampling temperature for the policy.\\n    force_beginning_resets: Whether to reset at the beginning of each episode.\\n\\n  Returns:\\n    Returns memory (observations, rewards, dones, actions,\\n    pdfs, values_functions)\\n    containing a rollout of environment from nested wrapped structure.\\n  \"\"\"\\n  epoch_length = ppo_hparams.epoch_length\\n\\n  to_initialize = []\\n  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\\n    num_agents = batch_env.batch_size\\n\\n    to_initialize.append(batch_env)\\n    wrappers = [(StackWrapper, {\\n        \"history\": frame_stack_size\\n    }), (_MemoryWrapper, {})]\\n    rollout_metadata = None\\n    speculum = None\\n    for w in wrappers:\\n      tf.logging.info(\"Applying wrapper %s(%s) to env %s.\" % (str(\\n          w[0]), str(w[1]), str(batch_env)))\\n      batch_env = w[0](batch_env, **w[1])\\n      to_initialize.append(batch_env)\\n\\n    rollout_metadata = _rollout_metadata(batch_env)\\n    speculum = batch_env.speculum\\n\\n    def initialization_lambda(sess):\\n      for batch_env in to_initialize:\\n        batch_env.initialize(sess)\\n\\n    memory = [\\n        tf.get_variable(  # pylint: disable=g-complex-comprehension\\n            \"collect_memory_%d_%s\" % (epoch_length, name),\\n            shape=[epoch_length] + shape,\\n            dtype=dtype,\\n            initializer=tf.zeros_initializer(),\\n            trainable=False) for (shape, dtype, name) in rollout_metadata\\n    ]\\n\\n    cumulative_rewards = tf.get_variable(\\n        \"cumulative_rewards\", len(batch_env), trainable=False)\\n\\n    eval_phase_t = tf.convert_to_tensor(eval_phase)\\n    should_reset_var = tf.Variable(True, trainable=False)\\n    zeros_tensor = tf.zeros(len(batch_env))\\n\\n  force_beginning_resets = tf.convert_to_tensor(force_beginning_resets)\\n\\n  def reset_ops_group():\\n    return tf.group(\\n        batch_env.reset(tf.range(len(batch_env))),\\n        tf.assign(cumulative_rewards, zeros_tensor))\\n\\n  reset_op = tf.cond(\\n      tf.logical_or(should_reset_var.read_value(), force_beginning_resets),\\n      reset_ops_group, tf.no_op)\\n\\n  with tf.control_dependencies([reset_op]):\\n    reset_once_op = tf.assign(should_reset_var, False)\\n\\n  with tf.control_dependencies([reset_once_op]):\\n\\n    def step(index, scores_sum, scores_num):\\n      \"\"\"Single step.\"\"\"\\n      index %= epoch_length  # Only needed in eval runs.\\n      # Note - the only way to ensure making a copy of tensor is to run simple\\n      # operation. We are waiting for tf.copy:\\n      # https://github.com/tensorflow/tensorflow/issues/11186\\n      obs_copy = batch_env.observ + 0\\n\\n      def env_step(arg1, arg2, arg3):  # pylint: disable=unused-argument\\n        \"\"\"Step of the environment.\"\"\"\\n\\n        (logits, value_function) = get_policy(\\n            obs_copy, ppo_hparams, batch_env.action_space\\n        )\\n        action = common_layers.sample_with_temperature(logits, sampling_temp)\\n        action = tf.cast(action, tf.int32)\\n        action = tf.reshape(action, shape=(num_agents,))\\n\\n        reward, done = batch_env.simulate(action)\\n\\n        pdf = tfp.distributions.Categorical(logits=logits).prob(action)\\n        pdf = tf.reshape(pdf, shape=(num_agents,))\\n        value_function = tf.reshape(value_function, shape=(num_agents,))\\n        done = tf.reshape(done, shape=(num_agents,))\\n\\n        with tf.control_dependencies([reward, done]):\\n          return tf.identity(pdf), tf.identity(value_function), \\\\\\n                 tf.identity(done)\\n\\n      # TODO(piotrmilos): while_body is executed at most once,\\n      # thus should be replaced with tf.cond\\n      pdf, value_function, top_level_done = tf.while_loop(\\n          lambda _1, _2, _3: tf.equal(speculum.size(), 0),\\n          env_step,\\n          [\\n              tf.constant(0.0, shape=(num_agents,)),\\n              tf.constant(0.0, shape=(num_agents,)),\\n              tf.constant(False, shape=(num_agents,))\\n          ],\\n          parallel_iterations=1,\\n          back_prop=False,\\n      )\\n\\n      with tf.control_dependencies([pdf, value_function]):\\n        obs, reward, done, action = speculum.dequeue()\\n\\n        to_save = [obs, reward, done, action, pdf, value_function]\\n        save_ops = [\\n            tf.scatter_update(memory_slot, index, value)\\n            for memory_slot, value in zip(memory, to_save)\\n        ]\\n        cumulate_rewards_op = cumulative_rewards.assign_add(reward)\\n\\n        agent_indices_to_reset = tf.where(top_level_done)[:, 0]\\n      with tf.control_dependencies([cumulate_rewards_op]):\\n        # TODO(piotrmilos): possibly we need cumulative_rewards.read_value()\\n        scores_sum_delta = tf.reduce_sum(\\n            tf.gather(cumulative_rewards.read_value(), agent_indices_to_reset))\\n        scores_num_delta = tf.count_nonzero(done, dtype=tf.int32)\\n      with tf.control_dependencies(save_ops +\\n                                   [scores_sum_delta, scores_num_delta]):\\n        reset_env_op = batch_env.reset(agent_indices_to_reset)\\n        reset_cumulative_rewards_op = tf.scatter_update(\\n            cumulative_rewards, agent_indices_to_reset,\\n            tf.gather(zeros_tensor, agent_indices_to_reset))\\n      with tf.control_dependencies([reset_env_op, reset_cumulative_rewards_op]):\\n        return [\\n            index + 1, scores_sum + scores_sum_delta,\\n            scores_num + scores_num_delta\\n        ]\\n\\n    def stop_condition(i, _, resets):\\n      return tf.cond(eval_phase_t, lambda: resets < num_agents,\\n                     lambda: i < epoch_length)\\n\\n    init = [tf.constant(0), tf.constant(0.0), tf.constant(0)]\\n    index, scores_sum, scores_num = tf.while_loop(\\n        stop_condition, step, init, parallel_iterations=1, back_prop=False)\\n\\n  # We handle force_beginning_resets differently. We assume that all envs are\\n  # reseted at the end of episod (though it happens at the beginning of the\\n  # next one\\n  scores_num = tf.cond(force_beginning_resets,\\n                       lambda: scores_num + len(batch_env), lambda: scores_num)\\n\\n  with tf.control_dependencies([scores_sum]):\\n    scores_sum = tf.cond(\\n        force_beginning_resets,\\n        lambda: scores_sum + tf.reduce_sum(cumulative_rewards.read_value()),\\n        lambda: scores_sum)\\n\\n  mean_score = tf.cond(\\n      tf.greater(scores_num, 0),\\n      lambda: scores_sum / tf.cast(scores_num, tf.float32), lambda: 0.)\\n  printing = tf.Print(0, [mean_score, scores_sum, scores_num], \"mean_score: \")\\n  with tf.control_dependencies([index, printing]):\\n    memory = [mem.read_value() for mem in memory]\\n    # When generating real data together with PPO training we must use single\\n    # agent. For PPO to work we reshape the history, as if it was generated\\n    # by real_ppo_effective_num_agents.\\n    if ppo_hparams.effective_num_agents is not None and not eval_phase:\\n      new_memory = []\\n      effective_num_agents = ppo_hparams.effective_num_agents\\n      assert epoch_length % ppo_hparams.effective_num_agents == 0, (\\n          \"The rollout of ppo_hparams.epoch_length will be distributed amongst\"\\n          \"effective_num_agents of agents\")\\n      new_epoch_length = int(epoch_length / effective_num_agents)\\n      for mem, info in zip(memory, rollout_metadata):\\n        shape, _, name = info\\n        new_shape = [effective_num_agents, new_epoch_length] + shape[1:]\\n        perm = list(range(len(shape) + 1))\\n        perm[0] = 1\\n        perm[1] = 0\\n        mem = tf.transpose(mem, perm=perm)\\n        mem = tf.reshape(mem, shape=new_shape)\\n        mem = tf.transpose(\\n            mem,\\n            perm=perm,\\n            name=\"collect_memory_%d_%s\" % (new_epoch_length, name))\\n        new_memory.append(mem)\\n      memory = new_memory\\n\\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\\n      mean_score_summary = tf.cond(\\n          tf.greater(scores_num, 0),\\n          lambda: tf.summary.scalar(\"mean_score_this_iter\", mean_score), str)\\n      summaries = tf.summary.merge([\\n          mean_score_summary,\\n          tf.summary.scalar(\"episodes_finished_this_iter\", scores_num)\\n      ])\\n      return memory, summaries, initialization_lambda',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'inputs',\n",
       "  'docstring': 'Make Inputs for built-in datasets.\\n\\n  Args:\\n    num_devices: how many devices to build the inputs for.\\n    dataset_name: a TFDS or T2T dataset name. If it\\'s a T2T dataset name, prefix\\n      with \"t2t_\".\\n    data_dir: data directory.\\n    input_name: optional, name of the inputs from the dictionary.\\n    num_chunks: optional, into how many pieces should we chunk (large inputs).\\n    append_targets: optional, instead of inputs return a pair (inputs, targets)\\n      which is useful for autoregressive models.\\n\\n  Returns:\\n    trax.inputs.Inputs',\n",
       "  'code': 'def inputs(num_devices, dataset_name, data_dir=None, input_name=None,\\n           num_chunks=0, append_targets=False):\\n  \"\"\"Make Inputs for built-in datasets.\\n\\n  Args:\\n    num_devices: how many devices to build the inputs for.\\n    dataset_name: a TFDS or T2T dataset name. If it\\'s a T2T dataset name, prefix\\n      with \"t2t_\".\\n    data_dir: data directory.\\n    input_name: optional, name of the inputs from the dictionary.\\n    num_chunks: optional, into how many pieces should we chunk (large inputs).\\n    append_targets: optional, instead of inputs return a pair (inputs, targets)\\n      which is useful for autoregressive models.\\n\\n  Returns:\\n    trax.inputs.Inputs\\n  \"\"\"\\n  assert data_dir, \"Must provide a data directory\"\\n  data_dir = os.path.expanduser(data_dir)\\n\\n  (train_batches, train_eval_batches, eval_batches,\\n   input_name, input_shape) = _train_and_eval_batches(\\n       dataset_name, data_dir, input_name, num_devices)\\n\\n  def numpy_stream(dataset):\\n    return dataset_to_stream(\\n        dataset, input_name,\\n        num_chunks=num_chunks, append_targets=append_targets)\\n\\n  if num_chunks > 0:\\n    length = input_shape[0]\\n    input_shape = tuple(\\n        [tuple([length // num_chunks] + list(input_shape)[1:])] * num_chunks)\\n\\n  return Inputs(train_stream=lambda: numpy_stream(train_batches),\\n                train_eval_stream=lambda: numpy_stream(train_eval_batches),\\n                eval_stream=lambda: numpy_stream(eval_batches),\\n                input_shape=input_shape)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'dataset_to_stream',\n",
       "  'docstring': 'Takes a tf.Dataset and creates a numpy stream of ready batches.',\n",
       "  'code': 'def dataset_to_stream(dataset, input_name, num_chunks=0, append_targets=False):\\n  \"\"\"Takes a tf.Dataset and creates a numpy stream of ready batches.\"\"\"\\n  for example in tfds.as_numpy(dataset):\\n    inp, out = example[0][input_name], example[1]\\n    if len(out.shape) > 1 and out.shape[-1] == 1:\\n      out = np.squeeze(out, axis=-1)\\n    if num_chunks > 0:\\n      inp = np.split(inp, num_chunks, axis=1)\\n      out = np.split(out, num_chunks, axis=1)\\n    if append_targets:\\n      inp = (inp, out)\\n    yield inp, out',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'create_border',\n",
       "  'docstring': 'Creates a border around each frame to differentiate input and target.\\n\\n  Args:\\n    video: 5-D NumPy array.\\n    color: string, \"blue\", \"red\" or \"green\".\\n    border_percent: Percentarge of the frame covered by the border.\\n  Returns:\\n    video: 5-D NumPy array.',\n",
       "  'code': 'def create_border(video, color=\"blue\", border_percent=2):\\n  \"\"\"Creates a border around each frame to differentiate input and target.\\n\\n  Args:\\n    video: 5-D NumPy array.\\n    color: string, \"blue\", \"red\" or \"green\".\\n    border_percent: Percentarge of the frame covered by the border.\\n  Returns:\\n    video: 5-D NumPy array.\\n  \"\"\"\\n  # Do not create border if the video is not in RGB format\\n  if video.shape[-1] != 3:\\n    return video\\n  color_to_axis = {\"blue\": 2, \"red\": 0, \"green\": 1}\\n  axis = color_to_axis[color]\\n  _, _, height, width, _ = video.shape\\n  border_height = np.ceil(border_percent * height / 100.0).astype(np.int)\\n  border_width = np.ceil(border_percent * width / 100.0).astype(np.int)\\n  video[:, :, :border_height, :, axis] = 255\\n  video[:, :, -border_height:, :, axis] = 255\\n  video[:, :, :, :border_width, axis] = 255\\n  video[:, :, :, -border_width:, axis] = 255\\n  return video',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'convert_videos_to_summaries',\n",
       "  'docstring': 'Converts input, output and target videos into video summaries.\\n\\n  Args:\\n    input_videos: 5-D NumPy array, (NTHWC) conditioning frames.\\n    output_videos: 5-D NumPy array, (NTHWC) model predictions.\\n    target_videos: 5-D NumPy array, (NTHWC) target frames.\\n    tag: tf summary tag.\\n    decode_hparams: HParams.\\n    display_ground_truth: Whether or not to display ground truth videos.\\n  Returns:\\n    summaries: a list of tf frame-by-frame and video summaries.',\n",
       "  'code': 'def convert_videos_to_summaries(input_videos, output_videos, target_videos,\\n                                tag, decode_hparams,\\n                                display_ground_truth=False):\\n  \"\"\"Converts input, output and target videos into video summaries.\\n\\n  Args:\\n    input_videos: 5-D NumPy array, (NTHWC) conditioning frames.\\n    output_videos: 5-D NumPy array, (NTHWC) model predictions.\\n    target_videos: 5-D NumPy array, (NTHWC) target frames.\\n    tag: tf summary tag.\\n    decode_hparams: HParams.\\n    display_ground_truth: Whether or not to display ground truth videos.\\n  Returns:\\n    summaries: a list of tf frame-by-frame and video summaries.\\n  \"\"\"\\n  fps = decode_hparams.frames_per_second\\n  border_percent = decode_hparams.border_percent\\n  max_outputs = decode_hparams.max_display_outputs\\n  target_steps = target_videos.shape[1]\\n  all_summaries = []\\n  input_videos = create_border(\\n      input_videos, color=\"blue\", border_percent=border_percent)\\n  target_videos = create_border(\\n      target_videos, color=\"red\", border_percent=border_percent)\\n  output_videos = create_border(\\n      output_videos, color=\"red\", border_percent=border_percent)\\n\\n  all_input = np.concatenate((input_videos, target_videos), axis=1)\\n  all_output = np.concatenate((input_videos, output_videos), axis=1)\\n  output_summ_vals, _ = common_video.py_gif_summary(\\n      \"%s/output\" % tag, all_output, max_outputs=max_outputs, fps=fps,\\n      return_summary_value=True)\\n  all_summaries.extend(output_summ_vals)\\n\\n  # Optionally display ground truth.\\n  if display_ground_truth:\\n    input_summ_vals, _ = common_video.py_gif_summary(\\n        \"%s/input\" % tag, all_input, max_outputs=max_outputs, fps=fps,\\n        return_summary_value=True)\\n    all_summaries.extend(input_summ_vals)\\n\\n  # Frame-by-frame summaries\\n  iterable = zip(output_videos[:max_outputs, :target_steps],\\n                 target_videos[:max_outputs])\\n  for ind, (input_video, output_video) in enumerate(iterable):\\n    t, h, w, c = input_video.shape\\n    # Tile vertically\\n    input_frames = np.reshape(input_video, (t*h, w, c))\\n    output_frames = np.reshape(output_video, (t*h, w, c))\\n\\n    # Concat across width.\\n    all_frames = np.concatenate((input_frames, output_frames), axis=1)\\n    tag = \"input/output/%s_sample_%d\" % (tag, ind)\\n    frame_by_frame_summ = image_utils.image_to_tf_summary_value(\\n        all_frames, tag=tag)\\n    all_summaries.append(frame_by_frame_summ)\\n  return all_summaries',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'VideoProblem.generate_encoded_samples',\n",
       "  'docstring': 'Generate samples of the encoded frames with possible extra data.\\n\\n    By default this function just encodes the numpy array returned as \"frame\"\\n    from `self.generate_samples` into a PNG image. Override this function to\\n    get other encodings on disk.\\n\\n    Args:\\n      data_dir: final data directory. Typically only used in this method to copy\\n        over user-supplied vocab files if there are extra fields needing them.\\n      tmp_dir: temporary directory that you can use for downloading and scratch.\\n      dataset_split: problem.DatasetSplit, which data split to generate samples\\n        for (for example, training and evaluation).\\n\\n    Yields:\\n      Sample: dict<str feature_name, feature value> which is in disk encoding.\\n\\n    Raises:\\n      ValueError: if the frame has a different number of channels than required.',\n",
       "  'code': 'def generate_encoded_samples(self, data_dir, tmp_dir, dataset_split):\\n    \"\"\"Generate samples of the encoded frames with possible extra data.\\n\\n    By default this function just encodes the numpy array returned as \"frame\"\\n    from `self.generate_samples` into a PNG image. Override this function to\\n    get other encodings on disk.\\n\\n    Args:\\n      data_dir: final data directory. Typically only used in this method to copy\\n        over user-supplied vocab files if there are extra fields needing them.\\n      tmp_dir: temporary directory that you can use for downloading and scratch.\\n      dataset_split: problem.DatasetSplit, which data split to generate samples\\n        for (for example, training and evaluation).\\n\\n    Yields:\\n      Sample: dict<str feature_name, feature value> which is in disk encoding.\\n\\n    Raises:\\n      ValueError: if the frame has a different number of channels than required.\\n    \"\"\"\\n    writer = None\\n\\n    with tf.Graph().as_default():\\n      image_t = tf.placeholder(dtype=tf.uint8, shape=(None, None, None))\\n      encoded_image_t = tf.image.encode_png(image_t)\\n      with tf.Session() as sess:\\n        for features in self.generate_samples(data_dir, tmp_dir, dataset_split):\\n          unencoded_frame = features.pop(\"frame\")\\n          self.validate_frame(unencoded_frame)\\n          height, width, _ = unencoded_frame.shape\\n          encoded_frame = sess.run(\\n              encoded_image_t, feed_dict={image_t: unencoded_frame})\\n          features[\"image/encoded\"] = [encoded_frame]\\n          features[\"image/format\"] = [\"png\"]\\n          features[\"image/height\"] = [height]\\n          features[\"image/width\"] = [width]\\n\\n          has_debug_image = \"image/debug\" in features\\n          if has_debug_image:\\n            unencoded_debug = features.pop(\"image/debug\")\\n            encoded_debug = sess.run(\\n                encoded_image_t, feed_dict={image_t: unencoded_debug})\\n            features[\"image/encoded_debug\"] = [encoded_debug]\\n\\n          if self.debug_dump_frames_path:\\n            # Defer creating debug writer until we know debug_dump_frames_path.\\n            if writer is None:\\n              if not tf.gfile.Exists(self.debug_dump_frames_path):\\n                tf.gfile.MkDir(self.debug_dump_frames_path)\\n              writer = debug_video_writer_factory(self.debug_dump_frames_path)\\n            img = unencoded_debug if has_debug_image else unencoded_frame\\n            encoded_img = encoded_debug if has_debug_image else encoded_frame\\n            writer.write(img, encoded_img)\\n\\n          yield features\\n\\n    if self.debug_dump_frames_path:\\n      writer.finish_to_disk()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'decode_once',\n",
       "  'docstring': \"Decodes once.\\n\\n  Args:\\n    estimator: tf.estimator.Estimator instance. Used to generate encoded\\n      predictions.\\n    problem_name: str. Name of problem.\\n    hparams: HParams instance. HParams for model training.\\n    infer_input_fn: zero-arg function. Input function for estimator.\\n    decode_hp: HParams instance. See decode_hparams() above.\\n    decode_to_file: str. Prefix for filenames. Used to generated filenames to\\n      which decoded predictions are written.\\n    output_dir: str. Output directory. Only used for writing images.\\n    log_results: bool. If False, return encoded predictions without any\\n      further processing.\\n    checkpoint_path: str. Path to load model checkpoint from. If unspecified,\\n      Estimator's default is used.\\n\\n  Returns:\\n    If decode_hp.decode_in_memory is True:\\n      List of dicts, one per example. Values are either numpy arrays or decoded\\n      strings.\\n    If decode_hp.decode_in_memory is False:\\n      An empty list.\",\n",
       "  'code': 'def decode_once(estimator,\\n                problem_name,\\n                hparams,\\n                infer_input_fn,\\n                decode_hp,\\n                decode_to_file,\\n                output_dir,\\n                log_results=True,\\n                checkpoint_path=None):\\n  \"\"\"Decodes once.\\n\\n  Args:\\n    estimator: tf.estimator.Estimator instance. Used to generate encoded\\n      predictions.\\n    problem_name: str. Name of problem.\\n    hparams: HParams instance. HParams for model training.\\n    infer_input_fn: zero-arg function. Input function for estimator.\\n    decode_hp: HParams instance. See decode_hparams() above.\\n    decode_to_file: str. Prefix for filenames. Used to generated filenames to\\n      which decoded predictions are written.\\n    output_dir: str. Output directory. Only used for writing images.\\n    log_results: bool. If False, return encoded predictions without any\\n      further processing.\\n    checkpoint_path: str. Path to load model checkpoint from. If unspecified,\\n      Estimator\\'s default is used.\\n\\n  Returns:\\n    If decode_hp.decode_in_memory is True:\\n      List of dicts, one per example. Values are either numpy arrays or decoded\\n      strings.\\n    If decode_hp.decode_in_memory is False:\\n      An empty list.\\n  \"\"\"\\n\\n  # Get the predictions as an iterable\\n  predictions = estimator.predict(infer_input_fn,\\n                                  checkpoint_path=checkpoint_path)\\n\\n  if not log_results:\\n    return list(predictions)\\n\\n  # Prepare output file writers if decode_to_file passed\\n  decode_to_file = decode_to_file or decode_hp.decode_to_file\\n  if decode_to_file:\\n    output_filepath = _decode_filename(decode_to_file, problem_name, decode_hp)\\n    parts = output_filepath.split(\".\")\\n    parts[-1] = \"targets\"\\n    target_filepath = \".\".join(parts)\\n    parts[-1] = \"inputs\"\\n    input_filepath = \".\".join(parts)\\n\\n    output_file = tf.gfile.Open(output_filepath, \"w\")\\n    target_file = tf.gfile.Open(target_filepath, \"w\")\\n    input_file = tf.gfile.Open(input_filepath, \"w\")\\n\\n  problem_hparams = hparams.problem_hparams\\n  # Inputs vocabulary is set to targets if there are no inputs in the problem,\\n  # e.g., for language models where the inputs are just a prefix of targets.\\n  has_input = \"inputs\" in problem_hparams.vocabulary\\n  inputs_vocab_key = \"inputs\" if has_input else \"targets\"\\n  inputs_vocab = problem_hparams.vocabulary[inputs_vocab_key]\\n  targets_vocab = problem_hparams.vocabulary[\"targets\"]\\n\\n  num_eval_samples = 0\\n\\n  # all_outputs[i][j] = (input: str, output: str, target: str). Input,\\n  # decoded output, and target strings for example i, beam rank j.\\n  all_outputs = []\\n  for num_predictions, prediction in enumerate(predictions):\\n    num_eval_samples += 1\\n    num_predictions += 1\\n    inputs = prediction.get(\"inputs\")\\n    targets = prediction.get(\"targets\")\\n    outputs = prediction.get(\"outputs\")\\n\\n    # Log predictions\\n    decoded_outputs = []  # [(str, str, str)]. See all_outputs above.\\n    if decode_hp.decode_in_memory:\\n      all_outputs.append(decoded_outputs)\\n    decoded_scores = []\\n\\n    if decode_hp.return_beams:\\n      output_beams = np.split(outputs, decode_hp.beam_size, axis=0)\\n      scores = None\\n      if \"scores\" in prediction:\\n        scores = np.split(prediction[\"scores\"], decode_hp.beam_size, axis=0)\\n      for i, beam in enumerate(output_beams):\\n        tf.logging.info(\"BEAM %d:\" % i)\\n        score = scores and scores[i]\\n        decoded = log_decode_results(\\n            inputs,\\n            beam,\\n            problem_name,\\n            num_predictions,\\n            inputs_vocab,\\n            targets_vocab,\\n            save_images=decode_hp.save_images,\\n            output_dir=output_dir,\\n            identity_output=decode_hp.identity_output,\\n            targets=targets,\\n            log_results=log_results)\\n        decoded_outputs.append(decoded)\\n        if decode_hp.write_beam_scores:\\n          decoded_scores.append(score)\\n    else:\\n      decoded = log_decode_results(\\n          inputs,\\n          outputs,\\n          problem_name,\\n          num_predictions,\\n          inputs_vocab,\\n          targets_vocab,\\n          save_images=decode_hp.save_images,\\n          output_dir=output_dir,\\n          identity_output=decode_hp.identity_output,\\n          targets=targets,\\n          log_results=log_results,\\n          skip_eos_postprocess=decode_hp.skip_eos_postprocess)\\n      decoded_outputs.append(decoded)\\n\\n    # Write out predictions if decode_to_file passed\\n    if decode_to_file:\\n      for i, (d_input, d_output, d_target) in enumerate(decoded_outputs):\\n        # Skip if all padding\\n        if d_input and re.match(\"^({})+$\".format(text_encoder.PAD), d_input):\\n          continue\\n        beam_score_str = \"\"\\n        if decode_hp.write_beam_scores:\\n          beam_score_str = \"\\\\t%.2f\" % decoded_scores[i]\\n        output_file.write(str(d_output) + beam_score_str + decode_hp.delimiter)\\n        target_file.write(str(d_target) + decode_hp.delimiter)\\n        input_file.write(str(d_input) + decode_hp.delimiter)\\n\\n    if (decode_hp.num_samples >= 0 and\\n        num_predictions >= decode_hp.num_samples):\\n      break\\n\\n  mlperf_log.transformer_print(key=mlperf_log.EVAL_SIZE,\\n                               value=num_eval_samples,\\n                               hparams=hparams)\\n\\n  if decode_to_file:\\n    output_file.close()\\n    target_file.close()\\n    input_file.close()\\n\\n  return all_outputs',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_interactive_input_fn',\n",
       "  'docstring': 'Generator that reads from the terminal and yields \"interactive inputs\".\\n\\n  Due to temporary limitations in tf.learn, if we don\\'t want to reload the\\n  whole graph, then we are stuck encoding all of the input as one fixed-size\\n  numpy array.\\n\\n  We yield int32 arrays with shape [const_array_size].  The format is:\\n  [num_samples, decode_length, len(input ids), <input ids>, <padding>]\\n\\n  Args:\\n    hparams: model hparams\\n    decode_hp: decode hparams\\n  Yields:\\n    numpy arrays\\n\\n  Raises:\\n    Exception: when `input_type` is invalid.',\n",
       "  'code': 'def _interactive_input_fn(hparams, decode_hp):\\n  \"\"\"Generator that reads from the terminal and yields \"interactive inputs\".\\n\\n  Due to temporary limitations in tf.learn, if we don\\'t want to reload the\\n  whole graph, then we are stuck encoding all of the input as one fixed-size\\n  numpy array.\\n\\n  We yield int32 arrays with shape [const_array_size].  The format is:\\n  [num_samples, decode_length, len(input ids), <input ids>, <padding>]\\n\\n  Args:\\n    hparams: model hparams\\n    decode_hp: decode hparams\\n  Yields:\\n    numpy arrays\\n\\n  Raises:\\n    Exception: when `input_type` is invalid.\\n  \"\"\"\\n  num_samples = decode_hp.num_samples if decode_hp.num_samples > 0 else 1\\n  decode_length = decode_hp.extra_length\\n  input_type = \"text\"\\n  p_hparams = hparams.problem_hparams\\n  has_input = \"inputs\" in p_hparams.modality\\n  vocabulary = p_hparams.vocabulary[\"inputs\" if has_input else \"targets\"]\\n  # This should be longer than the longest input.\\n  const_array_size = 10000\\n  # Import readline if available for command line editing and recall.\\n  try:\\n    import readline  # pylint: disable=g-import-not-at-top,unused-variable\\n  except ImportError:\\n    pass\\n  while True:\\n    prompt = (\"INTERACTIVE MODE  num_samples=%d  decode_length=%d  \\\\n\"\\n              \"  it=<input_type>     (\\'text\\' or \\'image\\' or \\'label\\', default: \"\\n              \"text)\\\\n\"\\n              \"  ns=<num_samples>    (changes number of samples, default: 1)\\\\n\"\\n              \"  dl=<decode_length>  (changes decode length, default: 100)\\\\n\"\\n              \"  <%s>                (decode)\\\\n\"\\n              \"  q                   (quit)\\\\n\"\\n              \">\" % (num_samples, decode_length,\\n                     \"source_string\" if has_input else \"target_prefix\"))\\n    input_string = input(prompt)\\n    if input_string == \"q\":\\n      return\\n    elif input_string[:3] == \"ns=\":\\n      num_samples = int(input_string[3:])\\n    elif input_string[:3] == \"dl=\":\\n      decode_length = int(input_string[3:])\\n    elif input_string[:3] == \"it=\":\\n      input_type = input_string[3:]\\n    else:\\n      if input_type == \"text\":\\n        input_ids = vocabulary.encode(input_string)\\n        if has_input:\\n          input_ids.append(text_encoder.EOS_ID)\\n        x = [num_samples, decode_length, len(input_ids)] + input_ids\\n        assert len(x) < const_array_size\\n        x += [0] * (const_array_size - len(x))\\n        features = {\\n            \"inputs\": np.array(x).astype(np.int32),\\n        }\\n      elif input_type == \"image\":\\n        input_path = input_string\\n        img = vocabulary.encode(input_path)\\n        features = {\\n            \"inputs\": img.astype(np.int32),\\n        }\\n      elif input_type == \"label\":\\n        input_ids = [int(input_string)]\\n        x = [num_samples, decode_length, len(input_ids)] + input_ids\\n        features = {\\n            \"inputs\": np.array(x).astype(np.int32),\\n        }\\n      else:\\n        raise Exception(\"Unsupported input type.\")\\n      for k, v in six.iteritems(\\n          problem_lib.problem_hparams_to_features(p_hparams)):\\n        features[k] = np.array(v).astype(np.int32)\\n      yield features',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'RenderedEnvProblem._generate_time_steps',\n",
       "  'docstring': 'Transforms time step observations to frames of a video.',\n",
       "  'code': 'def _generate_time_steps(self, trajectory_list):\\n    \"\"\"Transforms time step observations to frames of a video.\"\"\"\\n    for time_step in env_problem.EnvProblem._generate_time_steps(\\n        self, trajectory_list):\\n      # Convert the rendered observations from numpy to png format.\\n      frame_np = np.array(time_step.pop(env_problem.OBSERVATION_FIELD))\\n      frame_np = frame_np.reshape(\\n          [self.frame_height, self.frame_width, self.num_channels])\\n      # TODO(msaffar) Add support for non RGB rendered environments\\n      frame = png.from_array(frame_np, \"RGB\", info={\"bitdepth\": 8})\\n      frame_buffer = six.BytesIO()\\n      frame.save(frame_buffer)\\n\\n      # Put the encoded frame back.\\n      time_step[_IMAGE_ENCODED_FIELD] = [frame_buffer.getvalue()]\\n      time_step[_IMAGE_FORMAT_FIELD] = [_FORMAT]\\n      time_step[_IMAGE_HEIGHT_FIELD] = [self.frame_height]\\n      time_step[_IMAGE_WIDTH_FIELD] = [self.frame_width]\\n\\n      # Add the frame number\\n      time_step[_FRAME_NUMBER_FIELD] = time_step[env_problem.TIMESTEP_FIELD]\\n      yield time_step',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_positional_encoding_new_params',\n",
       "  'docstring': 'Helper: create positional encoding parameters.',\n",
       "  'code': 'def _positional_encoding_new_params(input_shape, rng, max_len=2048):  # pylint: disable=invalid-name\\n  \"\"\"Helper: create positional encoding parameters.\"\"\"\\n  del rng\\n  # Check if we are operating on chunked inputs by checking if the first\\n  # shape is a list/tuple of shapes (otherwise it\\'s an int or numpy array).\\n  is_chunked = isinstance(input_shape[0], (list, tuple))\\n  feature_depth = input_shape[0][-1] if is_chunked else input_shape[-1]\\n  pe = onp.zeros((max_len, feature_depth), dtype=onp.float32)\\n  position = onp.arange(0, max_len)[:, onp.newaxis]\\n  div_term = onp.exp(\\n      onp.arange(0, feature_depth, 2) * -(onp.log(10000.0) / feature_depth))\\n  pe[:, 0::2] = onp.sin(position * div_term)\\n  pe[:, 1::2] = onp.cos(position * div_term)\\n  pe = pe[onp.newaxis, :, :]  # [1, max_len, feature_depth]\\n  return np.array(pe)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_top_2_gating',\n",
       "  'docstring': 'Compute gating for mixture-of-experts in TensorFlow.\\n\\n  Note: until the algorithm and inferface solidify, we pass in a hyperparameters\\n  dictionary in order not to complicate the interface in mtf_transformer.py .\\n  Once this code moves out of \"research\", we should pass the hyperparameters\\n  separately.\\n\\n  Hyperparameters used:\\n    hparams.moe_use_second_place_loss: a boolean\\n    hparams.moe_second_policy_train: a string\\n    hparams.moe_second_policy_eval: a string\\n    hparams.moe_second_threshold: a float\\n\\n  The returned forward assignment is a tensor used to map (via einsum) from the\\n  inputs to the expert_inputs.  Likewise, the returned combine_tensor is\\n  used to map (via einsum) from the expert outputs to the outputs.  Both the\\n  forward and backward assignments are mostly zeros.  The shapes of the tensors\\n  are as follows.\\n\\n  inputs: [<batch_dims>, group_size_dim, input_dim]\\n  importance: [<batch_dims>, group_size_dim]\\n  dispatch_tensor:\\n    [<batch_dims>, group_size_dim, experts_dim, expert_capacity_dim]\\n  expert_inputs:\\n    [<batch_dims>, experts_dim, expert_capacity_dim, input_dim]\\n\\n  expert_outputs: [<batch_dims>, experts_dim, expert_capacity_dim, output_dim]\\n  combine_tensor:\\n    [<batch_dims>, group_size_dim, experts_dim, expert_capacity_dim]\\n  outputs: [<batch_dims>, group_size_dim, output_dim]\\n\\n  \"importance\" is an optional tensor with one floating-point value for each\\n  input vector.  If the importance of an input is 1.0, then we send it to\\n  up to 2 experts.  If 0.0 < importance < 1.0, then we send it to at most\\n  one expert.  If importance == 0.0, then we send it to no experts.\\n\\n  We use \"importance\" at the second-level gating function of a hierarchical\\n  mixture of experts.  Inputs to the first-choice expert-group get importance\\n  1.0.  Inputs to the second-choice expert group get importance 0.5.\\n  Inputs that represent padding get importance 0.0.\\n\\n  Args:\\n    inputs: a mtf.Tensor with shape [<batch_dims>, group_size_dim, input_dim]\\n    outer_expert_dims: an optional list of dimensions.  This is for the case\\n      where we are at an inner level of a hierarchical MoE.\\n    experts_dim: a Dimension (the number of experts)\\n    expert_capacity_dim: a Dimension (number of examples per group per expert)\\n    hparams: model hyperparameters.\\n    train: a boolean\\n    importance: an optional tensor with shape [<batch_dims>, group_size_dim]\\n\\n  Returns:\\n    dispatch_tensor: a Tensor with shape\\n      [<batch_dims>, group_size_dim, experts_dim, expert_capacity_dim]\\n    combine_tensor: a Tensor with shape\\n      [<batch_dims>, group_size_dim, experts_dim, expert_capacity_dim]\\n    loss: a mtf scalar\\n\\n  Raises:\\n    ValueError: on illegal hyperparameters',\n",
       "  'code': 'def _top_2_gating(\\n    inputs, outer_expert_dims, experts_dim, expert_capacity_dim,\\n    hparams, train, importance=None):\\n  \"\"\"Compute gating for mixture-of-experts in TensorFlow.\\n\\n  Note: until the algorithm and inferface solidify, we pass in a hyperparameters\\n  dictionary in order not to complicate the interface in mtf_transformer.py .\\n  Once this code moves out of \"research\", we should pass the hyperparameters\\n  separately.\\n\\n  Hyperparameters used:\\n    hparams.moe_use_second_place_loss: a boolean\\n    hparams.moe_second_policy_train: a string\\n    hparams.moe_second_policy_eval: a string\\n    hparams.moe_second_threshold: a float\\n\\n  The returned forward assignment is a tensor used to map (via einsum) from the\\n  inputs to the expert_inputs.  Likewise, the returned combine_tensor is\\n  used to map (via einsum) from the expert outputs to the outputs.  Both the\\n  forward and backward assignments are mostly zeros.  The shapes of the tensors\\n  are as follows.\\n\\n  inputs: [<batch_dims>, group_size_dim, input_dim]\\n  importance: [<batch_dims>, group_size_dim]\\n  dispatch_tensor:\\n    [<batch_dims>, group_size_dim, experts_dim, expert_capacity_dim]\\n  expert_inputs:\\n    [<batch_dims>, experts_dim, expert_capacity_dim, input_dim]\\n\\n  expert_outputs: [<batch_dims>, experts_dim, expert_capacity_dim, output_dim]\\n  combine_tensor:\\n    [<batch_dims>, group_size_dim, experts_dim, expert_capacity_dim]\\n  outputs: [<batch_dims>, group_size_dim, output_dim]\\n\\n  \"importance\" is an optional tensor with one floating-point value for each\\n  input vector.  If the importance of an input is 1.0, then we send it to\\n  up to 2 experts.  If 0.0 < importance < 1.0, then we send it to at most\\n  one expert.  If importance == 0.0, then we send it to no experts.\\n\\n  We use \"importance\" at the second-level gating function of a hierarchical\\n  mixture of experts.  Inputs to the first-choice expert-group get importance\\n  1.0.  Inputs to the second-choice expert group get importance 0.5.\\n  Inputs that represent padding get importance 0.0.\\n\\n  Args:\\n    inputs: a mtf.Tensor with shape [<batch_dims>, group_size_dim, input_dim]\\n    outer_expert_dims: an optional list of dimensions.  This is for the case\\n      where we are at an inner level of a hierarchical MoE.\\n    experts_dim: a Dimension (the number of experts)\\n    expert_capacity_dim: a Dimension (number of examples per group per expert)\\n    hparams: model hyperparameters.\\n    train: a boolean\\n    importance: an optional tensor with shape [<batch_dims>, group_size_dim]\\n\\n  Returns:\\n    dispatch_tensor: a Tensor with shape\\n      [<batch_dims>, group_size_dim, experts_dim, expert_capacity_dim]\\n    combine_tensor: a Tensor with shape\\n      [<batch_dims>, group_size_dim, experts_dim, expert_capacity_dim]\\n    loss: a mtf scalar\\n\\n  Raises:\\n    ValueError: on illegal hyperparameters\\n  \"\"\"\\n  group_size_dim, unused_input_dim = inputs.shape.dims[-2:]\\n\\n  raw_gates = mtf.softmax(mtf.layers.dense(\\n      inputs, experts_dim, use_bias=False,\\n      expert_dims=outer_expert_dims), experts_dim)\\n\\n  # The internals of this function run in float32.\\n  #   bfloat16 seems to reduce quality.\\n  raw_gates = mtf.to_float(raw_gates)\\n\\n  expert_capacity_f = float(expert_capacity_dim.size)\\n\\n  # FIND TOP 2 EXPERTS PER POSITON\\n  # Find the top expert for each position. shape=[batch, group]\\n  index_1, gate_1 = mtf.top_1(raw_gates, experts_dim)\\n  # [batch, group, experts]\\n  mask_1 = mtf.one_hot(index_1, experts_dim, dtype=raw_gates.dtype)\\n  density_1_proxy = raw_gates\\n  if importance is not None:\\n    mask_1 *= mtf.to_float(mtf.equal(importance, 1.0))\\n    gate_1 *= mtf.to_float(mtf.equal(importance, 1.0))\\n    density_1_proxy *= mtf.to_float(mtf.equal(importance, 1.0))\\n  gates_without_top_1 = raw_gates * (1.0 - mask_1)\\n  # [batch, group]\\n  index_2, gate_2 = mtf.top_1(gates_without_top_1, experts_dim)\\n  # [batch, group, experts]\\n  mask_2 = mtf.one_hot(index_2, experts_dim, dtype=raw_gates.dtype)\\n  if importance is not None:\\n    mask_2 *= mtf.to_float(mtf.greater(importance, 0.0))\\n\\n  denom = gate_1 + gate_2 + 1e-9\\n  gate_1 /= denom\\n  gate_2 /= denom\\n\\n  # BALANCING LOSSES\\n  # shape = [batch, experts]\\n  # We want to equalize the fraction of the batch assigned to each expert\\n  density_1 = mtf.reduce_mean(mask_1, reduced_dim=group_size_dim)\\n  # Something continuous that is correlated with what we want to equalize.\\n  density_1_proxy = mtf.reduce_mean(density_1_proxy, reduced_dim=group_size_dim)\\n  density_1 = mtf.Print(\\n      density_1, [mtf.reduce_mean(density_1, output_shape=[experts_dim])],\\n      \"density_1\", summarize=1000)\\n  loss = (mtf.reduce_mean(density_1_proxy * density_1)\\n          * float(experts_dim.size * experts_dim.size))\\n\\n  if hparams.moe_use_second_place_loss:\\n    # Also add a loss to encourage all experts to be used equally also as the\\n    # second-place expert.  Experimentally, this seems to be a wash.\\n    # We want to equalize the fraction of the batch assigned to each expert:\\n    density_2 = mtf.reduce_mean(mask_2, reduced_dim=group_size_dim)\\n    # As a proxy for density_2, we renormalize the raw gates after the top one\\n    # has been removed.\\n    normalized = gates_without_top_1 / (\\n        mtf.reduce_sum(gates_without_top_1, reduced_dim=experts_dim) + 1e-9)\\n    density_2_proxy = mtf.reduce_mean(normalized, reduced_dim=group_size_dim)\\n    loss_2 = (mtf.reduce_mean(density_2_proxy * density_2)\\n              * float(experts_dim.size * experts_dim.size))\\n    loss += loss_2 * 0.5\\n\\n  # Depending on the policy in the hparams, we may drop out some of the\\n  # second-place experts.\\n  policy = (\\n      hparams.moe_second_policy_train if train else\\n      hparams.moe_second_policy_eval)\\n  threshold = (\\n      hparams.moe_second_threshold_train if train else\\n      hparams.moe_second_threshold_eval)\\n  if policy == \"all\":\\n    # Use second-place experts for all examples.\\n    pass\\n  elif policy == \"none\":\\n    # Never use second-place experts for all examples.\\n    mask_2 = mtf.zeros_like(mask_2)\\n  elif policy == \"threshold\":\\n    # Use second-place experts if gate_2 > threshold.\\n    mask_2 *= mtf.to_float(mtf.greater(gate_2, threshold))\\n  elif policy == \"random\":\\n    # Use second-place experts with probablity min(1.0, gate_2 / threshold).\\n    mask_2 *= mtf.to_float(\\n        mtf.less(mtf.random_uniform(gate_2.mesh, gate_2.shape),\\n                 gate_2 / max(threshold, 1e-9)))\\n  else:\\n    raise ValueError(\"Unknown policy %s\" % policy)\\n  mask_2 = mtf.Print(\\n      mask_2, [mtf.reduce_mean(mask_2, output_shape=[experts_dim])],\\n      \"density_2\", summarize=1000)\\n\\n  # COMPUTE ASSIGNMENT TO EXPERTS\\n  # [batch, group, experts]\\n  # This is the position within the expert\\'s mini-batch for this sequence\\n  position_in_expert_1 = mtf.cumsum(\\n      mask_1, group_size_dim, exclusive=True) * mask_1\\n  # Remove the elements that don\\'t fit. [batch, group, experts]\\n  mask_1 *= mtf.to_float(mtf.less(position_in_expert_1, expert_capacity_f))\\n  # [batch, experts]\\n  # How many examples in this sequence go to this expert\\n  mask_1_count = mtf.reduce_sum(mask_1, reduced_dim=group_size_dim)\\n  # [batch, group] - mostly ones, but zeros where something didn\\'t fit\\n  mask_1_flat = mtf.reduce_sum(mask_1, reduced_dim=experts_dim)\\n  # [batch, group]\\n  position_in_expert_1 = mtf.reduce_sum(\\n      position_in_expert_1, reduced_dim=experts_dim)\\n  # Weight assigned to first expert.  [batch, group]\\n  gate_1 *= mask_1_flat\\n\\n  # [batch, group, experts]\\n  position_in_expert_2 = (\\n      mtf.cumsum(mask_2, group_size_dim, exclusive=True) + mask_1_count)\\n  position_in_expert_2 *= mask_2\\n  mask_2 *= mtf.to_float(mtf.less(position_in_expert_2, expert_capacity_f))\\n  # mask_2_count = mtf.reduce_sum(mask_2, reduced_dim=experts_dim)\\n  mask_2_flat = mtf.reduce_sum(mask_2, reduced_dim=experts_dim)\\n  gate_2 *= mask_2_flat\\n  position_in_expert_2 = mtf.reduce_sum(\\n      position_in_expert_2, reduced_dim=experts_dim)\\n\\n  # [batch, group, experts, expert_capacity]\\n  combine_tensor = (\\n      gate_1 * mask_1_flat\\n      * mtf.one_hot(index_1, experts_dim)\\n      * mtf.one_hot(mtf.to_int32(position_in_expert_1), expert_capacity_dim) +\\n      gate_2 * mask_2_flat\\n      * mtf.one_hot(index_2, experts_dim)\\n      * mtf.one_hot(mtf.to_int32(position_in_expert_2), expert_capacity_dim))\\n\\n  combine_tensor = mtf.cast(combine_tensor, inputs.dtype)\\n  loss = mtf.cast(loss, inputs.dtype)\\n\\n  dispatch_tensor = mtf.cast(\\n      mtf.cast(combine_tensor, tf.bool), combine_tensor.dtype)\\n\\n  return dispatch_tensor, combine_tensor, loss',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'list_to_1d_numpy',\n",
       "  'docstring': 'Convert data to 1-D numpy array.',\n",
       "  'code': 'def list_to_1d_numpy(data, dtype=np.float32, name=\\'list\\'):\\n    \"\"\"Convert data to 1-D numpy array.\"\"\"\\n    if is_numpy_1d_array(data):\\n        if data.dtype == dtype:\\n            return data\\n        else:\\n            return data.astype(dtype=dtype, copy=False)\\n    elif is_1d_list(data):\\n        return np.array(data, dtype=dtype, copy=False)\\n    elif isinstance(data, Series):\\n        return data.values.astype(dtype)\\n    else:\\n        raise TypeError(\"Wrong type({0}) for {1}.\\\\n\"\\n                        \"It should be list, numpy 1-D array or pandas Series\".format(type(data).__name__, name))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'cfloat32_array_to_numpy',\n",
       "  'docstring': 'Convert a ctypes float pointer array to a numpy array.',\n",
       "  'code': 'def cfloat32_array_to_numpy(cptr, length):\\n    \"\"\"Convert a ctypes float pointer array to a numpy array.\"\"\"\\n    if isinstance(cptr, ctypes.POINTER(ctypes.c_float)):\\n        return np.fromiter(cptr, dtype=np.float32, count=length)\\n    else:\\n        raise RuntimeError(\\'Expected float pointer\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'cfloat64_array_to_numpy',\n",
       "  'docstring': 'Convert a ctypes double pointer array to a numpy array.',\n",
       "  'code': 'def cfloat64_array_to_numpy(cptr, length):\\n    \"\"\"Convert a ctypes double pointer array to a numpy array.\"\"\"\\n    if isinstance(cptr, ctypes.POINTER(ctypes.c_double)):\\n        return np.fromiter(cptr, dtype=np.float64, count=length)\\n    else:\\n        raise RuntimeError(\\'Expected double pointer\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'cint32_array_to_numpy',\n",
       "  'docstring': 'Convert a ctypes int pointer array to a numpy array.',\n",
       "  'code': 'def cint32_array_to_numpy(cptr, length):\\n    \"\"\"Convert a ctypes int pointer array to a numpy array.\"\"\"\\n    if isinstance(cptr, ctypes.POINTER(ctypes.c_int32)):\\n        return np.fromiter(cptr, dtype=np.int32, count=length)\\n    else:\\n        raise RuntimeError(\\'Expected int pointer\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'cint8_array_to_numpy',\n",
       "  'docstring': 'Convert a ctypes int pointer array to a numpy array.',\n",
       "  'code': 'def cint8_array_to_numpy(cptr, length):\\n    \"\"\"Convert a ctypes int pointer array to a numpy array.\"\"\"\\n    if isinstance(cptr, ctypes.POINTER(ctypes.c_int8)):\\n        return np.fromiter(cptr, dtype=np.int8, count=length)\\n    else:\\n        raise RuntimeError(\\'Expected int pointer\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'param_dict_to_str',\n",
       "  'docstring': 'Convert Python dictionary to string, which is passed to C API.',\n",
       "  'code': 'def param_dict_to_str(data):\\n    \"\"\"Convert Python dictionary to string, which is passed to C API.\"\"\"\\n    if data is None or not data:\\n        return \"\"\\n    pairs = []\\n    for key, val in data.items():\\n        if isinstance(val, (list, tuple, set)) or is_numpy_1d_array(val):\\n            pairs.append(str(key) + \\'=\\' + \\',\\'.join(map(str, val)))\\n        elif isinstance(val, string_type) or isinstance(val, numeric_types) or is_numeric(val):\\n            pairs.append(str(key) + \\'=\\' + str(val))\\n        elif val is not None:\\n            raise TypeError(\\'Unknown type of parameter:%s, got:%s\\'\\n                            % (key, type(val).__name__))\\n    return \\' \\'.join(pairs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'c_float_array',\n",
       "  'docstring': 'Get pointer of float numpy array / list.',\n",
       "  'code': 'def c_float_array(data):\\n    \"\"\"Get pointer of float numpy array / list.\"\"\"\\n    if is_1d_list(data):\\n        data = np.array(data, copy=False)\\n    if is_numpy_1d_array(data):\\n        data = convert_from_sliced_object(data)\\n        assert data.flags.c_contiguous\\n        if data.dtype == np.float32:\\n            ptr_data = data.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\\n            type_data = C_API_DTYPE_FLOAT32\\n        elif data.dtype == np.float64:\\n            ptr_data = data.ctypes.data_as(ctypes.POINTER(ctypes.c_double))\\n            type_data = C_API_DTYPE_FLOAT64\\n        else:\\n            raise TypeError(\"Expected np.float32 or np.float64, met type({})\"\\n                            .format(data.dtype))\\n    else:\\n        raise TypeError(\"Unknown type({})\".format(type(data).__name__))\\n    return (ptr_data, type_data, data)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'c_int_array',\n",
       "  'docstring': 'Get pointer of int numpy array / list.',\n",
       "  'code': 'def c_int_array(data):\\n    \"\"\"Get pointer of int numpy array / list.\"\"\"\\n    if is_1d_list(data):\\n        data = np.array(data, copy=False)\\n    if is_numpy_1d_array(data):\\n        data = convert_from_sliced_object(data)\\n        assert data.flags.c_contiguous\\n        if data.dtype == np.int32:\\n            ptr_data = data.ctypes.data_as(ctypes.POINTER(ctypes.c_int32))\\n            type_data = C_API_DTYPE_INT32\\n        elif data.dtype == np.int64:\\n            ptr_data = data.ctypes.data_as(ctypes.POINTER(ctypes.c_int64))\\n            type_data = C_API_DTYPE_INT64\\n        else:\\n            raise TypeError(\"Expected np.int32 or np.int64, met type({})\"\\n                            .format(data.dtype))\\n    else:\\n        raise TypeError(\"Unknown type({})\".format(type(data).__name__))\\n    return (ptr_data, type_data, data)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_InnerPredictor.predict',\n",
       "  'docstring': \"Predict logic.\\n\\n        Parameters\\n        ----------\\n        data : string, numpy array, pandas DataFrame, H2O DataTable's Frame or scipy.sparse\\n            Data source for prediction.\\n            When data type is string, it represents the path of txt file.\\n        num_iteration : int, optional (default=-1)\\n            Iteration used for prediction.\\n        raw_score : bool, optional (default=False)\\n            Whether to predict raw scores.\\n        pred_leaf : bool, optional (default=False)\\n            Whether to predict leaf index.\\n        pred_contrib : bool, optional (default=False)\\n            Whether to predict feature contributions.\\n        data_has_header : bool, optional (default=False)\\n            Whether data has header.\\n            Used only for txt data.\\n        is_reshape : bool, optional (default=True)\\n            Whether to reshape to (nrow, ncol).\\n\\n        Returns\\n        -------\\n        result : numpy array\\n            Prediction result.\",\n",
       "  'code': 'def predict(self, data, num_iteration=-1,\\n                raw_score=False, pred_leaf=False, pred_contrib=False, data_has_header=False,\\n                is_reshape=True):\\n        \"\"\"Predict logic.\\n\\n        Parameters\\n        ----------\\n        data : string, numpy array, pandas DataFrame, H2O DataTable\\'s Frame or scipy.sparse\\n            Data source for prediction.\\n            When data type is string, it represents the path of txt file.\\n        num_iteration : int, optional (default=-1)\\n            Iteration used for prediction.\\n        raw_score : bool, optional (default=False)\\n            Whether to predict raw scores.\\n        pred_leaf : bool, optional (default=False)\\n            Whether to predict leaf index.\\n        pred_contrib : bool, optional (default=False)\\n            Whether to predict feature contributions.\\n        data_has_header : bool, optional (default=False)\\n            Whether data has header.\\n            Used only for txt data.\\n        is_reshape : bool, optional (default=True)\\n            Whether to reshape to (nrow, ncol).\\n\\n        Returns\\n        -------\\n        result : numpy array\\n            Prediction result.\\n        \"\"\"\\n        if isinstance(data, Dataset):\\n            raise TypeError(\"Cannot use Dataset instance for prediction, please use raw data instead\")\\n        data = _data_from_pandas(data, None, None, self.pandas_categorical)[0]\\n        predict_type = C_API_PREDICT_NORMAL\\n        if raw_score:\\n            predict_type = C_API_PREDICT_RAW_SCORE\\n        if pred_leaf:\\n            predict_type = C_API_PREDICT_LEAF_INDEX\\n        if pred_contrib:\\n            predict_type = C_API_PREDICT_CONTRIB\\n        int_data_has_header = 1 if data_has_header else 0\\n        if num_iteration > self.num_total_iteration:\\n            num_iteration = self.num_total_iteration\\n\\n        if isinstance(data, string_type):\\n            with _TempFile() as f:\\n                _safe_call(_LIB.LGBM_BoosterPredictForFile(\\n                    self.handle,\\n                    c_str(data),\\n                    ctypes.c_int(int_data_has_header),\\n                    ctypes.c_int(predict_type),\\n                    ctypes.c_int(num_iteration),\\n                    c_str(self.pred_parameter),\\n                    c_str(f.name)))\\n                lines = f.readlines()\\n                nrow = len(lines)\\n                preds = [float(token) for line in lines for token in line.split(\\'\\\\t\\')]\\n                preds = np.array(preds, dtype=np.float64, copy=False)\\n        elif isinstance(data, scipy.sparse.csr_matrix):\\n            preds, nrow = self.__pred_for_csr(data, num_iteration, predict_type)\\n        elif isinstance(data, scipy.sparse.csc_matrix):\\n            preds, nrow = self.__pred_for_csc(data, num_iteration, predict_type)\\n        elif isinstance(data, np.ndarray):\\n            preds, nrow = self.__pred_for_np2d(data, num_iteration, predict_type)\\n        elif isinstance(data, list):\\n            try:\\n                data = np.array(data)\\n            except BaseException:\\n                raise ValueError(\\'Cannot convert data list to numpy array.\\')\\n            preds, nrow = self.__pred_for_np2d(data, num_iteration, predict_type)\\n        elif isinstance(data, DataTable):\\n            preds, nrow = self.__pred_for_np2d(data.to_numpy(), num_iteration, predict_type)\\n        else:\\n            try:\\n                warnings.warn(\\'Converting data to scipy sparse matrix.\\')\\n                csr = scipy.sparse.csr_matrix(data)\\n            except BaseException:\\n                raise TypeError(\\'Cannot predict data for type {}\\'.format(type(data).__name__))\\n            preds, nrow = self.__pred_for_csr(csr, num_iteration, predict_type)\\n        if pred_leaf:\\n            preds = preds.astype(np.int32)\\n        if is_reshape and preds.size != nrow:\\n            if preds.size % nrow == 0:\\n                preds = preds.reshape(nrow, -1)\\n            else:\\n                raise ValueError(\\'Length of predict result (%d) cannot be divide nrow (%d)\\'\\n                                 % (preds.size, nrow))\\n        return preds',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_InnerPredictor.__pred_for_np2d',\n",
       "  'docstring': 'Predict for a 2-D numpy matrix.',\n",
       "  'code': 'def __pred_for_np2d(self, mat, num_iteration, predict_type):\\n        \"\"\"Predict for a 2-D numpy matrix.\"\"\"\\n        if len(mat.shape) != 2:\\n            raise ValueError(\\'Input numpy.ndarray or list must be 2 dimensional\\')\\n\\n        def inner_predict(mat, num_iteration, predict_type, preds=None):\\n            if mat.dtype == np.float32 or mat.dtype == np.float64:\\n                data = np.array(mat.reshape(mat.size), dtype=mat.dtype, copy=False)\\n            else:\\n                \"\"\"change non-float data to float data, need to copy\"\"\"\\n                data = np.array(mat.reshape(mat.size), dtype=np.float32)\\n            ptr_data, type_ptr_data, _ = c_float_array(data)\\n            n_preds = self.__get_num_preds(num_iteration, mat.shape[0], predict_type)\\n            if preds is None:\\n                preds = np.zeros(n_preds, dtype=np.float64)\\n            elif len(preds.shape) != 1 or len(preds) != n_preds:\\n                raise ValueError(\"Wrong length of pre-allocated predict array\")\\n            out_num_preds = ctypes.c_int64(0)\\n            _safe_call(_LIB.LGBM_BoosterPredictForMat(\\n                self.handle,\\n                ptr_data,\\n                ctypes.c_int(type_ptr_data),\\n                ctypes.c_int(mat.shape[0]),\\n                ctypes.c_int(mat.shape[1]),\\n                ctypes.c_int(C_API_IS_ROW_MAJOR),\\n                ctypes.c_int(predict_type),\\n                ctypes.c_int(num_iteration),\\n                c_str(self.pred_parameter),\\n                ctypes.byref(out_num_preds),\\n                preds.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\\n            if n_preds != out_num_preds.value:\\n                raise ValueError(\"Wrong length for predict results\")\\n            return preds, mat.shape[0]\\n\\n        nrow = mat.shape[0]\\n        if nrow > MAX_INT32:\\n            sections = np.arange(start=MAX_INT32, stop=nrow, step=MAX_INT32)\\n            # __get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal\\n            n_preds = [self.__get_num_preds(num_iteration, i, predict_type) for i in np.diff([0] + list(sections) + [nrow])]\\n            n_preds_sections = np.array([0] + n_preds, dtype=np.intp).cumsum()\\n            preds = np.zeros(sum(n_preds), dtype=np.float64)\\n            for chunk, (start_idx_pred, end_idx_pred) in zip_(np.array_split(mat, sections),\\n                                                              zip_(n_preds_sections, n_preds_sections[1:])):\\n                # avoid memory consumption by arrays concatenation operations\\n                inner_predict(chunk, num_iteration, predict_type, preds[start_idx_pred:end_idx_pred])\\n            return preds, nrow\\n        else:\\n            return inner_predict(mat, num_iteration, predict_type)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.__init_from_np2d',\n",
       "  'docstring': 'Initialize data from a 2-D numpy matrix.',\n",
       "  'code': 'def __init_from_np2d(self, mat, params_str, ref_dataset):\\n        \"\"\"Initialize data from a 2-D numpy matrix.\"\"\"\\n        if len(mat.shape) != 2:\\n            raise ValueError(\\'Input numpy.ndarray must be 2 dimensional\\')\\n\\n        self.handle = ctypes.c_void_p()\\n        if mat.dtype == np.float32 or mat.dtype == np.float64:\\n            data = np.array(mat.reshape(mat.size), dtype=mat.dtype, copy=False)\\n        else:\\n            # change non-float data to float data, need to copy\\n            data = np.array(mat.reshape(mat.size), dtype=np.float32)\\n\\n        ptr_data, type_ptr_data, _ = c_float_array(data)\\n        _safe_call(_LIB.LGBM_DatasetCreateFromMat(\\n            ptr_data,\\n            ctypes.c_int(type_ptr_data),\\n            ctypes.c_int(mat.shape[0]),\\n            ctypes.c_int(mat.shape[1]),\\n            ctypes.c_int(C_API_IS_ROW_MAJOR),\\n            c_str(params_str),\\n            ref_dataset,\\n            ctypes.byref(self.handle)))\\n        return self',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.__init_from_list_np2d',\n",
       "  'docstring': 'Initialize data from a list of 2-D numpy matrices.',\n",
       "  'code': 'def __init_from_list_np2d(self, mats, params_str, ref_dataset):\\n        \"\"\"Initialize data from a list of 2-D numpy matrices.\"\"\"\\n        ncol = mats[0].shape[1]\\n        nrow = np.zeros((len(mats),), np.int32)\\n        if mats[0].dtype == np.float64:\\n            ptr_data = (ctypes.POINTER(ctypes.c_double) * len(mats))()\\n        else:\\n            ptr_data = (ctypes.POINTER(ctypes.c_float) * len(mats))()\\n\\n        holders = []\\n        type_ptr_data = None\\n\\n        for i, mat in enumerate(mats):\\n            if len(mat.shape) != 2:\\n                raise ValueError(\\'Input numpy.ndarray must be 2 dimensional\\')\\n\\n            if mat.shape[1] != ncol:\\n                raise ValueError(\\'Input arrays must have same number of columns\\')\\n\\n            nrow[i] = mat.shape[0]\\n\\n            if mat.dtype == np.float32 or mat.dtype == np.float64:\\n                mats[i] = np.array(mat.reshape(mat.size), dtype=mat.dtype, copy=False)\\n            else:\\n                # change non-float data to float data, need to copy\\n                mats[i] = np.array(mat.reshape(mat.size), dtype=np.float32)\\n\\n            chunk_ptr_data, chunk_type_ptr_data, holder = c_float_array(mats[i])\\n            if type_ptr_data is not None and chunk_type_ptr_data != type_ptr_data:\\n                raise ValueError(\\'Input chunks must have same type\\')\\n            ptr_data[i] = chunk_ptr_data\\n            type_ptr_data = chunk_type_ptr_data\\n            holders.append(holder)\\n\\n        self.handle = ctypes.c_void_p()\\n        _safe_call(_LIB.LGBM_DatasetCreateFromMats(\\n            ctypes.c_int(len(mats)),\\n            ctypes.cast(ptr_data, ctypes.POINTER(ctypes.POINTER(ctypes.c_double))),\\n            ctypes.c_int(type_ptr_data),\\n            nrow.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)),\\n            ctypes.c_int(ncol),\\n            ctypes.c_int(C_API_IS_ROW_MAJOR),\\n            c_str(params_str),\\n            ref_dataset,\\n            ctypes.byref(self.handle)))\\n        return self',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.construct',\n",
       "  'docstring': 'Lazy init.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Constructed Dataset object.',\n",
       "  'code': 'def construct(self):\\n        \"\"\"Lazy init.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Constructed Dataset object.\\n        \"\"\"\\n        if self.handle is None:\\n            if self.reference is not None:\\n                if self.used_indices is None:\\n                    # create valid\\n                    self._lazy_init(self.data, label=self.label, reference=self.reference,\\n                                    weight=self.weight, group=self.group,\\n                                    init_score=self.init_score, predictor=self._predictor,\\n                                    silent=self.silent, feature_name=self.feature_name, params=self.params)\\n                else:\\n                    # construct subset\\n                    used_indices = list_to_1d_numpy(self.used_indices, np.int32, name=\\'used_indices\\')\\n                    assert used_indices.flags.c_contiguous\\n                    if self.reference.group is not None:\\n                        group_info = np.array(self.reference.group).astype(int)\\n                        _, self.group = np.unique(np.repeat(range_(len(group_info)), repeats=group_info)[self.used_indices],\\n                                                  return_counts=True)\\n                    self.handle = ctypes.c_void_p()\\n                    params_str = param_dict_to_str(self.params)\\n                    _safe_call(_LIB.LGBM_DatasetGetSubset(\\n                        self.reference.construct().handle,\\n                        used_indices.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)),\\n                        ctypes.c_int(used_indices.shape[0]),\\n                        c_str(params_str),\\n                        ctypes.byref(self.handle)))\\n                    self.data = self.reference.data\\n                    self.get_data()\\n                    if self.group is not None:\\n                        self.set_group(self.group)\\n                    if self.get_label() is None:\\n                        raise ValueError(\"Label should not be None.\")\\n            else:\\n                # create train\\n                self._lazy_init(self.data, label=self.label,\\n                                weight=self.weight, group=self.group,\\n                                init_score=self.init_score, predictor=self._predictor,\\n                                silent=self.silent, feature_name=self.feature_name,\\n                                categorical_feature=self.categorical_feature, params=self.params)\\n            if self.free_raw_data:\\n                self.data = None\\n        return self',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.create_valid',\n",
       "  'docstring': \"Create validation data align with current Dataset.\\n\\n        Parameters\\n        ----------\\n        data : string, numpy array, pandas DataFrame, H2O DataTable's Frame, scipy.sparse or list of numpy arrays\\n            Data source of Dataset.\\n            If string, it represents the path to txt file.\\n        label : list, numpy 1-D array, pandas Series / one-column DataFrame or None, optional (default=None)\\n            Label of the data.\\n        weight : list, numpy 1-D array, pandas Series or None, optional (default=None)\\n            Weight for each instance.\\n        group : list, numpy 1-D array, pandas Series or None, optional (default=None)\\n            Group/query size for Dataset.\\n        init_score : list, numpy 1-D array, pandas Series or None, optional (default=None)\\n            Init score for Dataset.\\n        silent : bool, optional (default=False)\\n            Whether to print messages during construction.\\n        params : dict or None, optional (default=None)\\n            Other parameters for validation Dataset.\\n\\n        Returns\\n        -------\\n        valid : Dataset\\n            Validation Dataset with reference to self.\",\n",
       "  'code': 'def create_valid(self, data, label=None, weight=None, group=None,\\n                     init_score=None, silent=False, params=None):\\n        \"\"\"Create validation data align with current Dataset.\\n\\n        Parameters\\n        ----------\\n        data : string, numpy array, pandas DataFrame, H2O DataTable\\'s Frame, scipy.sparse or list of numpy arrays\\n            Data source of Dataset.\\n            If string, it represents the path to txt file.\\n        label : list, numpy 1-D array, pandas Series / one-column DataFrame or None, optional (default=None)\\n            Label of the data.\\n        weight : list, numpy 1-D array, pandas Series or None, optional (default=None)\\n            Weight for each instance.\\n        group : list, numpy 1-D array, pandas Series or None, optional (default=None)\\n            Group/query size for Dataset.\\n        init_score : list, numpy 1-D array, pandas Series or None, optional (default=None)\\n            Init score for Dataset.\\n        silent : bool, optional (default=False)\\n            Whether to print messages during construction.\\n        params : dict or None, optional (default=None)\\n            Other parameters for validation Dataset.\\n\\n        Returns\\n        -------\\n        valid : Dataset\\n            Validation Dataset with reference to self.\\n        \"\"\"\\n        ret = Dataset(data, label=label, reference=self,\\n                      weight=weight, group=group, init_score=init_score,\\n                      silent=silent, params=params, free_raw_data=self.free_raw_data)\\n        ret._predictor = self._predictor\\n        ret.pandas_categorical = self.pandas_categorical\\n        return ret',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.set_field',\n",
       "  'docstring': 'Set property into the Dataset.\\n\\n        Parameters\\n        ----------\\n        field_name : string\\n            The field name of the information.\\n        data : list, numpy 1-D array, pandas Series or None\\n            The array of data to be set.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Dataset with set property.',\n",
       "  'code': 'def set_field(self, field_name, data):\\n        \"\"\"Set property into the Dataset.\\n\\n        Parameters\\n        ----------\\n        field_name : string\\n            The field name of the information.\\n        data : list, numpy 1-D array, pandas Series or None\\n            The array of data to be set.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Dataset with set property.\\n        \"\"\"\\n        if self.handle is None:\\n            raise Exception(\"Cannot set %s before construct dataset\" % field_name)\\n        if data is None:\\n            # set to None\\n            _safe_call(_LIB.LGBM_DatasetSetField(\\n                self.handle,\\n                c_str(field_name),\\n                None,\\n                ctypes.c_int(0),\\n                ctypes.c_int(FIELD_TYPE_MAPPER[field_name])))\\n            return self\\n        dtype = np.float32\\n        if field_name == \\'group\\':\\n            dtype = np.int32\\n        elif field_name == \\'init_score\\':\\n            dtype = np.float64\\n        data = list_to_1d_numpy(data, dtype, name=field_name)\\n        if data.dtype == np.float32 or data.dtype == np.float64:\\n            ptr_data, type_data, _ = c_float_array(data)\\n        elif data.dtype == np.int32:\\n            ptr_data, type_data, _ = c_int_array(data)\\n        else:\\n            raise TypeError(\"Excepted np.float32/64 or np.int32, meet type({})\".format(data.dtype))\\n        if type_data != FIELD_TYPE_MAPPER[field_name]:\\n            raise TypeError(\"Input type error for set_field\")\\n        _safe_call(_LIB.LGBM_DatasetSetField(\\n            self.handle,\\n            c_str(field_name),\\n            ptr_data,\\n            ctypes.c_int(len(data)),\\n            ctypes.c_int(type_data)))\\n        return self',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.get_field',\n",
       "  'docstring': 'Get property from the Dataset.\\n\\n        Parameters\\n        ----------\\n        field_name : string\\n            The field name of the information.\\n\\n        Returns\\n        -------\\n        info : numpy array\\n            A numpy array with information from the Dataset.',\n",
       "  'code': 'def get_field(self, field_name):\\n        \"\"\"Get property from the Dataset.\\n\\n        Parameters\\n        ----------\\n        field_name : string\\n            The field name of the information.\\n\\n        Returns\\n        -------\\n        info : numpy array\\n            A numpy array with information from the Dataset.\\n        \"\"\"\\n        if self.handle is None:\\n            raise Exception(\"Cannot get %s before construct Dataset\" % field_name)\\n        tmp_out_len = ctypes.c_int()\\n        out_type = ctypes.c_int()\\n        ret = ctypes.POINTER(ctypes.c_void_p)()\\n        _safe_call(_LIB.LGBM_DatasetGetField(\\n            self.handle,\\n            c_str(field_name),\\n            ctypes.byref(tmp_out_len),\\n            ctypes.byref(ret),\\n            ctypes.byref(out_type)))\\n        if out_type.value != FIELD_TYPE_MAPPER[field_name]:\\n            raise TypeError(\"Return type error for get_field\")\\n        if tmp_out_len.value == 0:\\n            return None\\n        if out_type.value == C_API_DTYPE_INT32:\\n            return cint32_array_to_numpy(ctypes.cast(ret, ctypes.POINTER(ctypes.c_int32)), tmp_out_len.value)\\n        elif out_type.value == C_API_DTYPE_FLOAT32:\\n            return cfloat32_array_to_numpy(ctypes.cast(ret, ctypes.POINTER(ctypes.c_float)), tmp_out_len.value)\\n        elif out_type.value == C_API_DTYPE_FLOAT64:\\n            return cfloat64_array_to_numpy(ctypes.cast(ret, ctypes.POINTER(ctypes.c_double)), tmp_out_len.value)\\n        elif out_type.value == C_API_DTYPE_INT8:\\n            return cint8_array_to_numpy(ctypes.cast(ret, ctypes.POINTER(ctypes.c_int8)), tmp_out_len.value)\\n        else:\\n            raise TypeError(\"Unknown type\")',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.set_label',\n",
       "  'docstring': 'Set label of Dataset.\\n\\n        Parameters\\n        ----------\\n        label : list, numpy 1-D array, pandas Series / one-column DataFrame or None\\n            The label information to be set into Dataset.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Dataset with set label.',\n",
       "  'code': 'def set_label(self, label):\\n        \"\"\"Set label of Dataset.\\n\\n        Parameters\\n        ----------\\n        label : list, numpy 1-D array, pandas Series / one-column DataFrame or None\\n            The label information to be set into Dataset.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Dataset with set label.\\n        \"\"\"\\n        self.label = label\\n        if self.handle is not None:\\n            label = list_to_1d_numpy(_label_from_pandas(label), name=\\'label\\')\\n            self.set_field(\\'label\\', label)\\n        return self',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.set_weight',\n",
       "  'docstring': 'Set weight of each instance.\\n\\n        Parameters\\n        ----------\\n        weight : list, numpy 1-D array, pandas Series or None\\n            Weight to be set for each data point.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Dataset with set weight.',\n",
       "  'code': 'def set_weight(self, weight):\\n        \"\"\"Set weight of each instance.\\n\\n        Parameters\\n        ----------\\n        weight : list, numpy 1-D array, pandas Series or None\\n            Weight to be set for each data point.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Dataset with set weight.\\n        \"\"\"\\n        if weight is not None and np.all(weight == 1):\\n            weight = None\\n        self.weight = weight\\n        if self.handle is not None and weight is not None:\\n            weight = list_to_1d_numpy(weight, name=\\'weight\\')\\n            self.set_field(\\'weight\\', weight)\\n        return self',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.set_init_score',\n",
       "  'docstring': 'Set init score of Booster to start from.\\n\\n        Parameters\\n        ----------\\n        init_score : list, numpy 1-D array, pandas Series or None\\n            Init score for Booster.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Dataset with set init score.',\n",
       "  'code': 'def set_init_score(self, init_score):\\n        \"\"\"Set init score of Booster to start from.\\n\\n        Parameters\\n        ----------\\n        init_score : list, numpy 1-D array, pandas Series or None\\n            Init score for Booster.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Dataset with set init score.\\n        \"\"\"\\n        self.init_score = init_score\\n        if self.handle is not None and init_score is not None:\\n            init_score = list_to_1d_numpy(init_score, np.float64, name=\\'init_score\\')\\n            self.set_field(\\'init_score\\', init_score)\\n        return self',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.set_group',\n",
       "  'docstring': 'Set group size of Dataset (used for ranking).\\n\\n        Parameters\\n        ----------\\n        group : list, numpy 1-D array, pandas Series or None\\n            Group size of each group.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Dataset with set group.',\n",
       "  'code': 'def set_group(self, group):\\n        \"\"\"Set group size of Dataset (used for ranking).\\n\\n        Parameters\\n        ----------\\n        group : list, numpy 1-D array, pandas Series or None\\n            Group size of each group.\\n\\n        Returns\\n        -------\\n        self : Dataset\\n            Dataset with set group.\\n        \"\"\"\\n        self.group = group\\n        if self.handle is not None and group is not None:\\n            group = list_to_1d_numpy(group, np.int32, name=\\'group\\')\\n            self.set_field(\\'group\\', group)\\n        return self',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.get_label',\n",
       "  'docstring': 'Get the label of the Dataset.\\n\\n        Returns\\n        -------\\n        label : numpy array or None\\n            The label information from the Dataset.',\n",
       "  'code': 'def get_label(self):\\n        \"\"\"Get the label of the Dataset.\\n\\n        Returns\\n        -------\\n        label : numpy array or None\\n            The label information from the Dataset.\\n        \"\"\"\\n        if self.label is None:\\n            self.label = self.get_field(\\'label\\')\\n        return self.label',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.get_weight',\n",
       "  'docstring': 'Get the weight of the Dataset.\\n\\n        Returns\\n        -------\\n        weight : numpy array or None\\n            Weight for each data point from the Dataset.',\n",
       "  'code': 'def get_weight(self):\\n        \"\"\"Get the weight of the Dataset.\\n\\n        Returns\\n        -------\\n        weight : numpy array or None\\n            Weight for each data point from the Dataset.\\n        \"\"\"\\n        if self.weight is None:\\n            self.weight = self.get_field(\\'weight\\')\\n        return self.weight',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.get_feature_penalty',\n",
       "  'docstring': 'Get the feature penalty of the Dataset.\\n\\n        Returns\\n        -------\\n        feature_penalty : numpy array or None\\n            Feature penalty for each feature in the Dataset.',\n",
       "  'code': 'def get_feature_penalty(self):\\n        \"\"\"Get the feature penalty of the Dataset.\\n\\n        Returns\\n        -------\\n        feature_penalty : numpy array or None\\n            Feature penalty for each feature in the Dataset.\\n        \"\"\"\\n        if self.feature_penalty is None:\\n            self.feature_penalty = self.get_field(\\'feature_penalty\\')\\n        return self.feature_penalty',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.get_monotone_constraints',\n",
       "  'docstring': 'Get the monotone constraints of the Dataset.\\n\\n        Returns\\n        -------\\n        monotone_constraints : numpy array or None\\n            Monotone constraints: -1, 0 or 1, for each feature in the Dataset.',\n",
       "  'code': 'def get_monotone_constraints(self):\\n        \"\"\"Get the monotone constraints of the Dataset.\\n\\n        Returns\\n        -------\\n        monotone_constraints : numpy array or None\\n            Monotone constraints: -1, 0 or 1, for each feature in the Dataset.\\n        \"\"\"\\n        if self.monotone_constraints is None:\\n            self.monotone_constraints = self.get_field(\\'monotone_constraints\\')\\n        return self.monotone_constraints',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.get_init_score',\n",
       "  'docstring': 'Get the initial score of the Dataset.\\n\\n        Returns\\n        -------\\n        init_score : numpy array or None\\n            Init score of Booster.',\n",
       "  'code': 'def get_init_score(self):\\n        \"\"\"Get the initial score of the Dataset.\\n\\n        Returns\\n        -------\\n        init_score : numpy array or None\\n            Init score of Booster.\\n        \"\"\"\\n        if self.init_score is None:\\n            self.init_score = self.get_field(\\'init_score\\')\\n        return self.init_score',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.get_data',\n",
       "  'docstring': \"Get the raw data of the Dataset.\\n\\n        Returns\\n        -------\\n        data : string, numpy array, pandas DataFrame, H2O DataTable's Frame, scipy.sparse, list of numpy arrays or None\\n            Raw data used in the Dataset construction.\",\n",
       "  'code': 'def get_data(self):\\n        \"\"\"Get the raw data of the Dataset.\\n\\n        Returns\\n        -------\\n        data : string, numpy array, pandas DataFrame, H2O DataTable\\'s Frame, scipy.sparse, list of numpy arrays or None\\n            Raw data used in the Dataset construction.\\n        \"\"\"\\n        if self.handle is None:\\n            raise Exception(\"Cannot get data before construct Dataset\")\\n        if self.data is not None and self.used_indices is not None and self.need_slice:\\n            if isinstance(self.data, np.ndarray) or scipy.sparse.issparse(self.data):\\n                self.data = self.data[self.used_indices, :]\\n            elif isinstance(self.data, DataFrame):\\n                self.data = self.data.iloc[self.used_indices].copy()\\n            elif isinstance(self.data, DataTable):\\n                self.data = self.data[self.used_indices, :]\\n            else:\\n                warnings.warn(\"Cannot subset {} type of raw data.\\\\n\"\\n                              \"Returning original raw data\".format(type(self.data).__name__))\\n            self.need_slice = False\\n        return self.data',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Dataset.get_group',\n",
       "  'docstring': 'Get the group of the Dataset.\\n\\n        Returns\\n        -------\\n        group : numpy array or None\\n            Group size of each group.',\n",
       "  'code': 'def get_group(self):\\n        \"\"\"Get the group of the Dataset.\\n\\n        Returns\\n        -------\\n        group : numpy array or None\\n            Group size of each group.\\n        \"\"\"\\n        if self.group is None:\\n            self.group = self.get_field(\\'group\\')\\n            if self.group is not None:\\n                # group data from LightGBM is boundaries data, need to convert to group size\\n                self.group = np.diff(self.group)\\n        return self.group',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Booster.__boost',\n",
       "  'docstring': 'Boost Booster for one iteration with customized gradient statistics.\\n\\n        Note\\n        ----\\n        For multi-class task, the score is group by class_id first, then group by row_id.\\n        If you want to get i-th row score in j-th class, the access way is score[j * num_data + i]\\n        and you should group grad and hess in this way as well.\\n\\n        Parameters\\n        ----------\\n        grad : 1-D numpy array or 1-D list\\n            The first order derivative (gradient).\\n        hess : 1-D numpy array or 1-D list\\n            The second order derivative (Hessian).\\n\\n        Returns\\n        -------\\n        is_finished : bool\\n            Whether the boost was successfully finished.',\n",
       "  'code': 'def __boost(self, grad, hess):\\n        \"\"\"Boost Booster for one iteration with customized gradient statistics.\\n\\n        Note\\n        ----\\n        For multi-class task, the score is group by class_id first, then group by row_id.\\n        If you want to get i-th row score in j-th class, the access way is score[j * num_data + i]\\n        and you should group grad and hess in this way as well.\\n\\n        Parameters\\n        ----------\\n        grad : 1-D numpy array or 1-D list\\n            The first order derivative (gradient).\\n        hess : 1-D numpy array or 1-D list\\n            The second order derivative (Hessian).\\n\\n        Returns\\n        -------\\n        is_finished : bool\\n            Whether the boost was successfully finished.\\n        \"\"\"\\n        grad = list_to_1d_numpy(grad, name=\\'gradient\\')\\n        hess = list_to_1d_numpy(hess, name=\\'hessian\\')\\n        assert grad.flags.c_contiguous\\n        assert hess.flags.c_contiguous\\n        if len(grad) != len(hess):\\n            raise ValueError(\"Lengths of gradient({}) and hessian({}) don\\'t match\"\\n                             .format(len(grad), len(hess)))\\n        is_finished = ctypes.c_int(0)\\n        _safe_call(_LIB.LGBM_BoosterUpdateOneIterCustom(\\n            self.handle,\\n            grad.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\\n            hess.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\\n            ctypes.byref(is_finished)))\\n        self.__is_predicted_cur_iter = [False for _ in range_(self.__num_dataset)]\\n        return is_finished.value == 1',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Booster.dump_model',\n",
       "  'docstring': 'Dump Booster to JSON format.\\n\\n        Parameters\\n        ----------\\n        num_iteration : int or None, optional (default=None)\\n            Index of the iteration that should be dumped.\\n            If None, if the best iteration exists, it is dumped; otherwise, all iterations are dumped.\\n            If <= 0, all iterations are dumped.\\n        start_iteration : int, optional (default=0)\\n            Start index of the iteration that should be dumped.\\n\\n        Returns\\n        -------\\n        json_repr : dict\\n            JSON format of Booster.',\n",
       "  'code': 'def dump_model(self, num_iteration=None, start_iteration=0):\\n        \"\"\"Dump Booster to JSON format.\\n\\n        Parameters\\n        ----------\\n        num_iteration : int or None, optional (default=None)\\n            Index of the iteration that should be dumped.\\n            If None, if the best iteration exists, it is dumped; otherwise, all iterations are dumped.\\n            If <= 0, all iterations are dumped.\\n        start_iteration : int, optional (default=0)\\n            Start index of the iteration that should be dumped.\\n\\n        Returns\\n        -------\\n        json_repr : dict\\n            JSON format of Booster.\\n        \"\"\"\\n        if num_iteration is None:\\n            num_iteration = self.best_iteration\\n        buffer_len = 1 << 20\\n        tmp_out_len = ctypes.c_int64(0)\\n        string_buffer = ctypes.create_string_buffer(buffer_len)\\n        ptr_string_buffer = ctypes.c_char_p(*[ctypes.addressof(string_buffer)])\\n        _safe_call(_LIB.LGBM_BoosterDumpModel(\\n            self.handle,\\n            ctypes.c_int(start_iteration),\\n            ctypes.c_int(num_iteration),\\n            ctypes.c_int64(buffer_len),\\n            ctypes.byref(tmp_out_len),\\n            ptr_string_buffer))\\n        actual_len = tmp_out_len.value\\n        # if buffer length is not long enough, reallocate a buffer\\n        if actual_len > buffer_len:\\n            string_buffer = ctypes.create_string_buffer(actual_len)\\n            ptr_string_buffer = ctypes.c_char_p(*[ctypes.addressof(string_buffer)])\\n            _safe_call(_LIB.LGBM_BoosterDumpModel(\\n                self.handle,\\n                ctypes.c_int(start_iteration),\\n                ctypes.c_int(num_iteration),\\n                ctypes.c_int64(actual_len),\\n                ctypes.byref(tmp_out_len),\\n                ptr_string_buffer))\\n        ret = json.loads(string_buffer.value.decode())\\n        ret[\\'pandas_categorical\\'] = json.loads(json.dumps(self.pandas_categorical,\\n                                                          default=json_default_with_numpy))\\n        return ret',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Booster.predict',\n",
       "  'docstring': \"Make a prediction.\\n\\n        Parameters\\n        ----------\\n        data : string, numpy array, pandas DataFrame, H2O DataTable's Frame or scipy.sparse\\n            Data source for prediction.\\n            If string, it represents the path to txt file.\\n        num_iteration : int or None, optional (default=None)\\n            Limit number of iterations in the prediction.\\n            If None, if the best iteration exists, it is used; otherwise, all iterations are used.\\n            If <= 0, all iterations are used (no limits).\\n        raw_score : bool, optional (default=False)\\n            Whether to predict raw scores.\\n        pred_leaf : bool, optional (default=False)\\n            Whether to predict leaf index.\\n        pred_contrib : bool, optional (default=False)\\n            Whether to predict feature contributions.\\n\\n            Note\\n            ----\\n            If you want to get more explanations for your model's predictions using SHAP values,\\n            like SHAP interaction values,\\n            you can install the shap package (https://github.com/slundberg/shap).\\n            Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\\n            column, where the last column is the expected value.\\n\\n        data_has_header : bool, optional (default=False)\\n            Whether the data has header.\\n            Used only if data is string.\\n        is_reshape : bool, optional (default=True)\\n            If True, result is reshaped to [nrow, ncol].\\n        **kwargs\\n            Other parameters for the prediction.\\n\\n        Returns\\n        -------\\n        result : numpy array\\n            Prediction result.\",\n",
       "  'code': 'def predict(self, data, num_iteration=None,\\n                raw_score=False, pred_leaf=False, pred_contrib=False,\\n                data_has_header=False, is_reshape=True, **kwargs):\\n        \"\"\"Make a prediction.\\n\\n        Parameters\\n        ----------\\n        data : string, numpy array, pandas DataFrame, H2O DataTable\\'s Frame or scipy.sparse\\n            Data source for prediction.\\n            If string, it represents the path to txt file.\\n        num_iteration : int or None, optional (default=None)\\n            Limit number of iterations in the prediction.\\n            If None, if the best iteration exists, it is used; otherwise, all iterations are used.\\n            If <= 0, all iterations are used (no limits).\\n        raw_score : bool, optional (default=False)\\n            Whether to predict raw scores.\\n        pred_leaf : bool, optional (default=False)\\n            Whether to predict leaf index.\\n        pred_contrib : bool, optional (default=False)\\n            Whether to predict feature contributions.\\n\\n            Note\\n            ----\\n            If you want to get more explanations for your model\\'s predictions using SHAP values,\\n            like SHAP interaction values,\\n            you can install the shap package (https://github.com/slundberg/shap).\\n            Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\\n            column, where the last column is the expected value.\\n\\n        data_has_header : bool, optional (default=False)\\n            Whether the data has header.\\n            Used only if data is string.\\n        is_reshape : bool, optional (default=True)\\n            If True, result is reshaped to [nrow, ncol].\\n        **kwargs\\n            Other parameters for the prediction.\\n\\n        Returns\\n        -------\\n        result : numpy array\\n            Prediction result.\\n        \"\"\"\\n        predictor = self._to_predictor(copy.deepcopy(kwargs))\\n        if num_iteration is None:\\n            num_iteration = self.best_iteration\\n        return predictor.predict(data, num_iteration,\\n                                 raw_score, pred_leaf, pred_contrib,\\n                                 data_has_header, is_reshape)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Booster.refit',\n",
       "  'docstring': \"Refit the existing Booster by new data.\\n\\n        Parameters\\n        ----------\\n        data : string, numpy array, pandas DataFrame, H2O DataTable's Frame or scipy.sparse\\n            Data source for refit.\\n            If string, it represents the path to txt file.\\n        label : list, numpy 1-D array or pandas Series / one-column DataFrame\\n            Label for refit.\\n        decay_rate : float, optional (default=0.9)\\n            Decay rate of refit,\\n            will use ``leaf_output = decay_rate * old_leaf_output + (1.0 - decay_rate) * new_leaf_output`` to refit trees.\\n        **kwargs\\n            Other parameters for refit.\\n            These parameters will be passed to ``predict`` method.\\n\\n        Returns\\n        -------\\n        result : Booster\\n            Refitted Booster.\",\n",
       "  'code': 'def refit(self, data, label, decay_rate=0.9, **kwargs):\\n        \"\"\"Refit the existing Booster by new data.\\n\\n        Parameters\\n        ----------\\n        data : string, numpy array, pandas DataFrame, H2O DataTable\\'s Frame or scipy.sparse\\n            Data source for refit.\\n            If string, it represents the path to txt file.\\n        label : list, numpy 1-D array or pandas Series / one-column DataFrame\\n            Label for refit.\\n        decay_rate : float, optional (default=0.9)\\n            Decay rate of refit,\\n            will use ``leaf_output = decay_rate * old_leaf_output + (1.0 - decay_rate) * new_leaf_output`` to refit trees.\\n        **kwargs\\n            Other parameters for refit.\\n            These parameters will be passed to ``predict`` method.\\n\\n        Returns\\n        -------\\n        result : Booster\\n            Refitted Booster.\\n        \"\"\"\\n        if self.__set_objective_to_none:\\n            raise LightGBMError(\\'Cannot refit due to null objective function.\\')\\n        predictor = self._to_predictor(copy.deepcopy(kwargs))\\n        leaf_preds = predictor.predict(data, -1, pred_leaf=True)\\n        nrow, ncol = leaf_preds.shape\\n        train_set = Dataset(data, label, silent=True)\\n        new_booster = Booster(self.params, train_set, silent=True)\\n        # Copy models\\n        _safe_call(_LIB.LGBM_BoosterMerge(\\n            new_booster.handle,\\n            predictor.handle))\\n        leaf_preds = leaf_preds.reshape(-1)\\n        ptr_data, type_ptr_data, _ = c_int_array(leaf_preds)\\n        _safe_call(_LIB.LGBM_BoosterRefit(\\n            new_booster.handle,\\n            ptr_data,\\n            ctypes.c_int(nrow),\\n            ctypes.c_int(ncol)))\\n        new_booster.network = self.network\\n        new_booster.__attr = self.__attr.copy()\\n        return new_booster',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Booster.feature_importance',\n",
       "  'docstring': 'Get feature importances.\\n\\n        Parameters\\n        ----------\\n        importance_type : string, optional (default=\"split\")\\n            How the importance is calculated.\\n            If \"split\", result contains numbers of times the feature is used in a model.\\n            If \"gain\", result contains total gains of splits which use the feature.\\n        iteration : int or None, optional (default=None)\\n            Limit number of iterations in the feature importance calculation.\\n            If None, if the best iteration exists, it is used; otherwise, all trees are used.\\n            If <= 0, all trees are used (no limits).\\n\\n        Returns\\n        -------\\n        result : numpy array\\n            Array with feature importances.',\n",
       "  'code': 'def feature_importance(self, importance_type=\\'split\\', iteration=None):\\n        \"\"\"Get feature importances.\\n\\n        Parameters\\n        ----------\\n        importance_type : string, optional (default=\"split\")\\n            How the importance is calculated.\\n            If \"split\", result contains numbers of times the feature is used in a model.\\n            If \"gain\", result contains total gains of splits which use the feature.\\n        iteration : int or None, optional (default=None)\\n            Limit number of iterations in the feature importance calculation.\\n            If None, if the best iteration exists, it is used; otherwise, all trees are used.\\n            If <= 0, all trees are used (no limits).\\n\\n        Returns\\n        -------\\n        result : numpy array\\n            Array with feature importances.\\n        \"\"\"\\n        if iteration is None:\\n            iteration = self.best_iteration\\n        if importance_type == \"split\":\\n            importance_type_int = 0\\n        elif importance_type == \"gain\":\\n            importance_type_int = 1\\n        else:\\n            importance_type_int = -1\\n        result = np.zeros(self.num_feature(), dtype=np.float64)\\n        _safe_call(_LIB.LGBM_BoosterFeatureImportance(\\n            self.handle,\\n            ctypes.c_int(iteration),\\n            ctypes.c_int(importance_type_int),\\n            result.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\\n        if importance_type_int == 0:\\n            return result.astype(int)\\n        else:\\n            return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Booster.get_split_value_histogram',\n",
       "  'docstring': 'Get split value histogram for the specified feature.\\n\\n        Parameters\\n        ----------\\n        feature : int or string\\n            The feature name or index the histogram is calculated for.\\n            If int, interpreted as index.\\n            If string, interpreted as name.\\n\\n            Note\\n            ----\\n            Categorical features are not supported.\\n\\n        bins : int, string or None, optional (default=None)\\n            The maximum number of bins.\\n            If None, or int and > number of unique split values and ``xgboost_style=True``,\\n            the number of bins equals number of unique split values.\\n            If string, it should be one from the list of the supported values by ``numpy.histogram()`` function.\\n        xgboost_style : bool, optional (default=False)\\n            Whether the returned result should be in the same form as it is in XGBoost.\\n            If False, the returned value is tuple of 2 numpy arrays as it is in ``numpy.histogram()`` function.\\n            If True, the returned value is matrix, in which the first column is the right edges of non-empty bins\\n            and the second one is the histogram values.\\n\\n        Returns\\n        -------\\n        result_tuple : tuple of 2 numpy arrays\\n            If ``xgboost_style=False``, the values of the histogram of used splitting values for the specified feature\\n            and the bin edges.\\n        result_array_like : numpy array or pandas DataFrame (if pandas is installed)\\n            If ``xgboost_style=True``, the histogram of used splitting values for the specified feature.',\n",
       "  'code': 'def get_split_value_histogram(self, feature, bins=None, xgboost_style=False):\\n        \"\"\"Get split value histogram for the specified feature.\\n\\n        Parameters\\n        ----------\\n        feature : int or string\\n            The feature name or index the histogram is calculated for.\\n            If int, interpreted as index.\\n            If string, interpreted as name.\\n\\n            Note\\n            ----\\n            Categorical features are not supported.\\n\\n        bins : int, string or None, optional (default=None)\\n            The maximum number of bins.\\n            If None, or int and > number of unique split values and ``xgboost_style=True``,\\n            the number of bins equals number of unique split values.\\n            If string, it should be one from the list of the supported values by ``numpy.histogram()`` function.\\n        xgboost_style : bool, optional (default=False)\\n            Whether the returned result should be in the same form as it is in XGBoost.\\n            If False, the returned value is tuple of 2 numpy arrays as it is in ``numpy.histogram()`` function.\\n            If True, the returned value is matrix, in which the first column is the right edges of non-empty bins\\n            and the second one is the histogram values.\\n\\n        Returns\\n        -------\\n        result_tuple : tuple of 2 numpy arrays\\n            If ``xgboost_style=False``, the values of the histogram of used splitting values for the specified feature\\n            and the bin edges.\\n        result_array_like : numpy array or pandas DataFrame (if pandas is installed)\\n            If ``xgboost_style=True``, the histogram of used splitting values for the specified feature.\\n        \"\"\"\\n        def add(root):\\n            \"\"\"Recursively add thresholds.\"\"\"\\n            if \\'split_index\\' in root:  # non-leaf\\n                if feature_names is not None and isinstance(feature, string_type):\\n                    split_feature = feature_names[root[\\'split_feature\\']]\\n                else:\\n                    split_feature = root[\\'split_feature\\']\\n                if split_feature == feature:\\n                    if isinstance(root[\\'threshold\\'], string_type):\\n                        raise LightGBMError(\\'Cannot compute split value histogram for the categorical feature\\')\\n                    else:\\n                        values.append(root[\\'threshold\\'])\\n                add(root[\\'left_child\\'])\\n                add(root[\\'right_child\\'])\\n\\n        model = self.dump_model()\\n        feature_names = model.get(\\'feature_names\\')\\n        tree_infos = model[\\'tree_info\\']\\n        values = []\\n        for tree_info in tree_infos:\\n            add(tree_info[\\'tree_structure\\'])\\n\\n        if bins is None or isinstance(bins, integer_types) and xgboost_style:\\n            n_unique = len(np.unique(values))\\n            bins = max(min(n_unique, bins) if bins is not None else n_unique, 1)\\n        hist, bin_edges = np.histogram(values, bins=bins)\\n        if xgboost_style:\\n            ret = np.column_stack((bin_edges[1:], hist))\\n            ret = ret[ret[:, 1] > 0]\\n            if PANDAS_INSTALLED:\\n                return DataFrame(ret, columns=[\\'SplitValue\\', \\'Count\\'])\\n            else:\\n                return ret\\n        else:\\n            return hist, bin_edges',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'json_default_with_numpy',\n",
       "  'docstring': 'Convert numpy classes to JSON serializable objects.',\n",
       "  'code': 'def json_default_with_numpy(obj):\\n    \"\"\"Convert numpy classes to JSON serializable objects.\"\"\"\\n    if isinstance(obj, (np.integer, np.floating, np.bool_)):\\n        return obj.item()\\n    elif isinstance(obj, np.ndarray):\\n        return obj.tolist()\\n    else:\\n        return obj',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'cv',\n",
       "  'docstring': 'Perform the cross-validation with given paramaters.\\n\\n    Parameters\\n    ----------\\n    params : dict\\n        Parameters for Booster.\\n    train_set : Dataset\\n        Data to be trained on.\\n    num_boost_round : int, optional (default=100)\\n        Number of boosting iterations.\\n    folds : generator or iterator of (train_idx, test_idx) tuples, scikit-learn splitter object or None, optional (default=None)\\n        If generator or iterator, it should yield the train and test indices for each fold.\\n        If object, it should be one of the scikit-learn splitter classes\\n        (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\\n        and have ``split`` method.\\n        This argument has highest priority over other data split arguments.\\n    nfold : int, optional (default=5)\\n        Number of folds in CV.\\n    stratified : bool, optional (default=True)\\n        Whether to perform stratified sampling.\\n    shuffle : bool, optional (default=True)\\n        Whether to shuffle before splitting data.\\n    metrics : string, list of strings or None, optional (default=None)\\n        Evaluation metrics to be monitored while CV.\\n        If not None, the metric in ``params`` will be overridden.\\n    fobj : callable or None, optional (default=None)\\n        Custom objective function.\\n    feval : callable or None, optional (default=None)\\n        Customized evaluation function.\\n        Should accept two parameters: preds, train_data,\\n        and return (eval_name, eval_result, is_higher_better) or list of such tuples.\\n        For multi-class task, the preds is group by class_id first, then group by row_id.\\n        If you want to get i-th row preds in j-th class, the access way is preds[j * num_data + i].\\n        To ignore the default metric corresponding to the used objective,\\n        set ``metrics`` to the string ``\"None\"``.\\n    init_model : string, Booster or None, optional (default=None)\\n        Filename of LightGBM model or Booster instance used for continue training.\\n    feature_name : list of strings or \\'auto\\', optional (default=\"auto\")\\n        Feature names.\\n        If \\'auto\\' and data is pandas DataFrame, data columns names are used.\\n    categorical_feature : list of strings or int, or \\'auto\\', optional (default=\"auto\")\\n        Categorical features.\\n        If list of int, interpreted as indices.\\n        If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\\n        If \\'auto\\' and data is pandas DataFrame, pandas unordered categorical columns are used.\\n        All values in categorical features should be less than int32 max value (2147483647).\\n        Large values could be memory consuming. Consider using consecutive integers starting from zero.\\n        All negative values in categorical features will be treated as missing values.\\n    early_stopping_rounds : int or None, optional (default=None)\\n        Activates early stopping.\\n        CV score needs to improve at least every ``early_stopping_rounds`` round(s)\\n        to continue.\\n        Requires at least one metric. If there\\'s more than one, will check all of them.\\n        To check only the first metric you can pass in ``callbacks``\\n        ``early_stopping`` callback with ``first_metric_only=True``.\\n        Last entry in evaluation history is the one from the best iteration.\\n    fpreproc : callable or None, optional (default=None)\\n        Preprocessing function that takes (dtrain, dtest, params)\\n        and returns transformed versions of those.\\n    verbose_eval : bool, int, or None, optional (default=None)\\n        Whether to display the progress.\\n        If None, progress will be displayed when np.ndarray is returned.\\n        If True, progress will be displayed at every boosting stage.\\n        If int, progress will be displayed at every given ``verbose_eval`` boosting stage.\\n    show_stdv : bool, optional (default=True)\\n        Whether to display the standard deviation in progress.\\n        Results are not affected by this parameter, and always contain std.\\n    seed : int, optional (default=0)\\n        Seed used to generate the folds (passed to numpy.random.seed).\\n    callbacks : list of callables or None, optional (default=None)\\n        List of callback functions that are applied at each iteration.\\n        See Callbacks in Python API for more information.\\n    eval_train_metric : bool, optional (default=False)\\n        Whether to display the train metric in progress.\\n        The score of the metric is calculated again after each training step, so there is some impact on performance.\\n\\n    Returns\\n    -------\\n    eval_hist : dict\\n        Evaluation history.\\n        The dictionary has the following format:\\n        {\\'metric1-mean\\': [values], \\'metric1-stdv\\': [values],\\n        \\'metric2-mean\\': [values], \\'metric2-stdv\\': [values],\\n        ...}.',\n",
       "  'code': 'def cv(params, train_set, num_boost_round=100,\\n       folds=None, nfold=5, stratified=True, shuffle=True,\\n       metrics=None, fobj=None, feval=None, init_model=None,\\n       feature_name=\\'auto\\', categorical_feature=\\'auto\\',\\n       early_stopping_rounds=None, fpreproc=None,\\n       verbose_eval=None, show_stdv=True, seed=0,\\n       callbacks=None, eval_train_metric=False):\\n    \"\"\"Perform the cross-validation with given paramaters.\\n\\n    Parameters\\n    ----------\\n    params : dict\\n        Parameters for Booster.\\n    train_set : Dataset\\n        Data to be trained on.\\n    num_boost_round : int, optional (default=100)\\n        Number of boosting iterations.\\n    folds : generator or iterator of (train_idx, test_idx) tuples, scikit-learn splitter object or None, optional (default=None)\\n        If generator or iterator, it should yield the train and test indices for each fold.\\n        If object, it should be one of the scikit-learn splitter classes\\n        (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\\n        and have ``split`` method.\\n        This argument has highest priority over other data split arguments.\\n    nfold : int, optional (default=5)\\n        Number of folds in CV.\\n    stratified : bool, optional (default=True)\\n        Whether to perform stratified sampling.\\n    shuffle : bool, optional (default=True)\\n        Whether to shuffle before splitting data.\\n    metrics : string, list of strings or None, optional (default=None)\\n        Evaluation metrics to be monitored while CV.\\n        If not None, the metric in ``params`` will be overridden.\\n    fobj : callable or None, optional (default=None)\\n        Custom objective function.\\n    feval : callable or None, optional (default=None)\\n        Customized evaluation function.\\n        Should accept two parameters: preds, train_data,\\n        and return (eval_name, eval_result, is_higher_better) or list of such tuples.\\n        For multi-class task, the preds is group by class_id first, then group by row_id.\\n        If you want to get i-th row preds in j-th class, the access way is preds[j * num_data + i].\\n        To ignore the default metric corresponding to the used objective,\\n        set ``metrics`` to the string ``\"None\"``.\\n    init_model : string, Booster or None, optional (default=None)\\n        Filename of LightGBM model or Booster instance used for continue training.\\n    feature_name : list of strings or \\'auto\\', optional (default=\"auto\")\\n        Feature names.\\n        If \\'auto\\' and data is pandas DataFrame, data columns names are used.\\n    categorical_feature : list of strings or int, or \\'auto\\', optional (default=\"auto\")\\n        Categorical features.\\n        If list of int, interpreted as indices.\\n        If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\\n        If \\'auto\\' and data is pandas DataFrame, pandas unordered categorical columns are used.\\n        All values in categorical features should be less than int32 max value (2147483647).\\n        Large values could be memory consuming. Consider using consecutive integers starting from zero.\\n        All negative values in categorical features will be treated as missing values.\\n    early_stopping_rounds : int or None, optional (default=None)\\n        Activates early stopping.\\n        CV score needs to improve at least every ``early_stopping_rounds`` round(s)\\n        to continue.\\n        Requires at least one metric. If there\\'s more than one, will check all of them.\\n        To check only the first metric you can pass in ``callbacks``\\n        ``early_stopping`` callback with ``first_metric_only=True``.\\n        Last entry in evaluation history is the one from the best iteration.\\n    fpreproc : callable or None, optional (default=None)\\n        Preprocessing function that takes (dtrain, dtest, params)\\n        and returns transformed versions of those.\\n    verbose_eval : bool, int, or None, optional (default=None)\\n        Whether to display the progress.\\n        If None, progress will be displayed when np.ndarray is returned.\\n        If True, progress will be displayed at every boosting stage.\\n        If int, progress will be displayed at every given ``verbose_eval`` boosting stage.\\n    show_stdv : bool, optional (default=True)\\n        Whether to display the standard deviation in progress.\\n        Results are not affected by this parameter, and always contain std.\\n    seed : int, optional (default=0)\\n        Seed used to generate the folds (passed to numpy.random.seed).\\n    callbacks : list of callables or None, optional (default=None)\\n        List of callback functions that are applied at each iteration.\\n        See Callbacks in Python API for more information.\\n    eval_train_metric : bool, optional (default=False)\\n        Whether to display the train metric in progress.\\n        The score of the metric is calculated again after each training step, so there is some impact on performance.\\n\\n    Returns\\n    -------\\n    eval_hist : dict\\n        Evaluation history.\\n        The dictionary has the following format:\\n        {\\'metric1-mean\\': [values], \\'metric1-stdv\\': [values],\\n        \\'metric2-mean\\': [values], \\'metric2-stdv\\': [values],\\n        ...}.\\n    \"\"\"\\n    if not isinstance(train_set, Dataset):\\n        raise TypeError(\"Traninig only accepts Dataset object\")\\n\\n    params = copy.deepcopy(params)\\n    if fobj is not None:\\n        params[\\'objective\\'] = \\'none\\'\\n    for alias in [\"num_iterations\", \"num_iteration\", \"n_iter\", \"num_tree\", \"num_trees\",\\n                  \"num_round\", \"num_rounds\", \"num_boost_round\", \"n_estimators\"]:\\n        if alias in params:\\n            warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\\n            num_boost_round = params.pop(alias)\\n            break\\n    for alias in [\"early_stopping_round\", \"early_stopping_rounds\", \"early_stopping\"]:\\n        if alias in params:\\n            warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\\n            early_stopping_rounds = params.pop(alias)\\n            break\\n\\n    if num_boost_round <= 0:\\n        raise ValueError(\"num_boost_round should be greater than zero.\")\\n    if isinstance(init_model, string_type):\\n        predictor = _InnerPredictor(model_file=init_model, pred_parameter=params)\\n    elif isinstance(init_model, Booster):\\n        predictor = init_model._to_predictor(dict(init_model.params, **params))\\n    else:\\n        predictor = None\\n    train_set._update_params(params) \\\\\\n             ._set_predictor(predictor) \\\\\\n             .set_feature_name(feature_name) \\\\\\n             .set_categorical_feature(categorical_feature)\\n\\n    if metrics is not None:\\n        params[\\'metric\\'] = metrics\\n\\n    results = collections.defaultdict(list)\\n    cvfolds = _make_n_folds(train_set, folds=folds, nfold=nfold,\\n                            params=params, seed=seed, fpreproc=fpreproc,\\n                            stratified=stratified, shuffle=shuffle,\\n                            eval_train_metric=eval_train_metric)\\n\\n    # setup callbacks\\n    if callbacks is None:\\n        callbacks = set()\\n    else:\\n        for i, cb in enumerate(callbacks):\\n            cb.__dict__.setdefault(\\'order\\', i - len(callbacks))\\n        callbacks = set(callbacks)\\n    if early_stopping_rounds is not None:\\n        callbacks.add(callback.early_stopping(early_stopping_rounds, verbose=False))\\n    if verbose_eval is True:\\n        callbacks.add(callback.print_evaluation(show_stdv=show_stdv))\\n    elif isinstance(verbose_eval, integer_types):\\n        callbacks.add(callback.print_evaluation(verbose_eval, show_stdv=show_stdv))\\n\\n    callbacks_before_iter = {cb for cb in callbacks if getattr(cb, \\'before_iteration\\', False)}\\n    callbacks_after_iter = callbacks - callbacks_before_iter\\n    callbacks_before_iter = sorted(callbacks_before_iter, key=attrgetter(\\'order\\'))\\n    callbacks_after_iter = sorted(callbacks_after_iter, key=attrgetter(\\'order\\'))\\n\\n    for i in range_(num_boost_round):\\n        for cb in callbacks_before_iter:\\n            cb(callback.CallbackEnv(model=cvfolds,\\n                                    params=params,\\n                                    iteration=i,\\n                                    begin_iteration=0,\\n                                    end_iteration=num_boost_round,\\n                                    evaluation_result_list=None))\\n        cvfolds.update(fobj=fobj)\\n        res = _agg_cv_result(cvfolds.eval_valid(feval), eval_train_metric)\\n        for _, key, mean, _, std in res:\\n            results[key + \\'-mean\\'].append(mean)\\n            results[key + \\'-stdv\\'].append(std)\\n        try:\\n            for cb in callbacks_after_iter:\\n                cb(callback.CallbackEnv(model=cvfolds,\\n                                        params=params,\\n                                        iteration=i,\\n                                        begin_iteration=0,\\n                                        end_iteration=num_boost_round,\\n                                        evaluation_result_list=res))\\n        except callback.EarlyStopException as earlyStopException:\\n            cvfolds.best_iteration = earlyStopException.best_iteration + 1\\n            for k in results:\\n                results[k] = results[k][:cvfolds.best_iteration]\\n            break\\n    return dict(results)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'find_nearest_neighbor',\n",
       "  'docstring': 'query is a 1d numpy array corresponding to the vector to which you want to\\n    find the closest vector\\n    vectors is a 2d numpy array corresponding to the vectors you want to consider\\n    ban_set is a set of indicies within vectors you want to ignore for nearest match\\n    cossims is a 1d numpy array of size len(vectors), which can be passed for efficiency\\n\\n    returns the index of the closest match to query within vectors',\n",
       "  'code': 'def find_nearest_neighbor(query, vectors, ban_set, cossims=None):\\n    \"\"\"\\n    query is a 1d numpy array corresponding to the vector to which you want to\\n    find the closest vector\\n    vectors is a 2d numpy array corresponding to the vectors you want to consider\\n    ban_set is a set of indicies within vectors you want to ignore for nearest match\\n    cossims is a 1d numpy array of size len(vectors), which can be passed for efficiency\\n\\n    returns the index of the closest match to query within vectors\\n\\n    \"\"\"\\n    if cossims is None:\\n        cossims = np.matmul(vectors, query, out=cossims)\\n    else:\\n        np.matmul(vectors, query, out=cossims)\\n    rank = len(cossims) - 1\\n    result_i = np.argpartition(cossims, rank)[rank]\\n    while result_i in ban_set:\\n        rank -= 1\\n        result_i = np.argpartition(cossims, rank)[rank]\\n    return result_i',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'sanitize',\n",
       "  'docstring': 'Sanitize turns PyTorch and Numpy types into basic Python types so they\\n    can be serialized into JSON.',\n",
       "  'code': 'def sanitize(x: Any) -> Any:  # pylint: disable=invalid-name,too-many-return-statements\\n    \"\"\"\\n    Sanitize turns PyTorch and Numpy types into basic Python types so they\\n    can be serialized into JSON.\\n    \"\"\"\\n    if isinstance(x, (str, float, int, bool)):\\n        # x is already serializable\\n        return x\\n    elif isinstance(x, torch.Tensor):\\n        # tensor needs to be converted to a list (and moved to cpu if necessary)\\n        return x.cpu().tolist()\\n    elif isinstance(x, numpy.ndarray):\\n        # array needs to be converted to a list\\n        return x.tolist()\\n    elif isinstance(x, numpy.number):  # pylint: disable=no-member\\n        # NumPy numbers need to be converted to Python numbers\\n        return x.item()\\n    elif isinstance(x, dict):\\n        # Dicts need their values sanitized\\n        return {key: sanitize(value) for key, value in x.items()}\\n    elif isinstance(x, (spacy.tokens.Token, allennlp.data.Token)):\\n        # Tokens get sanitized to just their text.\\n        return x.text\\n    elif isinstance(x, (list, tuple)):\\n        # Lists and Tuples need their values sanitized\\n        return [sanitize(x_i) for x_i in x]\\n    elif x is None:\\n        return \"None\"\\n    elif hasattr(x, \\'to_json\\'):\\n        return x.to_json()\\n    else:\\n        raise ValueError(f\"Cannot sanitize {x} of type {type(x)}. \"\\n                         \"If this is your own custom class, add a `to_json(self)` method \"\\n                         \"that returns a JSON-like object.\")',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'prepare_environment',\n",
       "  'docstring': 'Sets random seeds for reproducible experiments. This may not work as expected\\n    if you use this from within a python project in which you have already imported Pytorch.\\n    If you use the scripts/run_model.py entry point to training models with this library,\\n    your experiments should be reasonably reproducible. If you are using this from your own\\n    project, you will want to call this function before importing Pytorch. Complete determinism\\n    is very difficult to achieve with libraries doing optimized linear algebra due to massively\\n    parallel execution, which is exacerbated by using GPUs.\\n\\n    Parameters\\n    ----------\\n    params: Params object or dict, required.\\n        A ``Params`` object or dict holding the json parameters.',\n",
       "  'code': 'def prepare_environment(params: Params):\\n    \"\"\"\\n    Sets random seeds for reproducible experiments. This may not work as expected\\n    if you use this from within a python project in which you have already imported Pytorch.\\n    If you use the scripts/run_model.py entry point to training models with this library,\\n    your experiments should be reasonably reproducible. If you are using this from your own\\n    project, you will want to call this function before importing Pytorch. Complete determinism\\n    is very difficult to achieve with libraries doing optimized linear algebra due to massively\\n    parallel execution, which is exacerbated by using GPUs.\\n\\n    Parameters\\n    ----------\\n    params: Params object or dict, required.\\n        A ``Params`` object or dict holding the json parameters.\\n    \"\"\"\\n    seed = params.pop_int(\"random_seed\", 13370)\\n    numpy_seed = params.pop_int(\"numpy_seed\", 1337)\\n    torch_seed = params.pop_int(\"pytorch_seed\", 133)\\n\\n    if seed is not None:\\n        random.seed(seed)\\n    if numpy_seed is not None:\\n        numpy.random.seed(numpy_seed)\\n    if torch_seed is not None:\\n        torch.manual_seed(torch_seed)\\n        # Seed all GPUs with the same seed if available.\\n        if torch.cuda.is_available():\\n            torch.cuda.manual_seed_all(torch_seed)\\n\\n    log_pytorch_version_info()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'linkcode_resolve',\n",
       "  'docstring': 'Determine the URL corresponding to Python object\\n    This code is from\\n    https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L290\\n    and https://github.com/Lasagne/Lasagne/pull/262',\n",
       "  'code': 'def linkcode_resolve(domain, info):\\n    \"\"\"\\n    Determine the URL corresponding to Python object\\n    This code is from\\n    https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L290\\n    and https://github.com/Lasagne/Lasagne/pull/262\\n    \"\"\"\\n    if domain != \\'py\\':\\n        return None\\n\\n    modname = info[\\'module\\']\\n    fullname = info[\\'fullname\\']\\n\\n    submod = sys.modules.get(modname)\\n    if submod is None:\\n        return None\\n\\n    obj = submod\\n    for part in fullname.split(\\'.\\'):\\n        try:\\n            obj = getattr(obj, part)\\n        except:\\n            return None\\n\\n    try:\\n        fn = inspect.getsourcefile(obj)\\n    except:\\n        fn = None\\n    if not fn:\\n        return None\\n\\n    try:\\n        source, lineno = inspect.getsourcelines(obj)\\n    except:\\n        lineno = None\\n\\n    if lineno:\\n        linespec = \"#L%d-L%d\" % (lineno, lineno + len(source) - 1)\\n    else:\\n        linespec = \"\"\\n\\n    filename = info[\\'module\\'].replace(\\'.\\', \\'/\\')\\n    return \"http://github.com/allenai/allennlp/blob/master/%s.py%s\" % (filename, linespec)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'AtisWorld._flatten_entities',\n",
       "  'docstring': 'When we first get the entities and the linking scores in ``_get_linked_entities``\\n        we represent as dictionaries for easier updates to the grammar and valid actions.\\n        In this method, we flatten them for the model so that the entities are represented as\\n        a list, and the linking scores are a 2D numpy array of shape (num_entities, num_utterance_tokens).',\n",
       "  'code': 'def _flatten_entities(self) -> Tuple[List[str], numpy.ndarray]:\\n        \"\"\"\\n        When we first get the entities and the linking scores in ``_get_linked_entities``\\n        we represent as dictionaries for easier updates to the grammar and valid actions.\\n        In this method, we flatten them for the model so that the entities are represented as\\n        a list, and the linking scores are a 2D numpy array of shape (num_entities, num_utterance_tokens).\\n        \"\"\"\\n        entities = []\\n        linking_scores = []\\n        for entity in sorted(self.linked_entities[\\'number\\']):\\n            entities.append(entity)\\n            linking_scores.append(self.linked_entities[\\'number\\'][entity][2])\\n\\n        for entity in sorted(self.linked_entities[\\'string\\']):\\n            entities.append(entity)\\n            linking_scores.append(self.linked_entities[\\'string\\'][entity][2])\\n\\n        return entities, numpy.array(linking_scores)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Model.forward_on_instance',\n",
       "  'docstring': \"Takes an :class:`~allennlp.data.instance.Instance`, which typically has raw text in it,\\n        converts that text into arrays using this model's :class:`Vocabulary`, passes those arrays\\n        through :func:`self.forward()` and :func:`self.decode()` (which by default does nothing)\\n        and returns the result.  Before returning the result, we convert any\\n        ``torch.Tensors`` into numpy arrays and remove the batch dimension.\",\n",
       "  'code': 'def forward_on_instance(self, instance: Instance) -> Dict[str, numpy.ndarray]:\\n        \"\"\"\\n        Takes an :class:`~allennlp.data.instance.Instance`, which typically has raw text in it,\\n        converts that text into arrays using this model\\'s :class:`Vocabulary`, passes those arrays\\n        through :func:`self.forward()` and :func:`self.decode()` (which by default does nothing)\\n        and returns the result.  Before returning the result, we convert any\\n        ``torch.Tensors`` into numpy arrays and remove the batch dimension.\\n        \"\"\"\\n        return self.forward_on_instances([instance])[0]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Model.forward_on_instances',\n",
       "  'docstring': \"Takes a list of  :class:`~allennlp.data.instance.Instance`s, converts that text into\\n        arrays using this model's :class:`Vocabulary`, passes those arrays through\\n        :func:`self.forward()` and :func:`self.decode()` (which by default does nothing)\\n        and returns the result.  Before returning the result, we convert any\\n        ``torch.Tensors`` into numpy arrays and separate the\\n        batched output into a list of individual dicts per instance. Note that typically\\n        this will be faster on a GPU (and conditionally, on a CPU) than repeated calls to\\n        :func:`forward_on_instance`.\\n\\n        Parameters\\n        ----------\\n        instances : List[Instance], required\\n            The instances to run the model on.\\n\\n        Returns\\n        -------\\n        A list of the models output for each instance.\",\n",
       "  'code': 'def forward_on_instances(self,\\n                             instances: List[Instance]) -> List[Dict[str, numpy.ndarray]]:\\n        \"\"\"\\n        Takes a list of  :class:`~allennlp.data.instance.Instance`s, converts that text into\\n        arrays using this model\\'s :class:`Vocabulary`, passes those arrays through\\n        :func:`self.forward()` and :func:`self.decode()` (which by default does nothing)\\n        and returns the result.  Before returning the result, we convert any\\n        ``torch.Tensors`` into numpy arrays and separate the\\n        batched output into a list of individual dicts per instance. Note that typically\\n        this will be faster on a GPU (and conditionally, on a CPU) than repeated calls to\\n        :func:`forward_on_instance`.\\n\\n        Parameters\\n        ----------\\n        instances : List[Instance], required\\n            The instances to run the model on.\\n\\n        Returns\\n        -------\\n        A list of the models output for each instance.\\n        \"\"\"\\n        batch_size = len(instances)\\n        with torch.no_grad():\\n            cuda_device = self._get_prediction_device()\\n            dataset = Batch(instances)\\n            dataset.index_instances(self.vocab)\\n            model_input = util.move_to_device(dataset.as_tensor_dict(), cuda_device)\\n            outputs = self.decode(self(**model_input))\\n\\n            instance_separated_output: List[Dict[str, numpy.ndarray]] = [{} for _ in dataset.instances]\\n            for name, output in list(outputs.items()):\\n                if isinstance(output, torch.Tensor):\\n                    # NOTE(markn): This is a hack because 0-dim pytorch tensors are not iterable.\\n                    # This occurs with batch size 1, because we still want to include the loss in that case.\\n                    if output.dim() == 0:\\n                        output = output.unsqueeze(0)\\n\\n                    if output.size(0) != batch_size:\\n                        self._maybe_warn_for_unseparable_batches(name)\\n                        continue\\n                    output = output.detach().cpu().numpy()\\n                elif len(output) != batch_size:\\n                    self._maybe_warn_for_unseparable_batches(name)\\n                    continue\\n                for instance_output, batch_element in zip(instance_separated_output, output):\\n                    instance_output[name] = batch_element\\n            return instance_separated_output',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'viterbi_decode',\n",
       "  'docstring': 'Perform Viterbi decoding in log space over a sequence given a transition matrix\\n    specifying pairwise (transition) potentials between tags and a matrix of shape\\n    (sequence_length, num_tags) specifying unary potentials for possible tags per\\n    timestep.\\n\\n    Parameters\\n    ----------\\n    tag_sequence : torch.Tensor, required.\\n        A tensor of shape (sequence_length, num_tags) representing scores for\\n        a set of tags over a given sequence.\\n    transition_matrix : torch.Tensor, required.\\n        A tensor of shape (num_tags, num_tags) representing the binary potentials\\n        for transitioning between a given pair of tags.\\n    tag_observations : Optional[List[int]], optional, (default = None)\\n        A list of length ``sequence_length`` containing the class ids of observed\\n        elements in the sequence, with unobserved elements being set to -1. Note that\\n        it is possible to provide evidence which results in degenerate labelings if\\n        the sequences of tags you provide as evidence cannot transition between each\\n        other, or those transitions are extremely unlikely. In this situation we log a\\n        warning, but the responsibility for providing self-consistent evidence ultimately\\n        lies with the user.\\n\\n    Returns\\n    -------\\n    viterbi_path : List[int]\\n        The tag indices of the maximum likelihood tag sequence.\\n    viterbi_score : torch.Tensor\\n        The score of the viterbi path.',\n",
       "  'code': 'def viterbi_decode(tag_sequence: torch.Tensor,\\n                   transition_matrix: torch.Tensor,\\n                   tag_observations: Optional[List[int]] = None):\\n    \"\"\"\\n    Perform Viterbi decoding in log space over a sequence given a transition matrix\\n    specifying pairwise (transition) potentials between tags and a matrix of shape\\n    (sequence_length, num_tags) specifying unary potentials for possible tags per\\n    timestep.\\n\\n    Parameters\\n    ----------\\n    tag_sequence : torch.Tensor, required.\\n        A tensor of shape (sequence_length, num_tags) representing scores for\\n        a set of tags over a given sequence.\\n    transition_matrix : torch.Tensor, required.\\n        A tensor of shape (num_tags, num_tags) representing the binary potentials\\n        for transitioning between a given pair of tags.\\n    tag_observations : Optional[List[int]], optional, (default = None)\\n        A list of length ``sequence_length`` containing the class ids of observed\\n        elements in the sequence, with unobserved elements being set to -1. Note that\\n        it is possible to provide evidence which results in degenerate labelings if\\n        the sequences of tags you provide as evidence cannot transition between each\\n        other, or those transitions are extremely unlikely. In this situation we log a\\n        warning, but the responsibility for providing self-consistent evidence ultimately\\n        lies with the user.\\n\\n    Returns\\n    -------\\n    viterbi_path : List[int]\\n        The tag indices of the maximum likelihood tag sequence.\\n    viterbi_score : torch.Tensor\\n        The score of the viterbi path.\\n    \"\"\"\\n    sequence_length, num_tags = list(tag_sequence.size())\\n    if tag_observations:\\n        if len(tag_observations) != sequence_length:\\n            raise ConfigurationError(\"Observations were provided, but they were not the same length \"\\n                                     \"as the sequence. Found sequence of length: {} and evidence: {}\"\\n                                     .format(sequence_length, tag_observations))\\n    else:\\n        tag_observations = [-1 for _ in range(sequence_length)]\\n\\n    path_scores = []\\n    path_indices = []\\n\\n    if tag_observations[0] != -1:\\n        one_hot = torch.zeros(num_tags)\\n        one_hot[tag_observations[0]] = 100000.\\n        path_scores.append(one_hot)\\n    else:\\n        path_scores.append(tag_sequence[0, :])\\n\\n    # Evaluate the scores for all possible paths.\\n    for timestep in range(1, sequence_length):\\n        # Add pairwise potentials to current scores.\\n        summed_potentials = path_scores[timestep - 1].unsqueeze(-1) + transition_matrix\\n        scores, paths = torch.max(summed_potentials, 0)\\n\\n        # If we have an observation for this timestep, use it\\n        # instead of the distribution over tags.\\n        observation = tag_observations[timestep]\\n        # Warn the user if they have passed\\n        # invalid/extremely unlikely evidence.\\n        if tag_observations[timestep - 1] != -1:\\n            if transition_matrix[tag_observations[timestep - 1], observation] < -10000:\\n                logger.warning(\"The pairwise potential between tags you have passed as \"\\n                               \"observations is extremely unlikely. Double check your evidence \"\\n                               \"or transition potentials!\")\\n        if observation != -1:\\n            one_hot = torch.zeros(num_tags)\\n            one_hot[observation] = 100000.\\n            path_scores.append(one_hot)\\n        else:\\n            path_scores.append(tag_sequence[timestep, :] + scores.squeeze())\\n        path_indices.append(paths.squeeze())\\n\\n    # Construct the most likely sequence backwards.\\n    viterbi_score, best_path = torch.max(path_scores[-1], 0)\\n    viterbi_path = [int(best_path.numpy())]\\n    for backward_timestep in reversed(path_indices):\\n        viterbi_path.append(int(backward_timestep[viterbi_path[-1]]))\\n    # Reverse the backward path.\\n    viterbi_path.reverse()\\n    return viterbi_path, viterbi_score',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'add_sentence_boundary_token_ids',\n",
       "  'docstring': 'Add begin/end of sentence tokens to the batch of sentences.\\n    Given a batch of sentences with size ``(batch_size, timesteps)`` or\\n    ``(batch_size, timesteps, dim)`` this returns a tensor of shape\\n    ``(batch_size, timesteps + 2)`` or ``(batch_size, timesteps + 2, dim)`` respectively.\\n\\n    Returns both the new tensor and updated mask.\\n\\n    Parameters\\n    ----------\\n    tensor : ``torch.Tensor``\\n        A tensor of shape ``(batch_size, timesteps)`` or ``(batch_size, timesteps, dim)``\\n    mask : ``torch.Tensor``\\n         A tensor of shape ``(batch_size, timesteps)``\\n    sentence_begin_token: Any (anything that can be broadcast in torch for assignment)\\n        For 2D input, a scalar with the <S> id. For 3D input, a tensor with length dim.\\n    sentence_end_token: Any (anything that can be broadcast in torch for assignment)\\n        For 2D input, a scalar with the </S> id. For 3D input, a tensor with length dim.\\n\\n    Returns\\n    -------\\n    tensor_with_boundary_tokens : ``torch.Tensor``\\n        The tensor with the appended and prepended boundary tokens. If the input was 2D,\\n        it has shape (batch_size, timesteps + 2) and if the input was 3D, it has shape\\n        (batch_size, timesteps + 2, dim).\\n    new_mask : ``torch.Tensor``\\n        The new mask for the tensor, taking into account the appended tokens\\n        marking the beginning and end of the sentence.',\n",
       "  'code': 'def add_sentence_boundary_token_ids(tensor: torch.Tensor,\\n                                    mask: torch.Tensor,\\n                                    sentence_begin_token: Any,\\n                                    sentence_end_token: Any) -> Tuple[torch.Tensor, torch.Tensor]:\\n    \"\"\"\\n    Add begin/end of sentence tokens to the batch of sentences.\\n    Given a batch of sentences with size ``(batch_size, timesteps)`` or\\n    ``(batch_size, timesteps, dim)`` this returns a tensor of shape\\n    ``(batch_size, timesteps + 2)`` or ``(batch_size, timesteps + 2, dim)`` respectively.\\n\\n    Returns both the new tensor and updated mask.\\n\\n    Parameters\\n    ----------\\n    tensor : ``torch.Tensor``\\n        A tensor of shape ``(batch_size, timesteps)`` or ``(batch_size, timesteps, dim)``\\n    mask : ``torch.Tensor``\\n         A tensor of shape ``(batch_size, timesteps)``\\n    sentence_begin_token: Any (anything that can be broadcast in torch for assignment)\\n        For 2D input, a scalar with the <S> id. For 3D input, a tensor with length dim.\\n    sentence_end_token: Any (anything that can be broadcast in torch for assignment)\\n        For 2D input, a scalar with the </S> id. For 3D input, a tensor with length dim.\\n\\n    Returns\\n    -------\\n    tensor_with_boundary_tokens : ``torch.Tensor``\\n        The tensor with the appended and prepended boundary tokens. If the input was 2D,\\n        it has shape (batch_size, timesteps + 2) and if the input was 3D, it has shape\\n        (batch_size, timesteps + 2, dim).\\n    new_mask : ``torch.Tensor``\\n        The new mask for the tensor, taking into account the appended tokens\\n        marking the beginning and end of the sentence.\\n    \"\"\"\\n    # TODO: matthewp, profile this transfer\\n    sequence_lengths = mask.sum(dim=1).detach().cpu().numpy()\\n    tensor_shape = list(tensor.data.shape)\\n    new_shape = list(tensor_shape)\\n    new_shape[1] = tensor_shape[1] + 2\\n    tensor_with_boundary_tokens = tensor.new_zeros(*new_shape)\\n    if len(tensor_shape) == 2:\\n        tensor_with_boundary_tokens[:, 1:-1] = tensor\\n        tensor_with_boundary_tokens[:, 0] = sentence_begin_token\\n        for i, j in enumerate(sequence_lengths):\\n            tensor_with_boundary_tokens[i, j + 1] = sentence_end_token\\n        new_mask = (tensor_with_boundary_tokens != 0).long()\\n    elif len(tensor_shape) == 3:\\n        tensor_with_boundary_tokens[:, 1:-1, :] = tensor\\n        for i, j in enumerate(sequence_lengths):\\n            tensor_with_boundary_tokens[i, 0, :] = sentence_begin_token\\n            tensor_with_boundary_tokens[i, j + 1, :] = sentence_end_token\\n        new_mask = ((tensor_with_boundary_tokens > 0).long().sum(dim=-1) > 0).long()\\n    else:\\n        raise ValueError(\"add_sentence_boundary_token_ids only accepts 2D and 3D input\")\\n\\n    return tensor_with_boundary_tokens, new_mask',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'remove_sentence_boundaries',\n",
       "  'docstring': 'Remove begin/end of sentence embeddings from the batch of sentences.\\n    Given a batch of sentences with size ``(batch_size, timesteps, dim)``\\n    this returns a tensor of shape ``(batch_size, timesteps - 2, dim)`` after removing\\n    the beginning and end sentence markers.  The sentences are assumed to be padded on the right,\\n    with the beginning of each sentence assumed to occur at index 0 (i.e., ``mask[:, 0]`` is assumed\\n    to be 1).\\n\\n    Returns both the new tensor and updated mask.\\n\\n    This function is the inverse of ``add_sentence_boundary_token_ids``.\\n\\n    Parameters\\n    ----------\\n    tensor : ``torch.Tensor``\\n        A tensor of shape ``(batch_size, timesteps, dim)``\\n    mask : ``torch.Tensor``\\n         A tensor of shape ``(batch_size, timesteps)``\\n\\n    Returns\\n    -------\\n    tensor_without_boundary_tokens : ``torch.Tensor``\\n        The tensor after removing the boundary tokens of shape ``(batch_size, timesteps - 2, dim)``\\n    new_mask : ``torch.Tensor``\\n        The new mask for the tensor of shape ``(batch_size, timesteps - 2)``.',\n",
       "  'code': 'def remove_sentence_boundaries(tensor: torch.Tensor,\\n                               mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\\n    \"\"\"\\n    Remove begin/end of sentence embeddings from the batch of sentences.\\n    Given a batch of sentences with size ``(batch_size, timesteps, dim)``\\n    this returns a tensor of shape ``(batch_size, timesteps - 2, dim)`` after removing\\n    the beginning and end sentence markers.  The sentences are assumed to be padded on the right,\\n    with the beginning of each sentence assumed to occur at index 0 (i.e., ``mask[:, 0]`` is assumed\\n    to be 1).\\n\\n    Returns both the new tensor and updated mask.\\n\\n    This function is the inverse of ``add_sentence_boundary_token_ids``.\\n\\n    Parameters\\n    ----------\\n    tensor : ``torch.Tensor``\\n        A tensor of shape ``(batch_size, timesteps, dim)``\\n    mask : ``torch.Tensor``\\n         A tensor of shape ``(batch_size, timesteps)``\\n\\n    Returns\\n    -------\\n    tensor_without_boundary_tokens : ``torch.Tensor``\\n        The tensor after removing the boundary tokens of shape ``(batch_size, timesteps - 2, dim)``\\n    new_mask : ``torch.Tensor``\\n        The new mask for the tensor of shape ``(batch_size, timesteps - 2)``.\\n    \"\"\"\\n    # TODO: matthewp, profile this transfer\\n    sequence_lengths = mask.sum(dim=1).detach().cpu().numpy()\\n    tensor_shape = list(tensor.data.shape)\\n    new_shape = list(tensor_shape)\\n    new_shape[1] = tensor_shape[1] - 2\\n    tensor_without_boundary_tokens = tensor.new_zeros(*new_shape)\\n    new_mask = tensor.new_zeros((new_shape[0], new_shape[1]), dtype=torch.long)\\n    for i, j in enumerate(sequence_lengths):\\n        if j > 2:\\n            tensor_without_boundary_tokens[i, :(j - 2), :] = tensor[i, 1:(j - 1), :]\\n            new_mask[i, :(j - 2)] = 1\\n\\n    return tensor_without_boundary_tokens, new_mask',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ElmoEmbedder.embed_sentence',\n",
       "  'docstring': 'Computes the ELMo embeddings for a single tokenized sentence.\\n\\n        Please note that ELMo has internal state and will give different results for the same input.\\n        See the comment under the class definition.\\n\\n        Parameters\\n        ----------\\n        sentence : ``List[str]``, required\\n            A tokenized sentence.\\n\\n        Returns\\n        -------\\n        A tensor containing the ELMo vectors.',\n",
       "  'code': 'def embed_sentence(self, sentence: List[str]) -> numpy.ndarray:\\n        \"\"\"\\n        Computes the ELMo embeddings for a single tokenized sentence.\\n\\n        Please note that ELMo has internal state and will give different results for the same input.\\n        See the comment under the class definition.\\n\\n        Parameters\\n        ----------\\n        sentence : ``List[str]``, required\\n            A tokenized sentence.\\n\\n        Returns\\n        -------\\n        A tensor containing the ELMo vectors.\\n        \"\"\"\\n\\n        return self.embed_batch([sentence])[0]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ElmoEmbedder.embed_batch',\n",
       "  'docstring': 'Computes the ELMo embeddings for a batch of tokenized sentences.\\n\\n        Please note that ELMo has internal state and will give different results for the same input.\\n        See the comment under the class definition.\\n\\n        Parameters\\n        ----------\\n        batch : ``List[List[str]]``, required\\n            A list of tokenized sentences.\\n\\n        Returns\\n        -------\\n            A list of tensors, each representing the ELMo vectors for the input sentence at the same index.',\n",
       "  'code': 'def embed_batch(self, batch: List[List[str]]) -> List[numpy.ndarray]:\\n        \"\"\"\\n        Computes the ELMo embeddings for a batch of tokenized sentences.\\n\\n        Please note that ELMo has internal state and will give different results for the same input.\\n        See the comment under the class definition.\\n\\n        Parameters\\n        ----------\\n        batch : ``List[List[str]]``, required\\n            A list of tokenized sentences.\\n\\n        Returns\\n        -------\\n            A list of tensors, each representing the ELMo vectors for the input sentence at the same index.\\n        \"\"\"\\n        elmo_embeddings = []\\n\\n        # Batches with only an empty sentence will throw an exception inside AllenNLP, so we handle this case\\n        # and return an empty embedding instead.\\n        if batch == [[]]:\\n            elmo_embeddings.append(empty_embedding())\\n        else:\\n            embeddings, mask = self.batch_to_embeddings(batch)\\n            for i in range(len(batch)):\\n                length = int(mask[i, :].sum())\\n                # Slicing the embedding :0 throws an exception so we need to special case for empty sentences.\\n                if length == 0:\\n                    elmo_embeddings.append(empty_embedding())\\n                else:\\n                    elmo_embeddings.append(embeddings[i, :, :length, :].detach().cpu().numpy())\\n\\n        return elmo_embeddings',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ElmoEmbedder.embed_sentences',\n",
       "  'docstring': 'Computes the ELMo embeddings for a iterable of sentences.\\n\\n        Please note that ELMo has internal state and will give different results for the same input.\\n        See the comment under the class definition.\\n\\n        Parameters\\n        ----------\\n        sentences : ``Iterable[List[str]]``, required\\n            An iterable of tokenized sentences.\\n        batch_size : ``int``, required\\n            The number of sentences ELMo should process at once.\\n\\n        Returns\\n        -------\\n            A list of tensors, each representing the ELMo vectors for the input sentence at the same index.',\n",
       "  'code': 'def embed_sentences(self,\\n                        sentences: Iterable[List[str]],\\n                        batch_size: int = DEFAULT_BATCH_SIZE) -> Iterable[numpy.ndarray]:\\n        \"\"\"\\n        Computes the ELMo embeddings for a iterable of sentences.\\n\\n        Please note that ELMo has internal state and will give different results for the same input.\\n        See the comment under the class definition.\\n\\n        Parameters\\n        ----------\\n        sentences : ``Iterable[List[str]]``, required\\n            An iterable of tokenized sentences.\\n        batch_size : ``int``, required\\n            The number of sentences ELMo should process at once.\\n\\n        Returns\\n        -------\\n            A list of tensors, each representing the ELMo vectors for the input sentence at the same index.\\n        \"\"\"\\n        for batch in lazy_groups_of(iter(sentences), batch_size):\\n            yield from self.embed_batch(batch)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ElmoEmbedder.embed_file',\n",
       "  'docstring': 'Computes ELMo embeddings from an input_file where each line contains a sentence tokenized by whitespace.\\n        The ELMo embeddings are written out in HDF5 format, where each sentence embedding\\n        is saved in a dataset with the line number in the original file as the key.\\n\\n        Parameters\\n        ----------\\n        input_file : ``IO``, required\\n            A file with one tokenized sentence per line.\\n        output_file_path : ``str``, required\\n            A path to the output hdf5 file.\\n        output_format : ``str``, optional, (default = \"all\")\\n            The embeddings to output.  Must be one of \"all\", \"top\", or \"average\".\\n        batch_size : ``int``, optional, (default = 64)\\n            The number of sentences to process in ELMo at one time.\\n        forget_sentences : ``bool``, optional, (default = False).\\n            If use_sentence_keys is False, whether or not to include a string\\n            serialized JSON dictionary that associates sentences with their\\n            line number (its HDF5 key). The mapping is placed in the\\n            \"sentence_to_index\" HDF5 key. This is useful if\\n            you want to use the embeddings without keeping the original file\\n            of sentences around.\\n        use_sentence_keys : ``bool``, optional, (default = False).\\n            Whether or not to use full sentences as keys. By default,\\n            the line numbers of the input file are used as ids, which is more robust.',\n",
       "  'code': 'def embed_file(self,\\n                   input_file: IO,\\n                   output_file_path: str,\\n                   output_format: str = \"all\",\\n                   batch_size: int = DEFAULT_BATCH_SIZE,\\n                   forget_sentences: bool = False,\\n                   use_sentence_keys: bool = False) -> None:\\n        \"\"\"\\n        Computes ELMo embeddings from an input_file where each line contains a sentence tokenized by whitespace.\\n        The ELMo embeddings are written out in HDF5 format, where each sentence embedding\\n        is saved in a dataset with the line number in the original file as the key.\\n\\n        Parameters\\n        ----------\\n        input_file : ``IO``, required\\n            A file with one tokenized sentence per line.\\n        output_file_path : ``str``, required\\n            A path to the output hdf5 file.\\n        output_format : ``str``, optional, (default = \"all\")\\n            The embeddings to output.  Must be one of \"all\", \"top\", or \"average\".\\n        batch_size : ``int``, optional, (default = 64)\\n            The number of sentences to process in ELMo at one time.\\n        forget_sentences : ``bool``, optional, (default = False).\\n            If use_sentence_keys is False, whether or not to include a string\\n            serialized JSON dictionary that associates sentences with their\\n            line number (its HDF5 key). The mapping is placed in the\\n            \"sentence_to_index\" HDF5 key. This is useful if\\n            you want to use the embeddings without keeping the original file\\n            of sentences around.\\n        use_sentence_keys : ``bool``, optional, (default = False).\\n            Whether or not to use full sentences as keys. By default,\\n            the line numbers of the input file are used as ids, which is more robust.\\n        \"\"\"\\n\\n        assert output_format in [\"all\", \"top\", \"average\"]\\n\\n        # Tokenizes the sentences.\\n        sentences = [line.strip() for line in input_file]\\n\\n        blank_lines = [i for (i, line) in enumerate(sentences) if line == \"\"]\\n        if blank_lines:\\n            raise ConfigurationError(f\"Your input file contains empty lines at indexes \"\\n                                     f\"{blank_lines}. Please remove them.\")\\n        split_sentences = [sentence.split() for sentence in sentences]\\n        # Uses the sentence index as the key.\\n\\n        if use_sentence_keys:\\n            logger.warning(\"Using sentences as keys can fail if sentences \"\\n                           \"contain forward slashes or colons. Use with caution.\")\\n            embedded_sentences = zip(sentences, self.embed_sentences(split_sentences, batch_size))\\n        else:\\n            embedded_sentences = ((str(i), x) for i, x in\\n                                  enumerate(self.embed_sentences(split_sentences, batch_size)))\\n\\n        sentence_to_index = {}\\n        logger.info(\"Processing sentences.\")\\n        with h5py.File(output_file_path, \\'w\\') as fout:\\n            for key, embeddings in Tqdm.tqdm(embedded_sentences):\\n                if use_sentence_keys and key in fout.keys():\\n                    raise ConfigurationError(f\"Key already exists in {output_file_path}. \"\\n                                             f\"To encode duplicate sentences, do not pass \"\\n                                             f\"the --use-sentence-keys flag.\")\\n\\n                if not forget_sentences and not use_sentence_keys:\\n                    sentence = sentences[int(key)]\\n                    sentence_to_index[sentence] = key\\n\\n                if output_format == \"all\":\\n                    output = embeddings\\n                elif output_format == \"top\":\\n                    output = embeddings[-1]\\n                elif output_format == \"average\":\\n                    output = numpy.average(embeddings, axis=0)\\n\\n                fout.create_dataset(\\n                        str(key),\\n                        output.shape, dtype=\\'float32\\',\\n                        data=output\\n                )\\n            if not forget_sentences and not use_sentence_keys:\\n                sentence_index_dataset = fout.create_dataset(\\n                        \"sentence_to_index\",\\n                        (1,),\\n                        dtype=h5py.special_dtype(vlen=str))\\n                sentence_index_dataset[0] = json.dumps(sentence_to_index)\\n\\n        input_file.close()',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_docspec_comments',\n",
       "  'docstring': 'Inspect the docstring and get the comments for each parameter.',\n",
       "  'code': 'def _docspec_comments(obj) -> Dict[str, str]:\\n    \"\"\"\\n    Inspect the docstring and get the comments for each parameter.\\n    \"\"\"\\n    # Sometimes our docstring is on the class, and sometimes it\\'s on the initializer,\\n    # so we\\'ve got to check both.\\n    class_docstring = getattr(obj, \\'__doc__\\', None)\\n    init_docstring = getattr(obj.__init__, \\'__doc__\\', None) if hasattr(obj, \\'__init__\\') else None\\n\\n    docstring = class_docstring or init_docstring or \\'\\'\\n\\n    doc = NumpyDocString(docstring)\\n    params = doc[\"Parameters\"]\\n    comments: Dict[str, str] = {}\\n\\n    for line in params:\\n        # It looks like when there\\'s not a space after the parameter name,\\n        # numpydocstring parses it incorrectly.\\n        name_bad = line[0]\\n        name = name_bad.split(\":\")[0]\\n\\n        # Sometimes the line has 3 fields, sometimes it has 4 fields.\\n        comment = \"\\\\n\".join(line[-1])\\n\\n        comments[name] = comment\\n\\n    return comments',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'construct_prefix_tree',\n",
       "  'docstring': \"Takes a list of valid target action sequences and creates a mapping from all possible\\n    (valid) action prefixes to allowed actions given that prefix.  While the method is called\\n    ``construct_prefix_tree``, we're actually returning a map that has as keys the paths to\\n    `all internal nodes of the trie`, and as values all of the outgoing edges from that node.\\n\\n    ``targets`` is assumed to be a tensor of shape ``(batch_size, num_valid_sequences,\\n    sequence_length)``.  If the mask is not ``None``, it is assumed to have the same shape, and\\n    we will ignore any value in ``targets`` that has a value of ``0`` in the corresponding\\n    position in the mask.  We assume that the mask has the format 1*0* for each item in\\n    ``targets`` - that is, once we see our first zero, we stop processing that target.\\n\\n    For example, if ``targets`` is the following tensor: ``[[1, 2, 3], [1, 4, 5]]``, the return\\n    value will be: ``{(): set([1]), (1,): set([2, 4]), (1, 2): set([3]), (1, 4): set([5])}``.\\n\\n    This could be used, e.g., to do an efficient constrained beam search, or to efficiently\\n    evaluate the probability of all of the target sequences.\",\n",
       "  'code': 'def construct_prefix_tree(targets: Union[torch.Tensor, List[List[List[int]]]],\\n                          target_mask: Optional[torch.Tensor] = None) -> List[Dict[Tuple[int, ...], Set[int]]]:\\n    \"\"\"\\n    Takes a list of valid target action sequences and creates a mapping from all possible\\n    (valid) action prefixes to allowed actions given that prefix.  While the method is called\\n    ``construct_prefix_tree``, we\\'re actually returning a map that has as keys the paths to\\n    `all internal nodes of the trie`, and as values all of the outgoing edges from that node.\\n\\n    ``targets`` is assumed to be a tensor of shape ``(batch_size, num_valid_sequences,\\n    sequence_length)``.  If the mask is not ``None``, it is assumed to have the same shape, and\\n    we will ignore any value in ``targets`` that has a value of ``0`` in the corresponding\\n    position in the mask.  We assume that the mask has the format 1*0* for each item in\\n    ``targets`` - that is, once we see our first zero, we stop processing that target.\\n\\n    For example, if ``targets`` is the following tensor: ``[[1, 2, 3], [1, 4, 5]]``, the return\\n    value will be: ``{(): set([1]), (1,): set([2, 4]), (1, 2): set([3]), (1, 4): set([5])}``.\\n\\n    This could be used, e.g., to do an efficient constrained beam search, or to efficiently\\n    evaluate the probability of all of the target sequences.\\n    \"\"\"\\n    batched_allowed_transitions: List[Dict[Tuple[int, ...], Set[int]]] = []\\n\\n    if not isinstance(targets, list):\\n        assert targets.dim() == 3, \"targets tensor needs to be batched!\"\\n        targets = targets.detach().cpu().numpy().tolist()\\n    if target_mask is not None:\\n        target_mask = target_mask.detach().cpu().numpy().tolist()\\n    else:\\n        target_mask = [None for _ in targets]\\n\\n    for instance_targets, instance_mask in zip(targets, target_mask):\\n        allowed_transitions: Dict[Tuple[int, ...], Set[int]] = defaultdict(set)\\n        for i, target_sequence in enumerate(instance_targets):\\n            history: Tuple[int, ...] = ()\\n            for j, action in enumerate(target_sequence):\\n                if instance_mask and instance_mask[i][j] == 0:\\n                    break\\n                allowed_transitions[history].add(action)\\n                history = history + (action,)\\n        batched_allowed_transitions.append(allowed_transitions)\\n    return batched_allowed_transitions',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ElmoLstm.load_weights',\n",
       "  'docstring': 'Load the pre-trained weights from the file.',\n",
       "  'code': 'def load_weights(self, weight_file: str) -> None:\\n        \"\"\"\\n        Load the pre-trained weights from the file.\\n        \"\"\"\\n        requires_grad = self.requires_grad\\n\\n        with h5py.File(cached_path(weight_file), \\'r\\') as fin:\\n            for i_layer, lstms in enumerate(\\n                    zip(self.forward_layers, self.backward_layers)\\n            ):\\n                for j_direction, lstm in enumerate(lstms):\\n                    # lstm is an instance of LSTMCellWithProjection\\n                    cell_size = lstm.cell_size\\n\\n                    dataset = fin[\\'RNN_%s\\' % j_direction][\\'RNN\\'][\\'MultiRNNCell\\'][\\'Cell%s\\' % i_layer\\n                                                                                ][\\'LSTMCell\\']\\n\\n                    # tensorflow packs together both W and U matrices into one matrix,\\n                    # but pytorch maintains individual matrices.  In addition, tensorflow\\n                    # packs the gates as input, memory, forget, output but pytorch\\n                    # uses input, forget, memory, output.  So we need to modify the weights.\\n                    tf_weights = numpy.transpose(dataset[\\'W_0\\'][...])\\n                    torch_weights = tf_weights.copy()\\n\\n                    # split the W from U matrices\\n                    input_size = lstm.input_size\\n                    input_weights = torch_weights[:, :input_size]\\n                    recurrent_weights = torch_weights[:, input_size:]\\n                    tf_input_weights = tf_weights[:, :input_size]\\n                    tf_recurrent_weights = tf_weights[:, input_size:]\\n\\n                    # handle the different gate order convention\\n                    for torch_w, tf_w in [[input_weights, tf_input_weights],\\n                                          [recurrent_weights, tf_recurrent_weights]]:\\n                        torch_w[(1 * cell_size):(2 * cell_size), :] = tf_w[(2 * cell_size):(3 * cell_size), :]\\n                        torch_w[(2 * cell_size):(3 * cell_size), :] = tf_w[(1 * cell_size):(2 * cell_size), :]\\n\\n                    lstm.input_linearity.weight.data.copy_(torch.FloatTensor(input_weights))\\n                    lstm.state_linearity.weight.data.copy_(torch.FloatTensor(recurrent_weights))\\n                    lstm.input_linearity.weight.requires_grad = requires_grad\\n                    lstm.state_linearity.weight.requires_grad = requires_grad\\n\\n                    # the bias weights\\n                    tf_bias = dataset[\\'B\\'][...]\\n                    # tensorflow adds 1.0 to forget gate bias instead of modifying the\\n                    # parameters...\\n                    tf_bias[(2 * cell_size):(3 * cell_size)] += 1\\n                    torch_bias = tf_bias.copy()\\n                    torch_bias[(1 * cell_size):(2 * cell_size)\\n                              ] = tf_bias[(2 * cell_size):(3 * cell_size)]\\n                    torch_bias[(2 * cell_size):(3 * cell_size)\\n                              ] = tf_bias[(1 * cell_size):(2 * cell_size)]\\n                    lstm.state_linearity.bias.data.copy_(torch.FloatTensor(torch_bias))\\n                    lstm.state_linearity.bias.requires_grad = requires_grad\\n\\n                    # the projection weights\\n                    proj_weights = numpy.transpose(dataset[\\'W_P_0\\'][...])\\n                    lstm.state_projection.weight.data.copy_(torch.FloatTensor(proj_weights))\\n                    lstm.state_projection.weight.requires_grad = requires_grad',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'main',\n",
       "  'docstring': 'Creates ELMo word representations from a vocabulary file. These\\n    word representations are _independent_ - they are the result of running\\n    the CNN and Highway layers of the ELMo model, but not the Bidirectional LSTM.\\n    ELMo requires 2 additional tokens: <S> and </S>. The first token\\n    in this file is assumed to be an unknown token.\\n\\n    This script produces two artifacts: A new vocabulary file\\n    with the <S> and </S> tokens inserted and a glove formatted embedding\\n    file containing word : vector pairs, one per line, with all values\\n    separated by a space.',\n",
       "  'code': 'def main(vocab_path: str,\\n         elmo_config_path: str,\\n         elmo_weights_path: str,\\n         output_dir: str,\\n         batch_size: int,\\n         device: int,\\n         use_custom_oov_token: bool = False):\\n    \"\"\"\\n    Creates ELMo word representations from a vocabulary file. These\\n    word representations are _independent_ - they are the result of running\\n    the CNN and Highway layers of the ELMo model, but not the Bidirectional LSTM.\\n    ELMo requires 2 additional tokens: <S> and </S>. The first token\\n    in this file is assumed to be an unknown token.\\n\\n    This script produces two artifacts: A new vocabulary file\\n    with the <S> and </S> tokens inserted and a glove formatted embedding\\n    file containing word : vector pairs, one per line, with all values\\n    separated by a space.\\n    \"\"\"\\n\\n    # Load the vocabulary words and convert to char ids\\n    with open(vocab_path, \\'r\\') as vocab_file:\\n        tokens = vocab_file.read().strip().split(\\'\\\\n\\')\\n\\n    # Insert the sentence boundary tokens which elmo uses at positions 1 and 2.\\n    if tokens[0] != DEFAULT_OOV_TOKEN and not use_custom_oov_token:\\n        raise ConfigurationError(\"ELMo embeddings require the use of a OOV token.\")\\n\\n    tokens = [tokens[0]] + [\"<S>\", \"</S>\"] + tokens[1:]\\n\\n    indexer = ELMoTokenCharactersIndexer()\\n    indices = indexer.tokens_to_indices([Token(token) for token in tokens], Vocabulary(), \"indices\")[\"indices\"]\\n    sentences = []\\n    for k in range((len(indices) // 50) + 1):\\n        sentences.append(indexer.pad_token_sequence(indices[(k * 50):((k + 1) * 50)],\\n                                                    desired_num_tokens=50,\\n                                                    padding_lengths={}))\\n\\n    last_batch_remainder = 50 - (len(indices) % 50)\\n    if device != -1:\\n        elmo_token_embedder = _ElmoCharacterEncoder(elmo_config_path,\\n                                                    elmo_weights_path).cuda(device)\\n    else:\\n        elmo_token_embedder = _ElmoCharacterEncoder(elmo_config_path,\\n                                                    elmo_weights_path)\\n\\n    all_embeddings = []\\n    for i in range((len(sentences) // batch_size) + 1):\\n        array = numpy.array(sentences[i * batch_size: (i + 1) * batch_size])\\n        if device != -1:\\n            batch = torch.from_numpy(array).cuda(device)\\n        else:\\n            batch = torch.from_numpy(array)\\n\\n        token_embedding = elmo_token_embedder(batch)[\\'token_embedding\\'].data\\n\\n        # Reshape back to a list of words of shape (batch_size * 50, encoding_dim)\\n        # We also need to remove the <S>, </S> tokens appended by the encoder.\\n        per_word_embeddings = token_embedding[:, 1:-1, :].contiguous().view(-1, token_embedding.size(-1))\\n\\n        all_embeddings.append(per_word_embeddings)\\n\\n    # Remove the embeddings associated with padding in the last batch.\\n    all_embeddings[-1] = all_embeddings[-1][:-last_batch_remainder, :]\\n\\n    embedding_weight = torch.cat(all_embeddings, 0).cpu().numpy()\\n\\n    # Write out the embedding in a glove format.\\n    os.makedirs(output_dir, exist_ok=True)\\n    with gzip.open(os.path.join(output_dir, \"elmo_embeddings.txt.gz\"), \\'wb\\') as embeddings_file:\\n        for i, word in enumerate(tokens):\\n            string_array = \" \".join([str(x) for x in list(embedding_weight[i, :])])\\n            embeddings_file.write(f\"{word} {string_array}\\\\n\".encode(\\'utf-8\\'))\\n\\n    # Write out the new vocab with the <S> and </S> tokens.\\n    _, vocab_file_name = os.path.split(vocab_path)\\n    with open(os.path.join(output_dir, vocab_file_name), \"w\") as new_vocab_file:\\n        for word in tokens:\\n            new_vocab_file.write(f\"{word}\\\\n\")',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'decode_mst',\n",
       "  'docstring': 'Note: Counter to typical intuition, this function decodes the _maximum_\\n    spanning tree.\\n\\n    Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for\\n    maximum spanning arborescences on graphs.\\n\\n    Parameters\\n    ----------\\n    energy : ``numpy.ndarray``, required.\\n        A tensor with shape (num_labels, timesteps, timesteps)\\n        containing the energy of each edge. If has_labels is ``False``,\\n        the tensor should have shape (timesteps, timesteps) instead.\\n    length : ``int``, required.\\n        The length of this sequence, as the energy may have come\\n        from a padded batch.\\n    has_labels : ``bool``, optional, (default = True)\\n        Whether the graph has labels or not.',\n",
       "  'code': 'def decode_mst(energy: numpy.ndarray,\\n               length: int,\\n               has_labels: bool = True) -> Tuple[numpy.ndarray, numpy.ndarray]:\\n    \"\"\"\\n    Note: Counter to typical intuition, this function decodes the _maximum_\\n    spanning tree.\\n\\n    Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for\\n    maximum spanning arborescences on graphs.\\n\\n    Parameters\\n    ----------\\n    energy : ``numpy.ndarray``, required.\\n        A tensor with shape (num_labels, timesteps, timesteps)\\n        containing the energy of each edge. If has_labels is ``False``,\\n        the tensor should have shape (timesteps, timesteps) instead.\\n    length : ``int``, required.\\n        The length of this sequence, as the energy may have come\\n        from a padded batch.\\n    has_labels : ``bool``, optional, (default = True)\\n        Whether the graph has labels or not.\\n    \"\"\"\\n    if has_labels and energy.ndim != 3:\\n        raise ConfigurationError(\"The dimension of the energy array is not equal to 3.\")\\n    elif not has_labels and energy.ndim != 2:\\n        raise ConfigurationError(\"The dimension of the energy array is not equal to 2.\")\\n    input_shape = energy.shape\\n    max_length = input_shape[-1]\\n\\n    # Our energy matrix might have been batched -\\n    # here we clip it to contain only non padded tokens.\\n    if has_labels:\\n        energy = energy[:, :length, :length]\\n        # get best label for each edge.\\n        label_id_matrix = energy.argmax(axis=0)\\n        energy = energy.max(axis=0)\\n    else:\\n        energy = energy[:length, :length]\\n        label_id_matrix = None\\n    # get original score matrix\\n    original_score_matrix = energy\\n    # initialize score matrix to original score matrix\\n    score_matrix = numpy.array(original_score_matrix, copy=True)\\n\\n    old_input = numpy.zeros([length, length], dtype=numpy.int32)\\n    old_output = numpy.zeros([length, length], dtype=numpy.int32)\\n    current_nodes = [True for _ in range(length)]\\n    representatives: List[Set[int]] = []\\n\\n    for node1 in range(length):\\n        original_score_matrix[node1, node1] = 0.0\\n        score_matrix[node1, node1] = 0.0\\n        representatives.append({node1})\\n\\n        for node2 in range(node1 + 1, length):\\n            old_input[node1, node2] = node1\\n            old_output[node1, node2] = node2\\n\\n            old_input[node2, node1] = node2\\n            old_output[node2, node1] = node1\\n\\n    final_edges: Dict[int, int] = {}\\n\\n    # The main algorithm operates inplace.\\n    chu_liu_edmonds(length, score_matrix, current_nodes,\\n                    final_edges, old_input, old_output, representatives)\\n\\n    heads = numpy.zeros([max_length], numpy.int32)\\n    if has_labels:\\n        head_type = numpy.ones([max_length], numpy.int32)\\n    else:\\n        head_type = None\\n\\n    for child, parent in final_edges.items():\\n        heads[child] = parent\\n        if has_labels:\\n            head_type[child] = label_id_matrix[parent, child]\\n\\n    return heads, head_type',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'chu_liu_edmonds',\n",
       "  'docstring': \"Applies the chu-liu-edmonds algorithm recursively\\n    to a graph with edge weights defined by score_matrix.\\n\\n    Note that this function operates in place, so variables\\n    will be modified.\\n\\n    Parameters\\n    ----------\\n    length : ``int``, required.\\n        The number of nodes.\\n    score_matrix : ``numpy.ndarray``, required.\\n        The score matrix representing the scores for pairs\\n        of nodes.\\n    current_nodes : ``List[bool]``, required.\\n        The nodes which are representatives in the graph.\\n        A representative at it's most basic represents a node,\\n        but as the algorithm progresses, individual nodes will\\n        represent collapsed cycles in the graph.\\n    final_edges: ``Dict[int, int]``, required.\\n        An empty dictionary which will be populated with the\\n        nodes which are connected in the maximum spanning tree.\\n    old_input: ``numpy.ndarray``, required.\\n    old_output: ``numpy.ndarray``, required.\\n    representatives : ``List[Set[int]]``, required.\\n        A list containing the nodes that a particular node\\n        is representing at this iteration in the graph.\\n\\n    Returns\\n    -------\\n    Nothing - all variables are modified in place.\",\n",
       "  'code': 'def chu_liu_edmonds(length: int,\\n                    score_matrix: numpy.ndarray,\\n                    current_nodes: List[bool],\\n                    final_edges: Dict[int, int],\\n                    old_input: numpy.ndarray,\\n                    old_output: numpy.ndarray,\\n                    representatives: List[Set[int]]):\\n    \"\"\"\\n    Applies the chu-liu-edmonds algorithm recursively\\n    to a graph with edge weights defined by score_matrix.\\n\\n    Note that this function operates in place, so variables\\n    will be modified.\\n\\n    Parameters\\n    ----------\\n    length : ``int``, required.\\n        The number of nodes.\\n    score_matrix : ``numpy.ndarray``, required.\\n        The score matrix representing the scores for pairs\\n        of nodes.\\n    current_nodes : ``List[bool]``, required.\\n        The nodes which are representatives in the graph.\\n        A representative at it\\'s most basic represents a node,\\n        but as the algorithm progresses, individual nodes will\\n        represent collapsed cycles in the graph.\\n    final_edges: ``Dict[int, int]``, required.\\n        An empty dictionary which will be populated with the\\n        nodes which are connected in the maximum spanning tree.\\n    old_input: ``numpy.ndarray``, required.\\n    old_output: ``numpy.ndarray``, required.\\n    representatives : ``List[Set[int]]``, required.\\n        A list containing the nodes that a particular node\\n        is representing at this iteration in the graph.\\n\\n    Returns\\n    -------\\n    Nothing - all variables are modified in place.\\n\\n    \"\"\"\\n    # Set the initial graph to be the greedy best one.\\n    parents = [-1]\\n    for node1 in range(1, length):\\n        parents.append(0)\\n        if current_nodes[node1]:\\n            max_score = score_matrix[0, node1]\\n            for node2 in range(1, length):\\n                if node2 == node1 or not current_nodes[node2]:\\n                    continue\\n\\n                new_score = score_matrix[node2, node1]\\n                if new_score > max_score:\\n                    max_score = new_score\\n                    parents[node1] = node2\\n\\n    # Check if this solution has a cycle.\\n    has_cycle, cycle = _find_cycle(parents, length, current_nodes)\\n    # If there are no cycles, find all edges and return.\\n    if not has_cycle:\\n        final_edges[0] = -1\\n        for node in range(1, length):\\n            if not current_nodes[node]:\\n                continue\\n\\n            parent = old_input[parents[node], node]\\n            child = old_output[parents[node], node]\\n            final_edges[child] = parent\\n        return\\n\\n    # Otherwise, we have a cycle so we need to remove an edge.\\n    # From here until the recursive call is the contraction stage of the algorithm.\\n    cycle_weight = 0.0\\n    # Find the weight of the cycle.\\n    index = 0\\n    for node in cycle:\\n        index += 1\\n        cycle_weight += score_matrix[parents[node], node]\\n\\n    # For each node in the graph, find the maximum weight incoming\\n    # and outgoing edge into the cycle.\\n    cycle_representative = cycle[0]\\n    for node in range(length):\\n        if not current_nodes[node] or node in cycle:\\n            continue\\n\\n        in_edge_weight = float(\"-inf\")\\n        in_edge = -1\\n        out_edge_weight = float(\"-inf\")\\n        out_edge = -1\\n\\n        for node_in_cycle in cycle:\\n            if score_matrix[node_in_cycle, node] > in_edge_weight:\\n                in_edge_weight = score_matrix[node_in_cycle, node]\\n                in_edge = node_in_cycle\\n\\n            # Add the new edge score to the cycle weight\\n            # and subtract the edge we\\'re considering removing.\\n            score = (cycle_weight +\\n                     score_matrix[node, node_in_cycle] -\\n                     score_matrix[parents[node_in_cycle], node_in_cycle])\\n\\n            if score > out_edge_weight:\\n                out_edge_weight = score\\n                out_edge = node_in_cycle\\n\\n        score_matrix[cycle_representative, node] = in_edge_weight\\n        old_input[cycle_representative, node] = old_input[in_edge, node]\\n        old_output[cycle_representative, node] = old_output[in_edge, node]\\n\\n        score_matrix[node, cycle_representative] = out_edge_weight\\n        old_output[node, cycle_representative] = old_output[node, out_edge]\\n        old_input[node, cycle_representative] = old_input[node, out_edge]\\n\\n    # For the next recursive iteration, we want to consider the cycle as a\\n    # single node. Here we collapse the cycle into the first node in the\\n    # cycle (first node is arbitrary), set all the other nodes not be\\n    # considered in the next iteration. We also keep track of which\\n    # representatives we are considering this iteration because we need\\n    # them below to check if we\\'re done.\\n    considered_representatives: List[Set[int]] = []\\n    for i, node_in_cycle in enumerate(cycle):\\n        considered_representatives.append(set())\\n        if i > 0:\\n            # We need to consider at least one\\n            # node in the cycle, arbitrarily choose\\n            # the first.\\n            current_nodes[node_in_cycle] = False\\n\\n        for node in representatives[node_in_cycle]:\\n            considered_representatives[i].add(node)\\n            if i > 0:\\n                representatives[cycle_representative].add(node)\\n\\n    chu_liu_edmonds(length, score_matrix, current_nodes, final_edges, old_input, old_output, representatives)\\n\\n    # Expansion stage.\\n    # check each node in cycle, if one of its representatives\\n    # is a key in the final_edges, it is the one we need.\\n    found = False\\n    key_node = -1\\n    for i, node in enumerate(cycle):\\n        for cycle_rep in considered_representatives[i]:\\n            if cycle_rep in final_edges:\\n                key_node = node\\n                found = True\\n                break\\n        if found:\\n            break\\n\\n    previous = parents[key_node]\\n    while previous != key_node:\\n        child = old_output[parents[previous], previous]\\n        parent = old_input[parents[previous], previous]\\n        final_edges[child] = parent\\n        previous = parents[previous]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ExpectedRiskMinimization._prune_beam',\n",
       "  'docstring': 'This method can be used to prune the set of unfinished states on a beam or finished states\\n        at the end of search. In the former case, the states need not be sorted because the all come\\n        from the same decoding step, which does the sorting. However, if the states are finished and\\n        this method is called at the end of the search, they need to be sorted because they come\\n        from different decoding steps.',\n",
       "  'code': 'def _prune_beam(states: List[State],\\n                    beam_size: int,\\n                    sort_states: bool = False) -> List[State]:\\n        \"\"\"\\n        This method can be used to prune the set of unfinished states on a beam or finished states\\n        at the end of search. In the former case, the states need not be sorted because the all come\\n        from the same decoding step, which does the sorting. However, if the states are finished and\\n        this method is called at the end of the search, they need to be sorted because they come\\n        from different decoding steps.\\n        \"\"\"\\n        states_by_batch_index: Dict[int, List[State]] = defaultdict(list)\\n        for state in states:\\n            assert len(state.batch_indices) == 1\\n            batch_index = state.batch_indices[0]\\n            states_by_batch_index[batch_index].append(state)\\n        pruned_states = []\\n        for _, instance_states in states_by_batch_index.items():\\n            if sort_states:\\n                scores = torch.cat([state.score[0].view(-1) for state in instance_states])\\n                _, sorted_indices = scores.sort(-1, descending=True)\\n                sorted_states = [instance_states[i] for i in sorted_indices.detach().cpu().numpy()]\\n                instance_states = sorted_states\\n            for state in instance_states[:beam_size]:\\n                pruned_states.append(state)\\n        return pruned_states',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_read_embeddings_from_text_file',\n",
       "  'docstring': 'Read pre-trained word vectors from an eventually compressed text file, possibly contained\\n    inside an archive with multiple files. The text file is assumed to be utf-8 encoded with\\n    space-separated fields: [word] [dim 1] [dim 2] ...\\n\\n    Lines that contain more numerical tokens than ``embedding_dim`` raise a warning and are skipped.\\n\\n    The remainder of the docstring is identical to ``_read_pretrained_embeddings_file``.',\n",
       "  'code': 'def _read_embeddings_from_text_file(file_uri: str,\\n                                    embedding_dim: int,\\n                                    vocab: Vocabulary,\\n                                    namespace: str = \"tokens\") -> torch.FloatTensor:\\n    \"\"\"\\n    Read pre-trained word vectors from an eventually compressed text file, possibly contained\\n    inside an archive with multiple files. The text file is assumed to be utf-8 encoded with\\n    space-separated fields: [word] [dim 1] [dim 2] ...\\n\\n    Lines that contain more numerical tokens than ``embedding_dim`` raise a warning and are skipped.\\n\\n    The remainder of the docstring is identical to ``_read_pretrained_embeddings_file``.\\n    \"\"\"\\n    tokens_to_keep = set(vocab.get_index_to_token_vocabulary(namespace).values())\\n    vocab_size = vocab.get_vocab_size(namespace)\\n    embeddings = {}\\n\\n    # First we read the embeddings from the file, only keeping vectors for the words we need.\\n    logger.info(\"Reading pretrained embeddings from file\")\\n\\n    with EmbeddingsTextFile(file_uri) as embeddings_file:\\n        for line in Tqdm.tqdm(embeddings_file):\\n            token = line.split(\\' \\', 1)[0]\\n            if token in tokens_to_keep:\\n                fields = line.rstrip().split(\\' \\')\\n                if len(fields) - 1 != embedding_dim:\\n                    # Sometimes there are funny unicode parsing problems that lead to different\\n                    # fields lengths (e.g., a word with a unicode space character that splits\\n                    # into more than one column).  We skip those lines.  Note that if you have\\n                    # some kind of long header, this could result in all of your lines getting\\n                    # skipped.  It\\'s hard to check for that here; you just have to look in the\\n                    # embedding_misses_file and at the model summary to make sure things look\\n                    # like they are supposed to.\\n                    logger.warning(\"Found line with wrong number of dimensions (expected: %d; actual: %d): %s\",\\n                                   embedding_dim, len(fields) - 1, line)\\n                    continue\\n\\n                vector = numpy.asarray(fields[1:], dtype=\\'float32\\')\\n                embeddings[token] = vector\\n\\n    if not embeddings:\\n        raise ConfigurationError(\"No embeddings of correct dimension found; you probably \"\\n                                 \"misspecified your embedding_dim parameter, or didn\\'t \"\\n                                 \"pre-populate your Vocabulary\")\\n\\n    all_embeddings = numpy.asarray(list(embeddings.values()))\\n    embeddings_mean = float(numpy.mean(all_embeddings))\\n    embeddings_std = float(numpy.std(all_embeddings))\\n    # Now we initialize the weight matrix for an embedding layer, starting with random vectors,\\n    # then filling in the word vectors we just read.\\n    logger.info(\"Initializing pre-trained embedding layer\")\\n    embedding_matrix = torch.FloatTensor(vocab_size, embedding_dim).normal_(embeddings_mean,\\n                                                                            embeddings_std)\\n    num_tokens_found = 0\\n    index_to_token = vocab.get_index_to_token_vocabulary(namespace)\\n    for i in range(vocab_size):\\n        token = index_to_token[i]\\n\\n        # If we don\\'t have a pre-trained vector for this word, we\\'ll just leave this row alone,\\n        # so the word has a random initialization.\\n        if token in embeddings:\\n            embedding_matrix[i] = torch.FloatTensor(embeddings[token])\\n            num_tokens_found += 1\\n        else:\\n            logger.debug(\"Token %s was not found in the embedding file. Initialising randomly.\", token)\\n\\n    logger.info(\"Pretrained embeddings were found for %d out of %d tokens\",\\n                num_tokens_found, vocab_size)\\n\\n    return embedding_matrix',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Backend.run_node',\n",
       "  'docstring': 'Simple run one operator and return the results.\\n        Args:\\n            outputs_info: a list of tuples, which contains the element type and\\n            shape of each output. First element of the tuple is the dtype, and\\n            the second element is the shape. More use case can be found in\\n            https://github.com/onnx/onnx/blob/master/onnx/backend/test/runner/__init__.py',\n",
       "  'code': \"def run_node(cls,\\n                 node,  # type: NodeProto\\n                 inputs,  # type: Any\\n                 device='CPU',  # type: Text\\n                 outputs_info=None,  # type: Optional[Sequence[Tuple[numpy.dtype, Tuple[int, ...]]]]\\n                 **kwargs  # type: Dict[Text, Any]\\n                 ):  # type: (...) -> Optional[Tuple[Any, ...]]\\n        '''Simple run one operator and return the results.\\n        Args:\\n            outputs_info: a list of tuples, which contains the element type and\\n            shape of each output. First element of the tuple is the dtype, and\\n            the second element is the shape. More use case can be found in\\n            https://github.com/onnx/onnx/blob/master/onnx/backend/test/runner/__init__.py\\n        '''\\n        # TODO Remove Optional from return type\\n        if 'opset_version' in kwargs:\\n            special_context = c_checker.CheckerContext()\\n            special_context.ir_version = IR_VERSION\\n            special_context.opset_imports = {'': kwargs['opset_version']}  # type: ignore\\n            onnx.checker.check_node(node, special_context)\\n        else:\\n            onnx.checker.check_node(node)\\n        return None\",\n",
       "  'language': 'python'},\n",
       " {'function_name': 'to_array',\n",
       "  'docstring': 'Converts a tensor def object to a numpy array.\\n\\n    Inputs:\\n        tensor: a TensorProto object.\\n    Returns:\\n        arr: the converted array.',\n",
       "  'code': 'def to_array(tensor):  # type: (TensorProto) -> np.ndarray[Any]\\n    \"\"\"Converts a tensor def object to a numpy array.\\n\\n    Inputs:\\n        tensor: a TensorProto object.\\n    Returns:\\n        arr: the converted array.\\n    \"\"\"\\n    if tensor.HasField(\"segment\"):\\n        raise ValueError(\\n            \"Currently not supporting loading segments.\")\\n    if tensor.data_type == TensorProto.UNDEFINED:\\n        raise ValueError(\"The data type is not defined.\")\\n\\n    tensor_dtype = tensor.data_type\\n    np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[tensor_dtype]\\n    storage_type = mapping.TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE[tensor_dtype]\\n    storage_np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[storage_type]\\n    storage_field = mapping.STORAGE_TENSOR_TYPE_TO_FIELD[storage_type]\\n    dims = tensor.dims\\n\\n    if tensor.data_type == TensorProto.STRING:\\n        utf8_strings = getattr(tensor, storage_field)\\n        ss = list(s.decode(\\'utf-8\\') for s in utf8_strings)\\n        return np.asarray(ss).astype(np_dtype).reshape(dims)\\n\\n    if tensor.HasField(\"raw_data\"):\\n        # Raw_bytes support: using frombuffer.\\n        return np.frombuffer(\\n            tensor.raw_data,\\n            dtype=np_dtype).reshape(dims)\\n    else:\\n        data = getattr(tensor, storage_field),  # type: Sequence[np.complex64]\\n        if (tensor_dtype == TensorProto.COMPLEX64\\n                or tensor_dtype == TensorProto.COMPLEX128):\\n            data = combine_pairs_to_complex(data)\\n        return (\\n            np.asarray(\\n                data,\\n                dtype=storage_np_dtype)\\n            .astype(np_dtype)\\n            .reshape(dims)\\n        )',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'from_array',\n",
       "  'docstring': 'Converts a numpy array to a tensor def.\\n\\n    Inputs:\\n        arr: a numpy array.\\n        name: (optional) the name of the tensor.\\n    Returns:\\n        tensor_def: the converted tensor def.',\n",
       "  'code': 'def from_array(arr, name=None):  # type: (np.ndarray[Any], Optional[Text]) -> TensorProto\\n    \"\"\"Converts a numpy array to a tensor def.\\n\\n    Inputs:\\n        arr: a numpy array.\\n        name: (optional) the name of the tensor.\\n    Returns:\\n        tensor_def: the converted tensor def.\\n    \"\"\"\\n    tensor = TensorProto()\\n    tensor.dims.extend(arr.shape)\\n    if name:\\n        tensor.name = name\\n\\n    if arr.dtype == np.object:\\n        # Special care for strings.\\n        tensor.data_type = mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype]\\n        # TODO: Introduce full string support.\\n        # We flatten the array in case there are 2-D arrays are specified\\n        # We throw the error below if we have a 3-D array or some kind of other\\n        # object. If you want more complex shapes then follow the below instructions.\\n        # Unlike other types where the shape is automatically inferred from\\n        # nested arrays of values, the only reliable way now to feed strings\\n        # is to put them into a flat array then specify type astype(np.object)\\n        # (otherwise all strings may have different types depending on their length)\\n        # and then specify shape .reshape([x, y, z])\\n        flat_array = arr.flatten()\\n        for e in flat_array:\\n            if isinstance(e, text_type):\\n                tensor.string_data.append(e.encode(\\'utf-8\\'))\\n            elif isinstance(e, np.ndarray):\\n                for s in e:\\n                    if isinstance(s, text_type):\\n                        tensor.string_data.append(s.encode(\\'utf-8\\'))\\n            else:\\n                raise NotImplementedError(\\n                    \"Unrecognized object in the object array, expect a string, or array of bytes: \", str(type(e)))\\n        return tensor\\n\\n    # For numerical types, directly use numpy raw bytes.\\n    try:\\n        dtype = mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype]\\n    except KeyError:\\n        raise RuntimeError(\\n            \"Numpy data type not understood yet: {}\".format(str(arr.dtype)))\\n    tensor.data_type = dtype\\n    tensor.raw_data = arr.tobytes()  # note: tobytes() is only after 1.9.\\n\\n    return tensor',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'get_input_shape',\n",
       "  'docstring': 'Helper function to obtain the shape of an array',\n",
       "  'code': 'def get_input_shape(sym, proto_obj):\\n    \"\"\"Helper function to obtain the shape of an array\"\"\"\\n    arg_params = proto_obj.arg_dict\\n    aux_params = proto_obj.aux_dict\\n\\n    model_input_shape = [data[1] for data  in proto_obj.model_metadata.get(\\'input_tensor_data\\')]\\n    data_names = [data[0] for data  in proto_obj.model_metadata.get(\\'input_tensor_data\\')]\\n\\n    # creating dummy inputs\\n    inputs = []\\n    for  in_shape in model_input_shape:\\n        inputs.append(nd.ones(shape=in_shape))\\n\\n    data_shapes = []\\n    for idx, input_name in enumerate(data_names):\\n        data_shapes.append((input_name, inputs[idx].shape))\\n\\n    ctx = context.cpu()\\n    # create a module\\n    mod = module.Module(symbol=sym, data_names=data_names, context=ctx, label_names=None)\\n    mod.bind(for_training=False, data_shapes=data_shapes, label_shapes=None)\\n    mod.set_params(arg_params=arg_params, aux_params=aux_params)\\n\\n    data_forward = []\\n    for idx, input_name in enumerate(data_names):\\n        val = inputs[idx]\\n        data_forward.append(val)\\n\\n    mod.forward(io.DataBatch(data_forward))\\n    result = mod.get_outputs()[0].asnumpy()\\n\\n    return result.shape',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'imdecode',\n",
       "  'docstring': 'Decode an image to an NDArray.\\n\\n    .. note:: `imdecode` uses OpenCV (not the CV2 Python library).\\n       MXNet must have been built with USE_OPENCV=1 for `imdecode` to work.\\n\\n    Parameters\\n    ----------\\n    buf : str/bytes/bytearray or numpy.ndarray\\n        Binary image data as string or numpy ndarray.\\n    flag : int, optional, default=1\\n        1 for three channel color output. 0 for grayscale output.\\n    to_rgb : int, optional, default=1\\n        1 for RGB formatted output (MXNet default). 0 for BGR formatted output (OpenCV default).\\n    out : NDArray, optional\\n        Output buffer. Use `None` for automatic allocation.\\n\\n    Returns\\n    -------\\n    NDArray\\n        An `NDArray` containing the image.\\n\\n    Example\\n    -------\\n    >>> with open(\"flower.jpg\", \\'rb\\') as fp:\\n    ...     str_image = fp.read()\\n    ...\\n    >>> image = mx.img.imdecode(str_image)\\n    >>> image\\n    <NDArray 224x224x3 @cpu(0)>\\n\\n    Set `flag` parameter to 0 to get grayscale output\\n\\n    >>> with open(\"flower.jpg\", \\'rb\\') as fp:\\n    ...     str_image = fp.read()\\n    ...\\n    >>> image = mx.img.imdecode(str_image, flag=0)\\n    >>> image\\n    <NDArray 224x224x1 @cpu(0)>\\n\\n    Set `to_rgb` parameter to 0 to get output in OpenCV format (BGR)\\n\\n    >>> with open(\"flower.jpg\", \\'rb\\') as fp:\\n    ...     str_image = fp.read()\\n    ...\\n    >>> image = mx.img.imdecode(str_image, to_rgb=0)\\n    >>> image\\n    <NDArray 224x224x3 @cpu(0)>',\n",
       "  'code': 'def imdecode(buf, *args, **kwargs):\\n    \"\"\"Decode an image to an NDArray.\\n\\n    .. note:: `imdecode` uses OpenCV (not the CV2 Python library).\\n       MXNet must have been built with USE_OPENCV=1 for `imdecode` to work.\\n\\n    Parameters\\n    ----------\\n    buf : str/bytes/bytearray or numpy.ndarray\\n        Binary image data as string or numpy ndarray.\\n    flag : int, optional, default=1\\n        1 for three channel color output. 0 for grayscale output.\\n    to_rgb : int, optional, default=1\\n        1 for RGB formatted output (MXNet default). 0 for BGR formatted output (OpenCV default).\\n    out : NDArray, optional\\n        Output buffer. Use `None` for automatic allocation.\\n\\n    Returns\\n    -------\\n    NDArray\\n        An `NDArray` containing the image.\\n\\n    Example\\n    -------\\n    >>> with open(\"flower.jpg\", \\'rb\\') as fp:\\n    ...     str_image = fp.read()\\n    ...\\n    >>> image = mx.img.imdecode(str_image)\\n    >>> image\\n    <NDArray 224x224x3 @cpu(0)>\\n\\n    Set `flag` parameter to 0 to get grayscale output\\n\\n    >>> with open(\"flower.jpg\", \\'rb\\') as fp:\\n    ...     str_image = fp.read()\\n    ...\\n    >>> image = mx.img.imdecode(str_image, flag=0)\\n    >>> image\\n    <NDArray 224x224x1 @cpu(0)>\\n\\n    Set `to_rgb` parameter to 0 to get output in OpenCV format (BGR)\\n\\n    >>> with open(\"flower.jpg\", \\'rb\\') as fp:\\n    ...     str_image = fp.read()\\n    ...\\n    >>> image = mx.img.imdecode(str_image, to_rgb=0)\\n    >>> image\\n    <NDArray 224x224x3 @cpu(0)>\\n    \"\"\"\\n    if not isinstance(buf, nd.NDArray):\\n        if sys.version_info[0] == 3 and not isinstance(buf, (bytes, bytearray, np.ndarray)):\\n            raise ValueError(\\'buf must be of type bytes, bytearray or numpy.ndarray,\\'\\n                             \\'if you would like to input type str, please convert to bytes\\')\\n        buf = nd.array(np.frombuffer(buf, dtype=np.uint8), dtype=np.uint8)\\n\\n    return _internal._cvimdecode(buf, *args, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'RandCropper.sample',\n",
       "  'docstring': 'generate random cropping boxes according to parameters\\n        if satifactory crops generated, apply to ground-truth as well\\n\\n        Parameters:\\n        ----------\\n        label : numpy.array (n x 5 matrix)\\n            ground-truths\\n\\n        Returns:\\n        ----------\\n        list of (crop_box, label) tuples, if failed, return empty list []',\n",
       "  'code': 'def sample(self, label):\\n        \"\"\"\\n        generate random cropping boxes according to parameters\\n        if satifactory crops generated, apply to ground-truth as well\\n\\n        Parameters:\\n        ----------\\n        label : numpy.array (n x 5 matrix)\\n            ground-truths\\n\\n        Returns:\\n        ----------\\n        list of (crop_box, label) tuples, if failed, return empty list []\\n        \"\"\"\\n        samples = []\\n        count = 0\\n        for trial in range(self.max_trials):\\n            if count >= self.max_sample:\\n                return samples\\n            scale = np.random.uniform(self.min_scale, self.max_scale)\\n            min_ratio = max(self.min_aspect_ratio, scale * scale)\\n            max_ratio = min(self.max_aspect_ratio, 1. / scale / scale)\\n            ratio = math.sqrt(np.random.uniform(min_ratio, max_ratio))\\n            width = scale * ratio\\n            height = scale / ratio\\n            left = np.random.uniform(0., 1 - width)\\n            top = np.random.uniform(0., 1 - height)\\n            rand_box = (left, top, left + width, top + height)\\n            valid_mask = np.where(label[:, 0] > -1)[0]\\n            gt = label[valid_mask, :]\\n            ious = self._check_satisfy(rand_box, gt)\\n            if ious is not None:\\n                # transform gt labels after crop, discard bad ones\\n                l, t, r, b = rand_box\\n                new_gt_boxes = []\\n                new_width = r - l\\n                new_height = b - t\\n                for i in range(valid_mask.size):\\n                    if ious[i] > 0:\\n                        xmin = max(0., (gt[i, 1] - l) / new_width)\\n                        ymin = max(0., (gt[i, 2] - t) / new_height)\\n                        xmax = min(1., (gt[i, 3] - l) / new_width)\\n                        ymax = min(1., (gt[i, 4] - t) / new_height)\\n                        new_gt_boxes.append([gt[i, 0], xmin, ymin, xmax, ymax])\\n                if not new_gt_boxes:\\n                    continue\\n                new_gt_boxes = np.array(new_gt_boxes)\\n                label = np.lib.pad(new_gt_boxes,\\n                    ((0, label.shape[0]-new_gt_boxes.shape[0]), (0,0)), \\\\\\n                    \\'constant\\', constant_values=(-1, -1))\\n                samples.append((rand_box, label))\\n                count += 1\\n        return samples',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'RandPadder.sample',\n",
       "  'docstring': 'generate random padding boxes according to parameters\\n        if satifactory padding generated, apply to ground-truth as well\\n\\n        Parameters:\\n        ----------\\n        label : numpy.array (n x 5 matrix)\\n            ground-truths\\n\\n        Returns:\\n        ----------\\n        list of (crop_box, label) tuples, if failed, return empty list []',\n",
       "  'code': 'def sample(self, label):\\n        \"\"\"\\n        generate random padding boxes according to parameters\\n        if satifactory padding generated, apply to ground-truth as well\\n\\n        Parameters:\\n        ----------\\n        label : numpy.array (n x 5 matrix)\\n            ground-truths\\n\\n        Returns:\\n        ----------\\n        list of (crop_box, label) tuples, if failed, return empty list []\\n        \"\"\"\\n        samples = []\\n        count = 0\\n        for trial in range(self.max_trials):\\n            if count >= self.max_sample:\\n                return samples\\n            scale = np.random.uniform(self.min_scale, self.max_scale)\\n            min_ratio = max(self.min_aspect_ratio, scale * scale)\\n            max_ratio = min(self.max_aspect_ratio, 1. / scale / scale)\\n            ratio = math.sqrt(np.random.uniform(min_ratio, max_ratio))\\n            width = scale * ratio\\n            if width < 1:\\n                continue\\n            height = scale / ratio\\n            if height < 1:\\n                continue\\n            left = np.random.uniform(0., 1 - width)\\n            top = np.random.uniform(0., 1 - height)\\n            right = left + width\\n            bot = top + height\\n            rand_box = (left, top, right, bot)\\n            valid_mask = np.where(label[:, 0] > -1)[0]\\n            gt = label[valid_mask, :]\\n            new_gt_boxes = []\\n            for i in range(gt.shape[0]):\\n                xmin = (gt[i, 1] - left) / width\\n                ymin = (gt[i, 2] - top) / height\\n                xmax = (gt[i, 3] - left) / width\\n                ymax = (gt[i, 4] - top) / height\\n                new_size = min(xmax - xmin, ymax - ymin)\\n                if new_size < self.min_gt_scale:\\n                    new_gt_boxes = []\\n                    break\\n                new_gt_boxes.append([gt[i, 0], xmin, ymin, xmax, ymax])\\n            if not new_gt_boxes:\\n                continue\\n            new_gt_boxes = np.array(new_gt_boxes)\\n            label = np.lib.pad(new_gt_boxes,\\n                ((0, label.shape[0]-new_gt_boxes.shape[0]), (0,0)), \\\\\\n                \\'constant\\', constant_values=(-1, -1))\\n            samples.append((rand_box, label))\\n            count += 1\\n        return samples',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'COCO.loadNumpyAnnotations',\n",
       "  'docstring': 'Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\\n        :param  data (numpy.ndarray)\\n        :return: annotations (python nested list)',\n",
       "  'code': 'def loadNumpyAnnotations(self, data):\\n        \"\"\"\\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\\n        :param  data (numpy.ndarray)\\n        :return: annotations (python nested list)\\n        \"\"\"\\n        print(\\'Converting ndarray to lists...\\')\\n        assert(type(data) == np.ndarray)\\n        print(data.shape)\\n        assert(data.shape[1] == 7)\\n        N = data.shape[0]\\n        ann = []\\n        for i in range(N):\\n            if i % 1000000 == 0:\\n                print(\\'{}/{}\\'.format(i,N))\\n            ann += [{\\n                \\'image_id\\'  : int(data[i, 0]),\\n                \\'bbox\\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\\n                \\'score\\' : data[i, 5],\\n                \\'category_id\\': int(data[i, 6]),\\n                }]\\n        return ann',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'COCO.annToRLE',\n",
       "  'docstring': 'Convert annotation which can be polygons, uncompressed RLE to RLE.\\n        :return: binary mask (numpy 2D array)',\n",
       "  'code': 'def annToRLE(self, ann):\\n        \"\"\"\\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\\n        :return: binary mask (numpy 2D array)\\n        \"\"\"\\n        t = self.imgs[ann[\\'image_id\\']]\\n        h, w = t[\\'height\\'], t[\\'width\\']\\n        segm = ann[\\'segmentation\\']\\n        if type(segm) == list:\\n            # polygon -- a single object might consist of multiple parts\\n            # we merge all parts into one mask rle code\\n            # rles = maskUtils.frPyObjects(segm, h, w)\\n            # rle = maskUtils.merge(rles)\\n            raise NotImplementedError(\"maskUtils disabled!\")\\n        elif type(segm[\\'counts\\']) == list:\\n            # uncompressed RLE\\n            # rle = maskUtils.frPyObjects(segm, h, w)\\n            raise NotImplementedError(\"maskUtils disabled!\")\\n        else:\\n            # rle\\n            rle = ann[\\'segmentation\\']\\n        return rle',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'KVStore.init',\n",
       "  'docstring': \"Initializes a single or a sequence of key-value pairs into the store.\\n\\n        For each key, one must `init` it before calling `push` or `pull`.\\n        When multiple workers invoke `init` for the same key, only\\n        the value supplied by worker with rank `0` is used. This function returns\\n        after data has been initialized successfully.\\n\\n        Parameters\\n        ----------\\n        key : str, int, or sequence of str or int\\n            The keys.\\n        value : NDArray, RowSparseNDArray or sequence of NDArray or RowSparseNDArray\\n            Values corresponding to the keys.\\n\\n        Examples\\n        --------\\n        >>> # init a single key-value pair\\n        >>> shape = (2,3)\\n        >>> kv = mx.kv.create('local')\\n        >>> kv.init('3', mx.nd.ones(shape)*2)\\n        >>> a = mx.nd.zeros(shape)\\n        >>> kv.pull('3', out=a)\\n        >>> print a.asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\\n\\n        >>> # init a list of key-value pairs\\n        >>> keys = ['5', '7', '9']\\n        >>> kv.init(keys, [mx.nd.ones(shape)]*len(keys))\\n\\n        >>> # init a row_sparse value\\n        >>> kv.init('4', mx.nd.ones(shape).tostype('row_sparse'))\\n        >>> b = mx.nd.sparse.zeros('row_sparse', shape)\\n        >>> kv.row_sparse_pull('4', row_ids=mx.nd.array([0, 1]), out=b)\\n        >>> print b\\n        <RowSparseNDArray 2x3 @cpu(0)>\",\n",
       "  'code': 'def init(self, key, value):\\n        \"\"\" Initializes a single or a sequence of key-value pairs into the store.\\n\\n        For each key, one must `init` it before calling `push` or `pull`.\\n        When multiple workers invoke `init` for the same key, only\\n        the value supplied by worker with rank `0` is used. This function returns\\n        after data has been initialized successfully.\\n\\n        Parameters\\n        ----------\\n        key : str, int, or sequence of str or int\\n            The keys.\\n        value : NDArray, RowSparseNDArray or sequence of NDArray or RowSparseNDArray\\n            Values corresponding to the keys.\\n\\n        Examples\\n        --------\\n        >>> # init a single key-value pair\\n        >>> shape = (2,3)\\n        >>> kv = mx.kv.create(\\'local\\')\\n        >>> kv.init(\\'3\\', mx.nd.ones(shape)*2)\\n        >>> a = mx.nd.zeros(shape)\\n        >>> kv.pull(\\'3\\', out=a)\\n        >>> print a.asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\\n\\n        >>> # init a list of key-value pairs\\n        >>> keys = [\\'5\\', \\'7\\', \\'9\\']\\n        >>> kv.init(keys, [mx.nd.ones(shape)]*len(keys))\\n\\n        >>> # init a row_sparse value\\n        >>> kv.init(\\'4\\', mx.nd.ones(shape).tostype(\\'row_sparse\\'))\\n        >>> b = mx.nd.sparse.zeros(\\'row_sparse\\', shape)\\n        >>> kv.row_sparse_pull(\\'4\\', row_ids=mx.nd.array([0, 1]), out=b)\\n        >>> print b\\n        <RowSparseNDArray 2x3 @cpu(0)>\\n        \"\"\"\\n        ckeys, cvals, use_str_keys = _ctype_key_value(key, value)\\n        if use_str_keys:\\n            check_call(_LIB.MXKVStoreInitEx(self.handle, mx_uint(len(ckeys)), ckeys, cvals))\\n        else:\\n            check_call(_LIB.MXKVStoreInit(self.handle, mx_uint(len(ckeys)), ckeys, cvals))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'KVStore.push',\n",
       "  'docstring': \"Pushes a single or a sequence of key-value pairs into the store.\\n\\n        This function returns immediately after adding an operator to the engine.\\n        The actual operation is executed asynchronously. If there are consecutive\\n        pushes to the same key, there is no guarantee on the serialization of pushes.\\n        The execution of a push does not guarantee that all previous pushes are\\n        finished.\\n        There is no synchronization between workers.\\n        One can use ``_barrier()`` to sync all workers.\\n\\n        Parameters\\n        ----------\\n        key : str, int, or sequence of str or int\\n            Keys.\\n\\n        value : NDArray, RowSparseNDArray, list of NDArray or RowSparseNDArray,\\n                or list of list of NDArray or RowSparseNDArray\\n            Values corresponding to the keys.\\n\\n        priority : int, optional\\n            The priority of the push operation.\\n            Higher priority push operations are likely to be executed before\\n            other push actions.\\n\\n        Examples\\n        --------\\n        >>> # push a single key-value pair\\n        >>> kv.push('3', mx.nd.ones(shape)*8)\\n        >>> kv.pull('3', out=a) # pull out the value\\n        >>> print a.asnumpy()\\n        [[ 8.  8.  8.]\\n        [ 8.  8.  8.]]\\n\\n        >>> # aggregate the value and the push\\n        >>> gpus = [mx.gpu(i) for i in range(4)]\\n        >>> b = [mx.nd.ones(shape, gpu) for gpu in gpus]\\n        >>> kv.push('3', b)\\n        >>> kv.pull('3', out=a)\\n        >>> print a.asnumpy()\\n        [[ 4.  4.  4.]\\n        [ 4.  4.  4.]]\\n\\n        >>> # push a list of keys.\\n        >>> # single device\\n        >>> keys = ['4', '5', '6']\\n        >>> kv.push(keys, [mx.nd.ones(shape)]*len(keys))\\n        >>> b = [mx.nd.zeros(shape)]*len(keys)\\n        >>> kv.pull(keys, out=b)\\n        >>> print b[1].asnumpy()\\n        [[ 1.  1.  1.]\\n        [ 1.  1.  1.]]\\n\\n        >>> # multiple devices:\\n        >>> keys = ['7', '8', '9']\\n        >>> b = [[mx.nd.ones(shape, gpu) for gpu in gpus]] * len(keys)\\n        >>> kv.push(keys, b)\\n        >>> kv.pull(keys, out=b)\\n        >>> print b[1][1].asnumpy()\\n        [[ 4.  4.  4.]\\n        [ 4.  4.  4.]]\\n\\n        >>> # push a row_sparse value\\n        >>> b = mx.nd.sparse.zeros('row_sparse', shape)\\n        >>> kv.init('10', mx.nd.sparse.zeros('row_sparse', shape))\\n        >>> kv.push('10', mx.nd.ones(shape).tostype('row_sparse'))\\n        >>> # pull out the value\\n        >>> kv.row_sparse_pull('10', row_ids=mx.nd.array([0, 1]), out=b)\\n        >>> print b\\n        <RowSparseNDArray 2x3 @cpu(0)>\",\n",
       "  'code': 'def push(self, key, value, priority=0):\\n        \"\"\" Pushes a single or a sequence of key-value pairs into the store.\\n\\n        This function returns immediately after adding an operator to the engine.\\n        The actual operation is executed asynchronously. If there are consecutive\\n        pushes to the same key, there is no guarantee on the serialization of pushes.\\n        The execution of a push does not guarantee that all previous pushes are\\n        finished.\\n        There is no synchronization between workers.\\n        One can use ``_barrier()`` to sync all workers.\\n\\n        Parameters\\n        ----------\\n        key : str, int, or sequence of str or int\\n            Keys.\\n\\n        value : NDArray, RowSparseNDArray, list of NDArray or RowSparseNDArray,\\n                or list of list of NDArray or RowSparseNDArray\\n            Values corresponding to the keys.\\n\\n        priority : int, optional\\n            The priority of the push operation.\\n            Higher priority push operations are likely to be executed before\\n            other push actions.\\n\\n        Examples\\n        --------\\n        >>> # push a single key-value pair\\n        >>> kv.push(\\'3\\', mx.nd.ones(shape)*8)\\n        >>> kv.pull(\\'3\\', out=a) # pull out the value\\n        >>> print a.asnumpy()\\n        [[ 8.  8.  8.]\\n        [ 8.  8.  8.]]\\n\\n        >>> # aggregate the value and the push\\n        >>> gpus = [mx.gpu(i) for i in range(4)]\\n        >>> b = [mx.nd.ones(shape, gpu) for gpu in gpus]\\n        >>> kv.push(\\'3\\', b)\\n        >>> kv.pull(\\'3\\', out=a)\\n        >>> print a.asnumpy()\\n        [[ 4.  4.  4.]\\n        [ 4.  4.  4.]]\\n\\n        >>> # push a list of keys.\\n        >>> # single device\\n        >>> keys = [\\'4\\', \\'5\\', \\'6\\']\\n        >>> kv.push(keys, [mx.nd.ones(shape)]*len(keys))\\n        >>> b = [mx.nd.zeros(shape)]*len(keys)\\n        >>> kv.pull(keys, out=b)\\n        >>> print b[1].asnumpy()\\n        [[ 1.  1.  1.]\\n        [ 1.  1.  1.]]\\n\\n        >>> # multiple devices:\\n        >>> keys = [\\'7\\', \\'8\\', \\'9\\']\\n        >>> b = [[mx.nd.ones(shape, gpu) for gpu in gpus]] * len(keys)\\n        >>> kv.push(keys, b)\\n        >>> kv.pull(keys, out=b)\\n        >>> print b[1][1].asnumpy()\\n        [[ 4.  4.  4.]\\n        [ 4.  4.  4.]]\\n\\n        >>> # push a row_sparse value\\n        >>> b = mx.nd.sparse.zeros(\\'row_sparse\\', shape)\\n        >>> kv.init(\\'10\\', mx.nd.sparse.zeros(\\'row_sparse\\', shape))\\n        >>> kv.push(\\'10\\', mx.nd.ones(shape).tostype(\\'row_sparse\\'))\\n        >>> # pull out the value\\n        >>> kv.row_sparse_pull(\\'10\\', row_ids=mx.nd.array([0, 1]), out=b)\\n        >>> print b\\n        <RowSparseNDArray 2x3 @cpu(0)>\\n        \"\"\"\\n        ckeys, cvals, use_str_keys = _ctype_key_value(key, value)\\n        if use_str_keys:\\n            check_call(_LIB.MXKVStorePushEx(\\n                self.handle, mx_uint(len(ckeys)), ckeys, cvals, ctypes.c_int(priority)))\\n        else:\\n            check_call(_LIB.MXKVStorePush(\\n                self.handle, mx_uint(len(ckeys)), ckeys, cvals, ctypes.c_int(priority)))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'KVStore.pull',\n",
       "  'docstring': \"Pulls a single value or a sequence of values from the store.\\n\\n        This function returns immediately after adding an operator to the engine.\\n        Subsequent attempts to read from the `out` variable will be blocked until the\\n        pull operation completes.\\n\\n        `pull` is executed asynchronously after all previous `pull` calls and only\\n        the last `push` call for the same input key(s) are finished.\\n\\n        The returned values are guaranteed to be the latest values in the store.\\n\\n        pull with `RowSparseNDArray` is not supported for dist kvstore.\\n        Please use ``row_sparse_pull`` instead.\\n\\n        Parameters\\n        ----------\\n        key : str, int, or sequence of str or int\\n            Keys.\\n\\n        out: NDArray or list of NDArray or list of list of NDArray\\n            Values corresponding to the keys.\\n\\n        priority : int, optional\\n            The priority of the pull operation.\\n            Higher priority pull operations are likely to be executed before\\n            other pull actions.\\n\\n        ignore_sparse: bool, optional, default True\\n            Whether to ignore sparse arrays in the request.\\n\\n        Examples\\n        --------\\n        >>> # pull a single key-value pair\\n        >>> a = mx.nd.zeros(shape)\\n        >>> kv.pull('3', out=a)\\n        >>> print a.asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\\n\\n        >>> # pull into multiple devices\\n        >>> b = [mx.nd.ones(shape, gpu) for gpu in gpus]\\n        >>> kv.pull('3', out=b)\\n        >>> print b[1].asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\\n\\n        >>> # pull a list of key-value pairs.\\n        >>> # On single device\\n        >>> keys = ['5', '7', '9']\\n        >>> b = [mx.nd.zeros(shape)]*len(keys)\\n        >>> kv.pull(keys, out=b)\\n        >>> print b[1].asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\\n        >>> # On multiple devices\\n        >>> keys = ['6', '8', '10']\\n        >>> b = [[mx.nd.ones(shape, gpu) for gpu in gpus]] * len(keys)\\n        >>> kv.pull(keys, out=b)\\n        >>> print b[1][1].asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\",\n",
       "  'code': 'def pull(self, key, out=None, priority=0, ignore_sparse=True):\\n        \"\"\" Pulls a single value or a sequence of values from the store.\\n\\n        This function returns immediately after adding an operator to the engine.\\n        Subsequent attempts to read from the `out` variable will be blocked until the\\n        pull operation completes.\\n\\n        `pull` is executed asynchronously after all previous `pull` calls and only\\n        the last `push` call for the same input key(s) are finished.\\n\\n        The returned values are guaranteed to be the latest values in the store.\\n\\n        pull with `RowSparseNDArray` is not supported for dist kvstore.\\n        Please use ``row_sparse_pull`` instead.\\n\\n        Parameters\\n        ----------\\n        key : str, int, or sequence of str or int\\n            Keys.\\n\\n        out: NDArray or list of NDArray or list of list of NDArray\\n            Values corresponding to the keys.\\n\\n        priority : int, optional\\n            The priority of the pull operation.\\n            Higher priority pull operations are likely to be executed before\\n            other pull actions.\\n\\n        ignore_sparse: bool, optional, default True\\n            Whether to ignore sparse arrays in the request.\\n\\n        Examples\\n        --------\\n        >>> # pull a single key-value pair\\n        >>> a = mx.nd.zeros(shape)\\n        >>> kv.pull(\\'3\\', out=a)\\n        >>> print a.asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\\n\\n        >>> # pull into multiple devices\\n        >>> b = [mx.nd.ones(shape, gpu) for gpu in gpus]\\n        >>> kv.pull(\\'3\\', out=b)\\n        >>> print b[1].asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\\n\\n        >>> # pull a list of key-value pairs.\\n        >>> # On single device\\n        >>> keys = [\\'5\\', \\'7\\', \\'9\\']\\n        >>> b = [mx.nd.zeros(shape)]*len(keys)\\n        >>> kv.pull(keys, out=b)\\n        >>> print b[1].asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\\n        >>> # On multiple devices\\n        >>> keys = [\\'6\\', \\'8\\', \\'10\\']\\n        >>> b = [[mx.nd.ones(shape, gpu) for gpu in gpus]] * len(keys)\\n        >>> kv.pull(keys, out=b)\\n        >>> print b[1][1].asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\\n        \"\"\"\\n        assert(out is not None)\\n        ckeys, cvals, use_str_keys = _ctype_key_value(key, out)\\n        if use_str_keys:\\n            check_call(_LIB.MXKVStorePullWithSparseEx(self.handle, mx_uint(len(ckeys)), ckeys,\\n                                                      cvals, ctypes.c_int(priority),\\n                                                      ctypes.c_bool(ignore_sparse)))\\n        else:\\n            check_call(_LIB.MXKVStorePullWithSparse(self.handle, mx_uint(len(ckeys)), ckeys,\\n                                                    cvals, ctypes.c_int(priority),\\n                                                    ctypes.c_bool(ignore_sparse)))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'KVStore.row_sparse_pull',\n",
       "  'docstring': \"Pulls a single RowSparseNDArray value or a sequence of RowSparseNDArray values \\\\\\n        from the store with specified row_ids. When there is only one row_id, KVStoreRowSparsePull \\\\\\n        is invoked just once and the result is broadcast to all the rest of outputs.\\n\\n        `row_sparse_pull` is executed asynchronously after all previous\\n        `pull`/`row_sparse_pull` calls and the last `push` call for the\\n        same input key(s) are finished.\\n\\n        The returned values are guaranteed to be the latest values in the store.\\n\\n        Parameters\\n        ----------\\n        key : str, int, or sequence of str or int\\n            Keys.\\n\\n        out: RowSparseNDArray or list of RowSparseNDArray or list of list of RowSparseNDArray\\n            Values corresponding to the keys. The stype is expected to be row_sparse\\n\\n        priority : int, optional\\n            The priority of the pull operation.\\n            Higher priority pull operations are likely to be executed before\\n            other pull actions.\\n\\n        row_ids : NDArray or list of NDArray\\n            The row_ids for which to pull for each value. Each row_id is an 1-D NDArray \\\\\\n            whose values don't have to be unique nor sorted.\\n\\n        Examples\\n        --------\\n        >>> shape = (3, 3)\\n        >>> kv.init('3', mx.nd.ones(shape).tostype('row_sparse'))\\n        >>> a = mx.nd.sparse.zeros('row_sparse', shape)\\n        >>> row_ids = mx.nd.array([0, 2], dtype='int64')\\n        >>> kv.row_sparse_pull('3', out=a, row_ids=row_ids)\\n        >>> print a.asnumpy()\\n        [[ 1.  1.  1.]\\n        [ 0.  0.  0.]\\n        [ 1.  1.  1.]]\\n        >>> duplicate_row_ids = mx.nd.array([2, 2], dtype='int64')\\n        >>> kv.row_sparse_pull('3', out=a, row_ids=duplicate_row_ids)\\n        >>> print a.asnumpy()\\n        [[ 0.  0.  0.]\\n        [ 0.  0.  0.]\\n        [ 1.  1.  1.]]\\n        >>> unsorted_row_ids = mx.nd.array([1, 0], dtype='int64')\\n        >>> kv.row_sparse_pull('3', out=a, row_ids=unsorted_row_ids)\\n        >>> print a.asnumpy()\\n        [[ 1.  1.  1.]\\n        [ 1.  1.  1.]\\n        [ 0.  0.  0.]]\",\n",
       "  'code': 'def row_sparse_pull(self, key, out=None, priority=0, row_ids=None):\\n        \"\"\" Pulls a single RowSparseNDArray value or a sequence of RowSparseNDArray values \\\\\\n        from the store with specified row_ids. When there is only one row_id, KVStoreRowSparsePull \\\\\\n        is invoked just once and the result is broadcast to all the rest of outputs.\\n\\n        `row_sparse_pull` is executed asynchronously after all previous\\n        `pull`/`row_sparse_pull` calls and the last `push` call for the\\n        same input key(s) are finished.\\n\\n        The returned values are guaranteed to be the latest values in the store.\\n\\n        Parameters\\n        ----------\\n        key : str, int, or sequence of str or int\\n            Keys.\\n\\n        out: RowSparseNDArray or list of RowSparseNDArray or list of list of RowSparseNDArray\\n            Values corresponding to the keys. The stype is expected to be row_sparse\\n\\n        priority : int, optional\\n            The priority of the pull operation.\\n            Higher priority pull operations are likely to be executed before\\n            other pull actions.\\n\\n        row_ids : NDArray or list of NDArray\\n            The row_ids for which to pull for each value. Each row_id is an 1-D NDArray \\\\\\n            whose values don\\'t have to be unique nor sorted.\\n\\n        Examples\\n        --------\\n        >>> shape = (3, 3)\\n        >>> kv.init(\\'3\\', mx.nd.ones(shape).tostype(\\'row_sparse\\'))\\n        >>> a = mx.nd.sparse.zeros(\\'row_sparse\\', shape)\\n        >>> row_ids = mx.nd.array([0, 2], dtype=\\'int64\\')\\n        >>> kv.row_sparse_pull(\\'3\\', out=a, row_ids=row_ids)\\n        >>> print a.asnumpy()\\n        [[ 1.  1.  1.]\\n        [ 0.  0.  0.]\\n        [ 1.  1.  1.]]\\n        >>> duplicate_row_ids = mx.nd.array([2, 2], dtype=\\'int64\\')\\n        >>> kv.row_sparse_pull(\\'3\\', out=a, row_ids=duplicate_row_ids)\\n        >>> print a.asnumpy()\\n        [[ 0.  0.  0.]\\n        [ 0.  0.  0.]\\n        [ 1.  1.  1.]]\\n        >>> unsorted_row_ids = mx.nd.array([1, 0], dtype=\\'int64\\')\\n        >>> kv.row_sparse_pull(\\'3\\', out=a, row_ids=unsorted_row_ids)\\n        >>> print a.asnumpy()\\n        [[ 1.  1.  1.]\\n        [ 1.  1.  1.]\\n        [ 0.  0.  0.]]\\n        \"\"\"\\n        assert(out is not None)\\n        assert(row_ids is not None)\\n        if isinstance(row_ids, NDArray):\\n            row_ids = [row_ids]\\n        assert(isinstance(row_ids, list)), \\\\\\n            \"row_ids should be NDArray or list of NDArray\"\\n        first_out = out\\n        # whether row_ids are the same\\n        single_rowid = False\\n        if len(row_ids) == 1 and isinstance(out, list):\\n            single_rowid = True\\n            first_out = [out[0]]\\n        ckeys, cvals, use_str_keys = _ctype_key_value(key, first_out)\\n        _, crow_ids, _ = _ctype_key_value(key, row_ids)\\n        assert(len(crow_ids) == len(cvals)), \\\\\\n               \"the number of row_ids doesn\\'t match the number of values\"\\n        if use_str_keys:\\n            check_call(_LIB.MXKVStorePullRowSparseEx(\\n                self.handle, mx_uint(len(ckeys)), ckeys, cvals, crow_ids, ctypes.c_int(priority)))\\n        else:\\n            check_call(_LIB.MXKVStorePullRowSparse(\\n                self.handle, mx_uint(len(ckeys)), ckeys, cvals, crow_ids, ctypes.c_int(priority)))\\n        # the result can be copied to other devices without invoking row_sparse_pull\\n        # if the indices are the same\\n        if single_rowid:\\n            for out_i in out[1:]:\\n                out[0].copyto(out_i)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'KVStore.set_optimizer',\n",
       "  'docstring': 'Registers an optimizer with the kvstore.\\n\\n        When using a single machine, this function updates the local optimizer.\\n        If using multiple machines and this operation is invoked from a worker node,\\n        it will serialized the optimizer with pickle and send it to all servers.\\n        The function returns after all servers have been updated.\\n\\n        Parameters\\n        ----------\\n        optimizer : Optimizer\\n            The new optimizer for the store\\n\\n        Examples\\n        --------\\n\\n        >>> kv = mx.kv.create()\\n        >>> shape = (2, 2)\\n        >>> weight = mx.nd.zeros(shape)\\n        >>> kv.init(3, weight)\\n        >>> # set the optimizer for kvstore as the default SGD optimizer\\n        >>> kv.set_optimizer(mx.optimizer.SGD())\\n        >>> grad = mx.nd.ones(shape)\\n        >>> kv.push(3, grad)\\n        >>> kv.pull(3, out = weight)\\n        >>> # weight is updated via gradient descent\\n        >>> weight.asnumpy()\\n        array([[-0.01, -0.01],\\n               [-0.01, -0.01]], dtype=float32)',\n",
       "  'code': 'def set_optimizer(self, optimizer):\\n        \"\"\" Registers an optimizer with the kvstore.\\n\\n        When using a single machine, this function updates the local optimizer.\\n        If using multiple machines and this operation is invoked from a worker node,\\n        it will serialized the optimizer with pickle and send it to all servers.\\n        The function returns after all servers have been updated.\\n\\n        Parameters\\n        ----------\\n        optimizer : Optimizer\\n            The new optimizer for the store\\n\\n        Examples\\n        --------\\n\\n        >>> kv = mx.kv.create()\\n        >>> shape = (2, 2)\\n        >>> weight = mx.nd.zeros(shape)\\n        >>> kv.init(3, weight)\\n        >>> # set the optimizer for kvstore as the default SGD optimizer\\n        >>> kv.set_optimizer(mx.optimizer.SGD())\\n        >>> grad = mx.nd.ones(shape)\\n        >>> kv.push(3, grad)\\n        >>> kv.pull(3, out = weight)\\n        >>> # weight is updated via gradient descent\\n        >>> weight.asnumpy()\\n        array([[-0.01, -0.01],\\n               [-0.01, -0.01]], dtype=float32)\\n        \"\"\"\\n        is_worker = ctypes.c_int()\\n        check_call(_LIB.MXKVStoreIsWorkerNode(ctypes.byref(is_worker)))\\n\\n        # pylint: disable=invalid-name\\n        if \\'dist\\' in self.type and is_worker.value: # pylint: disable=unsupported-membership-test\\n            # send the optimizer to server\\n            try:\\n                # use ASCII protocol 0, might be slower, but not a big ideal\\n                optim_str = py_str(pickle.dumps(optimizer, 0))\\n            except:\\n                raise\\n            cmd = _get_kvstore_server_command_type(\\'kController\\')\\n            self._send_command_to_servers(cmd, optim_str)\\n            if optimizer.multi_precision:\\n                cmd = _get_kvstore_server_command_type(\\'kSetMultiPrecision\\')\\n                self._send_command_to_servers(cmd, \\'\\')\\n        else:\\n            self._set_updater(opt.get_updater(optimizer))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'KVStore._set_updater',\n",
       "  'docstring': 'Sets a push updater into the store.\\n\\n        This function only changes the local store. When running on multiple machines one must\\n        use `set_optimizer`.\\n\\n        Parameters\\n        ----------\\n        updater : function\\n            The updater function.\\n\\n        Examples\\n        --------\\n        >>> def update(key, input, stored):\\n        ...     print \"update on key: %d\" % key\\n        ...     stored += input * 2\\n        >>> kv._set_updater(update)\\n        >>> kv.pull(\\'3\\', out=a)\\n        >>> print a.asnumpy()\\n        [[ 4.  4.  4.]\\n        [ 4.  4.  4.]]\\n        >>> kv.push(\\'3\\', mx.nd.ones(shape))\\n        update on key: 3\\n        >>> kv.pull(\\'3\\', out=a)\\n        >>> print a.asnumpy()\\n        [[ 6.  6.  6.]\\n        [ 6.  6.  6.]]',\n",
       "  'code': 'def _set_updater(self, updater):\\n        \"\"\"Sets a push updater into the store.\\n\\n        This function only changes the local store. When running on multiple machines one must\\n        use `set_optimizer`.\\n\\n        Parameters\\n        ----------\\n        updater : function\\n            The updater function.\\n\\n        Examples\\n        --------\\n        >>> def update(key, input, stored):\\n        ...     print \"update on key: %d\" % key\\n        ...     stored += input * 2\\n        >>> kv._set_updater(update)\\n        >>> kv.pull(\\'3\\', out=a)\\n        >>> print a.asnumpy()\\n        [[ 4.  4.  4.]\\n        [ 4.  4.  4.]]\\n        >>> kv.push(\\'3\\', mx.nd.ones(shape))\\n        update on key: 3\\n        >>> kv.pull(\\'3\\', out=a)\\n        >>> print a.asnumpy()\\n        [[ 6.  6.  6.]\\n        [ 6.  6.  6.]]\\n        \"\"\"\\n        self._updater = updater\\n        # set updater with int keys\\n        _updater_proto = ctypes.CFUNCTYPE(\\n            None, ctypes.c_int, NDArrayHandle, NDArrayHandle, ctypes.c_void_p)\\n        self._updater_func = _updater_proto(_updater_wrapper(updater))\\n        # set updater with str keys\\n        _str_updater_proto = ctypes.CFUNCTYPE(\\n            None, ctypes.c_char_p, NDArrayHandle, NDArrayHandle, ctypes.c_void_p)\\n        self._str_updater_func = _str_updater_proto(_updater_wrapper(updater))\\n        check_call(_LIB.MXKVStoreSetUpdaterEx(self.handle, self._updater_func,\\n                                              self._str_updater_func, None))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'SequentialModule.get_outputs',\n",
       "  'docstring': 'Gets outputs from a previous forward computation.\\n\\n        Parameters\\n        ----------\\n        merge_multi_context : bool\\n            Default is ``True``. In the case when data-parallelism is used, the outputs\\n            will be collected from multiple devices. A ``True`` value indicate that we\\n            should merge the collected results so that they look like from a single\\n            executor.\\n\\n        Returns\\n        -------\\n        list of NDArray or list of list of NDArray\\n            If `merge_multi_context` is ``True``, it is like ``[out1,\\n            out2]``. Otherwise, it is like ``[[out1_dev1, out1_dev2], [out2_dev1,\\n            out2_dev2]]``. All the output elements are numpy arrays.',\n",
       "  'code': 'def get_outputs(self, merge_multi_context=True):\\n        \"\"\"Gets outputs from a previous forward computation.\\n\\n        Parameters\\n        ----------\\n        merge_multi_context : bool\\n            Default is ``True``. In the case when data-parallelism is used, the outputs\\n            will be collected from multiple devices. A ``True`` value indicate that we\\n            should merge the collected results so that they look like from a single\\n            executor.\\n\\n        Returns\\n        -------\\n        list of NDArray or list of list of NDArray\\n            If `merge_multi_context` is ``True``, it is like ``[out1,\\n            out2]``. Otherwise, it is like ``[[out1_dev1, out1_dev2], [out2_dev1,\\n            out2_dev2]]``. All the output elements are numpy arrays.\\n        \"\"\"\\n        assert self.binded and self.params_initialized\\n        return self._modules[-1].get_outputs(merge_multi_context=merge_multi_context)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ctypes2numpy_shared',\n",
       "  'docstring': 'Convert a ctypes pointer to a numpy array.\\n\\n    The resulting NumPy array shares the memory with the pointer.\\n\\n    Parameters\\n    ----------\\n    cptr : ctypes.POINTER(mx_float)\\n        pointer to the memory region\\n\\n    shape : tuple\\n        Shape of target `NDArray`.\\n\\n    Returns\\n    -------\\n    out : numpy_array\\n        A numpy array : numpy array.',\n",
       "  'code': 'def ctypes2numpy_shared(cptr, shape):\\n    \"\"\"Convert a ctypes pointer to a numpy array.\\n\\n    The resulting NumPy array shares the memory with the pointer.\\n\\n    Parameters\\n    ----------\\n    cptr : ctypes.POINTER(mx_float)\\n        pointer to the memory region\\n\\n    shape : tuple\\n        Shape of target `NDArray`.\\n\\n    Returns\\n    -------\\n    out : numpy_array\\n        A numpy array : numpy array.\\n    \"\"\"\\n    if not isinstance(cptr, ctypes.POINTER(mx_float)):\\n        raise RuntimeError(\\'expected float pointer\\')\\n    size = 1\\n    for s in shape:\\n        size *= s\\n    dbuffer = (mx_float * size).from_address(ctypes.addressof(cptr.contents))\\n    return _np.frombuffer(dbuffer, dtype=_np.float32).reshape(shape)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'set_np_compat',\n",
       "  'docstring': 'Turns on/off NumPy compatibility. NumPy-compatibility is turned off by default in backend.\\n\\n    Parameters\\n    ----------\\n    active : bool\\n        Indicates whether to turn on/off NumPy compatibility.\\n\\n    Returns\\n    -------\\n        A bool value indicating the previous state of NumPy compatibility.',\n",
       "  'code': 'def set_np_compat(active):\\n    \"\"\"\\n    Turns on/off NumPy compatibility. NumPy-compatibility is turned off by default in backend.\\n\\n    Parameters\\n    ----------\\n    active : bool\\n        Indicates whether to turn on/off NumPy compatibility.\\n\\n    Returns\\n    -------\\n        A bool value indicating the previous state of NumPy compatibility.\\n    \"\"\"\\n    prev = ctypes.c_int()\\n    check_call(_LIB.MXSetIsNumpyCompatible(ctypes.c_int(active), ctypes.byref(prev)))\\n    return bool(prev.value)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'is_np_compat',\n",
       "  'docstring': 'Checks whether the NumPy compatibility is currently turned on.\\n    NumPy-compatibility is turned off by default in backend.\\n\\n    Returns\\n    -------\\n        A bool value indicating whether the NumPy compatibility is currently on.',\n",
       "  'code': 'def is_np_compat():\\n    \"\"\"\\n    Checks whether the NumPy compatibility is currently turned on.\\n    NumPy-compatibility is turned off by default in backend.\\n\\n    Returns\\n    -------\\n        A bool value indicating whether the NumPy compatibility is currently on.\\n    \"\"\"\\n    curr = ctypes.c_bool()\\n    check_call(_LIB.MXIsNumpyCompatible(ctypes.byref(curr)))\\n    return curr.value',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'use_np_compat',\n",
       "  'docstring': 'Wraps a function with an activated NumPy-compatibility scope. This ensures\\n    that the execution of the function is guaranteed with NumPy compatible semantics,\\n    such as zero-dim and zero size tensors.\\n\\n    Example::\\n        import mxnet as mx\\n        @mx.use_np_compat\\n        def scalar_one():\\n            return mx.nd.ones(())\\n        print(scalar_one())\\n\\n    Parameters\\n    ----------\\n    func : a user-provided callable function to be scoped by the NumPy compatibility state.\\n\\n    Returns\\n    -------\\n    Function\\n        A function for wrapping the user functions in the NumPy compatibility scope.',\n",
       "  'code': 'def use_np_compat(func):\\n    \"\"\"Wraps a function with an activated NumPy-compatibility scope. This ensures\\n    that the execution of the function is guaranteed with NumPy compatible semantics,\\n    such as zero-dim and zero size tensors.\\n\\n    Example::\\n        import mxnet as mx\\n        @mx.use_np_compat\\n        def scalar_one():\\n            return mx.nd.ones(())\\n        print(scalar_one())\\n\\n    Parameters\\n    ----------\\n    func : a user-provided callable function to be scoped by the NumPy compatibility state.\\n\\n    Returns\\n    -------\\n    Function\\n        A function for wrapping the user functions in the NumPy compatibility scope.\\n    \"\"\"\\n    @wraps(func)\\n    def _with_np_compat(*args, **kwargs):\\n        with np_compat(active=True):\\n            return func(*args, **kwargs)\\n\\n    return _with_np_compat',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'train_episode',\n",
       "  'docstring': \"Complete an episode's worth of training for each environment.\",\n",
       "  'code': 'def train_episode(agent, envs, preprocessors, t_max, render):\\n    \"\"\"Complete an episode\\'s worth of training for each environment.\"\"\"\\n    num_envs = len(envs)\\n\\n    # Buffers to hold trajectories, e.g. `env_xs[i]` will hold the observations\\n    # for environment `i`.\\n    env_xs, env_as = _2d_list(num_envs), _2d_list(num_envs)\\n    env_rs, env_vs = _2d_list(num_envs), _2d_list(num_envs)\\n    episode_rs = np.zeros(num_envs, dtype=np.float)\\n\\n    for p in preprocessors:\\n        p.reset()\\n\\n    observations = [p.preprocess(e.reset())\\n                    for p, e in zip(preprocessors, envs)]\\n\\n    done = np.array([False for _ in range(num_envs)])\\n    all_done = False\\n    t = 1\\n\\n    while not all_done:\\n        if render:\\n            envs[0].render()\\n\\n        # NOTE(reed): Reshape to set the data shape.\\n        agent.model.reshape([(\\'data\\', (num_envs, preprocessors[0].obs_size))])\\n        step_xs = np.vstack([o.ravel() for o in observations])\\n\\n        # Get actions and values for all environments in a single forward pass.\\n        step_xs_nd = mx.nd.array(step_xs, ctx=agent.ctx)\\n        data_batch = mx.io.DataBatch(data=[step_xs_nd], label=None)\\n        agent.model.forward(data_batch, is_train=False)\\n        _, step_vs, _, step_ps = agent.model.get_outputs()\\n\\n        step_ps = step_ps.asnumpy()\\n        step_vs = step_vs.asnumpy()\\n        step_as = agent.act(step_ps)\\n\\n        # Step each environment whose episode has not completed.\\n        for i, env in enumerate(envs):\\n            if not done[i]:\\n                obs, r, done[i], _ = env.step(step_as[i])\\n\\n                # Record the observation, action, value, and reward in the\\n                # buffers.\\n                env_xs[i].append(step_xs[i].ravel())\\n                env_as[i].append(step_as[i])\\n                env_vs[i].append(step_vs[i][0])\\n                env_rs[i].append(r)\\n                episode_rs[i] += r\\n\\n                # Add 0 as the state value when done.\\n                if done[i]:\\n                    env_vs[i].append(0.0)\\n                else:\\n                    observations[i] = preprocessors[i].preprocess(obs)\\n\\n        # Perform an update every `t_max` steps.\\n        if t == t_max:\\n            # If the episode has not finished, add current state\\'s value. This\\n            # will be used to \\'bootstrap\\' the final return (see Algorithm S3\\n            # in A3C paper).\\n            step_xs = np.vstack([o.ravel() for o in observations])\\n            step_xs_nd = mx.nd.array(step_xs, ctx=agent.ctx)\\n            data_batch = mx.io.DataBatch(data=[step_xs_nd], label=None)\\n            agent.model.forward(data_batch, is_train=False)\\n            _, extra_vs, _, _ = agent.model.get_outputs()\\n            extra_vs = extra_vs.asnumpy()\\n            for i in range(num_envs):\\n                if not done[i]:\\n                    env_vs[i].append(extra_vs[i][0])\\n\\n            # Perform update and clear buffers.\\n            env_xs = np.vstack(list(chain.from_iterable(env_xs)))\\n            agent.train_step(env_xs, env_as, env_rs, env_vs)\\n            env_xs, env_as = _2d_list(num_envs), _2d_list(num_envs)\\n            env_rs, env_vs = _2d_list(num_envs), _2d_list(num_envs)\\n            t = 0\\n\\n        all_done = np.all(done)\\n        t += 1\\n\\n    return episode_rs',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'save_image',\n",
       "  'docstring': 'save image',\n",
       "  'code': 'def save_image(data, epoch, image_size, batch_size, output_dir, padding=2):\\n    \"\"\" save image \"\"\"\\n    data = data.asnumpy().transpose((0, 2, 3, 1))\\n    datanp = np.clip(\\n        (data - np.min(data))*(255.0/(np.max(data) - np.min(data))), 0, 255).astype(np.uint8)\\n    x_dim = min(8, batch_size)\\n    y_dim = int(math.ceil(float(batch_size) / x_dim))\\n    height, width = int(image_size + padding), int(image_size + padding)\\n    grid = np.zeros((height * y_dim + 1 + padding // 2, width *\\n                     x_dim + 1 + padding // 2, 3), dtype=np.uint8)\\n    k = 0\\n    for y in range(y_dim):\\n        for x in range(x_dim):\\n            if k >= batch_size:\\n                break\\n            start_y = y * height + 1 + padding // 2\\n            end_y = start_y + height - padding\\n            start_x = x * width + 1 + padding // 2\\n            end_x = start_x + width - padding\\n            np.copyto(grid[start_y:end_y, start_x:end_x, :], datanp[k])\\n            k += 1\\n    imageio.imwrite(\\n        \\'{}/fake_samples_epoch_{}.png\\'.format(output_dir, epoch), grid)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'load_mnist',\n",
       "  'docstring': 'Load mnist dataset',\n",
       "  'code': 'def load_mnist(training_num=50000):\\n    \"\"\"Load mnist dataset\"\"\"\\n    data_path = os.path.join(os.path.dirname(os.path.realpath(\\'__file__\\')), \\'mnist.npz\\')\\n    if not os.path.isfile(data_path):\\n        from six.moves import urllib\\n        origin = (\\n            \\'https://github.com/sxjscience/mxnet/raw/master/example/bayesian-methods/mnist.npz\\'\\n        )\\n        print(\\'Downloading data from %s to %s\\' % (origin, data_path))\\n        ctx = ssl._create_unverified_context()\\n        with urllib.request.urlopen(origin, context=ctx) as u, open(data_path, \\'wb\\') as f:\\n            f.write(u.read())\\n        print(\\'Done!\\')\\n    dat = numpy.load(data_path)\\n    X = (dat[\\'X\\'][:training_num] / 126.0).astype(\\'float32\\')\\n    Y = dat[\\'Y\\'][:training_num]\\n    X_test = (dat[\\'X_test\\'] / 126.0).astype(\\'float32\\')\\n    Y_test = dat[\\'Y_test\\']\\n    Y = Y.reshape((Y.shape[0],))\\n    Y_test = Y_test.reshape((Y_test.shape[0],))\\n    return X, Y, X_test, Y_test',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ImageDetIter._parse_label',\n",
       "  'docstring': 'Helper function to parse object detection label.\\n\\n        Format for raw label:\\n        n \\\\t k \\\\t ... \\\\t [id \\\\t xmin\\\\t ymin \\\\t xmax \\\\t ymax \\\\t ...] \\\\t [repeat]\\n        where n is the width of header, 2 or larger\\n        k is the width of each object annotation, can be arbitrary, at least 5',\n",
       "  'code': 'def _parse_label(self, label):\\n        \"\"\"Helper function to parse object detection label.\\n\\n        Format for raw label:\\n        n \\\\t k \\\\t ... \\\\t [id \\\\t xmin\\\\t ymin \\\\t xmax \\\\t ymax \\\\t ...] \\\\t [repeat]\\n        where n is the width of header, 2 or larger\\n        k is the width of each object annotation, can be arbitrary, at least 5\\n        \"\"\"\\n        if isinstance(label, nd.NDArray):\\n            label = label.asnumpy()\\n        raw = label.ravel()\\n        if raw.size < 7:\\n            raise RuntimeError(\"Label shape is invalid: \" + str(raw.shape))\\n        header_width = int(raw[0])\\n        obj_width = int(raw[1])\\n        if (raw.size - header_width) % obj_width != 0:\\n            msg = \"Label shape %s inconsistent with annotation width %d.\" \\\\\\n                %(str(raw.shape), obj_width)\\n            raise RuntimeError(msg)\\n        out = np.reshape(raw[header_width:], (-1, obj_width))\\n        # remove bad ground-truths\\n        valid = np.where(np.logical_and(out[:, 3] > out[:, 1], out[:, 4] > out[:, 2]))[0]\\n        if valid.size < 1:\\n            raise RuntimeError(\\'Encounter sample with no valid label.\\')\\n        return out[valid, :]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ImageDetIter.draw_next',\n",
       "  'docstring': \"Display next image with bounding boxes drawn.\\n\\n        Parameters\\n        ----------\\n        color : tuple\\n            Bounding box color in RGB, use None for random color\\n        thickness : int\\n            Bounding box border thickness\\n        mean : True or numpy.ndarray\\n            Compensate for the mean to have better visual effect\\n        std : True or numpy.ndarray\\n            Revert standard deviations\\n        clip : bool\\n            If true, clip to [0, 255] for better visual effect\\n        waitKey : None or int\\n            Hold the window for waitKey milliseconds if set, skip ploting if None\\n        window_name : str\\n            Plot window name if waitKey is set.\\n        id2labels : dict\\n            Mapping of labels id to labels name.\\n\\n        Returns\\n        -------\\n            numpy.ndarray\\n\\n        Examples\\n        --------\\n        >>> # use draw_next to get images with bounding boxes drawn\\n        >>> iterator = mx.image.ImageDetIter(1, (3, 600, 600), path_imgrec='train.rec')\\n        >>> for image in iterator.draw_next(waitKey=None):\\n        ...     # display image\\n        >>> # or let draw_next display using cv2 module\\n        >>> for image in iterator.draw_next(waitKey=0, window_name='disp'):\\n        ...     pass\",\n",
       "  'code': 'def draw_next(self, color=None, thickness=2, mean=None, std=None, clip=True,\\n                  waitKey=None, window_name=\\'draw_next\\', id2labels=None):\\n        \"\"\"Display next image with bounding boxes drawn.\\n\\n        Parameters\\n        ----------\\n        color : tuple\\n            Bounding box color in RGB, use None for random color\\n        thickness : int\\n            Bounding box border thickness\\n        mean : True or numpy.ndarray\\n            Compensate for the mean to have better visual effect\\n        std : True or numpy.ndarray\\n            Revert standard deviations\\n        clip : bool\\n            If true, clip to [0, 255] for better visual effect\\n        waitKey : None or int\\n            Hold the window for waitKey milliseconds if set, skip ploting if None\\n        window_name : str\\n            Plot window name if waitKey is set.\\n        id2labels : dict\\n            Mapping of labels id to labels name.\\n\\n        Returns\\n        -------\\n            numpy.ndarray\\n\\n        Examples\\n        --------\\n        >>> # use draw_next to get images with bounding boxes drawn\\n        >>> iterator = mx.image.ImageDetIter(1, (3, 600, 600), path_imgrec=\\'train.rec\\')\\n        >>> for image in iterator.draw_next(waitKey=None):\\n        ...     # display image\\n        >>> # or let draw_next display using cv2 module\\n        >>> for image in iterator.draw_next(waitKey=0, window_name=\\'disp\\'):\\n        ...     pass\\n        \"\"\"\\n        try:\\n            import cv2\\n        except ImportError as e:\\n            warnings.warn(\\'Unable to import cv2, skip drawing: %s\\', str(e))\\n            return\\n        count = 0\\n        try:\\n            while True:\\n                label, s = self.next_sample()\\n                data = self.imdecode(s)\\n                try:\\n                    self.check_valid_image([data])\\n                    label = self._parse_label(label)\\n                except RuntimeError as e:\\n                    logging.debug(\\'Invalid image, skipping:  %s\\', str(e))\\n                    continue\\n                count += 1\\n                data, label = self.augmentation_transform(data, label)\\n                image = data.asnumpy()\\n\\n                # revert color_normalize\\n                if std is True:\\n                    std = np.array([58.395, 57.12, 57.375])\\n                elif std is not None:\\n                    assert isinstance(std, np.ndarray) and std.shape[0] in [1, 3]\\n                if std is not None:\\n                    image *= std\\n\\n                if mean is True:\\n                    mean = np.array([123.68, 116.28, 103.53])\\n                elif mean is not None:\\n                    assert isinstance(mean, np.ndarray) and mean.shape[0] in [1, 3]\\n                if mean is not None:\\n                    image += mean\\n\\n                # swap RGB\\n                image[:, :, (0, 1, 2)] = image[:, :, (2, 1, 0)]\\n                if clip:\\n                    image = np.maximum(0, np.minimum(255, image))\\n                if color:\\n                    color = color[::-1]\\n                image = image.astype(np.uint8)\\n                height, width, _ = image.shape\\n                for i in range(label.shape[0]):\\n                    x1 = int(label[i, 1] * width)\\n                    if x1 < 0:\\n                        continue\\n                    y1 = int(label[i, 2] * height)\\n                    x2 = int(label[i, 3] * width)\\n                    y2 = int(label[i, 4] * height)\\n                    bc = np.random.rand(3) * 255 if not color else color\\n                    cv2.rectangle(image, (x1, y1), (x2, y2), bc, thickness)\\n                    if id2labels is not None:\\n                        cls_id = int(label[i, 0])\\n                        if cls_id in id2labels:\\n                            cls_name = id2labels[cls_id]\\n                            text = \"{:s}\".format(cls_name)\\n                            font = cv2.FONT_HERSHEY_SIMPLEX\\n                            font_scale = 0.5\\n                            text_height = cv2.getTextSize(text, font, font_scale, 2)[0][1]\\n                            tc = (255, 255, 255)\\n                            tpos = (x1 + 5, y1 + text_height + 5)\\n                            cv2.putText(image, text, tpos, font, font_scale, tc, 2)\\n                if waitKey is not None:\\n                    cv2.imshow(window_name, image)\\n                    cv2.waitKey(waitKey)\\n                yield image\\n        except StopIteration:\\n            if not count:\\n                return',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'char_beam_search',\n",
       "  'docstring': 'Description : apply beam search for prediction result',\n",
       "  'code': 'def char_beam_search(out):\\n    \"\"\"\\n    Description : apply beam search for prediction result\\n    \"\"\"\\n    out_conv = list()\\n    for idx in range(out.shape[0]):\\n        probs = out[idx]\\n        prob = probs.softmax().asnumpy()\\n        line_string_proposals = ctcBeamSearch(prob, ALPHABET, None, k=4, beamWidth=25)\\n        out_conv.append(line_string_proposals[0])\\n    return out_conv',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Train.infer',\n",
       "  'docstring': 'Description : Print sentence for prediction result',\n",
       "  'code': 'def infer(self, input_data, input_label):\\n        \"\"\"\\n        Description : Print sentence for prediction result\\n        \"\"\"\\n        sum_losses = 0\\n        len_losses = 0\\n        for data, label in zip(input_data, input_label):\\n            pred = self.net(data)\\n            sum_losses += mx.nd.array(self.loss_fn(pred, label)).sum().asscalar()\\n            len_losses += len(data)\\n            pred_convert = char_beam_search(pred)\\n            label_convert = char_conv(label.asnumpy())\\n            for target, pred in zip(label_convert, pred_convert):\\n                print(\"target:{t}  pred:{p}\".format(t=target, p=pred))\\n        return sum_losses, len_losses',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'sample_categorical',\n",
       "  'docstring': 'Sample from independent categorical distributions\\n\\n    Each batch is an independent categorical distribution.\\n\\n    Parameters\\n    ----------\\n    prob : numpy.ndarray\\n      Probability of the categorical distribution. Shape --> (batch_num, category_num)\\n    rng : numpy.random.RandomState\\n\\n    Returns\\n    -------\\n    ret : numpy.ndarray\\n      Sampling result. Shape --> (batch_num,)',\n",
       "  'code': 'def sample_categorical(prob, rng):\\n    \"\"\"Sample from independent categorical distributions\\n\\n    Each batch is an independent categorical distribution.\\n\\n    Parameters\\n    ----------\\n    prob : numpy.ndarray\\n      Probability of the categorical distribution. Shape --> (batch_num, category_num)\\n    rng : numpy.random.RandomState\\n\\n    Returns\\n    -------\\n    ret : numpy.ndarray\\n      Sampling result. Shape --> (batch_num,)\\n    \"\"\"\\n    ret = numpy.empty(prob.shape[0], dtype=numpy.float32)\\n    for ind in range(prob.shape[0]):\\n        ret[ind] = numpy.searchsorted(numpy.cumsum(prob[ind]), rng.rand()).clip(min=0.0,\\n                                                                                max=prob.shape[\\n                                                                                        1] - 0.5)\\n    return ret',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'sample_normal',\n",
       "  'docstring': 'Sample from independent normal distributions\\n\\n    Each element is an independent normal distribution.\\n\\n    Parameters\\n    ----------\\n    mean : numpy.ndarray\\n      Means of the normal distribution. Shape --> (batch_num, sample_dim)\\n    var : numpy.ndarray\\n      Variance of the normal distribution. Shape --> (batch_num, sample_dim)\\n    rng : numpy.random.RandomState\\n\\n    Returns\\n    -------\\n    ret : numpy.ndarray\\n       The sampling result. Shape --> (batch_num, sample_dim)',\n",
       "  'code': 'def sample_normal(mean, var, rng):\\n    \"\"\"Sample from independent normal distributions\\n\\n    Each element is an independent normal distribution.\\n\\n    Parameters\\n    ----------\\n    mean : numpy.ndarray\\n      Means of the normal distribution. Shape --> (batch_num, sample_dim)\\n    var : numpy.ndarray\\n      Variance of the normal distribution. Shape --> (batch_num, sample_dim)\\n    rng : numpy.random.RandomState\\n\\n    Returns\\n    -------\\n    ret : numpy.ndarray\\n       The sampling result. Shape --> (batch_num, sample_dim)\\n    \"\"\"\\n    ret = numpy.sqrt(var) * rng.randn(*mean.shape) + mean\\n    return ret',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'sample_mog',\n",
       "  'docstring': 'Sample from independent mixture of gaussian (MoG) distributions\\n\\n    Each batch is an independent MoG distribution.\\n\\n    Parameters\\n    ----------\\n    prob : numpy.ndarray\\n      mixture probability of each gaussian. Shape --> (batch_num, center_num)\\n    mean : numpy.ndarray\\n      mean of each gaussian. Shape --> (batch_num, center_num, sample_dim)\\n    var : numpy.ndarray\\n      variance of each gaussian. Shape --> (batch_num, center_num, sample_dim)\\n    rng : numpy.random.RandomState\\n\\n    Returns\\n    -------\\n    ret : numpy.ndarray\\n      sampling result. Shape --> (batch_num, sample_dim)',\n",
       "  'code': 'def sample_mog(prob, mean, var, rng):\\n    \"\"\"Sample from independent mixture of gaussian (MoG) distributions\\n\\n    Each batch is an independent MoG distribution.\\n\\n    Parameters\\n    ----------\\n    prob : numpy.ndarray\\n      mixture probability of each gaussian. Shape --> (batch_num, center_num)\\n    mean : numpy.ndarray\\n      mean of each gaussian. Shape --> (batch_num, center_num, sample_dim)\\n    var : numpy.ndarray\\n      variance of each gaussian. Shape --> (batch_num, center_num, sample_dim)\\n    rng : numpy.random.RandomState\\n\\n    Returns\\n    -------\\n    ret : numpy.ndarray\\n      sampling result. Shape --> (batch_num, sample_dim)\\n    \"\"\"\\n    gaussian_inds = sample_categorical(prob, rng).astype(numpy.int32)\\n    mean = mean[numpy.arange(mean.shape[0]), gaussian_inds, :]\\n    var = var[numpy.arange(mean.shape[0]), gaussian_inds, :]\\n    ret = sample_normal(mean=mean, var=var, rng=rng)\\n    return ret',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'GraphProto._parse_array',\n",
       "  'docstring': 'Grab data in TensorProto and convert to numpy array.',\n",
       "  'code': 'def _parse_array(self, tensor_proto):\\n        \"\"\"Grab data in TensorProto and convert to numpy array.\"\"\"\\n        try:\\n            from onnx.numpy_helper import to_array\\n        except ImportError:\\n            raise ImportError(\"Onnx and protobuf need to be installed. \"\\n                              + \"Instructions to install - https://github.com/onnx/onnx\")\\n        if len(tuple(tensor_proto.dims)) > 0:\\n            np_array = to_array(tensor_proto).reshape(tuple(tensor_proto.dims))\\n        else:\\n            # If onnx\\'s params are scalar values without dims mentioned.\\n            np_array = np.array([to_array(tensor_proto)])\\n        return nd.array(np_array)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'BucketingModule.get_outputs',\n",
       "  'docstring': 'Gets outputs from a previous forward computation.\\n\\n        Parameters\\n        ----------\\n        merge_multi_context : bool\\n            Defaults to ``True``. In the case when data-parallelism is used, the outputs\\n            will be collected from multiple devices. A ``True`` value indicate that we\\n            should merge the collected results so that they look like from a single\\n            executor.\\n\\n        Returns\\n        -------\\n        list of numpy arrays or list of list of numpy arrays\\n            If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it\\n            is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output\\n            elements are numpy arrays.',\n",
       "  'code': 'def get_outputs(self, merge_multi_context=True):\\n        \"\"\"Gets outputs from a previous forward computation.\\n\\n        Parameters\\n        ----------\\n        merge_multi_context : bool\\n            Defaults to ``True``. In the case when data-parallelism is used, the outputs\\n            will be collected from multiple devices. A ``True`` value indicate that we\\n            should merge the collected results so that they look like from a single\\n            executor.\\n\\n        Returns\\n        -------\\n        list of numpy arrays or list of list of numpy arrays\\n            If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it\\n            is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output\\n            elements are numpy arrays.\\n        \"\"\"\\n        assert self.binded and self.params_initialized\\n        return self._curr_module.get_outputs(merge_multi_context=merge_multi_context)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'train_nstyle',\n",
       "  'docstring': 'Train a neural style network.\\n    Args are from argparse and control input, output, hyper-parameters.\\n    callback allows for display of training progress.',\n",
       "  'code': 'def train_nstyle(args, callback=None):\\n    \"\"\"Train a neural style network.\\n    Args are from argparse and control input, output, hyper-parameters.\\n    callback allows for display of training progress.\\n    \"\"\"\\n    # input\\n    dev = mx.gpu(args.gpu) if args.gpu >= 0 else mx.cpu()\\n    content_np = PreprocessContentImage(args.content_image, args.max_long_edge)\\n    style_np = PreprocessStyleImage(args.style_image, shape=content_np.shape)\\n    size = content_np.shape[2:]\\n\\n    # model\\n    Executor = namedtuple(\\'Executor\\', [\\'executor\\', \\'data\\', \\'data_grad\\'])\\n\\n    model_module =  importlib.import_module(\\'model_\\' + args.model)\\n    style, content = model_module.get_symbol()\\n    gram, gscale = style_gram_symbol(size, style)\\n    model_executor = model_module.get_executor(gram, content, size, dev)\\n    model_executor.data[:] = style_np\\n    model_executor.executor.forward()\\n    style_array = []\\n    for i in range(len(model_executor.style)):\\n        style_array.append(model_executor.style[i].copyto(mx.cpu()))\\n\\n    model_executor.data[:] = content_np\\n    model_executor.executor.forward()\\n    content_array = model_executor.content.copyto(mx.cpu())\\n\\n    # delete the executor\\n    del model_executor\\n\\n    style_loss, content_loss = get_loss(gram, content)\\n    model_executor = model_module.get_executor(\\n        style_loss, content_loss, size, dev)\\n\\n    grad_array = []\\n    for i in range(len(style_array)):\\n        style_array[i].copyto(model_executor.arg_dict[\"target_gram_%d\" % i])\\n        grad_array.append(mx.nd.ones((1,), dev) * (float(args.style_weight) / gscale[i]))\\n    grad_array.append(mx.nd.ones((1,), dev) * (float(args.content_weight)))\\n\\n    print([x.asscalar() for x in grad_array])\\n    content_array.copyto(model_executor.arg_dict[\"target_content\"])\\n\\n    # train\\n    # initialize img with random noise\\n    img = mx.nd.zeros(content_np.shape, ctx=dev)\\n    img[:] = mx.rnd.uniform(-0.1, 0.1, img.shape)\\n\\n    lr = mx.lr_scheduler.FactorScheduler(step=args.lr_sched_delay,\\n            factor=args.lr_sched_factor)\\n\\n    optimizer = mx.optimizer.NAG(\\n        learning_rate = args.lr,\\n        wd = 0.0001,\\n        momentum=0.95,\\n        lr_scheduler = lr)\\n    optim_state = optimizer.create_state(0, img)\\n\\n    logging.info(\\'start training arguments %s\\', args)\\n    old_img = img.copyto(dev)\\n    clip_norm = 1 * np.prod(img.shape)\\n    tv_grad_executor = get_tv_grad_executor(img, dev, args.tv_weight)\\n\\n    for e in range(args.max_num_epochs):\\n        img.copyto(model_executor.data)\\n        model_executor.executor.forward()\\n        model_executor.executor.backward(grad_array)\\n        gnorm = mx.nd.norm(model_executor.data_grad).asscalar()\\n        if gnorm > clip_norm:\\n            model_executor.data_grad[:] *= clip_norm / gnorm\\n\\n        if tv_grad_executor is not None:\\n            tv_grad_executor.forward()\\n            optimizer.update(0, img,\\n                             model_executor.data_grad + tv_grad_executor.outputs[0],\\n                             optim_state)\\n        else:\\n            optimizer.update(0, img, model_executor.data_grad, optim_state)\\n        new_img = img\\n        eps = (mx.nd.norm(old_img - new_img) / mx.nd.norm(new_img)).asscalar()\\n\\n        old_img = new_img.copyto(dev)\\n        logging.info(\\'epoch %d, relative change %f\\', e, eps)\\n        if eps < args.stop_eps:\\n            logging.info(\\'eps < args.stop_eps, training finished\\')\\n            break\\n\\n        if callback:\\n            cbdata = {\\n                \\'eps\\': eps,\\n                \\'epoch\\': e+1,\\n            }\\n        if (e+1) % args.save_epochs == 0:\\n            outfn = args.output_dir + \\'e_\\'+str(e+1)+\\'.jpg\\'\\n            npimg = new_img.asnumpy()\\n            SaveImage(npimg, outfn, args.remove_noise)\\n            if callback:\\n                cbdata[\\'filename\\'] = outfn\\n                cbdata[\\'img\\'] = npimg\\n        if callback:\\n            callback(cbdata)\\n\\n    final_fn = args.output_dir + \\'/final.jpg\\'\\n    SaveImage(new_img.asnumpy(), final_fn)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'zeros',\n",
       "  'docstring': \"Return a new array of given shape and type, filled with zeros.\\n\\n    Parameters\\n    ----------\\n    shape : int or tuple of int\\n        The shape of the empty array\\n    ctx : Context, optional\\n        An optional device context (default is the current default context)\\n    dtype : str or numpy.dtype, optional\\n        An optional value type (default is `float32`)\\n    stype: string, optional\\n        The storage type of the empty array, such as 'row_sparse', 'csr', etc.\\n\\n    Returns\\n    -------\\n    NDArray, CSRNDArray or RowSparseNDArray\\n        A created array\\n    Examples\\n    --------\\n    >>> mx.nd.zeros((1,2), mx.cpu(), stype='csr')\\n    <CSRNDArray 1x2 @cpu(0)>\\n    >>> mx.nd.zeros((1,2), mx.cpu(), 'float16', stype='row_sparse').asnumpy()\\n    array([[ 0.,  0.]], dtype=float16)\",\n",
       "  'code': 'def zeros(shape, ctx=None, dtype=None, stype=None, **kwargs):\\n    \"\"\"Return a new array of given shape and type, filled with zeros.\\n\\n    Parameters\\n    ----------\\n    shape : int or tuple of int\\n        The shape of the empty array\\n    ctx : Context, optional\\n        An optional device context (default is the current default context)\\n    dtype : str or numpy.dtype, optional\\n        An optional value type (default is `float32`)\\n    stype: string, optional\\n        The storage type of the empty array, such as \\'row_sparse\\', \\'csr\\', etc.\\n\\n    Returns\\n    -------\\n    NDArray, CSRNDArray or RowSparseNDArray\\n        A created array\\n    Examples\\n    --------\\n    >>> mx.nd.zeros((1,2), mx.cpu(), stype=\\'csr\\')\\n    <CSRNDArray 1x2 @cpu(0)>\\n    >>> mx.nd.zeros((1,2), mx.cpu(), \\'float16\\', stype=\\'row_sparse\\').asnumpy()\\n    array([[ 0.,  0.]], dtype=float16)\\n    \"\"\"\\n\\n    if stype is None or stype == \\'default\\':\\n        return _zeros_ndarray(shape, ctx, dtype, **kwargs)\\n    else:\\n        return _zeros_sparse_ndarray(stype, shape, ctx, dtype, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'empty',\n",
       "  'docstring': \"Returns a new array of given shape and type, without initializing entries.\\n\\n    Parameters\\n    ----------\\n    shape : int or tuple of int\\n        The shape of the empty array.\\n    ctx : Context, optional\\n        An optional device context (default is the current default context).\\n    dtype : str or numpy.dtype, optional\\n        An optional value type (default is `float32`).\\n    stype : str, optional\\n        An optional storage type (default is `default`).\\n\\n    Returns\\n    -------\\n    NDArray, CSRNDArray or RowSparseNDArray\\n        A created array.\\n\\n    Examples\\n    --------\\n    >>> mx.nd.empty(1)\\n    <NDArray 1 @cpu(0)>\\n    >>> mx.nd.empty((1,2), mx.gpu(0))\\n    <NDArray 1x2 @gpu(0)>\\n    >>> mx.nd.empty((1,2), mx.gpu(0), 'float16')\\n    <NDArray 1x2 @gpu(0)>\\n    >>> mx.nd.empty((1,2), stype='csr')\\n    <CSRNDArray 1x2 @cpu(0)>\",\n",
       "  'code': 'def empty(shape, ctx=None, dtype=None, stype=None):\\n    \"\"\"Returns a new array of given shape and type, without initializing entries.\\n\\n    Parameters\\n    ----------\\n    shape : int or tuple of int\\n        The shape of the empty array.\\n    ctx : Context, optional\\n        An optional device context (default is the current default context).\\n    dtype : str or numpy.dtype, optional\\n        An optional value type (default is `float32`).\\n    stype : str, optional\\n        An optional storage type (default is `default`).\\n\\n    Returns\\n    -------\\n    NDArray, CSRNDArray or RowSparseNDArray\\n        A created array.\\n\\n    Examples\\n    --------\\n    >>> mx.nd.empty(1)\\n    <NDArray 1 @cpu(0)>\\n    >>> mx.nd.empty((1,2), mx.gpu(0))\\n    <NDArray 1x2 @gpu(0)>\\n    >>> mx.nd.empty((1,2), mx.gpu(0), \\'float16\\')\\n    <NDArray 1x2 @gpu(0)>\\n    >>> mx.nd.empty((1,2), stype=\\'csr\\')\\n    <CSRNDArray 1x2 @cpu(0)>\\n    \"\"\"\\n    if stype is None or stype == \\'default\\':\\n        return _empty_ndarray(shape, ctx, dtype)\\n    else:\\n        return _empty_sparse_ndarray(stype, shape, ctx, dtype)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'array',\n",
       "  'docstring': \"Creates an array from any object exposing the array interface.\\n\\n    Parameters\\n    ----------\\n    source_array : array_like\\n        An object exposing the array interface, an object whose `__array__`\\n        method returns an array, or any (nested) sequence.\\n    ctx : Context, optional\\n        Device context (default is the current default context).\\n    dtype : str or numpy.dtype, optional\\n        The data type of the output array. The default dtype is ``source_array.dtype``\\n        if `source_array` is an `NDArray`, `float32` otherwise.\\n\\n    Returns\\n    -------\\n    NDArray, RowSparseNDArray or CSRNDArray\\n        An array with the same contents as the `source_array`.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> mx.nd.array([1, 2, 3])\\n    <NDArray 3 @cpu(0)>\\n    >>> mx.nd.array([[1, 2], [3, 4]])\\n    <NDArray 2x2 @cpu(0)>\\n    >>> mx.nd.array(np.zeros((3, 2)))\\n    <NDArray 3x2 @cpu(0)>\\n    >>> mx.nd.array(np.zeros((3, 2)), mx.gpu(0))\\n    <NDArray 3x2 @gpu(0)>\\n    >>> mx.nd.array(mx.nd.zeros((3, 2), stype='row_sparse'))\\n    <RowSparseNDArray 3x2 @cpu(0)>\",\n",
       "  'code': 'def array(source_array, ctx=None, dtype=None):\\n    \"\"\"Creates an array from any object exposing the array interface.\\n\\n    Parameters\\n    ----------\\n    source_array : array_like\\n        An object exposing the array interface, an object whose `__array__`\\n        method returns an array, or any (nested) sequence.\\n    ctx : Context, optional\\n        Device context (default is the current default context).\\n    dtype : str or numpy.dtype, optional\\n        The data type of the output array. The default dtype is ``source_array.dtype``\\n        if `source_array` is an `NDArray`, `float32` otherwise.\\n\\n    Returns\\n    -------\\n    NDArray, RowSparseNDArray or CSRNDArray\\n        An array with the same contents as the `source_array`.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> mx.nd.array([1, 2, 3])\\n    <NDArray 3 @cpu(0)>\\n    >>> mx.nd.array([[1, 2], [3, 4]])\\n    <NDArray 2x2 @cpu(0)>\\n    >>> mx.nd.array(np.zeros((3, 2)))\\n    <NDArray 3x2 @cpu(0)>\\n    >>> mx.nd.array(np.zeros((3, 2)), mx.gpu(0))\\n    <NDArray 3x2 @gpu(0)>\\n    >>> mx.nd.array(mx.nd.zeros((3, 2), stype=\\'row_sparse\\'))\\n    <RowSparseNDArray 3x2 @cpu(0)>\\n    \"\"\"\\n    if spsp is not None and isinstance(source_array, spsp.csr.csr_matrix):\\n        return _sparse_array(source_array, ctx=ctx, dtype=dtype)\\n    elif isinstance(source_array, NDArray) and source_array.stype != \\'default\\':\\n        return _sparse_array(source_array, ctx=ctx, dtype=dtype)\\n    else:\\n        return _array(source_array, ctx=ctx, dtype=dtype)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_infer_param_types',\n",
       "  'docstring': \"Utility function that helps in inferring DType of args and auxs params\\n    from given input param.\\n\\n    Parameters\\n    ----------\\n    in_params: List of Symbol\\n        List of input symbol variables.\\n    out_params: Symbol\\n        Output symbol variable.\\n    arg_params: List of Str\\n        List of names of argument parametrs.\\n    aux_params: List of Str\\n        List of names of auxiliary parameters.\\n    default_dtype: numpy.dtype or str, default 'float32'\\n        Default data type for arg_params and aux_params, if unable to infer the type.\\n\\n    Returns\\n    -------\\n    arg_types: List of numpy.dtype\\n        List of arg_params type. Order is same as arg_params.\\n        Defaults to 'float32', if unable to infer type.\\n    aux_types: List of numpy.dtype\\n        List of aux_params type. Order is same as aux_params.\\n        Defaults to 'float32', if unable to infer type.\",\n",
       "  'code': 'def _infer_param_types(in_params, out_params, arg_params, aux_params, default_dtype=mx_real_t):\\n    \"\"\"Utility function that helps in inferring DType of args and auxs params\\n    from given input param.\\n\\n    Parameters\\n    ----------\\n    in_params: List of Symbol\\n        List of input symbol variables.\\n    out_params: Symbol\\n        Output symbol variable.\\n    arg_params: List of Str\\n        List of names of argument parametrs.\\n    aux_params: List of Str\\n        List of names of auxiliary parameters.\\n    default_dtype: numpy.dtype or str, default \\'float32\\'\\n        Default data type for arg_params and aux_params, if unable to infer the type.\\n\\n    Returns\\n    -------\\n    arg_types: List of numpy.dtype\\n        List of arg_params type. Order is same as arg_params.\\n        Defaults to \\'float32\\', if unable to infer type.\\n    aux_types: List of numpy.dtype\\n        List of aux_params type. Order is same as aux_params.\\n        Defaults to \\'float32\\', if unable to infer type.\\n    \"\"\"\\n    arg_types = None\\n    aux_types = None\\n\\n    # Get Input symbol details. This will be used to infer types of\\n    # other parameters.\\n    input_sym_names = [in_param.name for in_param in in_params]\\n\\n    # Try to infer input types. If not successful, we will set default dtype.\\n    # If successful, we will try to infer other params in the graph.\\n    input_sym_arg_types = []\\n    can_infer_input_type = True\\n    for in_param in in_params:\\n        input_sym_arg_type = in_param.infer_type()[0]\\n        if not input_sym_arg_type or len(input_sym_arg_type) < 1:\\n            can_infer_input_type = False\\n            break\\n        else:\\n            input_sym_arg_types.append(in_param.infer_type()[0][0])\\n\\n    # Try to infer types of other parameters.\\n    if can_infer_input_type:\\n        params = {k:v for k, v in zip(input_sym_names, input_sym_arg_types)}\\n        arg_types, _, aux_types = out_params.infer_type(**params)\\n\\n    if arg_types is None or len(arg_types) != len(arg_params):\\n        arg_types = []\\n        for _ in arg_params:\\n            arg_types.append(default_dtype)\\n\\n    if aux_types is None or len(aux_types) != len(aux_params):\\n        aux_types = []\\n        for _ in aux_params:\\n            aux_types.append(default_dtype)\\n\\n    return (arg_types, aux_types)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Block.cast',\n",
       "  'docstring': 'Cast this Block to use another data type.\\n\\n        Parameters\\n        ----------\\n        dtype : str or numpy.dtype\\n            The new data type.',\n",
       "  'code': 'def cast(self, dtype):\\n        \"\"\"Cast this Block to use another data type.\\n\\n        Parameters\\n        ----------\\n        dtype : str or numpy.dtype\\n            The new data type.\\n        \"\"\"\\n        for child in self._children.values():\\n            child.cast(dtype)\\n        for _, param in self.params.items():\\n            param.cast(dtype)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'scalar_op_helper',\n",
       "  'docstring': 'Helper function for scalar arithmetic operations',\n",
       "  'code': 'def scalar_op_helper(node, op_name, **kwargs):\\n    \"\"\"Helper function for scalar arithmetic operations\"\"\"\\n    name, input_nodes, attrs = get_inputs(node, kwargs)\\n    from onnx import numpy_helper\\n    input_type = kwargs[\"in_type\"]\\n    scalar_value = np.array([attrs.get(\"scalar\", 1)],\\n                            dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[input_type])\\n\\n    initializer = kwargs[\"initializer\"]\\n    flag = True\\n    # If the input value is in initializer, just multiply with scalar input\\n    # and create a new initializer\\n    for i in initializer:\\n        if i.name == input_nodes[0]:\\n            if op_name == \\'Mul\\':\\n                new_initializer = numpy_helper.to_array(i) * scalar_value[0]\\n            elif op_name == \\'Sub\\':\\n                if name.startswith(\"_rminusscalar\"):\\n                    new_initializer = scalar_value[0] - numpy_helper.to_array(i)\\n                else:\\n                    new_initializer = numpy_helper.to_array(i) - scalar_value[0]\\n            elif op_name == \\'Add\\':\\n                new_initializer = numpy_helper.to_array(i) + scalar_value[0]\\n            elif op_name == \\'Div\\':\\n                if name.startswith(\"_rdivscalar\"):\\n                    new_initializer = scalar_value[0] / numpy_helper.to_array(i)\\n                else:\\n                    new_initializer = numpy_helper.to_array(i) / scalar_value[0]\\n            elif op_name == \\'Pow\\':\\n                new_initializer = numpy_helper.to_array(i) ** scalar_value[0]\\n            flag = False\\n            break\\n\\n    # else create a new tensor of the scalar value, add it in initializer\\n    if flag is True:\\n        dims = np.shape(scalar_value)\\n\\n        scalar_op_name = \"scalar_op\" + str(kwargs[\"idx\"])\\n        tensor_node = onnx.helper.make_tensor_value_info(scalar_op_name, input_type, dims)\\n\\n        initializer.append(\\n            onnx.helper.make_tensor(\\n                name=scalar_op_name,\\n                data_type=input_type,\\n                dims=dims,\\n                vals=scalar_value,\\n                raw=False,\\n            )\\n        )\\n\\n        mul_node = onnx.helper.make_node(\\n            op_name,\\n            [input_nodes[0], scalar_op_name],\\n            [name],\\n            name=name\\n        )\\n\\n        return [tensor_node, mul_node]\\n    else:\\n        data_type = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[new_initializer.dtype]\\n        dims = np.shape(new_initializer)\\n\\n        new_a_node = input_nodes[0] + str(kwargs[\"idx\"])\\n        tensor_node = onnx.helper.make_tensor_value_info(new_a_node, data_type, dims)\\n\\n        initializer.append(\\n            onnx.helper.make_tensor(\\n                name=new_a_node,\\n                data_type=data_type,\\n                dims=dims,\\n                vals=new_initializer,\\n                raw=False,\\n            )\\n        )\\n        return [tensor_node]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'var',\n",
       "  'docstring': \"Creates a symbolic variable with specified name.\\n\\n    Example\\n    -------\\n    >>> data = mx.sym.Variable('data', attr={'a': 'b'})\\n    >>> data\\n    <Symbol data>\\n    >>> csr_data = mx.sym.Variable('csr_data', stype='csr')\\n    >>> csr_data\\n    <Symbol csr_data>\\n    >>> row_sparse_weight = mx.sym.Variable('weight', stype='row_sparse')\\n    >>> row_sparse_weight\\n    <Symbol weight>\\n\\n    Parameters\\n    ----------\\n    name : str\\n        Variable name.\\n    attr : Dict of strings\\n        Additional attributes to set on the variable. Format {string : string}.\\n    shape : tuple\\n        The shape of a variable. If specified, this will be used during the shape inference.\\n        If one has specified a different shape for this variable using\\n        a keyword argument when calling shape inference, this shape information will be ignored.\\n    lr_mult : float\\n        The learning rate multiplier for input variable.\\n    wd_mult : float\\n        Weight decay multiplier for input variable.\\n    dtype : str or numpy.dtype\\n        The dtype for input variable. If not specified, this value will be inferred.\\n    init : initializer (mxnet.init.*)\\n        Initializer for this variable to (optionally) override the default initializer.\\n    stype : str\\n        The storage type of the variable, such as 'row_sparse', 'csr', 'default', etc\\n    kwargs : Additional attribute variables\\n        Additional attributes must start and end with double underscores.\\n\\n    Returns\\n    -------\\n    variable : Symbol\\n        A symbol corresponding to an input to the computation graph.\",\n",
       "  'code': 'def var(name, attr=None, shape=None, lr_mult=None, wd_mult=None, dtype=None,\\n        init=None, stype=None, **kwargs):\\n    \"\"\"Creates a symbolic variable with specified name.\\n\\n    Example\\n    -------\\n    >>> data = mx.sym.Variable(\\'data\\', attr={\\'a\\': \\'b\\'})\\n    >>> data\\n    <Symbol data>\\n    >>> csr_data = mx.sym.Variable(\\'csr_data\\', stype=\\'csr\\')\\n    >>> csr_data\\n    <Symbol csr_data>\\n    >>> row_sparse_weight = mx.sym.Variable(\\'weight\\', stype=\\'row_sparse\\')\\n    >>> row_sparse_weight\\n    <Symbol weight>\\n\\n    Parameters\\n    ----------\\n    name : str\\n        Variable name.\\n    attr : Dict of strings\\n        Additional attributes to set on the variable. Format {string : string}.\\n    shape : tuple\\n        The shape of a variable. If specified, this will be used during the shape inference.\\n        If one has specified a different shape for this variable using\\n        a keyword argument when calling shape inference, this shape information will be ignored.\\n    lr_mult : float\\n        The learning rate multiplier for input variable.\\n    wd_mult : float\\n        Weight decay multiplier for input variable.\\n    dtype : str or numpy.dtype\\n        The dtype for input variable. If not specified, this value will be inferred.\\n    init : initializer (mxnet.init.*)\\n        Initializer for this variable to (optionally) override the default initializer.\\n    stype : str\\n        The storage type of the variable, such as \\'row_sparse\\', \\'csr\\', \\'default\\', etc\\n    kwargs : Additional attribute variables\\n        Additional attributes must start and end with double underscores.\\n\\n    Returns\\n    -------\\n    variable : Symbol\\n        A symbol corresponding to an input to the computation graph.\\n    \"\"\"\\n    if not isinstance(name, string_types):\\n        raise TypeError(\\'Expect a string for variable `name`\\')\\n    handle = SymbolHandle()\\n    check_call(_LIB.MXSymbolCreateVariable(c_str(name), ctypes.byref(handle)))\\n    ret = Symbol(handle)\\n    if not hasattr(AttrScope._current, \"value\"):\\n        AttrScope._current.value = AttrScope()\\n    attr = AttrScope._current.value.get(attr)\\n    attr = {} if attr is None else attr\\n    if shape is not None:\\n        attr[\\'__shape__\\'] = str(shape)\\n    if lr_mult is not None:\\n        attr[\\'__lr_mult__\\'] = str(lr_mult)\\n    if wd_mult is not None:\\n        attr[\\'__wd_mult__\\'] = str(wd_mult)\\n    if dtype is not None:\\n        attr[\\'__dtype__\\'] = str(_DTYPE_NP_TO_MX[_numpy.dtype(dtype).type])\\n    if init is not None:\\n        if not isinstance(init, string_types):\\n            init = init.dumps()\\n        attr[\\'__init__\\'] = init\\n    if stype is not None:\\n        attr[\\'__storage_type__\\'] = str(_STORAGE_TYPE_STR_TO_ID[stype])\\n    for k, v in kwargs.items():\\n        if k.startswith(\\'__\\') and k.endswith(\\'__\\'):\\n            attr[k] = str(v)\\n        else:\\n            raise ValueError(\\'Attribute name=%s is not supported.\\'\\n                             \\' Additional attributes must start and end with double underscores,\\'\\n                             \\' e.g, __yourattr__\\' % k)\\n    ret._set_attr(**attr)\\n    return ret',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'pow',\n",
       "  'docstring': \"Returns element-wise result of base element raised to powers from exp element.\\n\\n    Both inputs can be Symbol or scalar number.\\n    Broadcasting is not supported. Use `broadcast_pow` instead.\\n\\n    `sym.pow` is being deprecated, please use `sym.power` instead.\\n\\n    Parameters\\n    ---------\\n    base : Symbol or scalar\\n        The base symbol\\n    exp : Symbol or scalar\\n        The exponent symbol\\n\\n    Returns\\n    -------\\n    Symbol or scalar\\n        The bases in x raised to the exponents in y.\\n\\n    Examples\\n    --------\\n    >>> mx.sym.pow(2, 3)\\n    8\\n    >>> x = mx.sym.Variable('x')\\n    >>> y = mx.sym.Variable('y')\\n    >>> z = mx.sym.pow(x, 2)\\n    >>> z.eval(x=mx.nd.array([1,2]))[0].asnumpy()\\n    array([ 1.,  4.], dtype=float32)\\n    >>> z = mx.sym.pow(3, y)\\n    >>> z.eval(y=mx.nd.array([2,3]))[0].asnumpy()\\n    array([  9.,  27.], dtype=float32)\\n    >>> z = mx.sym.pow(x, y)\\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([2,3]))[0].asnumpy()\\n    array([  9.,  64.], dtype=float32)\",\n",
       "  'code': 'def pow(base, exp):\\n    \"\"\"Returns element-wise result of base element raised to powers from exp element.\\n\\n    Both inputs can be Symbol or scalar number.\\n    Broadcasting is not supported. Use `broadcast_pow` instead.\\n\\n    `sym.pow` is being deprecated, please use `sym.power` instead.\\n\\n    Parameters\\n    ---------\\n    base : Symbol or scalar\\n        The base symbol\\n    exp : Symbol or scalar\\n        The exponent symbol\\n\\n    Returns\\n    -------\\n    Symbol or scalar\\n        The bases in x raised to the exponents in y.\\n\\n    Examples\\n    --------\\n    >>> mx.sym.pow(2, 3)\\n    8\\n    >>> x = mx.sym.Variable(\\'x\\')\\n    >>> y = mx.sym.Variable(\\'y\\')\\n    >>> z = mx.sym.pow(x, 2)\\n    >>> z.eval(x=mx.nd.array([1,2]))[0].asnumpy()\\n    array([ 1.,  4.], dtype=float32)\\n    >>> z = mx.sym.pow(3, y)\\n    >>> z.eval(y=mx.nd.array([2,3]))[0].asnumpy()\\n    array([  9.,  27.], dtype=float32)\\n    >>> z = mx.sym.pow(x, y)\\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([2,3]))[0].asnumpy()\\n    array([  9.,  64.], dtype=float32)\\n    \"\"\"\\n    if isinstance(base, Symbol) and isinstance(exp, Symbol):\\n        return _internal._Power(base, exp)\\n    if isinstance(base, Symbol) and isinstance(exp, Number):\\n        return _internal._PowerScalar(base, scalar=exp)\\n    if isinstance(base, Number) and isinstance(exp, Symbol):\\n        return _internal._RPowerScalar(exp, scalar=base)\\n    if isinstance(base, Number) and isinstance(exp, Number):\\n        return base**exp\\n    else:\\n        raise TypeError(\\'types (%s, %s) not supported\\' % (str(type(base)), str(type(exp))))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'maximum',\n",
       "  'docstring': \"Returns element-wise maximum of the input elements.\\n\\n    Both inputs can be Symbol or scalar number. Broadcasting is not supported.\\n\\n    Parameters\\n    ---------\\n    left : Symbol or scalar\\n        First symbol to be compared.\\n    right : Symbol or scalar\\n        Second symbol to be compared.\\n\\n    Returns\\n    -------\\n    Symbol or scalar\\n        The element-wise maximum of the input symbols.\\n\\n    Examples\\n    --------\\n    >>> mx.sym.maximum(2, 3.5)\\n    3.5\\n    >>> x = mx.sym.Variable('x')\\n    >>> y = mx.sym.Variable('y')\\n    >>> z = mx.sym.maximum(x, 4)\\n    >>> z.eval(x=mx.nd.array([3,5,2,10]))[0].asnumpy()\\n    array([  4.,   5.,   4.,  10.], dtype=float32)\\n    >>> z = mx.sym.maximum(x, y)\\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([10,2]))[0].asnumpy()\\n    array([ 10.,   4.], dtype=float32)\",\n",
       "  'code': 'def maximum(left, right):\\n    \"\"\"Returns element-wise maximum of the input elements.\\n\\n    Both inputs can be Symbol or scalar number. Broadcasting is not supported.\\n\\n    Parameters\\n    ---------\\n    left : Symbol or scalar\\n        First symbol to be compared.\\n    right : Symbol or scalar\\n        Second symbol to be compared.\\n\\n    Returns\\n    -------\\n    Symbol or scalar\\n        The element-wise maximum of the input symbols.\\n\\n    Examples\\n    --------\\n    >>> mx.sym.maximum(2, 3.5)\\n    3.5\\n    >>> x = mx.sym.Variable(\\'x\\')\\n    >>> y = mx.sym.Variable(\\'y\\')\\n    >>> z = mx.sym.maximum(x, 4)\\n    >>> z.eval(x=mx.nd.array([3,5,2,10]))[0].asnumpy()\\n    array([  4.,   5.,   4.,  10.], dtype=float32)\\n    >>> z = mx.sym.maximum(x, y)\\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([10,2]))[0].asnumpy()\\n    array([ 10.,   4.], dtype=float32)\\n    \"\"\"\\n    if isinstance(left, Symbol) and isinstance(right, Symbol):\\n        return _internal._Maximum(left, right)\\n    if isinstance(left, Symbol) and isinstance(right, Number):\\n        return _internal._MaximumScalar(left, scalar=right)\\n    if isinstance(left, Number) and isinstance(right, Symbol):\\n        return _internal._MaximumScalar(right, scalar=left)\\n    if isinstance(left, Number) and isinstance(right, Number):\\n        return left if left > right else right\\n    else:\\n        raise TypeError(\\'types (%s, %s) not supported\\' % (str(type(left)), str(type(right))))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'minimum',\n",
       "  'docstring': \"Returns element-wise minimum of the input elements.\\n\\n    Both inputs can be Symbol or scalar number. Broadcasting is not supported.\\n\\n    Parameters\\n    ---------\\n    left : Symbol or scalar\\n        First symbol to be compared.\\n    right : Symbol or scalar\\n        Second symbol to be compared.\\n\\n    Returns\\n    -------\\n    Symbol or scalar\\n        The element-wise minimum of the input symbols.\\n\\n    Examples\\n    --------\\n    >>> mx.sym.minimum(2, 3.5)\\n    2\\n    >>> x = mx.sym.Variable('x')\\n    >>> y = mx.sym.Variable('y')\\n    >>> z = mx.sym.minimum(x, 4)\\n    >>> z.eval(x=mx.nd.array([3,5,2,10]))[0].asnumpy()\\n    array([ 3.,  4.,  2.,  4.], dtype=float32)\\n    >>> z = mx.sym.minimum(x, y)\\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([10,2]))[0].asnumpy()\\n    array([ 3.,  2.], dtype=float32)\",\n",
       "  'code': 'def minimum(left, right):\\n    \"\"\"Returns element-wise minimum of the input elements.\\n\\n    Both inputs can be Symbol or scalar number. Broadcasting is not supported.\\n\\n    Parameters\\n    ---------\\n    left : Symbol or scalar\\n        First symbol to be compared.\\n    right : Symbol or scalar\\n        Second symbol to be compared.\\n\\n    Returns\\n    -------\\n    Symbol or scalar\\n        The element-wise minimum of the input symbols.\\n\\n    Examples\\n    --------\\n    >>> mx.sym.minimum(2, 3.5)\\n    2\\n    >>> x = mx.sym.Variable(\\'x\\')\\n    >>> y = mx.sym.Variable(\\'y\\')\\n    >>> z = mx.sym.minimum(x, 4)\\n    >>> z.eval(x=mx.nd.array([3,5,2,10]))[0].asnumpy()\\n    array([ 3.,  4.,  2.,  4.], dtype=float32)\\n    >>> z = mx.sym.minimum(x, y)\\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([10,2]))[0].asnumpy()\\n    array([ 3.,  2.], dtype=float32)\\n    \"\"\"\\n    if isinstance(left, Symbol) and isinstance(right, Symbol):\\n        return _internal._Minimum(left, right)\\n    if isinstance(left, Symbol) and isinstance(right, Number):\\n        return _internal._MinimumScalar(left, scalar=right)\\n    if isinstance(left, Number) and isinstance(right, Symbol):\\n        return _internal._MinimumScalar(right, scalar=left)\\n    if isinstance(left, Number) and isinstance(right, Number):\\n        return left if left < right else right\\n    else:\\n        raise TypeError(\\'types (%s, %s) not supported\\' % (str(type(left)), str(type(right))))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'hypot',\n",
       "  'docstring': 'Given the \"legs\" of a right triangle, returns its hypotenuse.\\n\\n    Equivalent to :math:`\\\\\\\\sqrt(left^2 + right^2)`, element-wise.\\n    Both inputs can be Symbol or scalar number. Broadcasting is not supported.\\n\\n    Parameters\\n    ---------\\n    left : Symbol or scalar\\n        First leg of the triangle(s).\\n    right : Symbol or scalar\\n        Second leg of the triangle(s).\\n\\n    Returns\\n    -------\\n    Symbol or scalar\\n        The hypotenuse of the triangle(s)\\n\\n    Examples\\n    --------\\n    >>> mx.sym.hypot(3, 4)\\n    5.0\\n    >>> x = mx.sym.Variable(\\'x\\')\\n    >>> y = mx.sym.Variable(\\'y\\')\\n    >>> z = mx.sym.hypot(x, 4)\\n    >>> z.eval(x=mx.nd.array([3,5,2]))[0].asnumpy()\\n    array([ 5.,  6.40312433,  4.47213602], dtype=float32)\\n    >>> z = mx.sym.hypot(x, y)\\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([10,2]))[0].asnumpy()\\n    array([ 10.44030666,   4.47213602], dtype=float32)',\n",
       "  'code': 'def hypot(left, right):\\n    \"\"\"Given the \"legs\" of a right triangle, returns its hypotenuse.\\n\\n    Equivalent to :math:`\\\\\\\\sqrt(left^2 + right^2)`, element-wise.\\n    Both inputs can be Symbol or scalar number. Broadcasting is not supported.\\n\\n    Parameters\\n    ---------\\n    left : Symbol or scalar\\n        First leg of the triangle(s).\\n    right : Symbol or scalar\\n        Second leg of the triangle(s).\\n\\n    Returns\\n    -------\\n    Symbol or scalar\\n        The hypotenuse of the triangle(s)\\n\\n    Examples\\n    --------\\n    >>> mx.sym.hypot(3, 4)\\n    5.0\\n    >>> x = mx.sym.Variable(\\'x\\')\\n    >>> y = mx.sym.Variable(\\'y\\')\\n    >>> z = mx.sym.hypot(x, 4)\\n    >>> z.eval(x=mx.nd.array([3,5,2]))[0].asnumpy()\\n    array([ 5.,  6.40312433,  4.47213602], dtype=float32)\\n    >>> z = mx.sym.hypot(x, y)\\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([10,2]))[0].asnumpy()\\n    array([ 10.44030666,   4.47213602], dtype=float32)\\n    \"\"\"\\n    if isinstance(left, Symbol) and isinstance(right, Symbol):\\n        return _internal._Hypot(left, right)\\n    if isinstance(left, Symbol) and isinstance(right, Number):\\n        return _internal._HypotScalar(left, scalar=right)\\n    if isinstance(left, Number) and isinstance(right, Symbol):\\n        return _internal._HypotScalar(right, scalar=left)\\n    if isinstance(left, Number) and isinstance(right, Number):\\n        return _numpy.hypot(left, right)\\n    else:\\n        raise TypeError(\\'types (%s, %s) not supported\\' % (str(type(left)), str(type(right))))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'eye',\n",
       "  'docstring': 'Returns a new symbol of 2-D shpae, filled with ones on the diagonal and zeros elsewhere.\\n\\n    Parameters\\n    ----------\\n    N: int\\n        Number of rows in the output.\\n    M: int, optional\\n        Number of columns in the output. If 0, defaults to N.\\n    k: int, optional\\n        Index of the diagonal: 0 (the default) refers to the main diagonal,\\n        a positive value refers to an upper diagonal,\\n        and a negative value to a lower diagonal.\\n    dtype : str or numpy.dtype, optional\\n        The value type of the inner value, default to ``np.float32``.\\n\\n    Returns\\n    -------\\n    out : Symbol\\n        The created Symbol.',\n",
       "  'code': 'def eye(N, M=0, k=0, dtype=None, **kwargs):\\n    \"\"\"Returns a new symbol of 2-D shpae, filled with ones on the diagonal and zeros elsewhere.\\n\\n    Parameters\\n    ----------\\n    N: int\\n        Number of rows in the output.\\n    M: int, optional\\n        Number of columns in the output. If 0, defaults to N.\\n    k: int, optional\\n        Index of the diagonal: 0 (the default) refers to the main diagonal,\\n        a positive value refers to an upper diagonal,\\n        and a negative value to a lower diagonal.\\n    dtype : str or numpy.dtype, optional\\n        The value type of the inner value, default to ``np.float32``.\\n\\n    Returns\\n    -------\\n    out : Symbol\\n        The created Symbol.\\n    \"\"\"\\n    if dtype is None:\\n        dtype = _numpy.float32\\n    return _internal._eye(N, M, k, dtype=dtype, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'zeros',\n",
       "  'docstring': 'Returns a new symbol of given shape and type, filled with zeros.\\n\\n    Parameters\\n    ----------\\n    shape :  int or sequence of ints\\n        Shape of the new array.\\n    dtype : str or numpy.dtype, optional\\n        The value type of the inner value, default to ``np.float32``.\\n\\n    Returns\\n    -------\\n    out : Symbol\\n        The created Symbol.',\n",
       "  'code': 'def zeros(shape, dtype=None, **kwargs):\\n    \"\"\"Returns a new symbol of given shape and type, filled with zeros.\\n\\n    Parameters\\n    ----------\\n    shape :  int or sequence of ints\\n        Shape of the new array.\\n    dtype : str or numpy.dtype, optional\\n        The value type of the inner value, default to ``np.float32``.\\n\\n    Returns\\n    -------\\n    out : Symbol\\n        The created Symbol.\\n    \"\"\"\\n    if dtype is None:\\n        dtype = _numpy.float32\\n    return _internal._zeros(shape=shape, dtype=dtype, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'ones',\n",
       "  'docstring': 'Returns a new symbol of given shape and type, filled with ones.\\n\\n    Parameters\\n    ----------\\n    shape :  int or sequence of ints\\n        Shape of the new array.\\n    dtype : str or numpy.dtype, optional\\n        The value type of the inner value, default to ``np.float32``.\\n\\n    Returns\\n    -------\\n    out : Symbol\\n        The created Symbol',\n",
       "  'code': 'def ones(shape, dtype=None, **kwargs):\\n    \"\"\"Returns a new symbol of given shape and type, filled with ones.\\n\\n    Parameters\\n    ----------\\n    shape :  int or sequence of ints\\n        Shape of the new array.\\n    dtype : str or numpy.dtype, optional\\n        The value type of the inner value, default to ``np.float32``.\\n\\n    Returns\\n    -------\\n    out : Symbol\\n        The created Symbol\\n    \"\"\"\\n    if dtype is None:\\n        dtype = _numpy.float32\\n    return _internal._ones(shape=shape, dtype=dtype, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'full',\n",
       "  'docstring': 'Returns a new array of given shape and type, filled with the given value `val`.\\n\\n    Parameters\\n    ----------\\n    shape :  int or sequence of ints\\n        Shape of the new array.\\n    val : scalar\\n        Fill value.\\n    dtype : str or numpy.dtype, optional\\n        The value type of the inner value, default to ``np.float32``.\\n\\n    Returns\\n    -------\\n    out : Symbol\\n        The created Symbol',\n",
       "  'code': 'def full(shape, val, dtype=None, **kwargs):\\n    \"\"\"Returns a new array of given shape and type, filled with the given value `val`.\\n\\n    Parameters\\n    ----------\\n    shape :  int or sequence of ints\\n        Shape of the new array.\\n    val : scalar\\n        Fill value.\\n    dtype : str or numpy.dtype, optional\\n        The value type of the inner value, default to ``np.float32``.\\n\\n    Returns\\n    -------\\n    out : Symbol\\n        The created Symbol\\n    \"\"\"\\n    if dtype is None:\\n        dtype = _numpy.float32\\n    return _internal._full(shape=shape, dtype=dtype, value=float(val), **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'arange',\n",
       "  'docstring': 'Returns evenly spaced values within a given interval.\\n\\n    Values are generated within the half-open interval [`start`, `stop`). In other\\n    words, the interval includes `start` but excludes `stop`. The function is\\n    similar to the built-in Python function `range` and to `numpy.arange`,\\n    but returns a `Symbol`.\\n\\n    Parameters\\n    ----------\\n    start : number, optional\\n        Start of interval. The interval includes this value. The default start value is 0.\\n    stop : number\\n        End of interval. The interval does not include this value.\\n    step : number, optional\\n        Spacing between values.\\n    repeat : int, optional\\n        \"The repeating time of all elements.\\n        E.g repeat=3, the element a will be repeated three times --> a, a, a.\\n    infer_range : boolean, optional\\n        When set to True, infer the stop position from the start, step,\\n        repeat, and output tensor size.\\n    dtype : str or numpy.dtype, optional\\n        The value type of the inner value, default to ``np.float32``.\\n\\n    Returns\\n    -------\\n    out : Symbol\\n        The created Symbol',\n",
       "  'code': 'def arange(start, stop=None, step=1.0, repeat=1, infer_range=False, name=None, dtype=None):\\n    \"\"\"Returns evenly spaced values within a given interval.\\n\\n    Values are generated within the half-open interval [`start`, `stop`). In other\\n    words, the interval includes `start` but excludes `stop`. The function is\\n    similar to the built-in Python function `range` and to `numpy.arange`,\\n    but returns a `Symbol`.\\n\\n    Parameters\\n    ----------\\n    start : number, optional\\n        Start of interval. The interval includes this value. The default start value is 0.\\n    stop : number\\n        End of interval. The interval does not include this value.\\n    step : number, optional\\n        Spacing between values.\\n    repeat : int, optional\\n        \"The repeating time of all elements.\\n        E.g repeat=3, the element a will be repeated three times --> a, a, a.\\n    infer_range : boolean, optional\\n        When set to True, infer the stop position from the start, step,\\n        repeat, and output tensor size.\\n    dtype : str or numpy.dtype, optional\\n        The value type of the inner value, default to ``np.float32``.\\n\\n    Returns\\n    -------\\n    out : Symbol\\n        The created Symbol\\n    \"\"\"\\n    if dtype is None:\\n        dtype = _numpy.float32\\n    return _internal._arange(start=start, stop=stop, step=step, repeat=repeat,\\n                             infer_range=infer_range, name=name, dtype=dtype)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Symbol.infer_type',\n",
       "  'docstring': \"Infers the type of all arguments and all outputs, given the known types\\n        for some arguments.\\n\\n        This function takes the known types of some arguments in either positional way\\n        or keyword argument way as input. It returns a tuple of `None` values\\n        if there is not enough information to deduce the missing types.\\n\\n        Inconsistencies in the known types will cause an error to be raised.\\n\\n        Example\\n        -------\\n        >>> a = mx.sym.var('a')\\n        >>> b = mx.sym.var('b')\\n        >>> c = a + b\\n        >>> arg_types, out_types, aux_types = c.infer_type(a='float32')\\n        >>> arg_types\\n        [<type 'numpy.float32'>, <type 'numpy.float32'>]\\n        >>> out_types\\n        [<type 'numpy.float32'>]\\n        >>> aux_types\\n        []\\n\\n        Parameters\\n        ----------\\n        *args :\\n            Type of known arguments in a positional way.\\n            Unknown type can be marked as None.\\n\\n        **kwargs :\\n            Keyword arguments of known types.\\n\\n        Returns\\n        -------\\n        arg_types : list of numpy.dtype or None\\n            List of argument types.\\n            The order is same as the order of list_arguments().\\n        out_types : list of numpy.dtype or None\\n            List of output types.\\n            The order is same as the order of list_outputs().\\n        aux_types : list of numpy.dtype or None\\n            List of auxiliary state types.\\n            The order is same as the order of list_auxiliary_states().\",\n",
       "  'code': 'def infer_type(self, *args, **kwargs):\\n        \"\"\"Infers the type of all arguments and all outputs, given the known types\\n        for some arguments.\\n\\n        This function takes the known types of some arguments in either positional way\\n        or keyword argument way as input. It returns a tuple of `None` values\\n        if there is not enough information to deduce the missing types.\\n\\n        Inconsistencies in the known types will cause an error to be raised.\\n\\n        Example\\n        -------\\n        >>> a = mx.sym.var(\\'a\\')\\n        >>> b = mx.sym.var(\\'b\\')\\n        >>> c = a + b\\n        >>> arg_types, out_types, aux_types = c.infer_type(a=\\'float32\\')\\n        >>> arg_types\\n        [<type \\'numpy.float32\\'>, <type \\'numpy.float32\\'>]\\n        >>> out_types\\n        [<type \\'numpy.float32\\'>]\\n        >>> aux_types\\n        []\\n\\n        Parameters\\n        ----------\\n        *args :\\n            Type of known arguments in a positional way.\\n            Unknown type can be marked as None.\\n\\n        **kwargs :\\n            Keyword arguments of known types.\\n\\n        Returns\\n        -------\\n        arg_types : list of numpy.dtype or None\\n            List of argument types.\\n            The order is same as the order of list_arguments().\\n        out_types : list of numpy.dtype or None\\n            List of output types.\\n            The order is same as the order of list_outputs().\\n        aux_types : list of numpy.dtype or None\\n            List of auxiliary state types.\\n            The order is same as the order of list_auxiliary_states().\\n        \"\"\"\\n        try:\\n            res = self._infer_type_impl(False, *args, **kwargs)\\n            if res[1] is None:\\n                arg_shapes, _, _ = self._infer_type_impl(True, *args, **kwargs)\\n                arg_names = self.list_arguments()\\n                unknowns = []\\n                for name, dtype in zip(arg_names, arg_shapes):\\n                    if not dtype:\\n                        if len(unknowns) >= 10:\\n                            unknowns.append(\\'...\\')\\n                            break\\n                        unknowns.append(\\'%s: %s\\' % (name, str(dtype)))\\n                warnings.warn(\\n                    \"Cannot decide type for the following arguments. \" +\\n                    \"Consider providing them as input:\\\\n\\\\t\" +\\n                    \"\\\\n\\\\t\".join(unknowns), stacklevel=2)\\n            return res\\n        except MXNetError:\\n            print(\"infer_type error. Arguments:\")\\n            for i, arg in enumerate(args):\\n                print(\"  #%d: %s\" % (i, arg))\\n            for k, v in kwargs.items():\\n                print(\"  %s: %s\" % (k, v))\\n            raise',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Symbol._infer_type_impl',\n",
       "  'docstring': 'The actual implementation for calling type inference API.',\n",
       "  'code': 'def _infer_type_impl(self, partial, *args, **kwargs):\\n        \"\"\"The actual implementation for calling type inference API.\"\"\"\\n        # pylint: disable=too-many-locals\\n        if len(args) != 0 and len(kwargs) != 0:\\n            raise ValueError(\\'Can only specify known argument \\\\\\n                    types either by positional or kwargs way.\\')\\n        sdata = []\\n        if len(args) != 0:\\n            keys = c_array(ctypes.c_char_p, [])\\n            for s in args:\\n                if s is not None:\\n                    s = _numpy.dtype(s).type\\n                    if s not in _DTYPE_NP_TO_MX:\\n                        raise TypeError(\\'Argument need to be one of \\' + str(_DTYPE_NP_TO_MX))\\n                    sdata.append(_DTYPE_NP_TO_MX[s])\\n                else:\\n                    sdata.append(-1)\\n        else:\\n            str_keys = []\\n            for k, v in kwargs.items():\\n                v = _numpy.dtype(v).type\\n                if v in _DTYPE_NP_TO_MX:\\n                    str_keys.append(k)\\n                    sdata.append(_DTYPE_NP_TO_MX[v])\\n            keys = c_str_array(str_keys)\\n        arg_type_size = mx_uint()\\n        arg_type_data = ctypes.POINTER(ctypes.c_int)()\\n        out_type_size = mx_uint()\\n        out_type_data = ctypes.POINTER(ctypes.c_int)()\\n        aux_type_size = mx_uint()\\n        aux_type_data = ctypes.POINTER(ctypes.c_int)()\\n        complete = ctypes.c_int()\\n        if partial:\\n            infer_func = _LIB.MXSymbolInferTypePartial\\n        else:\\n            infer_func = _LIB.MXSymbolInferType\\n        check_call(infer_func(\\n            self.handle,\\n            mx_uint(len(sdata)),\\n            keys,\\n            c_array_buf(ctypes.c_int, array(\\'i\\', sdata)),\\n            ctypes.byref(arg_type_size),\\n            ctypes.byref(arg_type_data),\\n            ctypes.byref(out_type_size),\\n            ctypes.byref(out_type_data),\\n            ctypes.byref(aux_type_size),\\n            ctypes.byref(aux_type_data),\\n            ctypes.byref(complete)))\\n        if complete.value != 0:\\n            arg_types = [\\n                _DTYPE_MX_TO_NP[arg_type_data[i]] for i in range(arg_type_size.value)]\\n            out_types = [\\n                _DTYPE_MX_TO_NP[out_type_data[i]] for i in range(out_type_size.value)]\\n            aux_types = [\\n                _DTYPE_MX_TO_NP[aux_type_data[i]] for i in range(aux_type_size.value)]\\n            return (arg_types, out_types, aux_types)\\n        else:\\n            return (None, None, None)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Symbol.simple_bind',\n",
       "  'docstring': \"Bind current symbol to get an executor, allocate all the arguments needed.\\n        Allows specifying data types.\\n\\n        This function simplifies the binding procedure. You need to specify only input data shapes.\\n        Before binding the executor, the function allocates arguments and auxiliary states\\n        that were not explicitly specified. Allows specifying data types.\\n\\n        Example\\n        -------\\n        >>> x = mx.sym.Variable('x')\\n        >>> y = mx.sym.FullyConnected(x, num_hidden=4)\\n        >>> exe = y.simple_bind(mx.cpu(), x=(5,4), grad_req='null')\\n        >>> exe.forward()\\n        [<NDArray 5x4 @cpu(0)>]\\n        >>> exe.outputs[0].asnumpy()\\n        array([[ 0.,  0.,  0.,  0.],\\n               [ 0.,  0.,  0.,  0.],\\n               [ 0.,  0.,  0.,  0.],\\n               [ 0.,  0.,  0.,  0.],\\n               [ 0.,  0.,  0.,  0.]], dtype=float32)\\n        >>> exe.arg_arrays\\n        [<NDArray 5x4 @cpu(0)>, <NDArray 4x4 @cpu(0)>, <NDArray 4 @cpu(0)>]\\n        >>> exe.grad_arrays\\n        [<NDArray 5x4 @cpu(0)>, <NDArray 4x4 @cpu(0)>, <NDArray 4 @cpu(0)>]\\n\\n        Parameters\\n        ----------\\n        ctx : Context\\n            The device context the generated executor to run on.\\n\\n        grad_req: string\\n            {'write', 'add', 'null'}, or list of str or dict of str to str, optional\\n            To specify how we should update the gradient to the `args_grad`.\\n\\n            - 'write' means every time gradient is written to specified `args_grad` NDArray.\\n            - 'add' means every time gradient is added to the specified NDArray.\\n            - 'null' means no action is taken, the gradient may not be calculated.\\n\\n        type_dict  : Dict of str->numpy.dtype\\n            Input type dictionary, name->dtype\\n\\n        stype_dict  : Dict of str->str\\n            Input storage type dictionary, name->storage_type\\n\\n        group2ctx : Dict of string to mx.Context\\n            The dict mapping the `ctx_group` attribute to the context assignment.\\n\\n        shared_arg_names : List of string\\n            The argument names whose `NDArray` of shared_exec can be reused for initializing\\n            the current executor.\\n\\n        shared_exec : Executor\\n            The executor whose arg_arrays, arg_arrays, grad_arrays, and aux_arrays can be\\n            reused for initializing the current executor.\\n\\n        shared_buffer : Dict of string to `NDArray`\\n            The dict mapping argument names to the `NDArray` that can be reused for initializing\\n            the current executor. This buffer will be checked for reuse if one argument name\\n            of the current executor is not found in `shared_arg_names`. The `NDArray` s are\\n            expected have default storage type.\\n\\n        kwargs : Dict of str->shape\\n            Input shape dictionary, name->shape\\n\\n        Returns\\n        -------\\n        executor : mxnet.Executor\\n            The generated executor\",\n",
       "  'code': 'def simple_bind(self, ctx, grad_req=\\'write\\', type_dict=None, stype_dict=None,\\n                    group2ctx=None, shared_arg_names=None, shared_exec=None,\\n                    shared_buffer=None, **kwargs):\\n        \"\"\"Bind current symbol to get an executor, allocate all the arguments needed.\\n        Allows specifying data types.\\n\\n        This function simplifies the binding procedure. You need to specify only input data shapes.\\n        Before binding the executor, the function allocates arguments and auxiliary states\\n        that were not explicitly specified. Allows specifying data types.\\n\\n        Example\\n        -------\\n        >>> x = mx.sym.Variable(\\'x\\')\\n        >>> y = mx.sym.FullyConnected(x, num_hidden=4)\\n        >>> exe = y.simple_bind(mx.cpu(), x=(5,4), grad_req=\\'null\\')\\n        >>> exe.forward()\\n        [<NDArray 5x4 @cpu(0)>]\\n        >>> exe.outputs[0].asnumpy()\\n        array([[ 0.,  0.,  0.,  0.],\\n               [ 0.,  0.,  0.,  0.],\\n               [ 0.,  0.,  0.,  0.],\\n               [ 0.,  0.,  0.,  0.],\\n               [ 0.,  0.,  0.,  0.]], dtype=float32)\\n        >>> exe.arg_arrays\\n        [<NDArray 5x4 @cpu(0)>, <NDArray 4x4 @cpu(0)>, <NDArray 4 @cpu(0)>]\\n        >>> exe.grad_arrays\\n        [<NDArray 5x4 @cpu(0)>, <NDArray 4x4 @cpu(0)>, <NDArray 4 @cpu(0)>]\\n\\n        Parameters\\n        ----------\\n        ctx : Context\\n            The device context the generated executor to run on.\\n\\n        grad_req: string\\n            {\\'write\\', \\'add\\', \\'null\\'}, or list of str or dict of str to str, optional\\n            To specify how we should update the gradient to the `args_grad`.\\n\\n            - \\'write\\' means every time gradient is written to specified `args_grad` NDArray.\\n            - \\'add\\' means every time gradient is added to the specified NDArray.\\n            - \\'null\\' means no action is taken, the gradient may not be calculated.\\n\\n        type_dict  : Dict of str->numpy.dtype\\n            Input type dictionary, name->dtype\\n\\n        stype_dict  : Dict of str->str\\n            Input storage type dictionary, name->storage_type\\n\\n        group2ctx : Dict of string to mx.Context\\n            The dict mapping the `ctx_group` attribute to the context assignment.\\n\\n        shared_arg_names : List of string\\n            The argument names whose `NDArray` of shared_exec can be reused for initializing\\n            the current executor.\\n\\n        shared_exec : Executor\\n            The executor whose arg_arrays, arg_arrays, grad_arrays, and aux_arrays can be\\n            reused for initializing the current executor.\\n\\n        shared_buffer : Dict of string to `NDArray`\\n            The dict mapping argument names to the `NDArray` that can be reused for initializing\\n            the current executor. This buffer will be checked for reuse if one argument name\\n            of the current executor is not found in `shared_arg_names`. The `NDArray` s are\\n            expected have default storage type.\\n\\n        kwargs : Dict of str->shape\\n            Input shape dictionary, name->shape\\n\\n        Returns\\n        -------\\n        executor : mxnet.Executor\\n            The generated executor\\n        \"\"\"\\n        # data types\\n        num_provided_arg_types = 0\\n        provided_arg_type_names = ctypes.POINTER(ctypes.c_char_p)()  # provided type argument names\\n        provided_arg_type_data = ctypes.POINTER(mx_uint)()  # provided types\\n        if type_dict is not None:\\n            provided_arg_type_names = []\\n            provided_arg_type_data = []\\n            for k, v in type_dict.items():\\n                v = _numpy.dtype(v).type\\n                if v in _DTYPE_NP_TO_MX:\\n                    provided_arg_type_names.append(k)\\n                    provided_arg_type_data.append(_DTYPE_NP_TO_MX[v])\\n            num_provided_arg_types = mx_uint(len(provided_arg_type_names))\\n            provided_arg_type_names = c_str_array(provided_arg_type_names)\\n            provided_arg_type_data = c_array_buf(ctypes.c_int, array(\\'i\\', provided_arg_type_data))\\n\\n        # storage types\\n        num_provided_arg_stypes = 0\\n        # provided storage type argument names\\n        provided_arg_stype_names = ctypes.POINTER(ctypes.c_char_p)()\\n        provided_arg_stype_data = ctypes.POINTER(mx_uint)()  # provided storage types\\n        if stype_dict is not None:\\n            provided_arg_stype_names = []\\n            provided_arg_stype_data = []\\n            for k, v in stype_dict.items():\\n                if v in _STORAGE_TYPE_STR_TO_ID:\\n                    provided_arg_stype_names.append(k)\\n                    provided_arg_stype_data.append(_STORAGE_TYPE_STR_TO_ID[v])\\n            num_provided_arg_stypes = mx_uint(len(provided_arg_stype_names))\\n            provided_arg_stype_names = c_str_array(provided_arg_stype_names)\\n            provided_arg_stype_data = c_array_buf(ctypes.c_int, array(\\'i\\', provided_arg_stype_data))\\n\\n        provided_arg_shape_data = []  # shape data\\n        # argument shape index in sdata,\\n        # e.g. [sdata[indptr[0]], sdata[indptr[1]]) is the shape of the first arg\\n        provided_arg_shape_idx = [0]\\n        provided_arg_shape_names = []  # provided argument names\\n        for k, v in kwargs.items():\\n            # if k not in listed_arguments and k not in listed_aux_states:\\n            #   raise ValueError(\\'arg name %s is not valid\\', k)\\n            if isinstance(v, tuple):\\n                provided_arg_shape_names.append(k)\\n                provided_arg_shape_data.extend(v)\\n                provided_arg_shape_idx.append(len(provided_arg_shape_data))\\n\\n        provided_req_type_list_len = 0\\n        provided_grad_req_types = ctypes.POINTER(ctypes.c_char_p)()\\n        provided_grad_req_names = ctypes.POINTER(ctypes.c_char_p)()\\n        if grad_req is not None:\\n            if isinstance(grad_req, string_types):\\n                # use provided_req_type_list_len = 0 to indicate this situation\\n                provided_req_type_list_len = 0\\n                provided_grad_req_types = [grad_req]\\n            elif isinstance(grad_req, list):\\n                if len(grad_req) == 0:\\n                    raise RuntimeError(\\'grad_req in simple_bind cannot be an empty list\\')\\n                provided_grad_req_types = grad_req\\n                provided_req_type_list_len = len(provided_grad_req_types)\\n            elif isinstance(grad_req, dict):\\n                if len(grad_req) == 0:\\n                    raise RuntimeError(\\'grad_req in simple_bind cannot be an empty dict\\')\\n                provided_grad_req_names = []\\n                provided_grad_req_types = []\\n                for k, v in grad_req.items():\\n                    provided_grad_req_names.append(k)\\n                    provided_grad_req_types.append(v)\\n                provided_grad_req_names = c_str_array(provided_grad_req_names)\\n                provided_req_type_list_len = len(provided_grad_req_types)\\n            provided_grad_req_types = c_str_array(provided_grad_req_types)\\n\\n        num_ctx_map_keys = mx_uint(0)\\n        ctx_map_keys = ctypes.POINTER(ctypes.c_char_p)()\\n        ctx_map_dev_types = ctypes.POINTER(ctypes.c_int)()\\n        ctx_map_dev_ids = ctypes.POINTER(ctypes.c_int)()\\n        if group2ctx is not None:\\n            ctx_map_keys = []\\n            ctx_map_dev_types = []\\n            ctx_map_dev_ids = []\\n            for key, val in group2ctx.items():\\n                ctx_map_keys.append(key)\\n                ctx_map_dev_types.append(val.device_typeid)\\n                ctx_map_dev_ids.append(val.device_id)\\n            num_ctx_map_keys = mx_uint(len(ctx_map_keys))\\n            ctx_map_keys = c_str_array(ctx_map_keys)\\n            ctx_map_dev_types = c_array(ctypes.c_int, array(\\'i\\', ctx_map_dev_types))\\n            ctx_map_dev_ids = c_array(ctypes.c_int, array(\\'i\\', ctx_map_dev_ids))\\n\\n        # prepare param names\\n        shared_arg_name_list = []\\n        if shared_arg_names is not None:\\n            if not isinstance(shared_arg_names, list):\\n                raise ValueError(\\'shared_arg_names in simple_bind must be a list or None\\')\\n            shared_arg_name_list = shared_arg_names\\n\\n        # prepare shared_buffer\\n        if shared_buffer is None:\\n            shared_buffer_len = ctypes.c_int(-1)\\n            shared_buffer_names = ctypes.POINTER(ctypes.c_char_p)()\\n            shared_buffer_handles = ctypes.POINTER(NDArrayHandle)()\\n        else:\\n            if not isinstance(shared_buffer, dict):\\n                raise ValueError(\\'shared_buffer in simple_bind must be dict or None\\')\\n            buffer_names = shared_buffer.keys()\\n            buffer_arrays = shared_buffer.values()\\n            for v in buffer_arrays:\\n                assert(v.stype == \\'default\\'), \\\\\\n                    \"shared_buffer is expected to only contain NDArrays with default storage\"\\n            shared_buffer_names = c_str_array(buffer_names)\\n            shared_buffer_len = ctypes.c_int(len(buffer_arrays))\\n            shared_buffer_handles = c_handle_array(buffer_arrays)\\n        updated_shared_buffer_names = ctypes.POINTER(ctypes.c_char_p)()\\n        updated_shared_buffer_handles = ctypes.POINTER(NDArrayHandle)()\\n\\n        # prepare shared_exec_handle\\n        shared_exec_handle = shared_exec.handle if shared_exec is not None else ExecutorHandle()\\n\\n        # prepare current executor handle\\n        exe_handle = ExecutorHandle()\\n\\n        # prepare current executor\\'s in_args, arg_grads, and aux_states\\n        num_in_args = ctypes.c_uint()\\n        in_arg_handles = ctypes.POINTER(NDArrayHandle)()\\n        arg_grad_handles = ctypes.POINTER(NDArrayHandle)()\\n        num_aux_states = ctypes.c_uint()\\n        aux_state_handles = ctypes.POINTER(NDArrayHandle)()\\n\\n        try:\\n            check_call(_LIB.MXExecutorSimpleBindEx(self.handle,\\n                                                   ctypes.c_int(ctx.device_typeid),\\n                                                   ctypes.c_int(ctx.device_id),\\n                                                   num_ctx_map_keys,\\n                                                   ctx_map_keys,\\n                                                   ctx_map_dev_types,\\n                                                   ctx_map_dev_ids,\\n                                                   mx_uint(provided_req_type_list_len),\\n                                                   provided_grad_req_names,\\n                                                   provided_grad_req_types,\\n                                                   mx_uint(len(provided_arg_shape_names)),\\n                                                   c_str_array(provided_arg_shape_names),\\n                                                   c_array_buf(mx_int,\\n                                                               array(\\'I\\', provided_arg_shape_data)),\\n                                                   c_array_buf(mx_uint,\\n                                                               array(\\'i\\', provided_arg_shape_idx)),\\n                                                   num_provided_arg_types,\\n                                                   provided_arg_type_names,\\n                                                   provided_arg_type_data,\\n                                                   num_provided_arg_stypes,\\n                                                   provided_arg_stype_names,\\n                                                   provided_arg_stype_data,\\n                                                   mx_uint(len(shared_arg_name_list)),\\n                                                   c_str_array(shared_arg_name_list),\\n                                                   ctypes.byref(shared_buffer_len),\\n                                                   shared_buffer_names,\\n                                                   shared_buffer_handles,\\n                                                   ctypes.byref(updated_shared_buffer_names),\\n                                                   ctypes.byref(updated_shared_buffer_handles),\\n                                                   ctypes.byref(num_in_args),\\n                                                   ctypes.byref(in_arg_handles),\\n                                                   ctypes.byref(arg_grad_handles),\\n                                                   ctypes.byref(num_aux_states),\\n                                                   ctypes.byref(aux_state_handles),\\n                                                   shared_exec_handle,\\n                                                   ctypes.byref(exe_handle)))\\n        except MXNetError as e:\\n            error_msg = \"simple_bind error. Arguments:\\\\n\"\\n            for k, v in kwargs.items():\\n                error_msg += \"%s: %s\\\\n\" % (k, v)\\n            error_msg += \"%s\" % e\\n            raise RuntimeError(error_msg)\\n\\n        # update shared_buffer\\n        if shared_buffer is not None:\\n            for i in range(shared_buffer_len.value):\\n                k = py_str(updated_shared_buffer_names[i])\\n                v = NDArray(NDArrayHandle(updated_shared_buffer_handles[i]))\\n                shared_buffer[k] = v\\n\\n        # create in_args, arg_grads, and aux_states for the current executor\\n        arg_arrays = [_ndarray_cls(NDArrayHandle(in_arg_handles[i]))\\n                      for i in range(num_in_args.value)]\\n        grad_arrays = [_ndarray_cls(NDArrayHandle(arg_grad_handles[i]))\\n                       if arg_grad_handles[i] is not None\\n                       else None for i in range(num_in_args.value)]\\n        aux_arrays = [_ndarray_cls(NDArrayHandle(aux_state_handles[i]))\\n                      for i in range(num_aux_states.value)]\\n\\n        executor = Executor(exe_handle, self, ctx, grad_req, group2ctx)\\n        executor.arg_arrays = arg_arrays\\n        executor.grad_arrays = grad_arrays\\n        executor.aux_arrays = aux_arrays\\n        return executor',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Symbol.bind',\n",
       "  'docstring': \"Binds the current symbol to an executor and returns it.\\n\\n        We first declare the computation and then bind to the data to run.\\n        This function returns an executor which provides method `forward()` method for evaluation\\n        and a `outputs()` method to get all the results.\\n\\n        Example\\n        -------\\n        >>> a = mx.sym.Variable('a')\\n        >>> b = mx.sym.Variable('b')\\n        >>> c = a + b\\n        <Symbol _plus1>\\n        >>> ex = c.bind(ctx=mx.cpu(), args={'a' : mx.nd.ones([2,3]), 'b' : mx.nd.ones([2,3])})\\n        >>> ex.forward()\\n        [<NDArray 2x3 @cpu(0)>]\\n        >>> ex.outputs[0].asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\\n\\n        Parameters\\n        ----------\\n        ctx : Context\\n            The device context the generated executor to run on.\\n\\n        args : list of NDArray or dict of str to NDArray\\n            Input arguments to the symbol.\\n\\n            - If the input type is a list of `NDArray`, the order should be same as the order\\n              of `list_arguments()`.\\n            - If the input type is a dict of str to `NDArray`, then it maps the name of arguments\\n              to the corresponding `NDArray`.\\n            - In either case, all the arguments must be provided.\\n\\n        args_grad : list of NDArray or dict of str to `NDArray`, optional\\n            When specified, `args_grad` provides NDArrays to hold\\n            the result of gradient value in backward.\\n\\n            - If the input type is a list of `NDArray`, the order should be same as the order\\n              of `list_arguments()`.\\n            - If the input type is a dict of str to `NDArray`, then it maps the name of arguments\\n              to the corresponding NDArray.\\n            - When the type is a dict of str to `NDArray`, one only need to provide the dict\\n              for required argument gradient.\\n              Only the specified argument gradient will be calculated.\\n\\n        grad_req : {'write', 'add', 'null'}, or list of str or dict of str to str, optional\\n            To specify how we should update the gradient to the `args_grad`.\\n\\n            - 'write' means everytime gradient is write to specified `args_grad` `NDArray`.\\n            - 'add' means everytime gradient is add to the specified NDArray.\\n            - 'null' means no action is taken, the gradient may not be calculated.\\n\\n        aux_states : list of `NDArray`, or dict of str to `NDArray`, optional\\n            Input auxiliary states to the symbol, only needed when the output of\\n            `list_auxiliary_states()` is not empty.\\n\\n            - If the input type is a list of `NDArray`, the order should be same as the order\\n              of `list_auxiliary_states()`.\\n            - If the input type is a dict of str to `NDArray`, then it maps the name of\\n              `auxiliary_states` to the corresponding `NDArray`,\\n            - In either case, all the auxiliary states need to be provided.\\n\\n        group2ctx : Dict of string to mx.Context\\n            The dict mapping the `ctx_group` attribute to the context assignment.\\n\\n        shared_exec : mx.executor.Executor\\n            Executor to share memory with. This is intended for runtime reshaping, variable length\\n            sequences, etc. The returned executor shares state with `shared_exec`, and should not be\\n            used in parallel with it.\\n\\n        Returns\\n        -------\\n        executor : Executor\\n            The generated executor\\n\\n        Notes\\n        -----\\n        Auxiliary states are the special states of symbols that do not correspond\\n        to an argument, and do not have gradient but are still useful\\n        for the specific operations. Common examples of auxiliary states include\\n        the `moving_mean` and `moving_variance` states in `BatchNorm`.\\n        Most operators do not have auxiliary states and in those cases,\\n        this parameter can be safely ignored.\\n\\n        One can give up gradient by using a dict in `args_grad` and only specify\\n        gradient they interested in.\",\n",
       "  'code': 'def bind(self, ctx, args, args_grad=None, grad_req=\\'write\\',\\n             aux_states=None, group2ctx=None, shared_exec=None):\\n        \"\"\"Binds the current symbol to an executor and returns it.\\n\\n        We first declare the computation and then bind to the data to run.\\n        This function returns an executor which provides method `forward()` method for evaluation\\n        and a `outputs()` method to get all the results.\\n\\n        Example\\n        -------\\n        >>> a = mx.sym.Variable(\\'a\\')\\n        >>> b = mx.sym.Variable(\\'b\\')\\n        >>> c = a + b\\n        <Symbol _plus1>\\n        >>> ex = c.bind(ctx=mx.cpu(), args={\\'a\\' : mx.nd.ones([2,3]), \\'b\\' : mx.nd.ones([2,3])})\\n        >>> ex.forward()\\n        [<NDArray 2x3 @cpu(0)>]\\n        >>> ex.outputs[0].asnumpy()\\n        [[ 2.  2.  2.]\\n        [ 2.  2.  2.]]\\n\\n        Parameters\\n        ----------\\n        ctx : Context\\n            The device context the generated executor to run on.\\n\\n        args : list of NDArray or dict of str to NDArray\\n            Input arguments to the symbol.\\n\\n            - If the input type is a list of `NDArray`, the order should be same as the order\\n              of `list_arguments()`.\\n            - If the input type is a dict of str to `NDArray`, then it maps the name of arguments\\n              to the corresponding `NDArray`.\\n            - In either case, all the arguments must be provided.\\n\\n        args_grad : list of NDArray or dict of str to `NDArray`, optional\\n            When specified, `args_grad` provides NDArrays to hold\\n            the result of gradient value in backward.\\n\\n            - If the input type is a list of `NDArray`, the order should be same as the order\\n              of `list_arguments()`.\\n            - If the input type is a dict of str to `NDArray`, then it maps the name of arguments\\n              to the corresponding NDArray.\\n            - When the type is a dict of str to `NDArray`, one only need to provide the dict\\n              for required argument gradient.\\n              Only the specified argument gradient will be calculated.\\n\\n        grad_req : {\\'write\\', \\'add\\', \\'null\\'}, or list of str or dict of str to str, optional\\n            To specify how we should update the gradient to the `args_grad`.\\n\\n            - \\'write\\' means everytime gradient is write to specified `args_grad` `NDArray`.\\n            - \\'add\\' means everytime gradient is add to the specified NDArray.\\n            - \\'null\\' means no action is taken, the gradient may not be calculated.\\n\\n        aux_states : list of `NDArray`, or dict of str to `NDArray`, optional\\n            Input auxiliary states to the symbol, only needed when the output of\\n            `list_auxiliary_states()` is not empty.\\n\\n            - If the input type is a list of `NDArray`, the order should be same as the order\\n              of `list_auxiliary_states()`.\\n            - If the input type is a dict of str to `NDArray`, then it maps the name of\\n              `auxiliary_states` to the corresponding `NDArray`,\\n            - In either case, all the auxiliary states need to be provided.\\n\\n        group2ctx : Dict of string to mx.Context\\n            The dict mapping the `ctx_group` attribute to the context assignment.\\n\\n        shared_exec : mx.executor.Executor\\n            Executor to share memory with. This is intended for runtime reshaping, variable length\\n            sequences, etc. The returned executor shares state with `shared_exec`, and should not be\\n            used in parallel with it.\\n\\n        Returns\\n        -------\\n        executor : Executor\\n            The generated executor\\n\\n        Notes\\n        -----\\n        Auxiliary states are the special states of symbols that do not correspond\\n        to an argument, and do not have gradient but are still useful\\n        for the specific operations. Common examples of auxiliary states include\\n        the `moving_mean` and `moving_variance` states in `BatchNorm`.\\n        Most operators do not have auxiliary states and in those cases,\\n        this parameter can be safely ignored.\\n\\n        One can give up gradient by using a dict in `args_grad` and only specify\\n        gradient they interested in.\\n        \"\"\"\\n        # pylint: disable=too-many-locals, too-many-branches\\n        if not isinstance(ctx, Context):\\n            raise TypeError(\"Context type error\")\\n\\n        listed_arguments = self.list_arguments()\\n        args_handle, args = self._get_ndarray_inputs(\\'args\\', args, listed_arguments, False)\\n        # setup args gradient\\n        if args_grad is None:\\n            args_grad_handle = c_array(NDArrayHandle, [None] * len(args))\\n        else:\\n            args_grad_handle, args_grad = self._get_ndarray_inputs(\\n                \\'args_grad\\', args_grad, listed_arguments, True)\\n\\n        if aux_states is None:\\n            aux_states = []\\n        aux_args_handle, aux_states = self._get_ndarray_inputs(\\n            \\'aux_states\\', aux_states, self.list_auxiliary_states(), False)\\n\\n        # setup requirements\\n        if isinstance(grad_req, string_types):\\n            if grad_req not in _GRAD_REQ_MAP:\\n                raise ValueError(\\'grad_req must be in %s\\' % str(_GRAD_REQ_MAP))\\n            reqs_array = c_array_buf(mx_uint,\\n                                     array(\\'I\\', [_GRAD_REQ_MAP[grad_req]] * len(listed_arguments)))\\n        elif isinstance(grad_req, list):\\n            reqs_array = c_array_buf(mx_uint,\\n                                     array(\\'I\\', [_GRAD_REQ_MAP[item] for item in grad_req]))\\n        elif isinstance(grad_req, dict):\\n            req_array = []\\n            for name in listed_arguments:\\n                if name in grad_req:\\n                    req_array.append(_GRAD_REQ_MAP[grad_req[name]])\\n                else:\\n                    req_array.append(0)\\n            reqs_array = c_array_buf(mx_uint, array(\\'I\\', req_array))\\n\\n        ctx_map_keys = []\\n        ctx_map_dev_types = []\\n        ctx_map_dev_ids = []\\n\\n        if group2ctx:\\n            for key, val in group2ctx.items():\\n                ctx_map_keys.append(key)\\n                ctx_map_dev_types.append(val.device_typeid)\\n                ctx_map_dev_ids.append(val.device_id)\\n\\n        handle = ExecutorHandle()\\n        shared_handle = shared_exec.handle if shared_exec is not None else ExecutorHandle()\\n        check_call(_LIB.MXExecutorBindEX(self.handle,\\n                                         ctypes.c_int(ctx.device_typeid),\\n                                         ctypes.c_int(ctx.device_id),\\n                                         mx_uint(len(ctx_map_keys)),\\n                                         c_str_array(ctx_map_keys),\\n                                         c_array_buf(ctypes.c_int, array(\\'i\\', ctx_map_dev_types)),\\n                                         c_array_buf(ctypes.c_int, array(\\'i\\', ctx_map_dev_ids)),\\n                                         mx_uint(len(args)),\\n                                         args_handle,\\n                                         args_grad_handle,\\n                                         reqs_array,\\n                                         mx_uint(len(aux_states)),\\n                                         aux_args_handle,\\n                                         shared_handle,\\n                                         ctypes.byref(handle)))\\n        executor = Executor(handle, self, ctx, grad_req, group2ctx)\\n        executor.arg_arrays = args\\n        executor.grad_arrays = args_grad\\n        executor.aux_arrays = aux_states\\n        return executor',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Symbol.eval',\n",
       "  'docstring': \"Evaluates a symbol given arguments.\\n\\n        The `eval` method combines a call to `bind` (which returns an executor)\\n        with a call to `forward` (executor method).\\n        For the common use case, where you might repeatedly evaluate with same arguments,\\n        eval is slow.\\n        In that case, you should call `bind` once and then repeatedly call forward.\\n        This function allows simpler syntax for less cumbersome introspection.\\n\\n        Example\\n        -------\\n        >>> a = mx.sym.Variable('a')\\n        >>> b = mx.sym.Variable('b')\\n        >>> c = a + b\\n        >>> ex = c.eval(ctx = mx.cpu(), a = mx.nd.ones([2,3]), b = mx.nd.ones([2,3]))\\n        >>> ex\\n        [<NDArray 2x3 @cpu(0)>]\\n        >>> ex[0].asnumpy()\\n        array([[ 2.,  2.,  2.],\\n               [ 2.,  2.,  2.]], dtype=float32)\\n\\n        Parameters\\n        ----------\\n        ctx : Context\\n            The device context the generated executor to run on.\\n\\n        kwargs : Keyword arguments of type `NDArray`\\n            Input arguments to the symbol. All the arguments must be provided.\\n\\n        Returns\\n        ----------\\n        result :  a list of NDArrays corresponding to the values taken by each symbol when\\n        evaluated on given args. When called on a single symbol (not a group),\\n        the result will be a list with one element.\",\n",
       "  'code': 'def eval(self, ctx=None, **kwargs):\\n        \"\"\"Evaluates a symbol given arguments.\\n\\n        The `eval` method combines a call to `bind` (which returns an executor)\\n        with a call to `forward` (executor method).\\n        For the common use case, where you might repeatedly evaluate with same arguments,\\n        eval is slow.\\n        In that case, you should call `bind` once and then repeatedly call forward.\\n        This function allows simpler syntax for less cumbersome introspection.\\n\\n        Example\\n        -------\\n        >>> a = mx.sym.Variable(\\'a\\')\\n        >>> b = mx.sym.Variable(\\'b\\')\\n        >>> c = a + b\\n        >>> ex = c.eval(ctx = mx.cpu(), a = mx.nd.ones([2,3]), b = mx.nd.ones([2,3]))\\n        >>> ex\\n        [<NDArray 2x3 @cpu(0)>]\\n        >>> ex[0].asnumpy()\\n        array([[ 2.,  2.,  2.],\\n               [ 2.,  2.,  2.]], dtype=float32)\\n\\n        Parameters\\n        ----------\\n        ctx : Context\\n            The device context the generated executor to run on.\\n\\n        kwargs : Keyword arguments of type `NDArray`\\n            Input arguments to the symbol. All the arguments must be provided.\\n\\n        Returns\\n        ----------\\n        result :  a list of NDArrays corresponding to the values taken by each symbol when\\n        evaluated on given args. When called on a single symbol (not a group),\\n        the result will be a list with one element.\\n        \"\"\"\\n        if ctx is None:\\n            ctx = current_context()\\n        return self.bind(ctx, kwargs).forward()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'compare_layers_from_nets',\n",
       "  'docstring': 'Compare layer by layer of a caffe network with mxnet network\\n    :param caffe_net: loaded caffe network\\n    :param arg_params: arguments\\n    :param aux_params: auxiliary parameters\\n    :param exe: mxnet model\\n    :param layer_name_to_record: map between caffe layer and information record\\n    :param top_to_layers: map between caffe blob name to layers which outputs it (including inplace)\\n    :param mean_diff_allowed: mean difference allowed between caffe blob and mxnet blob\\n    :param max_diff_allowed: max difference allowed between caffe blob and mxnet blob',\n",
       "  'code': 'def compare_layers_from_nets(caffe_net, arg_params, aux_params, exe, layer_name_to_record,\\n                             top_to_layers, mean_diff_allowed, max_diff_allowed):\\n    \"\"\"\\n    Compare layer by layer of a caffe network with mxnet network\\n    :param caffe_net: loaded caffe network\\n    :param arg_params: arguments\\n    :param aux_params: auxiliary parameters\\n    :param exe: mxnet model\\n    :param layer_name_to_record: map between caffe layer and information record\\n    :param top_to_layers: map between caffe blob name to layers which outputs it (including inplace)\\n    :param mean_diff_allowed: mean difference allowed between caffe blob and mxnet blob\\n    :param max_diff_allowed: max difference allowed between caffe blob and mxnet blob\\n    \"\"\"\\n\\n    import re\\n\\n    log_format = \\'  {0:<40}  {1:<40}  {2:<8}  {3:>10}  {4:>10}  {5:<1}\\'\\n\\n    compare_layers_from_nets.is_first_convolution = True\\n\\n    def _compare_blob(caf_blob, mx_blob, caf_name, mx_name, blob_type, note):\\n        diff = np.abs(mx_blob - caf_blob)\\n        diff_mean = diff.mean()\\n        diff_max = diff.max()\\n        logging.info(log_format.format(caf_name, mx_name, blob_type, \\'%4.5f\\' % diff_mean,\\n                                       \\'%4.5f\\' % diff_max, note))\\n        assert diff_mean < mean_diff_allowed\\n        assert diff_max < max_diff_allowed\\n\\n    def _process_layer_parameters(layer):\\n\\n        logging.debug(\\'processing layer %s of type %s\\', layer.name, layer.type)\\n\\n        normalized_layer_name = re.sub(\\'[-/]\\', \\'_\\', layer.name)\\n\\n        # handle weight and bias of convolution and fully-connected layers\\n        if layer.name in caffe_net.params and layer.type in [\\'Convolution\\', \\'InnerProduct\\',\\n                                                             \\'Deconvolution\\']:\\n\\n            has_bias = len(caffe_net.params[layer.name]) > 1\\n\\n            mx_name_weight = \\'{}_weight\\'.format(normalized_layer_name)\\n            mx_beta = arg_params[mx_name_weight].asnumpy()\\n\\n            # first convolution should change from BGR to RGB\\n            if layer.type == \\'Convolution\\' and compare_layers_from_nets.is_first_convolution:\\n                compare_layers_from_nets.is_first_convolution = False\\n\\n                # if RGB or RGBA\\n                if mx_beta.shape[1] == 3 or mx_beta.shape[1] == 4:\\n                    # Swapping BGR of caffe into RGB in mxnet\\n                    mx_beta[:, [0, 2], :, :] = mx_beta[:, [2, 0], :, :]\\n\\n            caf_beta = caffe_net.params[layer.name][0].data\\n            _compare_blob(caf_beta, mx_beta, layer.name, mx_name_weight, \\'weight\\', \\'\\')\\n\\n            if has_bias:\\n                mx_name_bias = \\'{}_bias\\'.format(normalized_layer_name)\\n                mx_gamma = arg_params[mx_name_bias].asnumpy()\\n                caf_gamma = caffe_net.params[layer.name][1].data\\n                _compare_blob(caf_gamma, mx_gamma, layer.name, mx_name_bias, \\'bias\\', \\'\\')\\n\\n        elif layer.name in caffe_net.params and layer.type == \\'Scale\\':\\n\\n            if \\'scale\\' in normalized_layer_name:\\n                bn_name = normalized_layer_name.replace(\\'scale\\', \\'bn\\')\\n            elif \\'sc\\' in normalized_layer_name:\\n                bn_name = normalized_layer_name.replace(\\'sc\\', \\'bn\\')\\n            else:\\n                assert False, \\'Unknown name convention for bn/scale\\'\\n\\n            beta_name = \\'{}_beta\\'.format(bn_name)\\n            gamma_name = \\'{}_gamma\\'.format(bn_name)\\n\\n            mx_beta = arg_params[beta_name].asnumpy()\\n            caf_beta = caffe_net.params[layer.name][1].data\\n            _compare_blob(caf_beta, mx_beta, layer.name, beta_name, \\'mov_mean\\', \\'\\')\\n\\n            mx_gamma = arg_params[gamma_name].asnumpy()\\n            caf_gamma = caffe_net.params[layer.name][0].data\\n            _compare_blob(caf_gamma, mx_gamma, layer.name, gamma_name, \\'mov_var\\', \\'\\')\\n\\n        elif layer.name in caffe_net.params and layer.type == \\'BatchNorm\\':\\n\\n            mean_name = \\'{}_moving_mean\\'.format(normalized_layer_name)\\n            var_name = \\'{}_moving_var\\'.format(normalized_layer_name)\\n\\n            caf_rescale_factor = caffe_net.params[layer.name][2].data\\n\\n            mx_mean = aux_params[mean_name].asnumpy()\\n            caf_mean = caffe_net.params[layer.name][0].data / caf_rescale_factor\\n            _compare_blob(caf_mean, mx_mean, layer.name, mean_name, \\'mean\\', \\'\\')\\n\\n            mx_var = aux_params[var_name].asnumpy()\\n            caf_var = caffe_net.params[layer.name][1].data / caf_rescale_factor\\n            _compare_blob(caf_var, mx_var, layer.name, var_name, \\'var\\',\\n                          \\'expect 1e-04 change due to cudnn eps\\')\\n\\n        elif layer.type in [\\'Input\\', \\'Pooling\\', \\'ReLU\\', \\'Eltwise\\', \\'Softmax\\', \\'LRN\\', \\'Concat\\',\\n                            \\'Dropout\\', \\'Crop\\']:\\n            # no parameters to check for these layers\\n            pass\\n\\n        else:\\n            warnings.warn(\\'No handling for layer %s of type %s, should we ignore it?\\', layer.name,\\n                          layer.type)\\n\\n        return\\n\\n    def _process_layer_output(caffe_blob_name):\\n\\n        logging.debug(\\'processing blob %s\\', caffe_blob_name)\\n\\n        # skip blobs not originating from actual layers, e.g. artificial split layers added by caffe\\n        if caffe_blob_name not in top_to_layers:\\n            return\\n\\n        caf_blob = caffe_net.blobs[caffe_blob_name].data\\n\\n        # data should change from BGR to RGB\\n        if caffe_blob_name == \\'data\\':\\n\\n            # if RGB or RGBA\\n            if caf_blob.shape[1] == 3 or caf_blob.shape[1] == 4:\\n                # Swapping BGR of caffe into RGB in mxnet\\n                caf_blob[:, [0, 2], :, :] = caf_blob[:, [2, 0], :, :]\\n            mx_name = \\'data\\'\\n\\n        else:\\n            # get last layer name which outputs this blob name\\n            last_layer_name = top_to_layers[caffe_blob_name][-1]\\n            normalized_last_layer_name = re.sub(\\'[-/]\\', \\'_\\', last_layer_name)\\n            mx_name = \\'{}_output\\'.format(normalized_last_layer_name)\\n            if \\'scale\\' in mx_name:\\n                mx_name = mx_name.replace(\\'scale\\', \\'bn\\')\\n            elif \\'sc\\' in mx_name:\\n                mx_name = mx_name.replace(\\'sc\\', \\'bn\\')\\n\\n        if mx_name not in exe.output_dict:\\n            logging.error(\\'mxnet blob %s is missing, time to extend the compare tool..\\', mx_name)\\n            return\\n\\n        mx_blob = exe.output_dict[mx_name].asnumpy()\\n        _compare_blob(caf_blob, mx_blob, caffe_blob_name, mx_name, \\'output\\', \\'\\')\\n\\n        return\\n\\n    # check layer parameters\\n    logging.info(\\'\\\\n***** Network Parameters \\'.ljust(140, \\'*\\'))\\n    logging.info(log_format.format(\\'CAFFE\\', \\'MXNET\\', \\'Type\\', \\'Mean(diff)\\', \\'Max(diff)\\', \\'Note\\'))\\n    first_layer_name = layer_name_to_record.keys()[0]\\n    _bfs(layer_name_to_record[first_layer_name], _process_layer_parameters)\\n\\n    # check layer output\\n    logging.info(\\'\\\\n***** Network Outputs \\'.ljust(140, \\'*\\'))\\n    logging.info(log_format.format(\\'CAFFE\\', \\'MXNET\\', \\'Type\\', \\'Mean(diff)\\', \\'Max(diff)\\', \\'Note\\'))\\n    for caffe_blob_name in caffe_net.blobs.keys():\\n        _process_layer_output(caffe_blob_name)\\n\\n    return',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'optimize',\n",
       "  'docstring': 'Gatys et al. CVPR 2017\\n    ref: Image Style Transfer Using Convolutional Neural Networks',\n",
       "  'code': 'def optimize(args):\\n    \"\"\"    Gatys et al. CVPR 2017\\n    ref: Image Style Transfer Using Convolutional Neural Networks\\n    \"\"\"\\n    if args.cuda:\\n        ctx = mx.gpu(0)\\n    else:\\n        ctx = mx.cpu(0)\\n    # load the content and style target\\n    content_image = utils.tensor_load_rgbimage(args.content_image,ctx, size=args.content_size, keep_asp=True)\\n    content_image = utils.subtract_imagenet_mean_preprocess_batch(content_image)\\n    style_image = utils.tensor_load_rgbimage(args.style_image, ctx, size=args.style_size)\\n    style_image = utils.subtract_imagenet_mean_preprocess_batch(style_image)\\n    # load the pre-trained vgg-16 and extract features\\n    vgg = net.Vgg16()\\n    utils.init_vgg_params(vgg, \\'models\\', ctx=ctx)\\n    # content feature\\n    f_xc_c = vgg(content_image)[1]\\n    # style feature\\n    features_style = vgg(style_image)\\n    gram_style = [net.gram_matrix(y) for y in features_style]\\n    # output\\n    output = Parameter(\\'output\\', shape=content_image.shape)\\n    output.initialize(ctx=ctx)\\n    output.set_data(content_image)\\n    # optimizer\\n    trainer = gluon.Trainer([output], \\'adam\\',\\n                            {\\'learning_rate\\': args.lr})\\n    mse_loss = gluon.loss.L2Loss()\\n\\n    # optimizing the images\\n    for e in range(args.iters):\\n        utils.imagenet_clamp_batch(output.data(), 0, 255)\\n        # fix BN for pre-trained vgg\\n        with autograd.record():\\n            features_y = vgg(output.data())\\n            content_loss = 2 * args.content_weight * mse_loss(features_y[1], f_xc_c)\\n            style_loss = 0.\\n            for m in range(len(features_y)):\\n                gram_y = net.gram_matrix(features_y[m])\\n                gram_s = gram_style[m]\\n                style_loss = style_loss + 2 * args.style_weight * mse_loss(gram_y, gram_s)\\n            total_loss = content_loss + style_loss\\n            total_loss.backward()\\n\\n        trainer.step(1)\\n        if (e + 1) % args.log_interval == 0:\\n            print(\\'loss:{:.2f}\\'.format(total_loss.asnumpy()[0]))\\n\\n    # save the image\\n    output = utils.add_imagenet_mean_batch(output.data())\\n    utils.tensor_save_bgrimage(output[0], args.output_image, args.cuda)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'synthetic_grad',\n",
       "  'docstring': 'Get synthetic gradient value',\n",
       "  'code': 'def synthetic_grad(X, theta, sigma1, sigma2, sigmax, rescale_grad=1.0, grad=None):\\n    \"\"\"Get synthetic gradient value\"\"\"\\n    if grad is None:\\n        grad = nd.empty(theta.shape, theta.context)\\n    theta1 = theta.asnumpy()[0]\\n    theta2 = theta.asnumpy()[1]\\n    v1 = sigma1 ** 2\\n    v2 = sigma2 ** 2\\n    vx = sigmax ** 2\\n    denominator = numpy.exp(-(X - theta1) ** 2 / (2 * vx)) + numpy.exp(\\n        -(X - theta1 - theta2) ** 2 / (2 * vx))\\n    grad_npy = numpy.zeros(theta.shape)\\n    grad_npy[0] = -rescale_grad * ((numpy.exp(-(X - theta1) ** 2 / (2 * vx)) * (X - theta1) / vx\\n                                    + numpy.exp(-(X - theta1 - theta2) ** 2 / (2 * vx)) *\\n                                    (X - theta1 - theta2) / vx) / denominator).sum() + theta1 / v1\\n    grad_npy[1] = -rescale_grad * ((numpy.exp(-(X - theta1 - theta2) ** 2 / (2 * vx)) *\\n                                    (X - theta1 - theta2) / vx) / denominator).sum() + theta2 / v2\\n    grad[:] = grad_npy\\n    return grad',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'run_synthetic_SGLD',\n",
       "  'docstring': 'Run synthetic SGLD',\n",
       "  'code': 'def run_synthetic_SGLD():\\n    \"\"\"Run synthetic SGLD\"\"\"\\n    theta1 = 0\\n    theta2 = 1\\n    sigma1 = numpy.sqrt(10)\\n    sigma2 = 1\\n    sigmax = numpy.sqrt(2)\\n    X = load_synthetic(theta1=theta1, theta2=theta2, sigmax=sigmax, num=100)\\n    minibatch_size = 1\\n    total_iter_num = 1000000\\n    lr_scheduler = SGLDScheduler(begin_rate=0.01, end_rate=0.0001, total_iter_num=total_iter_num,\\n                                 factor=0.55)\\n    optimizer = mx.optimizer.create(\\'sgld\\',\\n                                    learning_rate=None,\\n                                    rescale_grad=1.0,\\n                                    lr_scheduler=lr_scheduler,\\n                                    wd=0)\\n    updater = mx.optimizer.get_updater(optimizer)\\n    theta = mx.random.normal(0, 1, (2,), mx.cpu())\\n    grad = nd.empty((2,), mx.cpu())\\n    samples = numpy.zeros((2, total_iter_num))\\n    start = time.time()\\n    for i in range(total_iter_num):\\n        if (i + 1) % 100000 == 0:\\n            end = time.time()\\n            print(\"Iter:%d, Time spent: %f\" % (i + 1, end - start))\\n            start = time.time()\\n        ind = numpy.random.randint(0, X.shape[0])\\n        synthetic_grad(X[ind], theta, sigma1, sigma2, sigmax,\\n                       rescale_grad=X.shape[0] / float(minibatch_size), grad=grad)\\n        updater(\\'theta\\', grad, theta)\\n        samples[:, i] = theta.asnumpy()\\n    plt.hist2d(samples[0, :], samples[1, :], (200, 200), cmap=plt.cm.jet)\\n    plt.colorbar()\\n    plt.show()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'convert_leakyrelu',\n",
       "  'docstring': 'Convert a leakyrelu layer from mxnet to coreml.\\n\\n    Parameters\\n    ----------\\n    network: net\\n        A mxnet network object.\\n\\n    layer: node\\n        Node to convert.\\n\\n    module: module\\n        An module for MXNet\\n\\n    builder: NeuralNetworkBuilder\\n        A neural network builder object.',\n",
       "  'code': 'def convert_leakyrelu(net, node, module, builder):\\n    \"\"\"Convert a leakyrelu layer from mxnet to coreml.\\n\\n    Parameters\\n    ----------\\n    network: net\\n        A mxnet network object.\\n\\n    layer: node\\n        Node to convert.\\n\\n    module: module\\n        An module for MXNet\\n\\n    builder: NeuralNetworkBuilder\\n        A neural network builder object.\\n    \"\"\"\\n\\n    input_name, output_name = _get_input_output_name(net, node)\\n    name = node[\\'name\\']\\n    inputs = node[\\'inputs\\']\\n    args, _ = module.get_params()\\n    mx_non_linearity = _get_attrs(node)[\\'act_type\\']\\n    if mx_non_linearity == \\'elu\\':\\n        non_linearity = \\'ELU\\'\\n        slope = _get_attrs(node)[\\'slope\\'] if \\'slope\\' in _get_attrs(node) else 0.25\\n        params = slope\\n    elif mx_non_linearity == \\'leaky\\':\\n        non_linearity = \\'LEAKYRELU\\'\\n        slope = _get_attrs(node)[\\'slope\\'] if \\'slope\\' in _get_attrs(node) else 0.25\\n        params = [slope]\\n    elif mx_non_linearity == \\'prelu\\':\\n        non_linearity = \\'PRELU\\'\\n        params = args[_get_node_name(net, inputs[1][0])].asnumpy()\\n    else:\\n        raise TypeError(\\'Unknown activation type %s\\' % mx_non_linearity)\\n    builder.add_activation(name = name,\\n                           non_linearity = non_linearity,\\n                           input_name = input_name,\\n                           output_name = output_name,\\n                           params = params)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'convert_convolution',\n",
       "  'docstring': 'Convert a convolution layer from mxnet to coreml.\\n\\n    Parameters\\n    ----------\\n    network: net\\n        A mxnet network object.\\n\\n    layer: node\\n        Node to convert.\\n\\n    module: module\\n        An module for MXNet\\n\\n    builder: NeuralNetworkBuilder\\n        A neural network builder object.',\n",
       "  'code': 'def convert_convolution(net, node, module, builder):\\n    \"\"\"Convert a convolution layer from mxnet to coreml.\\n\\n    Parameters\\n    ----------\\n    network: net\\n        A mxnet network object.\\n\\n    layer: node\\n        Node to convert.\\n\\n    module: module\\n        An module for MXNet\\n\\n    builder: NeuralNetworkBuilder\\n        A neural network builder object.\\n    \"\"\"\\n    input_name, output_name = _get_input_output_name(net, node)\\n    name = node[\\'name\\']\\n    param = _get_attrs(node)\\n    inputs = node[\\'inputs\\']\\n    args, _ = module.get_params()\\n\\n    if \\'no_bias\\' in param.keys():\\n        has_bias = not literal_eval(param[\\'no_bias\\'])\\n    else:\\n        has_bias = True\\n\\n    if \\'pad\\' in param.keys() and literal_eval(param[\\'pad\\']) != (0, 0):\\n        pad = literal_eval(param[\\'pad\\'])\\n        builder.add_padding(\\n            name=name+\"_pad\",\\n            left=pad[1],\\n            right=pad[1],\\n            top=pad[0],\\n            bottom=pad[0],\\n            value=0,\\n            input_name=input_name,\\n            output_name=name+\"_pad_output\")\\n        input_name = name+\"_pad_output\"\\n\\n    border_mode = \"valid\"\\n\\n    n_filters = int(param[\\'num_filter\\'])\\n    n_groups = int(param[\\'num_group\\']) if \\'num_group\\' in param else 1\\n\\n    W = args[_get_node_name(net, inputs[1][0])].asnumpy()\\n    if has_bias:\\n        Wb = args[_get_node_name(net, inputs[2][0])].asnumpy()\\n    else:\\n        Wb = None\\n\\n    channels = W.shape[1]\\n\\n    stride_height = 1\\n    stride_width = 1\\n    if \\'stride\\' in param.keys():\\n        stride_height, stride_width = literal_eval(param[\\'stride\\'])\\n\\n    kernel_height, kernel_width = literal_eval(param[\\'kernel\\'])\\n\\n    W = W.transpose((2, 3, 1, 0))\\n    builder.add_convolution(\\n        name=name,\\n        kernel_channels=channels,\\n        output_channels=n_filters,\\n        height=kernel_height,\\n        width=kernel_width,\\n        stride_height=stride_height,\\n        stride_width=stride_width,\\n        border_mode=border_mode,\\n        groups=n_groups,\\n        W=W,\\n        b=Wb,\\n        has_bias=has_bias,\\n        is_deconv=False,\\n        output_shape=None,\\n        input_name=input_name,\\n        output_name=output_name)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'convert_batchnorm',\n",
       "  'docstring': 'Convert a batchnorm layer from mxnet to coreml.\\n\\n    Parameters\\n    ----------\\n    network: net\\n        A mxnet network object.\\n\\n    layer: node\\n        Node to convert.\\n\\n    module: module\\n        An module for MXNet\\n\\n    builder: NeuralNetworkBuilder\\n        A neural network builder object.',\n",
       "  'code': 'def convert_batchnorm(net, node, module, builder):\\n    \"\"\"Convert a batchnorm layer from mxnet to coreml.\\n\\n    Parameters\\n    ----------\\n    network: net\\n        A mxnet network object.\\n\\n    layer: node\\n        Node to convert.\\n\\n    module: module\\n        An module for MXNet\\n\\n    builder: NeuralNetworkBuilder\\n        A neural network builder object.\\n    \"\"\"\\n    input_name, output_name = _get_input_output_name(net, node)\\n    name = node[\\'name\\']\\n    inputs = node[\\'inputs\\']\\n\\n\\n    eps = 1e-3  # Default value of eps for MXNet.\\n    use_global_stats = False  # Default value of use_global_stats for MXNet.\\n    fix_gamma = True  # Default value of fix_gamma for MXNet.\\n    attrs = _get_attrs(node)\\n    if \\'eps\\' in attrs:\\n        eps = literal_eval(attrs[\\'eps\\'])\\n    if \\'fix_gamma\\' in attrs:\\n        fix_gamma = literal_eval(attrs[\\'fix_gamma\\'])\\n\\n    args, aux = module.get_params()\\n    gamma = args[_get_node_name(net, inputs[1][0])].asnumpy()\\n    beta = args[_get_node_name(net, inputs[2][0])].asnumpy()\\n    mean = aux[_get_node_name(net, inputs[3][0])].asnumpy()\\n    variance = aux[_get_node_name(net, inputs[4][0])].asnumpy()\\n    nb_channels = gamma.shape[0]\\n    if fix_gamma:\\n        gamma.fill(1.)\\n    builder.add_batchnorm(\\n        name=name,\\n        channels=nb_channels,\\n        gamma=gamma,\\n        beta=beta,\\n        mean=mean,\\n        variance=variance,\\n        input_name=input_name,\\n        output_name=output_name,\\n        epsilon=eps)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'plot_network',\n",
       "  'docstring': 'Creates a visualization (Graphviz digraph object) of the given computation graph.\\n    Graphviz must be installed for this function to work.\\n\\n    Parameters\\n    ----------\\n    title: str, optional\\n        Title of the generated visualization.\\n    symbol: Symbol\\n        A symbol from the computation graph. The generated digraph will visualize the part\\n        of the computation graph required to compute `symbol`.\\n    shape: dict, optional\\n        Specifies the shape of the input tensors. If specified, the visualization will include\\n        the shape of the tensors between the nodes. `shape` is a dictionary mapping\\n        input symbol names (str) to the corresponding tensor shape (tuple).\\n    dtype: dict, optional\\n        Specifies the type of the input tensors. If specified, the visualization will include\\n        the type of the tensors between the nodes. `dtype` is a dictionary mapping\\n        input symbol names (str) to the corresponding tensor type (e.g. `numpy.float32`).\\n    node_attrs: dict, optional\\n        Specifies the attributes for nodes in the generated visualization. `node_attrs` is\\n        a dictionary of Graphviz attribute names and values. For example::\\n\\n            node_attrs={\"shape\":\"oval\",\"fixedsize\":\"false\"}\\n\\n        will use oval shape for nodes and allow variable sized nodes in the visualization.\\n    hide_weights: bool, optional\\n        If True (default), then inputs with names of form *_weight* (corresponding to weight\\n        tensors) or *_bias* (corresponding to bias vectors) will be hidden for a cleaner\\n        visualization.\\n\\n    Returns\\n    -------\\n    dot: Digraph\\n        A Graphviz digraph object visualizing the computation graph to compute `symbol`.\\n\\n    Example\\n    -------\\n    >>> net = mx.sym.Variable(\\'data\\')\\n    >>> net = mx.sym.FullyConnected(data=net, name=\\'fc1\\', num_hidden=128)\\n    >>> net = mx.sym.Activation(data=net, name=\\'relu1\\', act_type=\"relu\")\\n    >>> net = mx.sym.FullyConnected(data=net, name=\\'fc2\\', num_hidden=10)\\n    >>> net = mx.sym.SoftmaxOutput(data=net, name=\\'out\\')\\n    >>> digraph = mx.viz.plot_network(net, shape={\\'data\\':(100,200)},\\n    ... node_attrs={\"fixedsize\":\"false\"})\\n    >>> digraph.view()\\n\\n    Notes\\n    -----\\n    If ``mxnet`` is imported, the visualization module can be used in its short-form.\\n    For example, if we ``import mxnet`` as follows::\\n\\n        import mxnet\\n\\n    this method in visualization module can be used in its short-form as::\\n\\n        mxnet.viz.plot_network(...)',\n",
       "  'code': 'def plot_network(symbol, title=\"plot\", save_format=\\'pdf\\', shape=None, dtype=None, node_attrs={},\\n                 hide_weights=True):\\n    \"\"\"Creates a visualization (Graphviz digraph object) of the given computation graph.\\n    Graphviz must be installed for this function to work.\\n\\n    Parameters\\n    ----------\\n    title: str, optional\\n        Title of the generated visualization.\\n    symbol: Symbol\\n        A symbol from the computation graph. The generated digraph will visualize the part\\n        of the computation graph required to compute `symbol`.\\n    shape: dict, optional\\n        Specifies the shape of the input tensors. If specified, the visualization will include\\n        the shape of the tensors between the nodes. `shape` is a dictionary mapping\\n        input symbol names (str) to the corresponding tensor shape (tuple).\\n    dtype: dict, optional\\n        Specifies the type of the input tensors. If specified, the visualization will include\\n        the type of the tensors between the nodes. `dtype` is a dictionary mapping\\n        input symbol names (str) to the corresponding tensor type (e.g. `numpy.float32`).\\n    node_attrs: dict, optional\\n        Specifies the attributes for nodes in the generated visualization. `node_attrs` is\\n        a dictionary of Graphviz attribute names and values. For example::\\n\\n            node_attrs={\"shape\":\"oval\",\"fixedsize\":\"false\"}\\n\\n        will use oval shape for nodes and allow variable sized nodes in the visualization.\\n    hide_weights: bool, optional\\n        If True (default), then inputs with names of form *_weight* (corresponding to weight\\n        tensors) or *_bias* (corresponding to bias vectors) will be hidden for a cleaner\\n        visualization.\\n\\n    Returns\\n    -------\\n    dot: Digraph\\n        A Graphviz digraph object visualizing the computation graph to compute `symbol`.\\n\\n    Example\\n    -------\\n    >>> net = mx.sym.Variable(\\'data\\')\\n    >>> net = mx.sym.FullyConnected(data=net, name=\\'fc1\\', num_hidden=128)\\n    >>> net = mx.sym.Activation(data=net, name=\\'relu1\\', act_type=\"relu\")\\n    >>> net = mx.sym.FullyConnected(data=net, name=\\'fc2\\', num_hidden=10)\\n    >>> net = mx.sym.SoftmaxOutput(data=net, name=\\'out\\')\\n    >>> digraph = mx.viz.plot_network(net, shape={\\'data\\':(100,200)},\\n    ... node_attrs={\"fixedsize\":\"false\"})\\n    >>> digraph.view()\\n\\n    Notes\\n    -----\\n    If ``mxnet`` is imported, the visualization module can be used in its short-form.\\n    For example, if we ``import mxnet`` as follows::\\n\\n        import mxnet\\n\\n    this method in visualization module can be used in its short-form as::\\n\\n        mxnet.viz.plot_network(...)\\n\\n    \"\"\"\\n    # todo add shape support\\n    try:\\n        from graphviz import Digraph\\n    except:\\n        raise ImportError(\"Draw network requires graphviz library\")\\n    if not isinstance(symbol, Symbol):\\n        raise TypeError(\"symbol must be a Symbol\")\\n    internals = symbol.get_internals()\\n    draw_shape = shape is not None\\n    if draw_shape:\\n        _, out_shapes, _ = internals.infer_shape(**shape)\\n        if out_shapes is None:\\n            raise ValueError(\"Input shape is incomplete\")\\n        shape_dict = dict(zip(internals.list_outputs(), out_shapes))\\n    draw_type = dtype is not None\\n    if draw_type:\\n        _, out_types, _ = internals.infer_type(**dtype)\\n        if out_types is None:\\n            raise ValueError(\"Input type is incomplete\")\\n        type_dict = dict(zip(internals.list_outputs(), out_types))\\n    conf = json.loads(symbol.tojson())\\n    nodes = conf[\"nodes\"]\\n    # check if multiple nodes have the same name\\n    if len(nodes) != len(set([node[\"name\"] for node in nodes])):\\n        seen_nodes = set()\\n        # find all repeated names\\n        repeated = set(node[\\'name\\'] for node in nodes if node[\\'name\\'] in seen_nodes\\n                       or seen_nodes.add(node[\\'name\\']))\\n        warning_message = \"There are multiple variables with the same name in your graph, \" \\\\\\n                          \"this may result in cyclic graph. Repeated names: \" + \\',\\'.join(repeated)\\n        warnings.warn(warning_message, RuntimeWarning)\\n    # default attributes of node\\n    node_attr = {\"shape\": \"box\", \"fixedsize\": \"true\",\\n                 \"width\": \"1.3\", \"height\": \"0.8034\", \"style\": \"filled\"}\\n    # merge the dict provided by user and the default one\\n    node_attr.update(node_attrs)\\n    dot = Digraph(name=title, format=save_format)\\n    # color map\\n    cm = (\"#8dd3c7\", \"#fb8072\", \"#ffffb3\", \"#bebada\", \"#80b1d3\",\\n          \"#fdb462\", \"#b3de69\", \"#fccde5\")\\n\\n    def looks_like_weight(name):\\n        \"\"\"Internal helper to figure out if node should be hidden with `hide_weights`.\\n        \"\"\"\\n        weight_like = (\\'_weight\\', \\'_bias\\', \\'_beta\\', \\'_gamma\\',\\n                       \\'_moving_var\\', \\'_moving_mean\\', \\'_running_var\\', \\'_running_mean\\')\\n        return name.endswith(weight_like)\\n\\n    # make nodes\\n    hidden_nodes = set()\\n    for node in nodes:\\n        op = node[\"op\"]\\n        name = node[\"name\"]\\n        # input data\\n        attr = copy.deepcopy(node_attr)\\n        label = name\\n\\n        if op == \"null\":\\n            if looks_like_weight(node[\"name\"]):\\n                if hide_weights:\\n                    hidden_nodes.add(node[\"name\"])\\n                # else we don\\'t render a node, but\\n                # don\\'t add it to the hidden_nodes set\\n                # so it gets rendered as an empty oval\\n                continue\\n            attr[\"shape\"] = \"oval\" # inputs get their own shape\\n            label = node[\"name\"]\\n            attr[\"fillcolor\"] = cm[0]\\n        elif op == \"Convolution\":\\n            label = \"Convolution\\\\n{kernel}/{stride}, {filter}\".format(\\n                kernel=\"x\".join(_str2tuple(node[\"attrs\"][\"kernel\"])),\\n                stride=\"x\".join(_str2tuple(node[\"attrs\"][\"stride\"]))\\n                if \"stride\" in node[\"attrs\"] else \"1\",\\n                filter=node[\"attrs\"][\"num_filter\"]\\n            )\\n            attr[\"fillcolor\"] = cm[1]\\n        elif op == \"FullyConnected\":\\n            label = \"FullyConnected\\\\n{hidden}\".format(hidden=node[\"attrs\"][\"num_hidden\"])\\n            attr[\"fillcolor\"] = cm[1]\\n        elif op == \"BatchNorm\":\\n            attr[\"fillcolor\"] = cm[3]\\n        elif op == \\'Activation\\':\\n            act_type = node[\"attrs\"][\"act_type\"]\\n            label = \\'Activation\\\\n{activation}\\'.format(activation=act_type)\\n            attr[\"fillcolor\"] = cm[2]\\n        elif op == \\'LeakyReLU\\':\\n            attrs = node.get(\"attrs\")\\n            act_type = attrs.get(\"act_type\", \"Leaky\") if attrs else \"Leaky\"\\n            label = \\'LeakyReLU\\\\n{activation}\\'.format(activation=act_type)\\n            attr[\"fillcolor\"] = cm[2]\\n        elif op == \"Pooling\":\\n            label = \"Pooling\\\\n{pooltype}, {kernel}/{stride}\".format(pooltype=node[\"attrs\"][\"pool_type\"],\\n                                                                    kernel=\"x\".join(_str2tuple(node[\"attrs\"][\"kernel\"]))\\n                                                                    if \"kernel\" in node[\"attrs\"] else \"[]\",\\n                                                                    stride=\"x\".join(_str2tuple(node[\"attrs\"][\"stride\"]))\\n                                                                    if \"stride\" in node[\"attrs\"] else \"1\")\\n            attr[\"fillcolor\"] = cm[4]\\n        elif op in (\"Concat\", \"Flatten\", \"Reshape\"):\\n            attr[\"fillcolor\"] = cm[5]\\n        elif op == \"Softmax\":\\n            attr[\"fillcolor\"] = cm[6]\\n        else:\\n            attr[\"fillcolor\"] = cm[7]\\n            if op == \"Custom\":\\n                label = node[\"attrs\"][\"op_type\"]\\n\\n        dot.node(name=name, label=label, **attr)\\n\\n    # add edges\\n    for node in nodes:          # pylint: disable=too-many-nested-blocks\\n        op = node[\"op\"]\\n        name = node[\"name\"]\\n        if op == \"null\":\\n            continue\\n        else:\\n            inputs = node[\"inputs\"]\\n            for item in inputs:\\n                input_node = nodes[item[0]]\\n                input_name = input_node[\"name\"]\\n                if input_name not in hidden_nodes:\\n                    attr = {\"dir\": \"back\", \\'arrowtail\\':\\'open\\', \\'label\\': \\'\\'}\\n                    # add shapes\\n                    if draw_shape:\\n                        if input_node[\"op\"] != \"null\":\\n                            key = input_name + \"_output\"\\n                            if \"attrs\" in input_node:\\n                                params = input_node[\"attrs\"]\\n                                if \"num_outputs\" in params:\\n                                    key += str(int(params[\"num_outputs\"]) - 1)\\n                            shape = shape_dict[key][1:]\\n                            label = \"x\".join([str(x) for x in shape])\\n                            attr[\"label\"] = label\\n                        else:\\n                            key = input_name\\n                            shape = shape_dict[key][1:]\\n                            label = \"x\".join([str(x) for x in shape])\\n                            attr[\"label\"] = label\\n                    if draw_type:\\n                        if input_node[\"op\"] != \"null\":\\n                            key = input_name + \"_output\"\\n                            if \"attrs\" in input_node:\\n                                params = input_node[\"attrs\"]\\n                                if \"num_outputs\" in params:\\n                                    key += str(int(params[\"num_outputs\"]) - 1)\\n                            dtype = type_dict[key]\\n                            attr[\"label\"] += \\'(\\' + dtype.__name__ + \\')\\'\\n                        else:\\n                            key = input_name\\n                            dtype = type_dict[key]\\n                            attr[\"label\"] += \\'(\\' + dtype.__name__ + \\')\\'\\n                    dot.edge(tail_name=name, head_name=input_name, **attr)\\n\\n    return dot',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'tensorrt_bind',\n",
       "  'docstring': 'Bind current symbol to get an optimized trt executor.\\n\\n    Parameters\\n    ----------\\n    symbol : Symbol\\n        The symbol you wish to bind, and optimize with TensorRT.\\n\\n    ctx : Context\\n        The device context the generated executor to run on.\\n\\n    all_params : Dict of str->ndarray\\n        A dictionary of mappings from parameter names to parameter NDArrays.\\n\\n    type_dict  : Dict of str->numpy.dtype\\n        Input type dictionary, name->dtype\\n\\n    stype_dict  : Dict of str->str\\n        Input storage type dictionary, name->storage_type\\n\\n    group2ctx : Dict of string to mx.Context\\n        The dict mapping the `ctx_group` attribute to the context assignment.\\n\\n    kwargs : Dict of str->shape\\n        Input shape dictionary, name->shape\\n\\n    Returns\\n    -------\\n    executor : mxnet.Executor\\n        An optimized TensorRT executor.',\n",
       "  'code': 'def tensorrt_bind(symbol, ctx, all_params, type_dict=None, stype_dict=None, group2ctx=None,\\n                  **kwargs):\\n    \"\"\"Bind current symbol to get an optimized trt executor.\\n\\n    Parameters\\n    ----------\\n    symbol : Symbol\\n        The symbol you wish to bind, and optimize with TensorRT.\\n\\n    ctx : Context\\n        The device context the generated executor to run on.\\n\\n    all_params : Dict of str->ndarray\\n        A dictionary of mappings from parameter names to parameter NDArrays.\\n\\n    type_dict  : Dict of str->numpy.dtype\\n        Input type dictionary, name->dtype\\n\\n    stype_dict  : Dict of str->str\\n        Input storage type dictionary, name->storage_type\\n\\n    group2ctx : Dict of string to mx.Context\\n        The dict mapping the `ctx_group` attribute to the context assignment.\\n\\n    kwargs : Dict of str->shape\\n        Input shape dictionary, name->shape\\n\\n    Returns\\n    -------\\n    executor : mxnet.Executor\\n        An optimized TensorRT executor.\\n    \"\"\"\\n    kwargs[\\'shared_buffer\\'] = all_params\\n    return symbol.simple_bind(ctx, type_dict=type_dict, stype_dict=stype_dict,\\n                              group2ctx=group2ctx, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Detector.create_batch',\n",
       "  'docstring': ':param frame: an (w,h,channels) numpy array (image)\\n        :return: DataBatch of (1,channels,data_shape,data_shape)',\n",
       "  'code': 'def create_batch(self, frame):\\n        \"\"\"\\n        :param frame: an (w,h,channels) numpy array (image)\\n        :return: DataBatch of (1,channels,data_shape,data_shape)\\n        \"\"\"\\n        frame_resize = mx.nd.array(cv2.resize(frame, (self.data_shape[0], self.data_shape[1])))\\n        #frame_resize = mx.img.imresize(frame, self.data_shape[0], self.data_shape[1], cv2.INTER_LINEAR)\\n        # Change dimensions from (w,h,channels) to (channels, w, h)\\n        frame_t = mx.nd.transpose(frame_resize, axes=(2,0,1))\\n        frame_norm = frame_t - self.mean_pixels_nd\\n        # Add dimension for batch, results in (1,channels,w,h)\\n        batch_frame = [mx.nd.expand_dims(frame_norm, axis=0)]\\n        batch_shape = [DataDesc(\\'data\\', batch_frame[0].shape)]\\n        batch = DataBatch(data=batch_frame, provide_data=batch_shape)\\n        return batch',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Detector.detect_iter',\n",
       "  'docstring': 'detect all images in iterator\\n\\n        Parameters:\\n        ----------\\n        det_iter : DetIter\\n            iterator for all testing images\\n        show_timer : Boolean\\n            whether to print out detection exec time\\n\\n        Returns:\\n        ----------\\n        list of detection results',\n",
       "  'code': 'def detect_iter(self, det_iter, show_timer=False):\\n        \"\"\"\\n        detect all images in iterator\\n\\n        Parameters:\\n        ----------\\n        det_iter : DetIter\\n            iterator for all testing images\\n        show_timer : Boolean\\n            whether to print out detection exec time\\n\\n        Returns:\\n        ----------\\n        list of detection results\\n        \"\"\"\\n        num_images = det_iter._size\\n        if not isinstance(det_iter, mx.io.PrefetchingIter):\\n            det_iter = mx.io.PrefetchingIter(det_iter)\\n        start = timer()\\n        detections = self.mod.predict(det_iter).asnumpy()\\n        time_elapsed = timer() - start\\n        if show_timer:\\n            logging.info(\"Detection time for {} images: {:.4f} sec\".format(\\n                num_images, time_elapsed))\\n        result = Detector.filter_positive_detections(detections)\\n        return result',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Detector.visualize_detection',\n",
       "  'docstring': 'visualize detections in one image\\n\\n        Parameters:\\n        ----------\\n        img : numpy.array\\n            image, in bgr format\\n        dets : numpy.array\\n            ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\\n            each row is one object\\n        classes : tuple or list of str\\n            class names\\n        thresh : float\\n            score threshold',\n",
       "  'code': 'def visualize_detection(self, img, dets, classes=[], thresh=0.6):\\n        \"\"\"\\n        visualize detections in one image\\n\\n        Parameters:\\n        ----------\\n        img : numpy.array\\n            image, in bgr format\\n        dets : numpy.array\\n            ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\\n            each row is one object\\n        classes : tuple or list of str\\n            class names\\n        thresh : float\\n            score threshold\\n        \"\"\"\\n        import matplotlib.pyplot as plt\\n        import random\\n        plt.imshow(img)\\n        height = img.shape[0]\\n        width = img.shape[1]\\n        colors = dict()\\n        for det in dets:\\n            (klass, score, x0, y0, x1, y1) = det\\n            if score < thresh:\\n                continue\\n            cls_id = int(klass)\\n            if cls_id not in colors:\\n                colors[cls_id] = (random.random(), random.random(), random.random())\\n            xmin = int(x0 * width)\\n            ymin = int(y0 * height)\\n            xmax = int(x1 * width)\\n            ymax = int(y1 * height)\\n            rect = plt.Rectangle((xmin, ymin), xmax - xmin,\\n                                 ymax - ymin, fill=False,\\n                                 edgecolor=colors[cls_id],\\n                                 linewidth=3.5)\\n            plt.gca().add_patch(rect)\\n            class_name = str(cls_id)\\n            if classes and len(classes) > cls_id:\\n                class_name = classes[cls_id]\\n            plt.gca().text(xmin, ymin - 2,\\n                            \\'{:s} {:.3f}\\'.format(class_name, score),\\n                            bbox=dict(facecolor=colors[cls_id], alpha=0.5),\\n                                    fontsize=12, color=\\'white\\')\\n        plt.show()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'read_caffe_mean',\n",
       "  'docstring': \"Reads caffe formatted mean file\\n    :param caffe_mean_file: path to caffe mean file, presumably with 'binaryproto' suffix\\n    :return: mean image, converted from BGR to RGB format\",\n",
       "  'code': 'def read_caffe_mean(caffe_mean_file):\\n    \"\"\"\\n    Reads caffe formatted mean file\\n    :param caffe_mean_file: path to caffe mean file, presumably with \\'binaryproto\\' suffix\\n    :return: mean image, converted from BGR to RGB format\\n    \"\"\"\\n\\n    import caffe_parser\\n    import numpy as np\\n    mean_blob = caffe_parser.caffe_pb2.BlobProto()\\n    with open(caffe_mean_file, \\'rb\\') as f:\\n        mean_blob.ParseFromString(f.read())\\n\\n    img_mean_np = np.array(mean_blob.data)\\n    img_mean_np = img_mean_np.reshape(mean_blob.channels, mean_blob.height, mean_blob.width)\\n\\n    # swap channels from Caffe BGR to RGB\\n    img_mean_np[[0, 2], :, :] = img_mean_np[[2, 0], :, :]\\n\\n    return img_mean_np',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'build_iters',\n",
       "  'docstring': 'Load & generate training examples from multivariate time series data\\n    :return: data iters & variables required to define network architecture',\n",
       "  'code': 'def build_iters(data_dir, max_records, q, horizon, splits, batch_size):\\n    \"\"\"\\n    Load & generate training examples from multivariate time series data\\n    :return: data iters & variables required to define network architecture\\n    \"\"\"\\n    # Read in data as numpy array\\n    df = pd.read_csv(os.path.join(data_dir, \"electricity.txt\"), sep=\",\", header=None)\\n    feature_df = df.iloc[:, :].astype(float)\\n    x = feature_df.as_matrix()\\n    x = x[:max_records] if max_records else x\\n\\n    # Construct training examples based on horizon and window\\n    x_ts = np.zeros((x.shape[0] - q, q, x.shape[1]))\\n    y_ts = np.zeros((x.shape[0] - q, x.shape[1]))\\n    for n in range(x.shape[0]):\\n        if n + 1 < q:\\n            continue\\n        elif n + 1 + horizon > x.shape[0]:\\n            continue\\n        else:\\n            y_n = x[n + horizon, :]\\n            x_n = x[n + 1 - q:n + 1, :]\\n        x_ts[n-q] = x_n\\n        y_ts[n-q] = y_n\\n\\n    # Split into training and testing data\\n    training_examples = int(x_ts.shape[0] * splits[0])\\n    valid_examples = int(x_ts.shape[0] * splits[1])\\n    x_train, y_train = x_ts[:training_examples], \\\\\\n                       y_ts[:training_examples]\\n    x_valid, y_valid = x_ts[training_examples:training_examples + valid_examples], \\\\\\n                       y_ts[training_examples:training_examples + valid_examples]\\n    x_test, y_test = x_ts[training_examples + valid_examples:], \\\\\\n                     y_ts[training_examples + valid_examples:]\\n\\n    #build iterators to feed batches to network\\n    train_iter = mx.io.NDArrayIter(data=x_train,\\n                                   label=y_train,\\n                                   batch_size=batch_size)\\n    val_iter = mx.io.NDArrayIter(data=x_valid,\\n                                 label=y_valid,\\n                                 batch_size=batch_size)\\n    test_iter = mx.io.NDArrayIter(data=x_test,\\n                                  label=y_test,\\n                                  batch_size=batch_size)\\n    return train_iter, val_iter, test_iter',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_prepare_default_dtype',\n",
       "  'docstring': 'Prepare the value of dtype if `dtype` is None. If `src_array` is an NDArray, numpy.ndarray\\n    or scipy.sparse.csr.csr_matrix, return src_array.dtype. float32 is returned otherwise.',\n",
       "  'code': 'def _prepare_default_dtype(src_array, dtype):\\n    \"\"\"Prepare the value of dtype if `dtype` is None. If `src_array` is an NDArray, numpy.ndarray\\n    or scipy.sparse.csr.csr_matrix, return src_array.dtype. float32 is returned otherwise.\"\"\"\\n    if dtype is None:\\n        if isinstance(src_array, (NDArray, np.ndarray)):\\n            dtype = src_array.dtype\\n        elif spsp and isinstance(src_array, spsp.csr.csr_matrix):\\n            dtype = src_array.dtype\\n        else:\\n            dtype = mx_real_t\\n    return dtype',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'csr_matrix',\n",
       "  'docstring': 'Creates a `CSRNDArray`, an 2D array with compressed sparse row (CSR) format.\\n\\n    The CSRNDArray can be instantiated in several ways:\\n\\n    - csr_matrix(D):\\n        to construct a CSRNDArray with a dense 2D array ``D``\\n            -  **D** (*array_like*) - An object exposing the array interface, an object whose \\\\\\n            `__array__` method returns an array, or any (nested) sequence.\\n            - **ctx** (*Context, optional*) - Device context \\\\\\n            (default is the current default context).\\n            - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is ``D.dtype`` if ``D`` is an NDArray or numpy.ndarray, \\\\\\n            float32 otherwise.\\n\\n    - csr_matrix(S)\\n        to construct a CSRNDArray with a sparse 2D array ``S``\\n            -  **S** (*CSRNDArray or scipy.sparse.csr.csr_matrix*) - A sparse matrix.\\n            - **ctx** (*Context, optional*) - Device context \\\\\\n            (default is the current default context).\\n            - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is ``S.dtype``.\\n\\n    - csr_matrix((M, N))\\n        to construct an empty CSRNDArray with shape ``(M, N)``\\n            -  **M** (*int*) - Number of rows in the matrix\\n            -  **N** (*int*) - Number of columns in the matrix\\n            - **ctx** (*Context, optional*) - Device context \\\\\\n            (default is the current default context).\\n            - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is float32.\\n\\n    - csr_matrix((data, indices, indptr))\\n        to construct a CSRNDArray based on the definition of compressed sparse row format \\\\\\n        using three separate arrays, \\\\\\n        where the column indices for row i are stored in ``indices[indptr[i]:indptr[i+1]]`` \\\\\\n        and their corresponding values are stored in ``data[indptr[i]:indptr[i+1]]``. \\\\\\n        The column indices for a given row are expected to be **sorted in ascending order.** \\\\\\n        Duplicate column entries for the same row are not allowed.\\n            - **data** (*array_like*) - An object exposing the array interface, which \\\\\\n            holds all the non-zero entries of the matrix in row-major order.\\n            - **indices** (*array_like*) - An object exposing the array interface, which \\\\\\n            stores the column index for each non-zero element in ``data``.\\n            - **indptr** (*array_like*) - An object exposing the array interface, which \\\\\\n            stores the offset into ``data`` of the first non-zero element number of each \\\\\\n            row of the matrix.\\n            - **shape** (*tuple of int, optional*) - The shape of the array. The default \\\\\\n            shape is inferred from the indices and indptr arrays.\\n            - **ctx** (*Context, optional*) - Device context \\\\\\n            (default is the current default context).\\n            - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is ``data.dtype`` if ``data`` is an NDArray or numpy.ndarray, \\\\\\n            float32 otherwise.\\n\\n    - csr_matrix((data, (row, col)))\\n        to construct a CSRNDArray based on the COOrdinate format \\\\\\n        using three seperate arrays, \\\\\\n        where ``row[i]`` is the row index of the element, \\\\\\n        ``col[i]`` is the column index of the element \\\\\\n        and ``data[i]`` is the data corresponding to the element. All the missing \\\\\\n        elements in the input are taken to be zeroes.\\n            - **data** (*array_like*) - An object exposing the array interface, which \\\\\\n            holds all the non-zero entries of the matrix in COO format.\\n            - **row** (*array_like*) - An object exposing the array interface, which \\\\\\n            stores the row index for each non zero element in ``data``.\\n            - **col** (*array_like*) - An object exposing the array interface, which \\\\\\n            stores the col index for each non zero element in ``data``.\\n            - **shape** (*tuple of int, optional*) - The shape of the array. The default \\\\\\n            shape is inferred from the ``row`` and ``col`` arrays.\\n            - **ctx** (*Context, optional*) - Device context \\\\\\n            (default is the current default context).\\n            - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is float32.\\n\\n    Parameters\\n    ----------\\n    arg1: tuple of int, tuple of array_like, array_like, CSRNDArray, scipy.sparse.csr_matrix, \\\\\\n    scipy.sparse.coo_matrix, tuple of int or tuple of array_like\\n        The argument to help instantiate the csr matrix. See above for further details.\\n    shape : tuple of int, optional\\n        The shape of the csr matrix.\\n    ctx: Context, optional\\n        Device context (default is the current default context).\\n    dtype: str or numpy.dtype, optional\\n        The data type of the output array.\\n\\n    Returns\\n    -------\\n    CSRNDArray\\n        A `CSRNDArray` with the `csr` storage representation.\\n\\n    Example\\n    -------\\n    >>> a = mx.nd.sparse.csr_matrix(([1, 2, 3], [1, 0, 2], [0, 1, 2, 2, 3]), shape=(4, 3))\\n    >>> a.asnumpy()\\n    array([[ 0.,  1.,  0.],\\n           [ 2.,  0.,  0.],\\n           [ 0.,  0.,  0.],\\n           [ 0.,  0.,  3.]], dtype=float32)\\n\\n    See Also\\n    --------\\n    CSRNDArray : MXNet NDArray in compressed sparse row format.',\n",
       "  'code': 'def csr_matrix(arg1, shape=None, ctx=None, dtype=None):\\n    \"\"\"Creates a `CSRNDArray`, an 2D array with compressed sparse row (CSR) format.\\n\\n    The CSRNDArray can be instantiated in several ways:\\n\\n    - csr_matrix(D):\\n        to construct a CSRNDArray with a dense 2D array ``D``\\n            -  **D** (*array_like*) - An object exposing the array interface, an object whose \\\\\\n            `__array__` method returns an array, or any (nested) sequence.\\n            - **ctx** (*Context, optional*) - Device context \\\\\\n            (default is the current default context).\\n            - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is ``D.dtype`` if ``D`` is an NDArray or numpy.ndarray, \\\\\\n            float32 otherwise.\\n\\n    - csr_matrix(S)\\n        to construct a CSRNDArray with a sparse 2D array ``S``\\n            -  **S** (*CSRNDArray or scipy.sparse.csr.csr_matrix*) - A sparse matrix.\\n            - **ctx** (*Context, optional*) - Device context \\\\\\n            (default is the current default context).\\n            - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is ``S.dtype``.\\n\\n    - csr_matrix((M, N))\\n        to construct an empty CSRNDArray with shape ``(M, N)``\\n            -  **M** (*int*) - Number of rows in the matrix\\n            -  **N** (*int*) - Number of columns in the matrix\\n            - **ctx** (*Context, optional*) - Device context \\\\\\n            (default is the current default context).\\n            - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is float32.\\n\\n    - csr_matrix((data, indices, indptr))\\n        to construct a CSRNDArray based on the definition of compressed sparse row format \\\\\\n        using three separate arrays, \\\\\\n        where the column indices for row i are stored in ``indices[indptr[i]:indptr[i+1]]`` \\\\\\n        and their corresponding values are stored in ``data[indptr[i]:indptr[i+1]]``. \\\\\\n        The column indices for a given row are expected to be **sorted in ascending order.** \\\\\\n        Duplicate column entries for the same row are not allowed.\\n            - **data** (*array_like*) - An object exposing the array interface, which \\\\\\n            holds all the non-zero entries of the matrix in row-major order.\\n            - **indices** (*array_like*) - An object exposing the array interface, which \\\\\\n            stores the column index for each non-zero element in ``data``.\\n            - **indptr** (*array_like*) - An object exposing the array interface, which \\\\\\n            stores the offset into ``data`` of the first non-zero element number of each \\\\\\n            row of the matrix.\\n            - **shape** (*tuple of int, optional*) - The shape of the array. The default \\\\\\n            shape is inferred from the indices and indptr arrays.\\n            - **ctx** (*Context, optional*) - Device context \\\\\\n            (default is the current default context).\\n            - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is ``data.dtype`` if ``data`` is an NDArray or numpy.ndarray, \\\\\\n            float32 otherwise.\\n\\n    - csr_matrix((data, (row, col)))\\n        to construct a CSRNDArray based on the COOrdinate format \\\\\\n        using three seperate arrays, \\\\\\n        where ``row[i]`` is the row index of the element, \\\\\\n        ``col[i]`` is the column index of the element \\\\\\n        and ``data[i]`` is the data corresponding to the element. All the missing \\\\\\n        elements in the input are taken to be zeroes.\\n            - **data** (*array_like*) - An object exposing the array interface, which \\\\\\n            holds all the non-zero entries of the matrix in COO format.\\n            - **row** (*array_like*) - An object exposing the array interface, which \\\\\\n            stores the row index for each non zero element in ``data``.\\n            - **col** (*array_like*) - An object exposing the array interface, which \\\\\\n            stores the col index for each non zero element in ``data``.\\n            - **shape** (*tuple of int, optional*) - The shape of the array. The default \\\\\\n            shape is inferred from the ``row`` and ``col`` arrays.\\n            - **ctx** (*Context, optional*) - Device context \\\\\\n            (default is the current default context).\\n            - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is float32.\\n\\n    Parameters\\n    ----------\\n    arg1: tuple of int, tuple of array_like, array_like, CSRNDArray, scipy.sparse.csr_matrix, \\\\\\n    scipy.sparse.coo_matrix, tuple of int or tuple of array_like\\n        The argument to help instantiate the csr matrix. See above for further details.\\n    shape : tuple of int, optional\\n        The shape of the csr matrix.\\n    ctx: Context, optional\\n        Device context (default is the current default context).\\n    dtype: str or numpy.dtype, optional\\n        The data type of the output array.\\n\\n    Returns\\n    -------\\n    CSRNDArray\\n        A `CSRNDArray` with the `csr` storage representation.\\n\\n    Example\\n    -------\\n    >>> a = mx.nd.sparse.csr_matrix(([1, 2, 3], [1, 0, 2], [0, 1, 2, 2, 3]), shape=(4, 3))\\n    >>> a.asnumpy()\\n    array([[ 0.,  1.,  0.],\\n           [ 2.,  0.,  0.],\\n           [ 0.,  0.,  0.],\\n           [ 0.,  0.,  3.]], dtype=float32)\\n\\n    See Also\\n    --------\\n    CSRNDArray : MXNet NDArray in compressed sparse row format.\\n    \"\"\"\\n    # construct a csr matrix from (M, N) or (data, indices, indptr)\\n    if isinstance(arg1, tuple):\\n        arg_len = len(arg1)\\n        if arg_len == 2:\\n            # construct a sparse csr matrix from\\n            # scipy coo matrix if input format is coo\\n            if isinstance(arg1[1], tuple) and len(arg1[1]) == 2:\\n                data, (row, col) = arg1\\n                if isinstance(data, NDArray):\\n                    data = data.asnumpy()\\n                if isinstance(row, NDArray):\\n                    row = row.asnumpy()\\n                if isinstance(col, NDArray):\\n                    col = col.asnumpy()\\n                coo = spsp.coo_matrix((data, (row, col)), shape=shape)\\n                _check_shape(coo.shape, shape)\\n                csr = coo.tocsr()\\n                return array(csr, ctx=ctx, dtype=dtype)\\n            else:\\n                # empty matrix with shape\\n                _check_shape(arg1, shape)\\n                return empty(\\'csr\\', arg1, ctx=ctx, dtype=dtype)\\n        elif arg_len == 3:\\n            # data, indices, indptr\\n            return _csr_matrix_from_definition(arg1[0], arg1[1], arg1[2], shape=shape,\\n                                               ctx=ctx, dtype=dtype)\\n        else:\\n            raise ValueError(\"Unexpected length of input tuple: \" + str(arg_len))\\n    else:\\n        # construct a csr matrix from a sparse / dense one\\n        if isinstance(arg1, CSRNDArray) or (spsp and isinstance(arg1, spsp.csr.csr_matrix)):\\n            # construct a csr matrix from scipy or CSRNDArray\\n            _check_shape(arg1.shape, shape)\\n            return array(arg1, ctx=ctx, dtype=dtype)\\n        elif isinstance(arg1, RowSparseNDArray):\\n            raise ValueError(\"Unexpected input type: RowSparseNDArray\")\\n        else:\\n            # construct a csr matrix from a dense one\\n            # prepare default ctx and dtype since mx.nd.array doesn\\'t use default values\\n            # based on source_array\\n            dtype = _prepare_default_dtype(arg1, dtype)\\n            # create dns array with provided dtype. ctx is not passed since copy across\\n            # ctx requires dtype to be the same\\n            dns = _array(arg1, dtype=dtype)\\n            if ctx is not None and dns.context != ctx:\\n                dns = dns.as_in_context(ctx)\\n            _check_shape(dns.shape, shape)\\n            return dns.tostype(\\'csr\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'row_sparse_array',\n",
       "  'docstring': 'Creates a `RowSparseNDArray`, a multidimensional row sparse array with a set of \\\\\\n    tensor slices at given indices.\\n\\n    The RowSparseNDArray can be instantiated in several ways:\\n\\n    - row_sparse_array(D):\\n        to construct a RowSparseNDArray with a dense ndarray ``D``\\n        -  **D** (*array_like*) - An object exposing the array interface, an object whose \\\\\\n        `__array__` method returns an array, or any (nested) sequence.\\n        - **ctx** (*Context, optional*) - Device context \\\\\\n        (default is the current default context).\\n        - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n        The default dtype is ``D.dtype`` if ``D`` is an NDArray or numpy.ndarray, \\\\\\n        float32 otherwise.\\n\\n    - row_sparse_array(S)\\n        to construct a RowSparseNDArray with a sparse ndarray ``S``\\n        -  **S** (*RowSparseNDArray*) - A sparse ndarray.\\n        - **ctx** (*Context, optional*) - Device context \\\\\\n        (default is the current default context).\\n        - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n        The default dtype is ``S.dtype``.\\n\\n    - row_sparse_array((D0, D1 .. Dn))\\n        to construct an empty RowSparseNDArray with shape ``(D0, D1, ... Dn)``\\n        -  **D0, D1 .. Dn** (*int*) - The shape of the ndarray\\n        - **ctx** (*Context, optional*) - Device context \\\\\\n        (default is the current default context).\\n        - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is float32.\\n\\n    - row_sparse_array((data, indices))\\n        to construct a RowSparseNDArray based on the definition of row sparse format \\\\\\n        using two separate arrays, \\\\\\n        where the `indices` stores the indices of the row slices with non-zeros,\\n        while the values are stored in `data`. The corresponding NDArray ``dense``\\n        represented by RowSparseNDArray ``rsp`` has \\\\\\n        ``dense[rsp.indices[i], :, :, :, ...] = rsp.data[i, :, :, :, ...]``\\n        The row indices for are expected to be **sorted in ascending order.** \\\\\\n        - **data** (*array_like*) - An object exposing the array interface, which \\\\\\n        holds all the non-zero row slices of the array.\\n        - **indices** (*array_like*) - An object exposing the array interface, which \\\\\\n        stores the row index for each row slice with non-zero elements.\\n        - **shape** (*tuple of int, optional*) - The shape of the array. The default \\\\\\n        shape is inferred from the indices and indptr arrays.\\n        - **ctx** (*Context, optional*) - Device context \\\\\\n        (default is the current default context).\\n        - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n        The default dtype is float32.\\n\\n    Parameters\\n    ----------\\n    arg1 : NDArray, numpy.ndarray, RowSparseNDArray, tuple of int or tuple of array_like\\n        The argument to help instantiate the row sparse ndarray. See above for further details.\\n    shape : tuple of int, optional\\n        The shape of the row sparse ndarray. (Default value = None)\\n    ctx : Context, optional\\n        Device context (default is the current default context).\\n    dtype : str or numpy.dtype, optional\\n        The data type of the output array. (Default value = None)\\n\\n    Returns\\n    -------\\n    RowSparseNDArray\\n        An `RowSparseNDArray` with the `row_sparse` storage representation.\\n\\n    Examples\\n    --------\\n    >>> a = mx.nd.sparse.row_sparse_array(([[1, 2], [3, 4]], [1, 4]), shape=(6, 2))\\n    >>> a.asnumpy()\\n    array([[ 0.,  0.],\\n           [ 1.,  2.],\\n           [ 0.,  0.],\\n           [ 0.,  0.],\\n           [ 3.,  4.],\\n           [ 0.,  0.]], dtype=float32)\\n\\n    See Also\\n    --------\\n    RowSparseNDArray : MXNet NDArray in row sparse format.',\n",
       "  'code': 'def row_sparse_array(arg1, shape=None, ctx=None, dtype=None):\\n    \"\"\"Creates a `RowSparseNDArray`, a multidimensional row sparse array with a set of \\\\\\n    tensor slices at given indices.\\n\\n    The RowSparseNDArray can be instantiated in several ways:\\n\\n    - row_sparse_array(D):\\n        to construct a RowSparseNDArray with a dense ndarray ``D``\\n        -  **D** (*array_like*) - An object exposing the array interface, an object whose \\\\\\n        `__array__` method returns an array, or any (nested) sequence.\\n        - **ctx** (*Context, optional*) - Device context \\\\\\n        (default is the current default context).\\n        - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n        The default dtype is ``D.dtype`` if ``D`` is an NDArray or numpy.ndarray, \\\\\\n        float32 otherwise.\\n\\n    - row_sparse_array(S)\\n        to construct a RowSparseNDArray with a sparse ndarray ``S``\\n        -  **S** (*RowSparseNDArray*) - A sparse ndarray.\\n        - **ctx** (*Context, optional*) - Device context \\\\\\n        (default is the current default context).\\n        - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n        The default dtype is ``S.dtype``.\\n\\n    - row_sparse_array((D0, D1 .. Dn))\\n        to construct an empty RowSparseNDArray with shape ``(D0, D1, ... Dn)``\\n        -  **D0, D1 .. Dn** (*int*) - The shape of the ndarray\\n        - **ctx** (*Context, optional*) - Device context \\\\\\n        (default is the current default context).\\n        - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n            The default dtype is float32.\\n\\n    - row_sparse_array((data, indices))\\n        to construct a RowSparseNDArray based on the definition of row sparse format \\\\\\n        using two separate arrays, \\\\\\n        where the `indices` stores the indices of the row slices with non-zeros,\\n        while the values are stored in `data`. The corresponding NDArray ``dense``\\n        represented by RowSparseNDArray ``rsp`` has \\\\\\n        ``dense[rsp.indices[i], :, :, :, ...] = rsp.data[i, :, :, :, ...]``\\n        The row indices for are expected to be **sorted in ascending order.** \\\\\\n        - **data** (*array_like*) - An object exposing the array interface, which \\\\\\n        holds all the non-zero row slices of the array.\\n        - **indices** (*array_like*) - An object exposing the array interface, which \\\\\\n        stores the row index for each row slice with non-zero elements.\\n        - **shape** (*tuple of int, optional*) - The shape of the array. The default \\\\\\n        shape is inferred from the indices and indptr arrays.\\n        - **ctx** (*Context, optional*) - Device context \\\\\\n        (default is the current default context).\\n        - **dtype** (*str or numpy.dtype, optional*) - The data type of the output array. \\\\\\n        The default dtype is float32.\\n\\n    Parameters\\n    ----------\\n    arg1 : NDArray, numpy.ndarray, RowSparseNDArray, tuple of int or tuple of array_like\\n        The argument to help instantiate the row sparse ndarray. See above for further details.\\n    shape : tuple of int, optional\\n        The shape of the row sparse ndarray. (Default value = None)\\n    ctx : Context, optional\\n        Device context (default is the current default context).\\n    dtype : str or numpy.dtype, optional\\n        The data type of the output array. (Default value = None)\\n\\n    Returns\\n    -------\\n    RowSparseNDArray\\n        An `RowSparseNDArray` with the `row_sparse` storage representation.\\n\\n    Examples\\n    --------\\n    >>> a = mx.nd.sparse.row_sparse_array(([[1, 2], [3, 4]], [1, 4]), shape=(6, 2))\\n    >>> a.asnumpy()\\n    array([[ 0.,  0.],\\n           [ 1.,  2.],\\n           [ 0.,  0.],\\n           [ 0.,  0.],\\n           [ 3.,  4.],\\n           [ 0.,  0.]], dtype=float32)\\n\\n    See Also\\n    --------\\n    RowSparseNDArray : MXNet NDArray in row sparse format.\\n    \"\"\"\\n    # construct a row sparse array from (D0, D1 ..) or (data, indices)\\n    if isinstance(arg1, tuple):\\n        arg_len = len(arg1)\\n        if arg_len < 2:\\n            raise ValueError(\"Unexpected length of input tuple: \" + str(arg_len))\\n        elif arg_len > 2:\\n            # empty ndarray with shape\\n            _check_shape(arg1, shape)\\n            return empty(\\'row_sparse\\', arg1, ctx=ctx, dtype=dtype)\\n        else:\\n            # len(arg1) = 2, is either shape or (data, indices)\\n            if isinstance(arg1[0], integer_types) and isinstance(arg1[1], integer_types):\\n                # empty ndarray with shape\\n                _check_shape(arg1, shape)\\n                return empty(\\'row_sparse\\', arg1, ctx=ctx, dtype=dtype)\\n            else:\\n                # data, indices, indptr\\n                return _row_sparse_ndarray_from_definition(arg1[0], arg1[1], shape=shape,\\n                                                           ctx=ctx, dtype=dtype)\\n    else:\\n        # construct a row sparse ndarray from a dense / sparse array\\n        if isinstance(arg1, RowSparseNDArray):\\n            # construct a row sparse ndarray from RowSparseNDArray\\n            _check_shape(arg1.shape, shape)\\n            return array(arg1, ctx=ctx, dtype=dtype)\\n        elif isinstance(arg1, CSRNDArray):\\n            raise ValueError(\"Unexpected input type: CSRNDArray\")\\n        else:\\n            # construct a csr matrix from a dense one\\n            # prepare default dtype since mx.nd.array doesn\\'t use default values\\n            # based on source_array\\n            dtype = _prepare_default_dtype(arg1, dtype)\\n            # create dns array with provided dtype. ctx is not passed since copy across\\n            # ctx requires dtype to be the same\\n            dns = _array(arg1, dtype=dtype)\\n            if ctx is not None and dns.context != ctx:\\n                dns = dns.as_in_context(ctx)\\n            _check_shape(dns.shape, shape)\\n            return dns.tostype(\\'row_sparse\\')',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'add',\n",
       "  'docstring': \"Returns element-wise sum of the input arrays with broadcasting.\\n\\n    Equivalent to ``lhs + rhs``, ``mx.nd.broadcast_add(lhs, rhs)`` and\\n    ``mx.nd.broadcast_plus(lhs, rhs)`` when shapes of lhs and rhs do not\\n    match. If lhs.shape == rhs.shape, this is equivalent to\\n    ``mx.nd.elemwise_add(lhs, rhs)``\\n\\n    .. note::\\n\\n        If the corresponding dimensions of two arrays have the same size or one of them has size 1,\\n        then the arrays are broadcastable to a common shape.abs\\n\\n    Parameters\\n    ----------\\n    lhs : scalar or mxnet.ndarray.sparse.array\\n        First array to be added.\\n    rhs : scalar or mxnet.ndarray.sparse.array\\n         Second array to be added.\\n        If ``lhs.shape != rhs.shape``, they must be\\n        broadcastable to a common shape.\\n\\n    Returns\\n    -------\\n    NDArray\\n        The element-wise sum of the input arrays.\\n\\n    Examples\\n    --------\\n    >>> a = mx.nd.ones((2,3)).tostype('csr')\\n    >>> b = mx.nd.ones((2,3)).tostype('csr')\\n    >>> a.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> b.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> (a+b).asnumpy()\\n    array([[ 2.,  2.,  2.],\\n           [ 2.,  2.,  2.]], dtype=float32)\\n    >>> c = mx.nd.ones((2,3)).tostype('row_sparse')\\n    >>> d = mx.nd.ones((2,3)).tostype('row_sparse')\\n    >>> c.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> d.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> (c+d).asnumpy()\\n    array([[ 2.,  2.,  2.],\\n           [ 2.,  2.,  2.]], dtype=float32)\",\n",
       "  'code': 'def add(lhs, rhs):\\n    \"\"\"Returns element-wise sum of the input arrays with broadcasting.\\n\\n    Equivalent to ``lhs + rhs``, ``mx.nd.broadcast_add(lhs, rhs)`` and\\n    ``mx.nd.broadcast_plus(lhs, rhs)`` when shapes of lhs and rhs do not\\n    match. If lhs.shape == rhs.shape, this is equivalent to\\n    ``mx.nd.elemwise_add(lhs, rhs)``\\n\\n    .. note::\\n\\n        If the corresponding dimensions of two arrays have the same size or one of them has size 1,\\n        then the arrays are broadcastable to a common shape.abs\\n\\n    Parameters\\n    ----------\\n    lhs : scalar or mxnet.ndarray.sparse.array\\n        First array to be added.\\n    rhs : scalar or mxnet.ndarray.sparse.array\\n         Second array to be added.\\n        If ``lhs.shape != rhs.shape``, they must be\\n        broadcastable to a common shape.\\n\\n    Returns\\n    -------\\n    NDArray\\n        The element-wise sum of the input arrays.\\n\\n    Examples\\n    --------\\n    >>> a = mx.nd.ones((2,3)).tostype(\\'csr\\')\\n    >>> b = mx.nd.ones((2,3)).tostype(\\'csr\\')\\n    >>> a.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> b.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> (a+b).asnumpy()\\n    array([[ 2.,  2.,  2.],\\n           [ 2.,  2.,  2.]], dtype=float32)\\n    >>> c = mx.nd.ones((2,3)).tostype(\\'row_sparse\\')\\n    >>> d = mx.nd.ones((2,3)).tostype(\\'row_sparse\\')\\n    >>> c.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> d.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> (c+d).asnumpy()\\n    array([[ 2.,  2.,  2.],\\n           [ 2.,  2.,  2.]], dtype=float32)\\n    \"\"\"\\n    # pylint: disable= no-member, protected-access\\n    if isinstance(lhs, NDArray) and isinstance(rhs, NDArray) and lhs.shape == rhs.shape:\\n        return _ufunc_helper(\\n            lhs,\\n            rhs,\\n            op.elemwise_add,\\n            operator.add,\\n            _internal._plus_scalar,\\n            None)\\n\\n    return _ufunc_helper(\\n        lhs,\\n        rhs,\\n        op.broadcast_add,\\n        operator.add,\\n        _internal._plus_scalar,\\n        None)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'subtract',\n",
       "  'docstring': \"Returns element-wise difference of the input arrays with broadcasting.\\n\\n    Equivalent to ``lhs - rhs``, ``mx.nd.broadcast_sub(lhs, rhs)`` and\\n    ``mx.nd.broadcast_minus(lhs, rhs)`` when shapes of lhs and rhs do not\\n    match. If lhs.shape == rhs.shape, this is equivalent to\\n    ``mx.nd.elemwise_sub(lhs, rhs)``\\n\\n    .. note::\\n\\n        If the corresponding dimensions of two arrays have the same size or one of them has size 1,\\n        then the arrays are broadcastable to a common shape.\\n\\n    Parameters\\n    ----------\\n    lhs : scalar or mxnet.ndarray.sparse.array\\n        First array to be subtracted.\\n    rhs : scalar or mxnet.ndarray.sparse.array\\n         Second array to be subtracted.\\n        If ``lhs.shape != rhs.shape``, they must be\\n        broadcastable to a common shape.__spec__\\n\\n    Returns\\n    -------\\n    NDArray\\n        The element-wise difference of the input arrays.\\n\\n    Examples\\n    --------\\n    >>> a = mx.nd.ones((2,3)).tostype('csr')\\n    >>> b = mx.nd.ones((2,3)).tostype('csr')\\n    >>> a.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> b.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> (a-b).asnumpy()\\n    array([[ 0.,  0.,  0.],\\n           [ 0.,  0.,  0.]], dtype=float32)\\n    >>> c = mx.nd.ones((2,3)).tostype('row_sparse')\\n    >>> d = mx.nd.ones((2,3)).tostype('row_sparse')\\n    >>> c.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> d.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> (c-d).asnumpy()\\n    array([[ 0.,  0.,  0.],\\n           [ 0.,  0.,  0.]], dtype=float32)\",\n",
       "  'code': 'def subtract(lhs, rhs):\\n    \"\"\"Returns element-wise difference of the input arrays with broadcasting.\\n\\n    Equivalent to ``lhs - rhs``, ``mx.nd.broadcast_sub(lhs, rhs)`` and\\n    ``mx.nd.broadcast_minus(lhs, rhs)`` when shapes of lhs and rhs do not\\n    match. If lhs.shape == rhs.shape, this is equivalent to\\n    ``mx.nd.elemwise_sub(lhs, rhs)``\\n\\n    .. note::\\n\\n        If the corresponding dimensions of two arrays have the same size or one of them has size 1,\\n        then the arrays are broadcastable to a common shape.\\n\\n    Parameters\\n    ----------\\n    lhs : scalar or mxnet.ndarray.sparse.array\\n        First array to be subtracted.\\n    rhs : scalar or mxnet.ndarray.sparse.array\\n         Second array to be subtracted.\\n        If ``lhs.shape != rhs.shape``, they must be\\n        broadcastable to a common shape.__spec__\\n\\n    Returns\\n    -------\\n    NDArray\\n        The element-wise difference of the input arrays.\\n\\n    Examples\\n    --------\\n    >>> a = mx.nd.ones((2,3)).tostype(\\'csr\\')\\n    >>> b = mx.nd.ones((2,3)).tostype(\\'csr\\')\\n    >>> a.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> b.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> (a-b).asnumpy()\\n    array([[ 0.,  0.,  0.],\\n           [ 0.,  0.,  0.]], dtype=float32)\\n    >>> c = mx.nd.ones((2,3)).tostype(\\'row_sparse\\')\\n    >>> d = mx.nd.ones((2,3)).tostype(\\'row_sparse\\')\\n    >>> c.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> d.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> (c-d).asnumpy()\\n    array([[ 0.,  0.,  0.],\\n           [ 0.,  0.,  0.]], dtype=float32)\\n    \"\"\"\\n    # pylint: disable= no-member, protected-access\\n    if isinstance(lhs, NDArray) and isinstance(rhs, NDArray) and lhs.shape == rhs.shape:\\n        return _ufunc_helper(\\n            lhs,\\n            rhs,\\n            op.elemwise_sub,\\n            operator.sub,\\n            _internal._minus_scalar,\\n            None)\\n\\n    return _ufunc_helper(\\n        lhs,\\n        rhs,\\n        op.broadcast_sub,\\n        operator.sub,\\n        _internal._minus_scalar,\\n        None)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'multiply',\n",
       "  'docstring': \"Returns element-wise product of the input arrays with broadcasting.\\n\\n        Equivalent to ``lhs * rhs`` and ``mx.nd.broadcast_mul(lhs, rhs)``\\n        when shapes of lhs and rhs do not match. If lhs.shape == rhs.shape,\\n        this is equivalent to ``mx.nd.elemwise_mul(lhs, rhs)``\\n\\n    .. note::\\n\\n        If the corresponding dimensions of two arrays have the same size or one of them has size 1,\\n        then the arrays are broadcastable to a common shape.\\n\\n    Parameters\\n    ----------\\n    lhs : scalar or mxnet.ndarray.sparse.array\\n        First array to be multiplied.\\n    rhs : scalar or mxnet.ndarray.sparse.array\\n         Second array to be multiplied.\\n        If ``lhs.shape != rhs.shape``, they must be\\n        broadcastable to a common shape.\\n\\n    Returns\\n    -------\\n    NDArray\\n        The element-wise multiplication of the input arrays.\\n\\n    Examples\\n    --------\\n    >>> x = mx.nd.ones((2,3)).tostype('csr')\\n    >>> y = mx.nd.arange(2).reshape((2,1))\\n    >>> z = mx.nd.arange(3)\\n    >>> x.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> y.asnumpy()\\n    array([[ 0.],\\n           [ 1.]], dtype=float32)\\n    >>> z.asnumpy()\\n    array([ 0.,  1.,  2.], dtype=float32)\\n    >>> (x*2).asnumpy()\\n    array([[ 2.,  2.,  2.],\\n           [ 2.,  2.,  2.]], dtype=float32)\\n    >>> (x*y).asnumpy()\\n    array([[ 0.,  0.,  0.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> mx.nd.sparse.multiply(x, y).asnumpy()\\n    array([[ 0.,  0.,  0.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> (x*z).asnumpy()\\n    array([[ 0.,  1.,  2.],\\n           [ 0.,  1.,  2.]], dtype=float32)\\n    >>> mx.nd.sparse.multiply(x, z).asnumpy()\\n    array([[ 0.,  1.,  2.],\\n           [ 0.,  1.,  2.]], dtype=float32)\\n    >>> z = z.reshape((1, 3))\\n    >>> z.asnumpy()\\n    array([[ 0.,  1.,  2.]], dtype=float32)\\n    >>> (x*z).asnumpy()\\n    array([[ 0.,  1.,  2.],\\n           [ 0.,  1.,  2.]], dtype=float32)\\n    >>> mx.nd.sparse.multiply(x, z).asnumpy()\\n    array([[ 0.,  1.,  2.],\\n           [ 0.,  1.,  2.]], dtype=float32)\",\n",
       "  'code': 'def multiply(lhs, rhs):\\n    \"\"\"Returns element-wise product of the input arrays with broadcasting.\\n\\n        Equivalent to ``lhs * rhs`` and ``mx.nd.broadcast_mul(lhs, rhs)``\\n        when shapes of lhs and rhs do not match. If lhs.shape == rhs.shape,\\n        this is equivalent to ``mx.nd.elemwise_mul(lhs, rhs)``\\n\\n    .. note::\\n\\n        If the corresponding dimensions of two arrays have the same size or one of them has size 1,\\n        then the arrays are broadcastable to a common shape.\\n\\n    Parameters\\n    ----------\\n    lhs : scalar or mxnet.ndarray.sparse.array\\n        First array to be multiplied.\\n    rhs : scalar or mxnet.ndarray.sparse.array\\n         Second array to be multiplied.\\n        If ``lhs.shape != rhs.shape``, they must be\\n        broadcastable to a common shape.\\n\\n    Returns\\n    -------\\n    NDArray\\n        The element-wise multiplication of the input arrays.\\n\\n    Examples\\n    --------\\n    >>> x = mx.nd.ones((2,3)).tostype(\\'csr\\')\\n    >>> y = mx.nd.arange(2).reshape((2,1))\\n    >>> z = mx.nd.arange(3)\\n    >>> x.asnumpy()\\n    array([[ 1.,  1.,  1.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> y.asnumpy()\\n    array([[ 0.],\\n           [ 1.]], dtype=float32)\\n    >>> z.asnumpy()\\n    array([ 0.,  1.,  2.], dtype=float32)\\n    >>> (x*2).asnumpy()\\n    array([[ 2.,  2.,  2.],\\n           [ 2.,  2.,  2.]], dtype=float32)\\n    >>> (x*y).asnumpy()\\n    array([[ 0.,  0.,  0.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> mx.nd.sparse.multiply(x, y).asnumpy()\\n    array([[ 0.,  0.,  0.],\\n           [ 1.,  1.,  1.]], dtype=float32)\\n    >>> (x*z).asnumpy()\\n    array([[ 0.,  1.,  2.],\\n           [ 0.,  1.,  2.]], dtype=float32)\\n    >>> mx.nd.sparse.multiply(x, z).asnumpy()\\n    array([[ 0.,  1.,  2.],\\n           [ 0.,  1.,  2.]], dtype=float32)\\n    >>> z = z.reshape((1, 3))\\n    >>> z.asnumpy()\\n    array([[ 0.,  1.,  2.]], dtype=float32)\\n    >>> (x*z).asnumpy()\\n    array([[ 0.,  1.,  2.],\\n           [ 0.,  1.,  2.]], dtype=float32)\\n    >>> mx.nd.sparse.multiply(x, z).asnumpy()\\n    array([[ 0.,  1.,  2.],\\n           [ 0.,  1.,  2.]], dtype=float32)\\n    \"\"\"\\n    # pylint: disable= no-member, protected-access\\n    if isinstance(lhs, NDArray) and isinstance(rhs, NDArray) and lhs.shape == rhs.shape:\\n        return _ufunc_helper(\\n            lhs,\\n            rhs,\\n            op.elemwise_mul,\\n            operator.mul,\\n            _internal._mul_scalar,\\n            None)\\n\\n    return _ufunc_helper(\\n        lhs,\\n        rhs,\\n        op.broadcast_mul,\\n        operator.mul,\\n        _internal._mul_scalar,\\n        None)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'divide',\n",
       "  'docstring': \"Returns element-wise division of the input arrays with broadcasting.\\n\\n    Equivalent to ``lhs / rhs`` and ``mx.nd.broadcast_div(lhs, rhs)``\\n    when shapes of lhs and rhs do not match. If lhs.shape == rhs.shape,\\n    this is equivalent to ``mx.nd.elemwise_div(lhs, rhs)``\\n\\n    .. note::\\n\\n        If the corresponding dimensions of two arrays have the same size or one of them has size 1,\\n        then the arrays are broadcastable to a common shape.\\n\\n    Parameters\\n    ----------\\n    lhs : scalar or mxnet.ndarray.sparse.array\\n        First array in division.\\n    rhs : scalar or mxnet.ndarray.sparse.array\\n         Second array in division.\\n        The arrays to be divided. If ``lhs.shape != rhs.shape``, they must be\\n        broadcastable to a common shape.\\n\\n    Returns\\n    -------\\n    NDArray\\n        The element-wise division of the input arrays.\\n\\n    Examples\\n    --------\\n    >>> x = (mx.nd.ones((2,3))*6).tostype('csr')\\n    >>> y = mx.nd.arange(2).reshape((2,1)) + 1\\n    >>> z = mx.nd.arange(3) + 1\\n    >>> x.asnumpy()\\n    array([[ 6.,  6.,  6.],\\n           [ 6.,  6.,  6.]], dtype=float32)\\n    >>> y.asnumpy()\\n    array([[ 1.],\\n           [ 2.]], dtype=float32)\\n    >>> z.asnumpy()\\n    array([ 1.,  2.,  3.], dtype=float32)\\n    >>> x/2\\n    <NDArray 2x3 @cpu(0)>\\n    >>> (x/3).asnumpy()\\n    array([[ 2.,  2.,  2.],\\n           [ 2.,  2.,  2.]], dtype=float32)\\n    >>> (x/y).asnumpy()\\n    array([[ 6.,  6.,  6.],\\n           [ 3.,  3.,  3.]], dtype=float32)\\n    >>> mx.nd.sparse.divide(x,y).asnumpy()\\n    array([[ 6.,  6.,  6.],\\n           [ 3.,  3.,  3.]], dtype=float32)\\n    >>> (x/z).asnumpy()\\n    array([[ 6.,  3.,  2.],\\n           [ 6.,  3.,  2.]], dtype=float32)\\n    >>> mx.nd.sprase.divide(x,z).asnumpy()\\n    array([[ 6.,  3.,  2.],\\n           [ 6.,  3.,  2.]], dtype=float32)\\n    >>> z = z.reshape((1,3))\\n    >>> z.asnumpy()\\n    array([[ 1.,  2.,  3.]], dtype=float32)\\n    >>> (x/z).asnumpy()\\n    array([[ 6.,  3.,  2.],\\n           [ 6.,  3.,  2.]], dtype=float32)\\n    >>> mx.nd.sparse.divide(x,z).asnumpy()\\n    array([[ 6.,  3.,  2.],\\n           [ 6.,  3.,  2.]], dtype=float32)\",\n",
       "  'code': 'def divide(lhs, rhs):\\n    \"\"\"Returns element-wise division of the input arrays with broadcasting.\\n\\n    Equivalent to ``lhs / rhs`` and ``mx.nd.broadcast_div(lhs, rhs)``\\n    when shapes of lhs and rhs do not match. If lhs.shape == rhs.shape,\\n    this is equivalent to ``mx.nd.elemwise_div(lhs, rhs)``\\n\\n    .. note::\\n\\n        If the corresponding dimensions of two arrays have the same size or one of them has size 1,\\n        then the arrays are broadcastable to a common shape.\\n\\n    Parameters\\n    ----------\\n    lhs : scalar or mxnet.ndarray.sparse.array\\n        First array in division.\\n    rhs : scalar or mxnet.ndarray.sparse.array\\n         Second array in division.\\n        The arrays to be divided. If ``lhs.shape != rhs.shape``, they must be\\n        broadcastable to a common shape.\\n\\n    Returns\\n    -------\\n    NDArray\\n        The element-wise division of the input arrays.\\n\\n    Examples\\n    --------\\n    >>> x = (mx.nd.ones((2,3))*6).tostype(\\'csr\\')\\n    >>> y = mx.nd.arange(2).reshape((2,1)) + 1\\n    >>> z = mx.nd.arange(3) + 1\\n    >>> x.asnumpy()\\n    array([[ 6.,  6.,  6.],\\n           [ 6.,  6.,  6.]], dtype=float32)\\n    >>> y.asnumpy()\\n    array([[ 1.],\\n           [ 2.]], dtype=float32)\\n    >>> z.asnumpy()\\n    array([ 1.,  2.,  3.], dtype=float32)\\n    >>> x/2\\n    <NDArray 2x3 @cpu(0)>\\n    >>> (x/3).asnumpy()\\n    array([[ 2.,  2.,  2.],\\n           [ 2.,  2.,  2.]], dtype=float32)\\n    >>> (x/y).asnumpy()\\n    array([[ 6.,  6.,  6.],\\n           [ 3.,  3.,  3.]], dtype=float32)\\n    >>> mx.nd.sparse.divide(x,y).asnumpy()\\n    array([[ 6.,  6.,  6.],\\n           [ 3.,  3.,  3.]], dtype=float32)\\n    >>> (x/z).asnumpy()\\n    array([[ 6.,  3.,  2.],\\n           [ 6.,  3.,  2.]], dtype=float32)\\n    >>> mx.nd.sprase.divide(x,z).asnumpy()\\n    array([[ 6.,  3.,  2.],\\n           [ 6.,  3.,  2.]], dtype=float32)\\n    >>> z = z.reshape((1,3))\\n    >>> z.asnumpy()\\n    array([[ 1.,  2.,  3.]], dtype=float32)\\n    >>> (x/z).asnumpy()\\n    array([[ 6.,  3.,  2.],\\n           [ 6.,  3.,  2.]], dtype=float32)\\n    >>> mx.nd.sparse.divide(x,z).asnumpy()\\n    array([[ 6.,  3.,  2.],\\n           [ 6.,  3.,  2.]], dtype=float32)\\n    \"\"\"\\n    # pylint: disable= no-member, protected-access\\n    if isinstance(lhs, NDArray) and isinstance(rhs, NDArray) and lhs.shape == rhs.shape:\\n        return _ufunc_helper(\\n            lhs,\\n            rhs,\\n            op.elemwise_div,\\n            operator.truediv,\\n            _internal._div_scalar,\\n            None)\\n\\n    return _ufunc_helper(\\n        lhs,\\n        rhs,\\n        op.broadcast_div,\\n        operator.truediv,\\n        _internal._div_scalar,\\n        None)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'zeros',\n",
       "  'docstring': \"Return a new array of given shape and type, filled with zeros.\\n\\n    Parameters\\n    ----------\\n    stype: string\\n        The storage type of the empty array, such as 'row_sparse', 'csr', etc\\n    shape : int or tuple of int\\n        The shape of the empty array\\n    ctx : Context, optional\\n        An optional device context (default is the current default context)\\n    dtype : str or numpy.dtype, optional\\n        An optional value type (default is `float32`)\\n\\n    Returns\\n    -------\\n    RowSparseNDArray or CSRNDArray\\n        A created array\\n    Examples\\n    --------\\n    >>> mx.nd.sparse.zeros('csr', (1,2))\\n    <CSRNDArray 1x2 @cpu(0)>\\n    >>> mx.nd.sparse.zeros('row_sparse', (1,2), ctx=mx.cpu(), dtype='float16').asnumpy()\\n    array([[ 0.,  0.]], dtype=float16)\",\n",
       "  'code': 'def zeros(stype, shape, ctx=None, dtype=None, **kwargs):\\n    \"\"\"Return a new array of given shape and type, filled with zeros.\\n\\n    Parameters\\n    ----------\\n    stype: string\\n        The storage type of the empty array, such as \\'row_sparse\\', \\'csr\\', etc\\n    shape : int or tuple of int\\n        The shape of the empty array\\n    ctx : Context, optional\\n        An optional device context (default is the current default context)\\n    dtype : str or numpy.dtype, optional\\n        An optional value type (default is `float32`)\\n\\n    Returns\\n    -------\\n    RowSparseNDArray or CSRNDArray\\n        A created array\\n    Examples\\n    --------\\n    >>> mx.nd.sparse.zeros(\\'csr\\', (1,2))\\n    <CSRNDArray 1x2 @cpu(0)>\\n    >>> mx.nd.sparse.zeros(\\'row_sparse\\', (1,2), ctx=mx.cpu(), dtype=\\'float16\\').asnumpy()\\n    array([[ 0.,  0.]], dtype=float16)\\n    \"\"\"\\n    # pylint: disable= no-member, protected-access\\n    if stype == \\'default\\':\\n        return _zeros_ndarray(shape, ctx=ctx, dtype=dtype, **kwargs)\\n    if ctx is None:\\n        ctx = current_context()\\n    dtype = mx_real_t if dtype is None else dtype\\n    if stype in (\\'row_sparse\\', \\'csr\\'):\\n        aux_types = _STORAGE_AUX_TYPES[stype]\\n    else:\\n        raise ValueError(\"unknown storage type\" + stype)\\n    out = _ndarray_cls(_new_alloc_handle(stype, shape, ctx, True, dtype, aux_types))\\n    return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, out=out, **kwargs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'empty',\n",
       "  'docstring': \"Returns a new array of given shape and type, without initializing entries.\\n\\n    Parameters\\n    ----------\\n    stype: string\\n        The storage type of the empty array, such as 'row_sparse', 'csr', etc\\n    shape : int or tuple of int\\n        The shape of the empty array.\\n    ctx : Context, optional\\n        An optional device context (default is the current default context).\\n    dtype : str or numpy.dtype, optional\\n        An optional value type (default is `float32`).\\n\\n    Returns\\n    -------\\n    CSRNDArray or RowSparseNDArray\\n        A created array.\",\n",
       "  'code': 'def empty(stype, shape, ctx=None, dtype=None):\\n    \"\"\"Returns a new array of given shape and type, without initializing entries.\\n\\n    Parameters\\n    ----------\\n    stype: string\\n        The storage type of the empty array, such as \\'row_sparse\\', \\'csr\\', etc\\n    shape : int or tuple of int\\n        The shape of the empty array.\\n    ctx : Context, optional\\n        An optional device context (default is the current default context).\\n    dtype : str or numpy.dtype, optional\\n        An optional value type (default is `float32`).\\n\\n    Returns\\n    -------\\n    CSRNDArray or RowSparseNDArray\\n        A created array.\\n    \"\"\"\\n    if isinstance(shape, int):\\n        shape = (shape, )\\n    if ctx is None:\\n        ctx = current_context()\\n    if dtype is None:\\n        dtype = mx_real_t\\n    assert(stype is not None)\\n    if stype in (\\'csr\\', \\'row_sparse\\'):\\n        return zeros(stype, shape, ctx=ctx, dtype=dtype)\\n    else:\\n        raise Exception(\"unknown stype : \" + str(stype))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'array',\n",
       "  'docstring': \"Creates a sparse array from any object exposing the array interface.\\n\\n    Parameters\\n    ----------\\n    source_array : RowSparseNDArray, CSRNDArray or scipy.sparse.csr.csr_matrix\\n        The source sparse array\\n    ctx : Context, optional\\n        The default context is ``source_array.context`` if ``source_array`` is an NDArray. \\\\\\n        The current default context otherwise.\\n    dtype : str or numpy.dtype, optional\\n        The data type of the output array. The default dtype is ``source_array.dtype``\\n        if `source_array` is an `NDArray`, `numpy.ndarray` or `scipy.sparse.csr.csr_matrix`, \\\\\\n        `float32` otherwise.\\n\\n    Returns\\n    -------\\n    RowSparseNDArray or CSRNDArray\\n        An array with the same contents as the `source_array`.\\n\\n    Examples\\n    --------\\n    >>> import scipy.sparse as spsp\\n    >>> csr = spsp.csr_matrix((2, 100))\\n    >>> mx.nd.sparse.array(csr)\\n    <CSRNDArray 2x100 @cpu(0)>\\n    >>> mx.nd.sparse.array(mx.nd.sparse.zeros('csr', (3, 2)))\\n    <CSRNDArray 3x2 @cpu(0)>\\n    >>> mx.nd.sparse.array(mx.nd.sparse.zeros('row_sparse', (3, 2)))\\n    <RowSparseNDArray 3x2 @cpu(0)>\",\n",
       "  'code': 'def array(source_array, ctx=None, dtype=None):\\n    \"\"\"Creates a sparse array from any object exposing the array interface.\\n\\n    Parameters\\n    ----------\\n    source_array : RowSparseNDArray, CSRNDArray or scipy.sparse.csr.csr_matrix\\n        The source sparse array\\n    ctx : Context, optional\\n        The default context is ``source_array.context`` if ``source_array`` is an NDArray. \\\\\\n        The current default context otherwise.\\n    dtype : str or numpy.dtype, optional\\n        The data type of the output array. The default dtype is ``source_array.dtype``\\n        if `source_array` is an `NDArray`, `numpy.ndarray` or `scipy.sparse.csr.csr_matrix`, \\\\\\n        `float32` otherwise.\\n\\n    Returns\\n    -------\\n    RowSparseNDArray or CSRNDArray\\n        An array with the same contents as the `source_array`.\\n\\n    Examples\\n    --------\\n    >>> import scipy.sparse as spsp\\n    >>> csr = spsp.csr_matrix((2, 100))\\n    >>> mx.nd.sparse.array(csr)\\n    <CSRNDArray 2x100 @cpu(0)>\\n    >>> mx.nd.sparse.array(mx.nd.sparse.zeros(\\'csr\\', (3, 2)))\\n    <CSRNDArray 3x2 @cpu(0)>\\n    >>> mx.nd.sparse.array(mx.nd.sparse.zeros(\\'row_sparse\\', (3, 2)))\\n    <RowSparseNDArray 3x2 @cpu(0)>\\n    \"\"\"\\n    ctx = current_context() if ctx is None else ctx\\n    if isinstance(source_array, NDArray):\\n        assert(source_array.stype != \\'default\\'), \\\\\\n               \"Please use `tostype` to create RowSparseNDArray or CSRNDArray from an NDArray\"\\n        # prepare dtype and ctx based on source_array, if not provided\\n        dtype = _prepare_default_dtype(source_array, dtype)\\n        # if both dtype and ctx are different from source_array, we cannot copy directly\\n        if source_array.dtype != dtype and source_array.context != ctx:\\n            arr = empty(source_array.stype, source_array.shape, dtype=dtype)\\n            arr[:] = source_array\\n            arr = arr.as_in_context(ctx)\\n        else:\\n            arr = empty(source_array.stype, source_array.shape, dtype=dtype, ctx=ctx)\\n            arr[:] = source_array\\n        return arr\\n    elif spsp and isinstance(source_array, spsp.csr.csr_matrix):\\n        # TODO(haibin) implement `_sync_copy_from` with scipy csr object to reduce a copy\\n        # preprocess scipy csr to canonical form\\n        csr = source_array.sorted_indices()\\n        csr.sum_duplicates()\\n        dtype = _prepare_default_dtype(source_array, dtype)\\n        return csr_matrix((csr.data, csr.indices, csr.indptr), shape=csr.shape, \\\\\\n                          dtype=dtype, ctx=ctx)\\n    elif isinstance(source_array, (np.ndarray, np.generic)):\\n        raise ValueError(\"Please use mx.nd.array to create an NDArray with source_array of type \",\\n                         type(source_array))\\n    else:\\n        raise ValueError(\"Unexpected source_array type: \", type(source_array))',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'BaseSparseNDArray._aux_type',\n",
       "  'docstring': \"Data-type of the array's ith aux data.\\n\\n        Returns\\n        -------\\n        numpy.dtype\\n            This BaseSparseNDArray's aux data type.\",\n",
       "  'code': 'def _aux_type(self, i):\\n        \"\"\"Data-type of the array\\'s ith aux data.\\n\\n        Returns\\n        -------\\n        numpy.dtype\\n            This BaseSparseNDArray\\'s aux data type.\\n        \"\"\"\\n        aux_type = ctypes.c_int()\\n        check_call(_LIB.MXNDArrayGetAuxType(self.handle, i, ctypes.byref(aux_type)))\\n        return _DTYPE_MX_TO_NP[aux_type.value]',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'BaseSparseNDArray.astype',\n",
       "  'docstring': \"Return a copy of the array after casting to a specified type.\\n\\n        Parameters\\n        ----------\\n        dtype : numpy.dtype or str\\n            The type of the returned array.\\n        copy : bool\\n            Default `True`. By default, astype always returns a newly\\n            allocated ndarray on the same context. If this is set to\\n            `False`, and the dtype requested is the same as the ndarray's\\n            dtype, the ndarray is returned instead of a copy.\\n\\n        Examples\\n        --------\\n        >>> x = mx.nd.sparse.zeros('row_sparse', (2,3), dtype='float32')\\n        >>> y = x.astype('int32')\\n        >>> y.dtype\\n        <type 'numpy.int32'>\",\n",
       "  'code': 'def astype(self, dtype, copy=True):\\n        \"\"\"Return a copy of the array after casting to a specified type.\\n\\n        Parameters\\n        ----------\\n        dtype : numpy.dtype or str\\n            The type of the returned array.\\n        copy : bool\\n            Default `True`. By default, astype always returns a newly\\n            allocated ndarray on the same context. If this is set to\\n            `False`, and the dtype requested is the same as the ndarray\\'s\\n            dtype, the ndarray is returned instead of a copy.\\n\\n        Examples\\n        --------\\n        >>> x = mx.nd.sparse.zeros(\\'row_sparse\\', (2,3), dtype=\\'float32\\')\\n        >>> y = x.astype(\\'int32\\')\\n        >>> y.dtype\\n        <type \\'numpy.int32\\'>\\n        \"\"\"\\n        if not copy and np.dtype(dtype) == self.dtype:\\n            return self\\n\\n        res = zeros(shape=self.shape, ctx=self.context,\\n                    dtype=dtype, stype=self.stype)\\n        self.copyto(res)\\n        return res',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'CSRNDArray.asscipy',\n",
       "  'docstring': \"Returns a ``scipy.sparse.csr.csr_matrix`` object with value copied from this array\\n\\n        Examples\\n        --------\\n        >>> x = mx.nd.sparse.zeros('csr', (2,3))\\n        >>> y = x.asscipy()\\n        >>> type(y)\\n        <type 'scipy.sparse.csr.csr_matrix'>\\n        >>> y\\n        <2x3 sparse matrix of type '<type 'numpy.float32'>'\\n        with 0 stored elements in Compressed Sparse Row format>\",\n",
       "  'code': 'def asscipy(self):\\n        \"\"\"Returns a ``scipy.sparse.csr.csr_matrix`` object with value copied from this array\\n\\n        Examples\\n        --------\\n        >>> x = mx.nd.sparse.zeros(\\'csr\\', (2,3))\\n        >>> y = x.asscipy()\\n        >>> type(y)\\n        <type \\'scipy.sparse.csr.csr_matrix\\'>\\n        >>> y\\n        <2x3 sparse matrix of type \\'<type \\'numpy.float32\\'>\\'\\n        with 0 stored elements in Compressed Sparse Row format>\\n        \"\"\"\\n        data = self.data.asnumpy()\\n        indices = self.indices.asnumpy()\\n        indptr = self.indptr.asnumpy()\\n        if not spsp:\\n            raise ImportError(\"scipy is not available. \\\\\\n                               Please check if the scipy python bindings are installed.\")\\n        return spsp.csr_matrix((data, indices, indptr), shape=self.shape, dtype=self.dtype)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_get_grad',\n",
       "  'docstring': \"This is an internal helper function that can be used for either of these\\n    but not both at the same time:\\n    1. Record the output and gradient of output of an intermediate convolutional layer.\\n    2. Record the gradients of the image.\\n\\n    Parameters\\n    ----------\\n    image : NDArray\\n        Image to visuaize. This is an NDArray with the preprocessed image.\\n    class_id : int\\n        Category ID this image belongs to. If not provided,\\n        network's prediction will be used.\\n    conv_layer_name: str\\n        Name of the convolutional layer whose output and output's gradients need to be acptured.\\n    image_grad: bool\\n        Whether to capture gradients of the image.\",\n",
       "  'code': 'def _get_grad(net, image, class_id=None, conv_layer_name=None, image_grad=False):\\n    \"\"\"This is an internal helper function that can be used for either of these\\n    but not both at the same time:\\n    1. Record the output and gradient of output of an intermediate convolutional layer.\\n    2. Record the gradients of the image.\\n\\n    Parameters\\n    ----------\\n    image : NDArray\\n        Image to visuaize. This is an NDArray with the preprocessed image.\\n    class_id : int\\n        Category ID this image belongs to. If not provided,\\n        network\\'s prediction will be used.\\n    conv_layer_name: str\\n        Name of the convolutional layer whose output and output\\'s gradients need to be acptured.\\n    image_grad: bool\\n        Whether to capture gradients of the image.\"\"\"\\n\\n    if image_grad:\\n        image.attach_grad()\\n        Conv2D.capture_layer_name = None\\n        Activation.set_guided_backprop(True)\\n    else:\\n        # Tell convviz.Conv2D which layer\\'s output and gradient needs to be recorded\\n        Conv2D.capture_layer_name = conv_layer_name\\n        Activation.set_guided_backprop(False)\\n    \\n    # Run the network\\n    with autograd.record(train_mode=False):\\n        out = net(image)\\n    \\n    # If user didn\\'t provide a class id, we\\'ll use the class that the network predicted\\n    if class_id == None:\\n        model_output = out.asnumpy()\\n        class_id = np.argmax(model_output)\\n\\n    # Create a one-hot target with class_id and backprop with the created target\\n    one_hot_target = mx.nd.one_hot(mx.nd.array([class_id]), 1000)\\n    out.backward(one_hot_target, train_mode=False)\\n\\n    if image_grad:\\n        return image.grad[0].asnumpy()\\n    else:\\n        # Return the recorded convolution output and gradient\\n        conv_out = Conv2D.conv_output\\n        return conv_out[0].asnumpy(), conv_out.grad[0].asnumpy()',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'np',\n",
       "  'docstring': 'Creates a custom evaluation metric that receives its inputs as numpy arrays.\\n\\n    Parameters\\n    ----------\\n    numpy_feval : callable(label, pred)\\n        Custom evaluation function that receives labels and predictions for a minibatch\\n        as numpy arrays and returns the corresponding custom metric as a floating point number.\\n    name : str, optional\\n        Name of the custom metric.\\n    allow_extra_outputs : bool, optional\\n        Whether prediction output is allowed to have extra outputs. This is useful in cases\\n        like RNN where states are also part of output which can then be fed back to the RNN\\n        in the next step. By default, extra outputs are not allowed.\\n\\n    Returns\\n    -------\\n    float\\n        Custom metric corresponding to the provided labels and predictions.\\n\\n    Example\\n    -------\\n    >>> def custom_metric(label, pred):\\n    ...     return np.mean(np.abs(label-pred))\\n    ...\\n    >>> metric = mx.metric.np(custom_metric)',\n",
       "  'code': 'def np(numpy_feval, name=None, allow_extra_outputs=False):\\n    \"\"\"Creates a custom evaluation metric that receives its inputs as numpy arrays.\\n\\n    Parameters\\n    ----------\\n    numpy_feval : callable(label, pred)\\n        Custom evaluation function that receives labels and predictions for a minibatch\\n        as numpy arrays and returns the corresponding custom metric as a floating point number.\\n    name : str, optional\\n        Name of the custom metric.\\n    allow_extra_outputs : bool, optional\\n        Whether prediction output is allowed to have extra outputs. This is useful in cases\\n        like RNN where states are also part of output which can then be fed back to the RNN\\n        in the next step. By default, extra outputs are not allowed.\\n\\n    Returns\\n    -------\\n    float\\n        Custom metric corresponding to the provided labels and predictions.\\n\\n    Example\\n    -------\\n    >>> def custom_metric(label, pred):\\n    ...     return np.mean(np.abs(label-pred))\\n    ...\\n    >>> metric = mx.metric.np(custom_metric)\\n    \"\"\"\\n    def feval(label, pred):\\n        \"\"\"Internal eval function.\"\"\"\\n        return numpy_feval(label, pred)\\n    feval.__name__ = numpy_feval.__name__\\n    return CustomMetric(feval, name, allow_extra_outputs)',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_BinaryClassificationMetrics.update_binary_stats',\n",
       "  'docstring': 'Update various binary classification counts for a single (label, pred)\\n        pair.\\n\\n        Parameters\\n        ----------\\n        label : `NDArray`\\n            The labels of the data.\\n\\n        pred : `NDArray`\\n            Predicted values.',\n",
       "  'code': 'def update_binary_stats(self, label, pred):\\n        \"\"\"\\n        Update various binary classification counts for a single (label, pred)\\n        pair.\\n\\n        Parameters\\n        ----------\\n        label : `NDArray`\\n            The labels of the data.\\n\\n        pred : `NDArray`\\n            Predicted values.\\n        \"\"\"\\n        pred = pred.asnumpy()\\n        label = label.asnumpy().astype(\\'int32\\')\\n        pred_label = numpy.argmax(pred, axis=1)\\n\\n        check_label_shapes(label, pred)\\n        if len(numpy.unique(label)) > 2:\\n            raise ValueError(\"%s currently only supports binary classification.\"\\n                             % self.__class__.__name__)\\n        pred_true = (pred_label == 1)\\n        pred_false = 1 - pred_true\\n        label_true = (label == 1)\\n        label_false = 1 - label_true\\n\\n        true_pos = (pred_true * label_true).sum()\\n        false_pos = (pred_true * label_false).sum()\\n        false_neg = (pred_false * label_true).sum()\\n        true_neg = (pred_false * label_false).sum()\\n        self.true_positives += true_pos\\n        self.global_true_positives += true_pos\\n        self.false_positives += false_pos\\n        self.global_false_positives += false_pos\\n        self.false_negatives += false_neg\\n        self.global_false_negatives += false_neg\\n        self.true_negatives += true_neg\\n        self.global_true_negatives += true_neg',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'MApMetric.update',\n",
       "  'docstring': 'Update internal records. This function now only update internal buffer,\\n        sum_metric and num_inst are updated in _update() function instead when\\n        get() is called to return results.\\n\\n        Params:\\n        ----------\\n        labels: mx.nd.array (n * 6) or (n * 5), difficult column is optional\\n            2-d array of ground-truths, n objects(id-xmin-ymin-xmax-ymax-[difficult])\\n        preds: mx.nd.array (m * 6)\\n            2-d array of detections, m objects(id-score-xmin-ymin-xmax-ymax)',\n",
       "  'code': 'def update(self, labels, preds):\\n        \"\"\"\\n        Update internal records. This function now only update internal buffer,\\n        sum_metric and num_inst are updated in _update() function instead when\\n        get() is called to return results.\\n\\n        Params:\\n        ----------\\n        labels: mx.nd.array (n * 6) or (n * 5), difficult column is optional\\n            2-d array of ground-truths, n objects(id-xmin-ymin-xmax-ymax-[difficult])\\n        preds: mx.nd.array (m * 6)\\n            2-d array of detections, m objects(id-score-xmin-ymin-xmax-ymax)\\n        \"\"\"\\n        def iou(x, ys):\\n            \"\"\"\\n            Calculate intersection-over-union overlap\\n            Params:\\n            ----------\\n            x : numpy.array\\n                single box [xmin, ymin ,xmax, ymax]\\n            ys : numpy.array\\n                multiple box [[xmin, ymin, xmax, ymax], [...], ]\\n            Returns:\\n            -----------\\n            numpy.array\\n                [iou1, iou2, ...], size == ys.shape[0]\\n            \"\"\"\\n            ixmin = np.maximum(ys[:, 0], x[0])\\n            iymin = np.maximum(ys[:, 1], x[1])\\n            ixmax = np.minimum(ys[:, 2], x[2])\\n            iymax = np.minimum(ys[:, 3], x[3])\\n            iw = np.maximum(ixmax - ixmin, 0.)\\n            ih = np.maximum(iymax - iymin, 0.)\\n            inters = iw * ih\\n            uni = (x[2] - x[0]) * (x[3] - x[1]) + (ys[:, 2] - ys[:, 0]) * \\\\\\n                (ys[:, 3] - ys[:, 1]) - inters\\n            ious = inters / uni\\n            ious[uni < 1e-12] = 0  # in case bad boxes\\n            return ious\\n\\n        # independant execution for each image\\n        for i in range(labels[0].shape[0]):\\n            # get as numpy arrays\\n            label = labels[0][i].asnumpy()\\n            if np.sum(label[:, 0] >= 0) < 1:\\n                continue\\n            pred = preds[self.pred_idx][i].asnumpy()\\n            # calculate for each class\\n            while (pred.shape[0] > 0):\\n                cid = int(pred[0, 0])\\n                indices = np.where(pred[:, 0].astype(int) == cid)[0]\\n                if cid < 0:\\n                    pred = np.delete(pred, indices, axis=0)\\n                    continue\\n                dets = pred[indices]\\n                pred = np.delete(pred, indices, axis=0)\\n                # sort by score, desceding\\n                dets = dets[dets[:,1].argsort()[::-1]]\\n                records = np.hstack((dets[:, 1][:, np.newaxis], np.zeros((dets.shape[0], 1))))\\n                # ground-truths\\n                label_indices = np.where(label[:, 0].astype(int) == cid)[0]\\n                gts = label[label_indices, :]\\n                label = np.delete(label, label_indices, axis=0)\\n                if gts.size > 0:\\n                    found = [False] * gts.shape[0]\\n                    for j in range(dets.shape[0]):\\n                        # compute overlaps\\n                        ious = iou(dets[j, 2:], gts[:, 1:5])\\n                        ovargmax = np.argmax(ious)\\n                        ovmax = ious[ovargmax]\\n                        if ovmax > self.ovp_thresh:\\n                            if (not self.use_difficult and\\n                                gts.shape[1] >= 6 and\\n                                gts[ovargmax, 5] > 0):\\n                                pass\\n                            else:\\n                                if not found[ovargmax]:\\n                                    records[j, -1] = 1  # tp\\n                                    found[ovargmax] = True\\n                                else:\\n                                    # duplicate\\n                                    records[j, -1] = 2  # fp\\n                        else:\\n                            records[j, -1] = 2 # fp\\n                else:\\n                    # no gt, mark all fp\\n                    records[:, -1] = 2\\n\\n                # ground truth count\\n                if (not self.use_difficult and gts.shape[1] >= 6):\\n                    gt_count = np.sum(gts[:, 5] < 1)\\n                else:\\n                    gt_count = gts.shape[0]\\n\\n                # now we push records to buffer\\n                # first column: score, second column: tp/fp\\n                # 0: not set(matched to difficult or something), 1: tp, 2: fp\\n                records = records[np.where(records[:, -1] > 0)[0], :]\\n                if records.size > 0:\\n                    self._insert(cid, records, gt_count)\\n\\n            # add missing class if not present in prediction\\n            while (label.shape[0] > 0):\\n                cid = int(label[0, 0])\\n                label_indices = np.where(label[:, 0].astype(int) == cid)[0]\\n                label = np.delete(label, label_indices, axis=0)\\n                if cid < 0:\\n                    continue\\n                gt_count = label_indices.size\\n                self._insert(cid, np.array([[0, 0]]), gt_count)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'MApMetric._average_precision',\n",
       "  'docstring': 'calculate average precision\\n\\n        Params:\\n        ----------\\n        rec : numpy.array\\n            cumulated recall\\n        prec : numpy.array\\n            cumulated precision\\n        Returns:\\n        ----------\\n        ap as float',\n",
       "  'code': 'def _average_precision(self, rec, prec):\\n        \"\"\"\\n        calculate average precision\\n\\n        Params:\\n        ----------\\n        rec : numpy.array\\n            cumulated recall\\n        prec : numpy.array\\n            cumulated precision\\n        Returns:\\n        ----------\\n        ap as float\\n        \"\"\"\\n        # append sentinel values at both ends\\n        mrec = np.concatenate(([0.], rec, [1.]))\\n        mpre = np.concatenate(([0.], prec, [0.]))\\n\\n        # compute precision integration ladder\\n        for i in range(mpre.size - 1, 0, -1):\\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\\n\\n        # look for recall value changes\\n        i = np.where(mrec[1:] != mrec[:-1])[0]\\n\\n        # sum (\\\\delta recall) * prec\\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\\n        return ap',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'VOC07MApMetric._average_precision',\n",
       "  'docstring': 'calculate average precision, override the default one,\\n        special 11-point metric\\n\\n        Params:\\n        ----------\\n        rec : numpy.array\\n            cumulated recall\\n        prec : numpy.array\\n            cumulated precision\\n        Returns:\\n        ----------\\n        ap as float',\n",
       "  'code': 'def _average_precision(self, rec, prec):\\n        \"\"\"\\n        calculate average precision, override the default one,\\n        special 11-point metric\\n\\n        Params:\\n        ----------\\n        rec : numpy.array\\n            cumulated recall\\n        prec : numpy.array\\n            cumulated precision\\n        Returns:\\n        ----------\\n        ap as float\\n        \"\"\"\\n        ap = 0.\\n        for t in np.arange(0., 1.1, 0.1):\\n            if np.sum(rec >= t) == 0:\\n                p = 0\\n            else:\\n                p = np.max(prec[rec >= t])\\n            ap += p / 11.\\n        return ap',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'read_data',\n",
       "  'docstring': 'download and read data into numpy',\n",
       "  'code': 'def read_data(label, image):\\n    \"\"\"\\n    download and read data into numpy\\n    \"\"\"\\n    base_url = \\'http://yann.lecun.com/exdb/mnist/\\'\\n    with gzip.open(download_file(base_url+label, os.path.join(\\'data\\',label))) as flbl:\\n        magic, num = struct.unpack(\">II\", flbl.read(8))\\n        label = np.fromstring(flbl.read(), dtype=np.int8)\\n    with gzip.open(download_file(base_url+image, os.path.join(\\'data\\',image)), \\'rb\\') as fimg:\\n        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\\n        image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\\n    return (label, image)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'main',\n",
       "  'docstring': 'Module main execution',\n",
       "  'code': 'def main():\\n    \"\"\"Module main execution\"\"\"\\n    # Initialization variables - update to change your model and execution context\\n    model_prefix = \"FCN8s_VGG16\"\\n    epoch = 19\\n\\n    # By default, MXNet will run on the CPU. Change to ctx = mx.gpu() to run on GPU.\\n    ctx = mx.cpu()\\n\\n    fcnxs, fcnxs_args, fcnxs_auxs = mx.model.load_checkpoint(model_prefix, epoch)\\n    fcnxs_args[\"data\"] = mx.nd.array(get_data(args.input), ctx)\\n    data_shape = fcnxs_args[\"data\"].shape\\n    label_shape = (1, data_shape[2]*data_shape[3])\\n    fcnxs_args[\"softmax_label\"] = mx.nd.empty(label_shape, ctx)\\n    exector = fcnxs.bind(ctx, fcnxs_args, args_grad=None, grad_req=\"null\", aux_states=fcnxs_args)\\n    exector.forward(is_train=False)\\n    output = exector.outputs[0]\\n    out_img = np.uint8(np.squeeze(output.asnumpy().argmax(axis=1)))\\n    out_img = Image.fromarray(out_img)\\n    out_img.putpalette(get_palette())\\n    out_img.save(args.output)',\n",
       "  'language': 'python'},\n",
       " {'function_name': 'Monitor.toc',\n",
       "  'docstring': 'End collecting for current batch and return results.\\n        Call after computation of current batch.\\n\\n        Returns\\n        -------\\n        res : list of',\n",
       "  'code': 'def toc(self):\\n        \"\"\"End collecting for current batch and return results.\\n        Call after computation of current batch.\\n\\n        Returns\\n        -------\\n        res : list of \"\"\"\\n        if not self.activated:\\n            return []\\n        for exe in self.exes:\\n            for array in exe.arg_arrays:\\n                array.wait_to_read()\\n            for array in exe.aux_arrays:\\n                array.wait_to_read()\\n        for exe in self.exes:\\n            for name, array in zip(exe._symbol.list_arguments(), exe.arg_arrays):\\n                if self.re_prog.match(name):\\n                    self.queue.append((self.step, name, self.stat_func(array)))\\n            for name, array in zip(exe._symbol.list_auxiliary_states(), exe.aux_arrays):\\n                if self.re_prog.match(name):\\n                    self.queue.append((self.step, name, self.stat_func(array)))\\n        self.activated = False\\n        res = []\\n        if self.sort:\\n            self.queue.sort(key=lambda x: x[1])\\n        for n, k, v_list in self.queue:\\n            if isinstance(v_list, NDArray):\\n                v_list = [v_list]\\n            assert isinstance(v_list, list)\\n            s = \\'\\'\\n            for v in v_list:\\n                assert isinstance(v, NDArray)\\n                if v.shape == (1,):\\n                    s += str(v.asscalar()) + \\'\\\\t\\'\\n                else:\\n                    s += str(v.asnumpy()) + \\'\\\\t\\'\\n            res.append((n, k, s))\\n        self.queue = []\\n        return res',\n",
       "  'language': 'python'},\n",
       " {'function_name': '_TokenEmbedding.get_vecs_by_tokens',\n",
       "  'docstring': 'Look up embedding vectors of tokens.\\n\\n\\n        Parameters\\n        ----------\\n        tokens : str or list of strs\\n            A token or a list of tokens.\\n        lower_case_backup : bool, default False\\n            If False, each token in the original case will be looked up; if True, each token in the\\n            original case will be looked up first, if not found in the keys of the property\\n            `token_to_idx`, the token in the lower case will be looked up.\\n\\n\\n        Returns\\n        -------\\n        mxnet.ndarray.NDArray:\\n            The embedding vector(s) of the token(s). According to numpy conventions, if `tokens` is\\n            a string, returns a 1-D NDArray of shape `self.vec_len`; if `tokens` is a list of\\n            strings, returns a 2-D NDArray of shape=(len(tokens), self.vec_len).',\n",
       "  'code': 'def get_vecs_by_tokens(self, tokens, lower_case_backup=False):\\n        \"\"\"Look up embedding vectors of tokens.\\n\\n\\n        Parameters\\n        ----------\\n        tokens : str or list of strs\\n            A token or a list of tokens.\\n        lower_case_backup : bool, default False\\n            If False, each token in the original case will be looked up; if True, each token in the\\n            original case will be looked up first, if not found in the keys of the property\\n            `token_to_idx`, the token in the lower case will be looked up.\\n\\n\\n        Returns\\n        -------\\n        mxnet.ndarray.NDArray:\\n            The embedding vector(s) of the token(s). According to numpy conventions, if `tokens` is\\n            a string, returns a 1-D NDArray of shape `self.vec_len`; if `tokens` is a list of\\n            strings, returns a 2-D NDArray of shape=(len(tokens), self.vec_len).\\n        \"\"\"\\n\\n        to_reduce = False\\n        if not isinstance(tokens, list):\\n            tokens = [tokens]\\n            to_reduce = True\\n\\n        if not lower_case_backup:\\n            indices = [self.token_to_idx.get(token, C.UNKNOWN_IDX) for token in tokens]\\n        else:\\n            indices = [self.token_to_idx[token] if token in self.token_to_idx\\n                       else self.token_to_idx.get(token.lower(), C.UNKNOWN_IDX)\\n                       for token in tokens]\\n\\n        vecs = nd.Embedding(nd.array(indices), self.idx_to_vec, self.idx_to_vec.shape[0],\\n                            self.idx_to_vec.shape[1])\\n\\n        return vecs[0] if to_reduce else vecs',\n",
       "  'language': 'python'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "635c3508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_name': 'face_distance',\n",
       " 'docstring': \"Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\\n    for each comparison face. The distance tells you how similar the faces are.\\n\\n    :param faces: List of face encodings to compare\\n    :param face_to_compare: A face encoding to compare against\\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\",\n",
       " 'code': 'def face_distance(face_encodings, face_to_compare):\\n    \"\"\"\\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\\n    for each comparison face. The distance tells you how similar the faces are.\\n\\n    :param faces: List of face encodings to compare\\n    :param face_to_compare: A face encoding to compare against\\n    :return: A numpy ndarray with the distance for each face in the same order as the \\'faces\\' array\\n    \"\"\"\\n    if len(face_encodings) == 0:\\n        return np.empty((0))\\n\\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)',\n",
       " 'language': 'python'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86cf9871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1500 training samples from 500 examples\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a code-optimized tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "\n",
    "\n",
    "def prepare_training_examples(examples, tokenizer, max_length=512):\n",
    "    \"\"\"Convert raw code examples into training samples for code completion.\"\"\"\n",
    "    training_samples = []\n",
    "\n",
    "    for example in examples:\n",
    "        code = example[\"code\"]\n",
    "\n",
    "        # Skip empty or very short functions\n",
    "        if len(code.strip()) < 20:\n",
    "            continue\n",
    "\n",
    "        # Tokenize the code\n",
    "        tokenized = tokenizer(code, truncation=True, max_length=max_length)\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "\n",
    "        # Create training examples with different completion points\n",
    "        # We'll randomly mask 10-50% of the tokens at the end\n",
    "        seq_length = len(input_ids)\n",
    "        if seq_length > 20:  # Ensure there's enough content to mask\n",
    "            # Create a few different masking positions for each example\n",
    "            for _ in range(3):\n",
    "                # Decide how much to keep (50-90% of tokens)\n",
    "                keep_percent = random.uniform(0.5, 0.9)\n",
    "                keep_tokens = int(seq_length * keep_percent)\n",
    "\n",
    "                # Create input/target pairs\n",
    "                input_sample = input_ids[:keep_tokens]\n",
    "                target_sample = input_ids[keep_tokens:]\n",
    "\n",
    "                training_samples.append(\n",
    "                    {\n",
    "                        \"input_ids\": input_sample,\n",
    "                        \"labels\": target_sample,\n",
    "                        \"original_code\": code,\n",
    "                        \"function_name\": example[\"function_name\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    print(\n",
    "        f\"Created {len(training_samples)} training samples from {len(examples)} examples\"\n",
    "    )\n",
    "    return training_samples\n",
    "\n",
    "\n",
    "# Prepare the training data\n",
    "training_samples = prepare_training_examples(training_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53ce1718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 655360\n",
      "Total parameters: 357367808\n",
      "Percentage of trainable parameters: 0.18%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "\n",
    "# 1. Load a pre-trained code model (small enough for Colab)\n",
    "model_name = \"Salesforce/codegen-350M-mono\"  # A relatively small code model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 2. Configure LoRA adapter\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,  # Rank of update matrices\n",
    "    lora_alpha=32,  # Parameter scaling factor\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# 3. Create PEFT model\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\n",
    "    f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\"\n",
    ")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(\n",
    "    f\"Percentage of trainable parameters: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()):.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a26a871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def read_json(filename, print=False):\n",
      "    \"\"\"\n",
      "    Reads a JSON file and returns a list of dictionaries.\n",
      "    \"\"\"\n",
      "    with open(filename, 'r') as f:\n",
      "        return json.load(f)\n",
      "\n",
      "def write_json(filename, data):\n",
      "    \"\"\"\n",
      "    Writes a JSON file.\n",
      "    \"\"\"\n",
      "    with open(filename, 'w') as f:\n",
      "        json.dump(data, f, indent=4)\n",
      "\n",
      "def read_csv(filename, print=False):\n",
      "    \"\"\"\n",
      "    Reads a CSV\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "\n",
    "text = \"def read_json(filename, print=False):\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generated_ids = model.generate(input_ids, max_length=128)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01479b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m CodeCompletionDataset(val_samples, tokenizer)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Set up training arguments\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./code-completion-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# To simulate larger batch size\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enable mixed-precision training\u001b[39;49;00m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Create trainer\u001b[39;00m\n\u001b[1;32m     79\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     80\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     81\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     82\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     83\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[1;32m     84\u001b[0m )\n",
      "File \u001b[0;32m<string>:124\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.10/site-packages/transformers/training_args.py:1557\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1546\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1556\u001b[0m ):\n\u001b[0;32m-> 1557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1560\u001b[0m     )\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1572\u001b[0m ):\n\u001b[1;32m   1573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1576\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CodeCompletionDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, max_length=512):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        # Get input and target\n",
    "        input_ids = sample[\"input_ids\"]\n",
    "        labels = sample[\"labels\"]\n",
    "\n",
    "        # Combine input with a special token and the beginning of labels for training\n",
    "        # We typically include the first token of labels in the input to help the model start\n",
    "        combined_ids = input_ids + labels\n",
    "\n",
    "        # Handle truncation if needed\n",
    "        if len(combined_ids) > self.max_length:\n",
    "            combined_ids = combined_ids[: self.max_length]\n",
    "\n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(combined_ids)\n",
    "\n",
    "        # Pad sequences if needed\n",
    "        padding_length = self.max_length - len(combined_ids)\n",
    "        if padding_length > 0:\n",
    "            combined_ids = combined_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length\n",
    "\n",
    "        # Set up labels (set to -100 for input portion to ignore in loss)\n",
    "        labels = [-100] * len(input_ids) + combined_ids[len(input_ids) :]\n",
    "\n",
    "        # Ensure all sequences have the right length\n",
    "        if len(labels) > self.max_length:\n",
    "            labels = labels[: self.max_length]\n",
    "        elif len(labels) < self.max_length:\n",
    "            labels = labels + [-100] * (self.max_length - len(labels))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(combined_ids),\n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "            \"labels\": torch.tensor(labels),\n",
    "        }\n",
    "\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_samples = training_samples[: int(0.9 * len(training_samples))]\n",
    "val_samples = training_samples[int(0.9 * len(training_samples)) :]\n",
    "\n",
    "train_dataset = CodeCompletionDataset(train_samples, tokenizer)\n",
    "val_dataset = CodeCompletionDataset(val_samples, tokenizer)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./code-completion-model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # To simulate larger batch size\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # Enable mixed-precision training\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned adapter\n",
    "model.save_pretrained(\"./code-completion-adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cdb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(model, tokenizer, function_prefix, max_new_tokens=100):\n",
    "    inputs = tokenizer(function_prefix, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    completed_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return only the newly generated part\n",
    "    return completed_code[\n",
    "        len(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)) :\n",
    "    ]\n",
    "\n",
    "\n",
    "# Test with some examples\n",
    "test_prefixes = [\n",
    "    \"def train_model(X_train, y_train):\\n    # Create TensorFlow model\\n    model = tf.keras\",\n",
    "    \"def process_image(image_path):\\n    # Load and preprocess image\\n    import numpy as np\\n    img = \",\n",
    "    \"def create_bert_classifier():\\n    # Initialize a BERT model from HuggingFace\\n    from transformers import \",\n",
    "]\n",
    "\n",
    "for prefix in test_prefixes:\n",
    "    completion = generate_completion(model, tokenizer, prefix)\n",
    "    print(f\"\\nPrefix:\\n{prefix}\")\n",
    "    print(f\"\\nCompletion:\\n{completion}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
