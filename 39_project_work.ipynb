{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d492e463",
      "metadata": {
        "id": "d492e463"
      },
      "source": [
        "## AI-powered Code Autocompletion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0f0ed0fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f0ed0fe",
        "outputId": "2119f6e3-b94e-4788-fc63-47ae9037c968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import (\n",
        "    get_peft_config,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    get_peft_model,\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        ")\n",
        "import random"
      ],
      "metadata": {
        "id": "lx8vbmOAVstB"
      },
      "id": "lx8vbmOAVstB",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = load_dataset(\"ieadoboe/python-function-examples\", split=\"train\")\n",
        "val_ds = load_dataset(\"ieadoboe/python-function-examples\", split=\"validation\")\n",
        "test_ds = load_dataset(\"ieadoboe/python-function-examples\", split=\"test\")"
      ],
      "metadata": {
        "id": "2-__diwcSqMs"
      },
      "id": "2-__diwcSqMs",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI6EbuvNTNK8",
        "outputId": "e39277ca-c43d-4eee-d704-942d634ba295"
      },
      "id": "oI6EbuvNTNK8",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'repo': 'ageitgey/face_recognition',\n",
              " 'path': 'examples/face_recognition_knn.py',\n",
              " 'func_name': 'train',\n",
              " 'original_string': 'def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo=\\'ball_tree\\', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.\\n    \"\"\"\\n    X = []\\n    y = []\\n\\n    # Loop through each person in the training set\\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        # Loop through each training image for the current person\\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                # If there are no people (or too many people) in a training image, skip the image.\\n                if verbose:\\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\\n            else:\\n                # Add face encoding for current image to the training set\\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    # Determine how many neighbors to use for weighting in the KNN classifier\\n    if n_neighbors is None:\\n        n_neighbors = int(round(math.sqrt(len(X))))\\n        if verbose:\\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\\n\\n    # Create and train the KNN classifier\\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(X, y)\\n\\n    # Save the trained KNN classifier\\n    if model_save_path is not None:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf',\n",
              " 'language': 'python',\n",
              " 'code': 'def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo=\\'ball_tree\\', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.\\n    \"\"\"\\n    X = []\\n    y = []\\n\\n    # Loop through each person in the training set\\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        # Loop through each training image for the current person\\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                # If there are no people (or too many people) in a training image, skip the image.\\n                if verbose:\\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\\n            else:\\n                # Add face encoding for current image to the training set\\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    # Determine how many neighbors to use for weighting in the KNN classifier\\n    if n_neighbors is None:\\n        n_neighbors = int(round(math.sqrt(len(X))))\\n        if verbose:\\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\\n\\n    # Create and train the KNN classifier\\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(X, y)\\n\\n    # Save the trained KNN classifier\\n    if model_save_path is not None:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf',\n",
              " 'code_tokens': ['def',\n",
              "  'train',\n",
              "  '(',\n",
              "  'train_dir',\n",
              "  ',',\n",
              "  'model_save_path',\n",
              "  '=',\n",
              "  'None',\n",
              "  ',',\n",
              "  'n_neighbors',\n",
              "  '=',\n",
              "  'None',\n",
              "  ',',\n",
              "  'knn_algo',\n",
              "  '=',\n",
              "  \"'ball_tree'\",\n",
              "  ',',\n",
              "  'verbose',\n",
              "  '=',\n",
              "  'False',\n",
              "  ')',\n",
              "  ':',\n",
              "  'X',\n",
              "  '=',\n",
              "  '[',\n",
              "  ']',\n",
              "  'y',\n",
              "  '=',\n",
              "  '[',\n",
              "  ']',\n",
              "  '# Loop through each person in the training set',\n",
              "  'for',\n",
              "  'class_dir',\n",
              "  'in',\n",
              "  'os',\n",
              "  '.',\n",
              "  'listdir',\n",
              "  '(',\n",
              "  'train_dir',\n",
              "  ')',\n",
              "  ':',\n",
              "  'if',\n",
              "  'not',\n",
              "  'os',\n",
              "  '.',\n",
              "  'path',\n",
              "  '.',\n",
              "  'isdir',\n",
              "  '(',\n",
              "  'os',\n",
              "  '.',\n",
              "  'path',\n",
              "  '.',\n",
              "  'join',\n",
              "  '(',\n",
              "  'train_dir',\n",
              "  ',',\n",
              "  'class_dir',\n",
              "  ')',\n",
              "  ')',\n",
              "  ':',\n",
              "  'continue',\n",
              "  '# Loop through each training image for the current person',\n",
              "  'for',\n",
              "  'img_path',\n",
              "  'in',\n",
              "  'image_files_in_folder',\n",
              "  '(',\n",
              "  'os',\n",
              "  '.',\n",
              "  'path',\n",
              "  '.',\n",
              "  'join',\n",
              "  '(',\n",
              "  'train_dir',\n",
              "  ',',\n",
              "  'class_dir',\n",
              "  ')',\n",
              "  ')',\n",
              "  ':',\n",
              "  'image',\n",
              "  '=',\n",
              "  'face_recognition',\n",
              "  '.',\n",
              "  'load_image_file',\n",
              "  '(',\n",
              "  'img_path',\n",
              "  ')',\n",
              "  'face_bounding_boxes',\n",
              "  '=',\n",
              "  'face_recognition',\n",
              "  '.',\n",
              "  'face_locations',\n",
              "  '(',\n",
              "  'image',\n",
              "  ')',\n",
              "  'if',\n",
              "  'len',\n",
              "  '(',\n",
              "  'face_bounding_boxes',\n",
              "  ')',\n",
              "  '!=',\n",
              "  '1',\n",
              "  ':',\n",
              "  '# If there are no people (or too many people) in a training image, skip the image.',\n",
              "  'if',\n",
              "  'verbose',\n",
              "  ':',\n",
              "  'print',\n",
              "  '(',\n",
              "  '\"Image {} not suitable for training: {}\"',\n",
              "  '.',\n",
              "  'format',\n",
              "  '(',\n",
              "  'img_path',\n",
              "  ',',\n",
              "  '\"Didn\\'t find a face\"',\n",
              "  'if',\n",
              "  'len',\n",
              "  '(',\n",
              "  'face_bounding_boxes',\n",
              "  ')',\n",
              "  '<',\n",
              "  '1',\n",
              "  'else',\n",
              "  '\"Found more than one face\"',\n",
              "  ')',\n",
              "  ')',\n",
              "  'else',\n",
              "  ':',\n",
              "  '# Add face encoding for current image to the training set',\n",
              "  'X',\n",
              "  '.',\n",
              "  'append',\n",
              "  '(',\n",
              "  'face_recognition',\n",
              "  '.',\n",
              "  'face_encodings',\n",
              "  '(',\n",
              "  'image',\n",
              "  ',',\n",
              "  'known_face_locations',\n",
              "  '=',\n",
              "  'face_bounding_boxes',\n",
              "  ')',\n",
              "  '[',\n",
              "  '0',\n",
              "  ']',\n",
              "  ')',\n",
              "  'y',\n",
              "  '.',\n",
              "  'append',\n",
              "  '(',\n",
              "  'class_dir',\n",
              "  ')',\n",
              "  '# Determine how many neighbors to use for weighting in the KNN classifier',\n",
              "  'if',\n",
              "  'n_neighbors',\n",
              "  'is',\n",
              "  'None',\n",
              "  ':',\n",
              "  'n_neighbors',\n",
              "  '=',\n",
              "  'int',\n",
              "  '(',\n",
              "  'round',\n",
              "  '(',\n",
              "  'math',\n",
              "  '.',\n",
              "  'sqrt',\n",
              "  '(',\n",
              "  'len',\n",
              "  '(',\n",
              "  'X',\n",
              "  ')',\n",
              "  ')',\n",
              "  ')',\n",
              "  ')',\n",
              "  'if',\n",
              "  'verbose',\n",
              "  ':',\n",
              "  'print',\n",
              "  '(',\n",
              "  '\"Chose n_neighbors automatically:\"',\n",
              "  ',',\n",
              "  'n_neighbors',\n",
              "  ')',\n",
              "  '# Create and train the KNN classifier',\n",
              "  'knn_clf',\n",
              "  '=',\n",
              "  'neighbors',\n",
              "  '.',\n",
              "  'KNeighborsClassifier',\n",
              "  '(',\n",
              "  'n_neighbors',\n",
              "  '=',\n",
              "  'n_neighbors',\n",
              "  ',',\n",
              "  'algorithm',\n",
              "  '=',\n",
              "  'knn_algo',\n",
              "  ',',\n",
              "  'weights',\n",
              "  '=',\n",
              "  \"'distance'\",\n",
              "  ')',\n",
              "  'knn_clf',\n",
              "  '.',\n",
              "  'fit',\n",
              "  '(',\n",
              "  'X',\n",
              "  ',',\n",
              "  'y',\n",
              "  ')',\n",
              "  '# Save the trained KNN classifier',\n",
              "  'if',\n",
              "  'model_save_path',\n",
              "  'is',\n",
              "  'not',\n",
              "  'None',\n",
              "  ':',\n",
              "  'with',\n",
              "  'open',\n",
              "  '(',\n",
              "  'model_save_path',\n",
              "  ',',\n",
              "  \"'wb'\",\n",
              "  ')',\n",
              "  'as',\n",
              "  'f',\n",
              "  ':',\n",
              "  'pickle',\n",
              "  '.',\n",
              "  'dump',\n",
              "  '(',\n",
              "  'knn_clf',\n",
              "  ',',\n",
              "  'f',\n",
              "  ')',\n",
              "  'return',\n",
              "  'knn_clf'],\n",
              " 'docstring': 'Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.',\n",
              " 'docstring_tokens': ['Trains',\n",
              "  'a',\n",
              "  'k',\n",
              "  '-',\n",
              "  'nearest',\n",
              "  'neighbors',\n",
              "  'classifier',\n",
              "  'for',\n",
              "  'face',\n",
              "  'recognition',\n",
              "  '.'],\n",
              " 'sha': 'c96b010c02f15e8eeb0f71308c641179ac1f19bb',\n",
              " 'url': 'https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L46-L108',\n",
              " 'partition': 'train'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6eb2d032",
      "metadata": {
        "id": "6eb2d032"
      },
      "outputs": [],
      "source": [
        "def print_code_examples(dataset, num_examples=5):\n",
        "    count = 0\n",
        "    for example in dataset:\n",
        "        print(f\"\\n--- Example {count+1} ---\")\n",
        "        print(f\"Function name: {example.get('func_name', 'N/A')}\")\n",
        "        print(f\"Docstring: {example.get('docstring', 'N/A')[:100]}...\")  # Print first 100 chars\n",
        "        print(f\"Code:\\n{example.get('code', 'N/A')}\")\n",
        "        count += 1\n",
        "        if count >= num_examples:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ba5f3a3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba5f3a3b",
        "outputId": "1a76fb1a-3cfd-47a2-c562-57f9b686ae0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 1 ---\n",
            "Function name: train\n",
            "Docstring: Trains a k-nearest neighbors classifier for face recognition.\n",
            "\n",
            "    :param train_dir: directory that ...\n",
            "Code:\n",
            "def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n",
            "    \"\"\"\n",
            "    Trains a k-nearest neighbors classifier for face recognition.\n",
            "\n",
            "    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n",
            "\n",
            "     (View in source code to see train_dir example tree structure)\n",
            "\n",
            "     Structure:\n",
            "        <train_dir>/\n",
            "        ├── <person1>/\n",
            "        │   ├── <somename1>.jpeg\n",
            "        │   ├── <somename2>.jpeg\n",
            "        │   ├── ...\n",
            "        ├── <person2>/\n",
            "        │   ├── <somename1>.jpeg\n",
            "        │   └── <somename2>.jpeg\n",
            "        └── ...\n",
            "\n",
            "    :param model_save_path: (optional) path to save model on disk\n",
            "    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n",
            "    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n",
            "    :param verbose: verbosity of training\n",
            "    :return: returns knn classifier that was trained on the given data.\n",
            "    \"\"\"\n",
            "    X = []\n",
            "    y = []\n",
            "\n",
            "    # Loop through each person in the training set\n",
            "    for class_dir in os.listdir(train_dir):\n",
            "        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n",
            "            continue\n",
            "\n",
            "        # Loop through each training image for the current person\n",
            "        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n",
            "            image = face_recognition.load_image_file(img_path)\n",
            "            face_bounding_boxes = face_recognition.face_locations(image)\n",
            "\n",
            "            if len(face_bounding_boxes) != 1:\n",
            "                # If there are no people (or too many people) in a training image, skip the image.\n",
            "                if verbose:\n",
            "                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n",
            "            else:\n",
            "                # Add face encoding for current image to the training set\n",
            "                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n",
            "                y.append(class_dir)\n",
            "\n",
            "    # Determine how many neighbors to use for weighting in the KNN classifier\n",
            "    if n_neighbors is None:\n",
            "        n_neighbors = int(round(math.sqrt(len(X))))\n",
            "        if verbose:\n",
            "            print(\"Chose n_neighbors automatically:\", n_neighbors)\n",
            "\n",
            "    # Create and train the KNN classifier\n",
            "    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n",
            "    knn_clf.fit(X, y)\n",
            "\n",
            "    # Save the trained KNN classifier\n",
            "    if model_save_path is not None:\n",
            "        with open(model_save_path, 'wb') as f:\n",
            "            pickle.dump(knn_clf, f)\n",
            "\n",
            "    return knn_clf\n",
            "\n",
            "--- Example 2 ---\n",
            "Function name: predict\n",
            "Docstring: Recognizes faces in given image using a trained KNN classifier\n",
            "\n",
            "    :param X_img_path: path to image...\n",
            "Code:\n",
            "def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\n",
            "    \"\"\"\n",
            "    Recognizes faces in given image using a trained KNN classifier\n",
            "\n",
            "    :param X_img_path: path to image to be recognized\n",
            "    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n",
            "    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n",
            "    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n",
            "           of mis-classifying an unknown person as a known one.\n",
            "    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n",
            "        For faces of unrecognized persons, the name 'unknown' will be returned.\n",
            "    \"\"\"\n",
            "    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\n",
            "        raise Exception(\"Invalid image path: {}\".format(X_img_path))\n",
            "\n",
            "    if knn_clf is None and model_path is None:\n",
            "        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\n",
            "\n",
            "    # Load a trained KNN model (if one was passed in)\n",
            "    if knn_clf is None:\n",
            "        with open(model_path, 'rb') as f:\n",
            "            knn_clf = pickle.load(f)\n",
            "\n",
            "    # Load image file and find face locations\n",
            "    X_img = face_recognition.load_image_file(X_img_path)\n",
            "    X_face_locations = face_recognition.face_locations(X_img)\n",
            "\n",
            "    # If no faces are found in the image, return an empty result.\n",
            "    if len(X_face_locations) == 0:\n",
            "        return []\n",
            "\n",
            "    # Find encodings for faces in the test iamge\n",
            "    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\n",
            "\n",
            "    # Use the KNN model to find the best matches for the test face\n",
            "    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\n",
            "    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\n",
            "\n",
            "    # Predict classes and remove classifications that aren't within the threshold\n",
            "    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]\n",
            "\n",
            "--- Example 3 ---\n",
            "Function name: show_prediction_labels_on_image\n",
            "Docstring: Shows the face recognition results visually.\n",
            "\n",
            "    :param img_path: path to image to be recognized\n",
            "  ...\n",
            "Code:\n",
            "def show_prediction_labels_on_image(img_path, predictions):\n",
            "    \"\"\"\n",
            "    Shows the face recognition results visually.\n",
            "\n",
            "    :param img_path: path to image to be recognized\n",
            "    :param predictions: results of the predict function\n",
            "    :return:\n",
            "    \"\"\"\n",
            "    pil_image = Image.open(img_path).convert(\"RGB\")\n",
            "    draw = ImageDraw.Draw(pil_image)\n",
            "\n",
            "    for name, (top, right, bottom, left) in predictions:\n",
            "        # Draw a box around the face using the Pillow module\n",
            "        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n",
            "\n",
            "        # There's a bug in Pillow where it blows up with non-UTF-8 text\n",
            "        # when using the default bitmap font\n",
            "        name = name.encode(\"UTF-8\")\n",
            "\n",
            "        # Draw a label with a name below the face\n",
            "        text_width, text_height = draw.textsize(name)\n",
            "        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n",
            "        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n",
            "\n",
            "    # Remove the drawing library from memory as per the Pillow docs\n",
            "    del draw\n",
            "\n",
            "    # Display the resulting image\n",
            "    pil_image.show()\n"
          ]
        }
      ],
      "source": [
        "# Print training set examples\n",
        "print_code_examples(train_ds, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "218f63f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "218f63f0",
        "outputId": "f9faa2a0-ac99-464b-8835-94e57c6a0e16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 1 ---\n",
            "Function name: learn\n",
            "Docstring: Train a deepq model.\n",
            "\n",
            "    Parameters\n",
            "    -------\n",
            "    env: gym.Env\n",
            "        environment to train on\n",
            "  ...\n",
            "Code:\n",
            "def learn(env,\n",
            "          network,\n",
            "          seed=None,\n",
            "          lr=5e-4,\n",
            "          total_timesteps=100000,\n",
            "          buffer_size=50000,\n",
            "          exploration_fraction=0.1,\n",
            "          exploration_final_eps=0.02,\n",
            "          train_freq=1,\n",
            "          batch_size=32,\n",
            "          print_freq=100,\n",
            "          checkpoint_freq=10000,\n",
            "          checkpoint_path=None,\n",
            "          learning_starts=1000,\n",
            "          gamma=1.0,\n",
            "          target_network_update_freq=500,\n",
            "          prioritized_replay=False,\n",
            "          prioritized_replay_alpha=0.6,\n",
            "          prioritized_replay_beta0=0.4,\n",
            "          prioritized_replay_beta_iters=None,\n",
            "          prioritized_replay_eps=1e-6,\n",
            "          param_noise=False,\n",
            "          callback=None,\n",
            "          load_path=None,\n",
            "          **network_kwargs\n",
            "            ):\n",
            "    \"\"\"Train a deepq model.\n",
            "\n",
            "    Parameters\n",
            "    -------\n",
            "    env: gym.Env\n",
            "        environment to train on\n",
            "    network: string or a function\n",
            "        neural network to use as a q function approximator. If string, has to be one of the names of registered models in baselines.common.models\n",
            "        (mlp, cnn, conv_only). If a function, should take an observation tensor and return a latent variable tensor, which\n",
            "        will be mapped to the Q function heads (see build_q_func in baselines.deepq.models for details on that)\n",
            "    seed: int or None\n",
            "        prng seed. The runs with the same seed \"should\" give the same results. If None, no seeding is used.\n",
            "    lr: float\n",
            "        learning rate for adam optimizer\n",
            "    total_timesteps: int\n",
            "        number of env steps to optimizer for\n",
            "    buffer_size: int\n",
            "        size of the replay buffer\n",
            "    exploration_fraction: float\n",
            "        fraction of entire training period over which the exploration rate is annealed\n",
            "    exploration_final_eps: float\n",
            "        final value of random action probability\n",
            "    train_freq: int\n",
            "        update the model every `train_freq` steps.\n",
            "        set to None to disable printing\n",
            "    batch_size: int\n",
            "        size of a batched sampled from replay buffer for training\n",
            "    print_freq: int\n",
            "        how often to print out training progress\n",
            "        set to None to disable printing\n",
            "    checkpoint_freq: int\n",
            "        how often to save the model. This is so that the best version is restored\n",
            "        at the end of the training. If you do not wish to restore the best version at\n",
            "        the end of the training set this variable to None.\n",
            "    learning_starts: int\n",
            "        how many steps of the model to collect transitions for before learning starts\n",
            "    gamma: float\n",
            "        discount factor\n",
            "    target_network_update_freq: int\n",
            "        update the target network every `target_network_update_freq` steps.\n",
            "    prioritized_replay: True\n",
            "        if True prioritized replay buffer will be used.\n",
            "    prioritized_replay_alpha: float\n",
            "        alpha parameter for prioritized replay buffer\n",
            "    prioritized_replay_beta0: float\n",
            "        initial value of beta for prioritized replay buffer\n",
            "    prioritized_replay_beta_iters: int\n",
            "        number of iterations over which beta will be annealed from initial value\n",
            "        to 1.0. If set to None equals to total_timesteps.\n",
            "    prioritized_replay_eps: float\n",
            "        epsilon to add to the TD errors when updating priorities.\n",
            "    param_noise: bool\n",
            "        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n",
            "    callback: (locals, globals) -> None\n",
            "        function called at every steps with state of the algorithm.\n",
            "        If callback returns true training stops.\n",
            "    load_path: str\n",
            "        path to load the model from. (default: None)\n",
            "    **network_kwargs\n",
            "        additional keyword arguments to pass to the network builder.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    act: ActWrapper\n",
            "        Wrapper over act function. Adds ability to save it and load it.\n",
            "        See header of baselines/deepq/categorical.py for details on the act function.\n",
            "    \"\"\"\n",
            "    # Create all the functions necessary to train the model\n",
            "\n",
            "    sess = get_session()\n",
            "    set_global_seeds(seed)\n",
            "\n",
            "    q_func = build_q_func(network, **network_kwargs)\n",
            "\n",
            "    # capture the shape outside the closure so that the env object is not serialized\n",
            "    # by cloudpickle when serializing make_obs_ph\n",
            "\n",
            "    observation_space = env.observation_space\n",
            "    def make_obs_ph(name):\n",
            "        return ObservationInput(observation_space, name=name)\n",
            "\n",
            "    act, train, update_target, debug = deepq.build_train(\n",
            "        make_obs_ph=make_obs_ph,\n",
            "        q_func=q_func,\n",
            "        num_actions=env.action_space.n,\n",
            "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
            "        gamma=gamma,\n",
            "        grad_norm_clipping=10,\n",
            "        param_noise=param_noise\n",
            "    )\n",
            "\n",
            "    act_params = {\n",
            "        'make_obs_ph': make_obs_ph,\n",
            "        'q_func': q_func,\n",
            "        'num_actions': env.action_space.n,\n",
            "    }\n",
            "\n",
            "    act = ActWrapper(act, act_params)\n",
            "\n",
            "    # Create the replay buffer\n",
            "    if prioritized_replay:\n",
            "        replay_buffer = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
            "        if prioritized_replay_beta_iters is None:\n",
            "            prioritized_replay_beta_iters = total_timesteps\n",
            "        beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
            "                                       initial_p=prioritized_replay_beta0,\n",
            "                                       final_p=1.0)\n",
            "    else:\n",
            "        replay_buffer = ReplayBuffer(buffer_size)\n",
            "        beta_schedule = None\n",
            "    # Create the schedule for exploration starting from 1.\n",
            "    exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\n",
            "                                 initial_p=1.0,\n",
            "                                 final_p=exploration_final_eps)\n",
            "\n",
            "    # Initialize the parameters and copy them to the target network.\n",
            "    U.initialize()\n",
            "    update_target()\n",
            "\n",
            "    episode_rewards = [0.0]\n",
            "    saved_mean_reward = None\n",
            "    obs = env.reset()\n",
            "    reset = True\n",
            "\n",
            "    with tempfile.TemporaryDirectory() as td:\n",
            "        td = checkpoint_path or td\n",
            "\n",
            "        model_file = os.path.join(td, \"model\")\n",
            "        model_saved = False\n",
            "\n",
            "        if tf.train.latest_checkpoint(td) is not None:\n",
            "            load_variables(model_file)\n",
            "            logger.log('Loaded model from {}'.format(model_file))\n",
            "            model_saved = True\n",
            "        elif load_path is not None:\n",
            "            load_variables(load_path)\n",
            "            logger.log('Loaded model from {}'.format(load_path))\n",
            "\n",
            "\n",
            "        for t in range(total_timesteps):\n",
            "            if callback is not None:\n",
            "                if callback(locals(), globals()):\n",
            "                    break\n",
            "            # Take action and update exploration to the newest value\n",
            "            kwargs = {}\n",
            "            if not param_noise:\n",
            "                update_eps = exploration.value(t)\n",
            "                update_param_noise_threshold = 0.\n",
            "            else:\n",
            "                update_eps = 0.\n",
            "                # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n",
            "                # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n",
            "                # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n",
            "                # for detailed explanation.\n",
            "                update_param_noise_threshold = -np.log(1. - exploration.value(t) + exploration.value(t) / float(env.action_space.n))\n",
            "                kwargs['reset'] = reset\n",
            "                kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n",
            "                kwargs['update_param_noise_scale'] = True\n",
            "            action = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
            "            env_action = action\n",
            "            reset = False\n",
            "            new_obs, rew, done, _ = env.step(env_action)\n",
            "            # Store transition in the replay buffer.\n",
            "            replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
            "            obs = new_obs\n",
            "\n",
            "            episode_rewards[-1] += rew\n",
            "            if done:\n",
            "                obs = env.reset()\n",
            "                episode_rewards.append(0.0)\n",
            "                reset = True\n",
            "\n",
            "            if t > learning_starts and t % train_freq == 0:\n",
            "                # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
            "                if prioritized_replay:\n",
            "                    experience = replay_buffer.sample(batch_size, beta=beta_schedule.value(t))\n",
            "                    (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
            "                else:\n",
            "                    obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
            "                    weights, batch_idxes = np.ones_like(rewards), None\n",
            "                td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
            "                if prioritized_replay:\n",
            "                    new_priorities = np.abs(td_errors) + prioritized_replay_eps\n",
            "                    replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
            "\n",
            "            if t > learning_starts and t % target_network_update_freq == 0:\n",
            "                # Update target network periodically.\n",
            "                update_target()\n",
            "\n",
            "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
            "            num_episodes = len(episode_rewards)\n",
            "            if done and print_freq is not None and len(episode_rewards) % print_freq == 0:\n",
            "                logger.record_tabular(\"steps\", t)\n",
            "                logger.record_tabular(\"episodes\", num_episodes)\n",
            "                logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n",
            "                logger.record_tabular(\"% time spent exploring\", int(100 * exploration.value(t)))\n",
            "                logger.dump_tabular()\n",
            "\n",
            "            if (checkpoint_freq is not None and t > learning_starts and\n",
            "                    num_episodes > 100 and t % checkpoint_freq == 0):\n",
            "                if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward:\n",
            "                    if print_freq is not None:\n",
            "                        logger.log(\"Saving model due to mean reward increase: {} -> {}\".format(\n",
            "                                   saved_mean_reward, mean_100ep_reward))\n",
            "                    save_variables(model_file)\n",
            "                    model_saved = True\n",
            "                    saved_mean_reward = mean_100ep_reward\n",
            "        if model_saved:\n",
            "            if print_freq is not None:\n",
            "                logger.log(\"Restored model with mean reward: {}\".format(saved_mean_reward))\n",
            "            load_variables(model_file)\n",
            "\n",
            "    return act\n",
            "\n",
            "--- Example 2 ---\n",
            "Function name: ActWrapper.save_act\n",
            "Docstring: Save model to a pickle located at `path`...\n",
            "Code:\n",
            "def save_act(self, path=None):\n",
            "        \"\"\"Save model to a pickle located at `path`\"\"\"\n",
            "        if path is None:\n",
            "            path = os.path.join(logger.get_dir(), \"model.pkl\")\n",
            "\n",
            "        with tempfile.TemporaryDirectory() as td:\n",
            "            save_variables(os.path.join(td, \"model\"))\n",
            "            arc_name = os.path.join(td, \"packed.zip\")\n",
            "            with zipfile.ZipFile(arc_name, 'w') as zipf:\n",
            "                for root, dirs, files in os.walk(td):\n",
            "                    for fname in files:\n",
            "                        file_path = os.path.join(root, fname)\n",
            "                        if file_path != arc_name:\n",
            "                            zipf.write(file_path, os.path.relpath(file_path, td))\n",
            "            with open(arc_name, \"rb\") as f:\n",
            "                model_data = f.read()\n",
            "        with open(path, \"wb\") as f:\n",
            "            cloudpickle.dump((model_data, self._act_params), f)\n",
            "\n",
            "--- Example 3 ---\n",
            "Function name: nature_cnn\n",
            "Docstring: CNN from Nature paper....\n",
            "Code:\n",
            "def nature_cnn(unscaled_images, **conv_kwargs):\n",
            "    \"\"\"\n",
            "    CNN from Nature paper.\n",
            "    \"\"\"\n",
            "    scaled_images = tf.cast(unscaled_images, tf.float32) / 255.\n",
            "    activ = tf.nn.relu\n",
            "    h = activ(conv(scaled_images, 'c1', nf=32, rf=8, stride=4, init_scale=np.sqrt(2),\n",
            "                   **conv_kwargs))\n",
            "    h2 = activ(conv(h, 'c2', nf=64, rf=4, stride=2, init_scale=np.sqrt(2), **conv_kwargs))\n",
            "    h3 = activ(conv(h2, 'c3', nf=64, rf=3, stride=1, init_scale=np.sqrt(2), **conv_kwargs))\n",
            "    h3 = conv_to_fc(h3)\n",
            "    return activ(fc(h3, 'fc1', nh=512, init_scale=np.sqrt(2)))\n"
          ]
        }
      ],
      "source": [
        "print_code_examples(val_ds, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "8161aef8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8161aef8",
        "outputId": "89744f08-da40-46f0-c250-ad0c6b5e3840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 1000 examples\n"
          ]
        }
      ],
      "source": [
        "def collect_training_data_from_dataset(\n",
        "    dataset, max_examples=1000\n",
        "):\n",
        "    examples = []\n",
        "    for example in dataset:\n",
        "        code = example.get(\"code\", \"\")\n",
        "        examples.append(\n",
        "            {\n",
        "                \"function_name\": example.get(\"func_name\", \"\"),\n",
        "                \"docstring\": example.get(\"docstring\", \"\"),\n",
        "                \"code\": code,\n",
        "                \"language\": example.get(\"language\", \"python\"),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if len(examples) >= max_examples:\n",
        "            break\n",
        "\n",
        "    print(f\"Collected {len(examples)} examples\")\n",
        "    return examples\n",
        "\n",
        "\n",
        "# Collect training data from the dataset\n",
        "training_data = collect_training_data_from_dataset(train_ds, max_examples=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "635c3508",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "635c3508",
        "outputId": "48230228-b3b2-4209-ca13-346d2173b246"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'function_name': 'face_distance',\n",
              " 'docstring': \"Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\\n    for each comparison face. The distance tells you how similar the faces are.\\n\\n    :param faces: List of face encodings to compare\\n    :param face_to_compare: A face encoding to compare against\\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\",\n",
              " 'code': 'def face_distance(face_encodings, face_to_compare):\\n    \"\"\"\\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\\n    for each comparison face. The distance tells you how similar the faces are.\\n\\n    :param faces: List of face encodings to compare\\n    :param face_to_compare: A face encoding to compare against\\n    :return: A numpy ndarray with the distance for each face in the same order as the \\'faces\\' array\\n    \"\"\"\\n    if len(face_encodings) == 0:\\n        return np.empty((0))\\n\\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)',\n",
              " 'language': 'python'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "training_data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "86cf9871",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86cf9871",
        "outputId": "c8725259-0421-4189-c3f0-b5e7a47bb47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 3000 training samples from 1000 examples\n"
          ]
        }
      ],
      "source": [
        "# Load a code-optimized tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
        "\n",
        "\n",
        "def prepare_training_examples(examples, tokenizer, max_length=512):\n",
        "    \"\"\"Convert raw code examples into training samples for code completion.\"\"\"\n",
        "    training_samples = []\n",
        "\n",
        "    for example in examples:\n",
        "        code = example[\"code\"]\n",
        "        # Skip empty or very short functions\n",
        "        if len(code.strip()) < 20:\n",
        "            continue\n",
        "\n",
        "        # Tokenize the code\n",
        "        tokenized = tokenizer(code, truncation=True, max_length=max_length)\n",
        "        input_ids = tokenized[\"input_ids\"]\n",
        "\n",
        "        # Create training examples with different completion points\n",
        "        # Randomly mask 10-50% of the tokens at the end\n",
        "        seq_length = len(input_ids)\n",
        "        if seq_length > 20:\n",
        "            for _ in range(3):  # Create a few different masking positions for each example\n",
        "                # Decide how much to keep (50-90% of tokens)\n",
        "                keep_percent = random.uniform(0.5, 0.9)\n",
        "                keep_tokens = int(seq_length * keep_percent)\n",
        "\n",
        "                # Create input/target pairs\n",
        "                input_sample = input_ids[:keep_tokens]\n",
        "                target_sample = input_ids[keep_tokens:]\n",
        "\n",
        "                training_samples.append({\n",
        "                    \"input_ids\": input_sample,\n",
        "                    \"labels\": target_sample,\n",
        "                })\n",
        "\n",
        "    print(f\"Created {len(training_samples)} training samples from {len(examples)} examples\")\n",
        "    return training_samples\n",
        "\n",
        "\n",
        "# Prepare the training data\n",
        "training_samples = prepare_training_examples(training_data, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_samples[0]"
      ],
      "metadata": {
        "id": "ixGw8e3reLPj",
        "outputId": "aecd685c-aa1d-45ee-fdcb-f61666fe98e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ixGw8e3reLPj",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [4299,\n",
              "  4512,\n",
              "  7,\n",
              "  27432,\n",
              "  62,\n",
              "  15908,\n",
              "  11,\n",
              "  2746,\n",
              "  62,\n",
              "  21928,\n",
              "  62,\n",
              "  6978,\n",
              "  28,\n",
              "  14202,\n",
              "  11,\n",
              "  299,\n",
              "  62,\n",
              "  710,\n",
              "  394,\n",
              "  32289,\n",
              "  28,\n",
              "  14202,\n",
              "  11,\n",
              "  638,\n",
              "  77,\n",
              "  62,\n",
              "  282,\n",
              "  2188,\n",
              "  11639,\n",
              "  1894,\n",
              "  62,\n",
              "  21048,\n",
              "  3256,\n",
              "  15942,\n",
              "  577,\n",
              "  28,\n",
              "  25101,\n",
              "  2599,\n",
              "  198,\n",
              "  50284,\n",
              "  37811,\n",
              "  198,\n",
              "  50284,\n",
              "  2898,\n",
              "  1299,\n",
              "  257,\n",
              "  479,\n",
              "  12,\n",
              "  710,\n",
              "  12423,\n",
              "  12020,\n",
              "  1398,\n",
              "  7483,\n",
              "  329,\n",
              "  1986,\n",
              "  9465,\n",
              "  13,\n",
              "  628,\n",
              "  50284,\n",
              "  25,\n",
              "  17143,\n",
              "  4512,\n",
              "  62,\n",
              "  15908,\n",
              "  25,\n",
              "  8619,\n",
              "  326,\n",
              "  4909,\n",
              "  257,\n",
              "  850,\n",
              "  12,\n",
              "  34945,\n",
              "  329,\n",
              "  1123,\n",
              "  1900,\n",
              "  1048,\n",
              "  11,\n",
              "  351,\n",
              "  663,\n",
              "  1438,\n",
              "  13,\n",
              "  628,\n",
              "  50283,\n",
              "  7,\n",
              "  7680,\n",
              "  287,\n",
              "  2723,\n",
              "  2438,\n",
              "  284,\n",
              "  766,\n",
              "  4512,\n",
              "  62,\n",
              "  15908,\n",
              "  1672,\n",
              "  5509,\n",
              "  4645,\n",
              "  8,\n",
              "  628,\n",
              "  50283,\n",
              "  1273,\n",
              "  5620,\n",
              "  25,\n",
              "  198,\n",
              "  50280,\n",
              "  27,\n",
              "  27432,\n",
              "  62,\n",
              "  15908,\n",
              "  29,\n",
              "  14,\n",
              "  198,\n",
              "  50280,\n",
              "  6552,\n",
              "  250,\n",
              "  8418,\n",
              "  1279,\n",
              "  6259,\n",
              "  16,\n",
              "  29,\n",
              "  14,\n",
              "  198,\n",
              "  50280,\n",
              "  6552,\n",
              "  224,\n",
              "  50285,\n",
              "  6552,\n",
              "  250,\n",
              "  8418,\n",
              "  1279,\n",
              "  82,\n",
              "  3674,\n",
              "  480,\n",
              "  16,\n",
              "  28401,\n",
              "  73,\n",
              "  22071,\n",
              "  198,\n",
              "  50280,\n",
              "  6552,\n",
              "  224,\n",
              "  50285,\n",
              "  6552,\n",
              "  250,\n",
              "  8418,\n",
              "  1279,\n",
              "  82,\n",
              "  3674,\n",
              "  480,\n",
              "  17,\n",
              "  28401,\n",
              "  73,\n",
              "  22071,\n",
              "  198,\n",
              "  50280,\n",
              "  6552,\n",
              "  224,\n",
              "  50285,\n",
              "  6552,\n",
              "  250,\n",
              "  8418,\n",
              "  2644,\n",
              "  198,\n",
              "  50280,\n",
              "  6552,\n",
              "  250,\n",
              "  8418,\n",
              "  1279,\n",
              "  6259,\n",
              "  17,\n",
              "  29,\n",
              "  14,\n",
              "  198,\n",
              "  50280,\n",
              "  6552,\n",
              "  224,\n",
              "  50285,\n",
              "  6552,\n",
              "  250,\n",
              "  8418,\n",
              "  1279,\n",
              "  82,\n",
              "  3674,\n",
              "  480,\n",
              "  16,\n",
              "  28401,\n",
              "  73,\n",
              "  22071,\n",
              "  198,\n",
              "  50280,\n",
              "  6552,\n",
              "  224,\n",
              "  50285,\n",
              "  6552,\n",
              "  242,\n",
              "  8418,\n",
              "  1279,\n",
              "  82,\n",
              "  3674,\n",
              "  480,\n",
              "  17,\n",
              "  28401,\n",
              "  73,\n",
              "  22071,\n",
              "  198,\n",
              "  50280,\n",
              "  6552,\n",
              "  242,\n",
              "  8418,\n",
              "  2644,\n",
              "  628,\n",
              "  50284,\n",
              "  25,\n",
              "  17143,\n",
              "  2746,\n",
              "  62,\n",
              "  21928,\n",
              "  62,\n",
              "  6978,\n",
              "  25,\n",
              "  357,\n",
              "  25968,\n",
              "  8,\n",
              "  3108,\n",
              "  284,\n",
              "  3613,\n",
              "  2746,\n",
              "  319,\n",
              "  11898,\n",
              "  198,\n",
              "  50284,\n",
              "  25,\n",
              "  17143,\n",
              "  299,\n",
              "  62,\n",
              "  710,\n",
              "  394,\n",
              "  32289,\n",
              "  25,\n",
              "  357,\n",
              "  25968,\n",
              "  8,\n",
              "  1271,\n",
              "  286,\n",
              "  12020,\n",
              "  284,\n",
              "  10164,\n",
              "  287,\n",
              "  17923,\n",
              "  13,\n",
              "  39884,\n",
              "  6338,\n",
              "  611,\n",
              "  407,\n",
              "  7368,\n",
              "  198,\n",
              "  50284,\n",
              "  25,\n",
              "  17143,\n",
              "  638,\n",
              "  77,\n",
              "  62,\n",
              "  282,\n",
              "  2188,\n",
              "  25,\n",
              "  357,\n",
              "  25968,\n",
              "  8,\n",
              "  10238,\n",
              "  1366,\n",
              "  4645,\n",
              "  284,\n",
              "  1104,\n",
              "  638,\n",
              "  77,\n",
              "  13,\n",
              "  12286,\n",
              "  318,\n",
              "  2613,\n",
              "  62,\n",
              "  21048,\n",
              "  198,\n",
              "  50284,\n",
              "  25,\n",
              "  17143,\n",
              "  15942,\n",
              "  577,\n",
              "  25,\n",
              "  15942,\n",
              "  16579,\n",
              "  286,\n",
              "  3047,\n",
              "  198],\n",
              " 'labels': [50284,\n",
              "  25,\n",
              "  7783,\n",
              "  25,\n",
              "  5860,\n",
              "  638,\n",
              "  77,\n",
              "  1398,\n",
              "  7483,\n",
              "  326,\n",
              "  373,\n",
              "  8776,\n",
              "  319,\n",
              "  262,\n",
              "  1813,\n",
              "  1366,\n",
              "  13,\n",
              "  198,\n",
              "  50284,\n",
              "  37811,\n",
              "  198,\n",
              "  50284,\n",
              "  55,\n",
              "  796,\n",
              "  17635,\n",
              "  198,\n",
              "  50284,\n",
              "  88,\n",
              "  796,\n",
              "  17635,\n",
              "  628,\n",
              "  50284,\n",
              "  2,\n",
              "  26304,\n",
              "  832,\n",
              "  1123,\n",
              "  1048,\n",
              "  287,\n",
              "  262,\n",
              "  3047,\n",
              "  900,\n",
              "  198,\n",
              "  50284,\n",
              "  1640,\n",
              "  1398,\n",
              "  62,\n",
              "  15908,\n",
              "  287,\n",
              "  28686,\n",
              "  13,\n",
              "  4868,\n",
              "  15908,\n",
              "  7,\n",
              "  27432,\n",
              "  62,\n",
              "  15908,\n",
              "  2599,\n",
              "  198,\n",
              "  50280,\n",
              "  361,\n",
              "  407,\n",
              "  28686,\n",
              "  13,\n",
              "  6978,\n",
              "  13,\n",
              "  9409,\n",
              "  343,\n",
              "  7,\n",
              "  418,\n",
              "  13,\n",
              "  6978,\n",
              "  13,\n",
              "  22179,\n",
              "  7,\n",
              "  27432,\n",
              "  62,\n",
              "  15908,\n",
              "  11,\n",
              "  1398,\n",
              "  62,\n",
              "  15908,\n",
              "  8,\n",
              "  2599,\n",
              "  198,\n",
              "  50276,\n",
              "  43043,\n",
              "  628,\n",
              "  50280,\n",
              "  2,\n",
              "  26304,\n",
              "  832,\n",
              "  1123,\n",
              "  3047,\n",
              "  2939,\n",
              "  329,\n",
              "  262,\n",
              "  1459,\n",
              "  1048,\n",
              "  198,\n",
              "  50280,\n",
              "  1640,\n",
              "  33705,\n",
              "  62,\n",
              "  6978,\n",
              "  287,\n",
              "  2939,\n",
              "  62,\n",
              "  16624,\n",
              "  62,\n",
              "  259,\n",
              "  62,\n",
              "  43551,\n",
              "  7,\n",
              "  418,\n",
              "  13,\n",
              "  6978,\n",
              "  13,\n",
              "  22179,\n",
              "  7,\n",
              "  27432,\n",
              "  62,\n",
              "  15908,\n",
              "  11,\n",
              "  1398,\n",
              "  62,\n",
              "  15908,\n",
              "  8,\n",
              "  2599,\n",
              "  198,\n",
              "  50276,\n",
              "  9060,\n",
              "  796,\n",
              "  1986,\n",
              "  62,\n",
              "  26243,\n",
              "  653,\n",
              "  13,\n",
              "  2220,\n",
              "  62,\n",
              "  9060,\n",
              "  62,\n",
              "  7753,\n",
              "  7,\n",
              "  9600,\n",
              "  62,\n",
              "  6978,\n",
              "  8,\n",
              "  198,\n",
              "  50276,\n",
              "  2550,\n",
              "  62,\n",
              "  7784,\n",
              "  278,\n",
              "  62,\n",
              "  29305,\n",
              "  796,\n",
              "  1986,\n",
              "  62,\n",
              "  26243,\n",
              "  653,\n",
              "  13,\n",
              "  2550,\n",
              "  62,\n",
              "  17946,\n",
              "  602,\n",
              "  7,\n",
              "  9060,\n",
              "  8,\n",
              "  628,\n",
              "  50276,\n",
              "  361,\n",
              "  18896,\n",
              "  7,\n",
              "  2550,\n",
              "  62,\n",
              "  7784,\n",
              "  278,\n",
              "  62,\n",
              "  29305,\n",
              "  8,\n",
              "  14512,\n",
              "  352,\n",
              "  25,\n",
              "  198,\n",
              "  50272,\n",
              "  2,\n",
              "  1002,\n",
              "  612,\n",
              "  389,\n",
              "  645,\n",
              "  661,\n",
              "  357,\n",
              "  273,\n",
              "  1165,\n",
              "  867,\n",
              "  661,\n",
              "  8,\n",
              "  287,\n",
              "  257,\n",
              "  3047,\n",
              "  2939,\n",
              "  11,\n",
              "  14267,\n",
              "  262,\n",
              "  2939,\n",
              "  13,\n",
              "  198,\n",
              "  50272,\n",
              "  361,\n",
              "  15942,\n",
              "  577,\n",
              "  25,\n",
              "  198,\n",
              "  50268,\n",
              "  4798,\n",
              "  7203,\n",
              "  5159,\n",
              "  23884,\n",
              "  407,\n",
              "  11080]}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "53ce1718",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235,
          "referenced_widgets": [
            "51f05fed622e4cc3bb2ef98534cafde6",
            "c7c44ff9be374da5a21d26ba304c7a13",
            "ab765e57d317437b8b7b472b7368f1c9",
            "8d42e2d388984117ba602a321be42796",
            "a4b4613c82c64de6a96e6a8183b67b7e",
            "8913a1b667ce4b709383dc09ede565a6",
            "4bb0df3e82a44822b942a3669d73c8d2",
            "46ba684ae0b94a07b0ea2d6b7f45b4a2",
            "7e9ebceeec0d41a3b9ba2e2ad2000f0e",
            "a36582bade7e4df9bfda1197ff529a21",
            "2f183fa6fadb447b939eb1cac04365a0",
            "58889f45b71240b1b7801c9a289ee5bb",
            "a61729ed4fef4ceba317bef057cb2bf8",
            "ec51d731c3064346a346c6440697b44f",
            "55393c2339ac4e419eff33cc8e73c6f2",
            "7037c6d14c2647c7b47e3ba875724229",
            "f829b217a9b443b793d6cf7b46d6193f",
            "3c57bc598184440692f7005a4895125d",
            "d1f6789cd8f14354b69adccaea0fa653",
            "b8fee14aa5ee4627b5b11eac97fd685a",
            "fe89a29eb09c481d86365600dcb4268a",
            "64bea273ac0146e5af1addc9d93fc00f",
            "2c46ececc82f4564a7df9295cb7b45d2",
            "3de265bd548243ceae913fbfa7b93969",
            "9954828497cf4d6e8cdd833faba14476",
            "23da245e88b94439946f7f0e8566d93d",
            "b00ed2aa4f3649bd9bac4156f2cbe6a7",
            "032d3e8155284182bf2ff217e2f11858",
            "9e63e2fa46c94621ae824318ce4815f7",
            "f461177a41b14dee9e5dab8acbab59fb",
            "6903557a27f84f29b9f25d6bec0c1876",
            "fc426de78f864b7ba1e37a5555f6a3ba",
            "107853bc57a04c639e9f7787238995a8"
          ]
        },
        "id": "53ce1718",
        "outputId": "38547796-3a52-49b1-ef1a-54993c92ed30"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/999 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51f05fed622e4cc3bb2ef98534cafde6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/797M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58889f45b71240b1b7801c9a289ee5bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/797M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c46ececc82f4564a7df9295cb7b45d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
            "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 655360\n",
            "Total parameters: 357367808\n",
            "Percentage of trainable parameters: 0.18%\n"
          ]
        }
      ],
      "source": [
        "# Load a pre-trained code model\n",
        "model_name = \"Salesforce/codegen-350M-mono\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Configure LoRA adapter\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        ")\n",
        "\n",
        "# Create PEFT model\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(\n",
        "    f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\"\n",
        ")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "print(\n",
        "    f\"Percentage of trainable parameters: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()):.2f}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4a26a871",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a26a871",
        "outputId": "c1cf8a4d-560f-4935-9db3-57315b3d0b8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def read_json(filename, print=False):\n",
            "    \"\"\"\n",
            "    Reads a JSON file and returns a list of dictionaries.\n",
            "    \"\"\"\n",
            "    with open(filename, 'r') as f:\n",
            "        return json.load(f)\n",
            "\n",
            "def write_json(filename, data):\n",
            "    \"\"\"\n",
            "    Writes a JSON file.\n",
            "    \"\"\"\n",
            "    with open(filename, 'w') as f:\n",
            "        json.dump(data, f, indent=4)\n",
            "\n",
            "def read_csv(filename, print=False):\n",
            "    \"\"\"\n",
            "    Reads a CSV\n"
          ]
        }
      ],
      "source": [
        "text = \"def read_json(filename, print=False):\"\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "generated_ids = model.generate(input_ids, max_length=128)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CodeCompletionDataset(Dataset):\n",
        "    def __init__(self, samples, tokenizer, max_length=512):\n",
        "        self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Get input and target\n",
        "        input_ids = sample[\"input_ids\"]\n",
        "        labels = sample[\"labels\"]\n",
        "\n",
        "        # Combine input with labels for training\n",
        "        combined_ids = input_ids + labels\n",
        "\n",
        "        # Handle truncation if needed\n",
        "        if len(combined_ids) > self.max_length:\n",
        "            combined_ids = combined_ids[:self.max_length]\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = [1] * len(combined_ids)\n",
        "\n",
        "        # Pad sequences if needed\n",
        "        padding_length = self.max_length - len(combined_ids)\n",
        "        if padding_length > 0:\n",
        "            combined_ids = combined_ids + [self.tokenizer.pad_token_id] * padding_length\n",
        "            attention_mask = attention_mask + [0] * padding_length\n",
        "\n",
        "        # Set up labels (set to -100 for input portion to ignore in loss)\n",
        "        labels = [-100] * len(input_ids) + combined_ids[len(input_ids):]\n",
        "\n",
        "        # Ensure all sequences have the right length\n",
        "        if len(labels) > self.max_length:\n",
        "            labels = labels[:self.max_length]\n",
        "        elif len(labels) < self.max_length:\n",
        "            labels = labels + [-100] * (self.max_length - len(labels))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(combined_ids),\n",
        "            \"attention_mask\": torch.tensor(attention_mask),\n",
        "            \"labels\": torch.tensor(labels),\n",
        "        }\n",
        "\n",
        "def train_code_completion_model(\n",
        "    training_samples,\n",
        "    model_name=\"Salesforce/codegen-350M-mono\",\n",
        "    output_dir=\"./code-completion-model\",\n",
        "    num_epochs=3,\n",
        "    batch_size=4,\n",
        "    grad_accum_steps=4,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    max_length=512,\n",
        "):\n",
        "    \"\"\"Train a code completion model using PyTorch and Hugging Face Transformers.\"\"\"\n",
        "    # Load tokenizer and model\n",
        "    print(f\"Loading model and tokenizer: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    # Ensure padding token is set\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        if tokenizer.eos_token_id is not None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "            print(f\"Setting pad_token_id to eos_token_id: {tokenizer.pad_token_id}\")\n",
        "        else:\n",
        "            tokenizer.pad_token_id = 0\n",
        "            print(\"Setting pad_token_id to 0\")\n",
        "\n",
        "    # Split data into train and validation\n",
        "    train_size = int(0.9 * len(training_samples))\n",
        "    train_samples = training_samples[:train_size]\n",
        "    val_samples = training_samples[train_size:]\n",
        "\n",
        "    print(f\"Training samples: {len(train_samples)}\")\n",
        "    print(f\"Validation samples: {len(val_samples)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = CodeCompletionDataset(train_samples, tokenizer, max_length)\n",
        "    val_dataset = CodeCompletionDataset(val_samples, tokenizer, max_length)\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=grad_accum_steps,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        logging_steps=10,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        fp16=True,  # Enable mixed-precision training\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "y1P33B3cg8_I"
      },
      "id": "y1P33B3cg8_I",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Dummy training data\n",
        "    training_data = [\n",
        "        {\"code\": \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)\", \"function_name\": \"factorial\"},\n",
        "        {\"code\": \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\", \"function_name\": \"fibonacci\"},\n",
        "        # Add more examples...\n",
        "    ]\n",
        "\n",
        "    # Prepare the training samples\n",
        "    training_samples = prepare_training_examples(training_data, tokenizer)\n",
        "\n",
        "    # Train the model\n",
        "    model, tokenizer = train_code_completion_model(training_samples)"
      ],
      "metadata": {
        "id": "bpd8vnb1i9TO",
        "outputId": "88034a4c-bf0d-4af7-ea54-227db180c132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "id": "bpd8vnb1i9TO",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 6 training samples from 2 examples\n",
            "Loading model and tokenizer: Salesforce/codegen-350M-mono\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
            "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting pad_token_id to eos_token_id: 50256\n",
            "Training samples: 5\n",
            "Validation samples: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2376 has 14.73 GiB memory in use. Of the allocated memory 6.57 GiB is allocated by PyTorch, and 28.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-0f9bbb1fd655>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_code_completion_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-61-253660eba0b9>\u001b[0m in \u001b[0;36mtrain_code_completion_model\u001b[0;34m(training_samples, model_name, output_dir, num_epochs, batch_size, grad_accum_steps, learning_rate, weight_decay, max_length)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# Create trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     trainer = Trainer(\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quantization_method\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mQuantizationMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBITS_AND_BYTES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         ):\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3710\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m                 )\n\u001b[0;32m-> 3712\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2376 has 14.73 GiB memory in use. Of the allocated memory 6.57 GiB is allocated by PyTorch, and 28.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load a code-optimized tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
        "\n",
        "# Prepare training data\n",
        "# (Your prepare_training_examples function as provided earlier)\n",
        "training_samples = prepare_training_examples(training_data, tokenizer)\n",
        "\n",
        "# Train the model\n",
        "model, tokenizer = train_code_completion_model(\n",
        "    training_samples,\n",
        "    model_name=\"Salesforce/codegen-350M-mono\",\n",
        "    output_dir=\"./codegen-completion-model\",\n",
        "    num_epochs=3,\n",
        "    batch_size=4\n",
        ")\n"
      ],
      "metadata": {
        "id": "51615LSIg_Ue",
        "outputId": "374c9112-6246-430a-c3c4-5454c41ce6ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "id": "51615LSIg_Ue",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 3000 training samples from 1000 examples\n",
            "Loading model and tokenizer: Salesforce/codegen-350M-mono\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
            "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting pad_token_id to eos_token_id: 50256\n",
            "Training samples: 2700\n",
            "Validation samples: 300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2376 has 14.73 GiB memory in use. Of the allocated memory 6.57 GiB is allocated by PyTorch, and 28.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-e7dc2469891b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m model, tokenizer = train_code_completion_model(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtraining_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Salesforce/codegen-350M-mono\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-253660eba0b9>\u001b[0m in \u001b[0;36mtrain_code_completion_model\u001b[0;34m(training_samples, model_name, output_dir, num_epochs, batch_size, grad_accum_steps, learning_rate, weight_decay, max_length)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# Create trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     trainer = Trainer(\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quantization_method\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mQuantizationMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBITS_AND_BYTES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         ):\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3710\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m                 )\n\u001b[0;32m-> 3712\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2376 has 14.73 GiB memory in use. Of the allocated memory 6.57 GiB is allocated by PyTorch, and 28.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9WuMJde8hDkj"
      },
      "id": "9WuMJde8hDkj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6cdb6a",
      "metadata": {
        "id": "8e6cdb6a"
      },
      "outputs": [],
      "source": [
        "def generate_completion(model, tokenizer, function_prefix, max_new_tokens=100):\n",
        "    inputs = tokenizer(function_prefix, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate completion\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    completed_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Return only the newly generated part\n",
        "    return completed_code[\n",
        "        len(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)) :\n",
        "    ]\n",
        "\n",
        "\n",
        "# Test with some examples\n",
        "test_prefixes = [\n",
        "    \"def train_model(X_train, y_train):\\n    # Create TensorFlow model\\n    model = tf.keras\",\n",
        "    \"def process_image(image_path):\\n    # Load and preprocess image\\n    import numpy as np\\n    img = \",\n",
        "    \"def create_bert_classifier():\\n    # Initialize a BERT model from HuggingFace\\n    from transformers import \",\n",
        "]\n",
        "\n",
        "for prefix in test_prefixes:\n",
        "    completion = generate_completion(model, tokenizer, prefix)\n",
        "    print(f\"\\nPrefix:\\n{prefix}\")\n",
        "    print(f\"\\nCompletion:\\n{completion}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "class CodeCompletionDataset(Dataset):\n",
        "    def __init__(self, samples, tokenizer, max_length=512):\n",
        "        self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Get input and target\n",
        "        input_ids = sample[\"input_ids\"]\n",
        "        labels = sample[\"labels\"]\n",
        "\n",
        "        # Combine input with labels for training\n",
        "        combined_ids = input_ids + labels\n",
        "\n",
        "        # Handle truncation if needed\n",
        "        if len(combined_ids) > self.max_length:\n",
        "            combined_ids = combined_ids[:self.max_length]\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = [1] * len(combined_ids)\n",
        "\n",
        "        # Pad sequences if needed\n",
        "        padding_length = self.max_length - len(combined_ids)\n",
        "        if padding_length > 0:\n",
        "            combined_ids = combined_ids + [self.tokenizer.pad_token_id] * padding_length\n",
        "            attention_mask = attention_mask + [0] * padding_length\n",
        "\n",
        "        # Set up labels (set to -100 for input portion to ignore in loss)\n",
        "        labels = [-100] * len(input_ids) + combined_ids[len(input_ids):]\n",
        "\n",
        "        # Ensure all sequences have the right length\n",
        "        if len(labels) > self.max_length:\n",
        "            labels = labels[:self.max_length]\n",
        "        elif len(labels) < self.max_length:\n",
        "            labels = labels + [-100] * (self.max_length - len(labels))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(combined_ids),\n",
        "            \"attention_mask\": torch.tensor(attention_mask),\n",
        "            \"labels\": torch.tensor(labels),\n",
        "        }\n",
        "\n",
        "def train_code_completion_model(\n",
        "    training_samples,\n",
        "    model_name=\"Salesforce/codegen-350M-mono\",\n",
        "    output_dir=\"./code-completion-model\",\n",
        "    num_epochs=3,\n",
        "    batch_size=4,\n",
        "    grad_accum_steps=4,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    max_length=512,\n",
        "):\n",
        "    \"\"\"Train a code completion model using PyTorch and Hugging Face Transformers.\"\"\"\n",
        "    # Load tokenizer and model\n",
        "    print(f\"Loading model and tokenizer: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    # Ensure padding token is set\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        if tokenizer.eos_token_id is not None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "            print(f\"Setting pad_token_id to eos_token_id: {tokenizer.pad_token_id}\")\n",
        "        else:\n",
        "            tokenizer.pad_token_id = 0\n",
        "            print(\"Setting pad_token_id to 0\")\n",
        "\n",
        "    # Split data into train and validation\n",
        "    train_size = int(0.9 * len(training_samples))\n",
        "    train_samples = training_samples[:train_size]\n",
        "    val_samples = training_samples[train_size:]\n",
        "\n",
        "    print(f\"Training samples: {len(train_samples)}\")\n",
        "    print(f\"Validation samples: {len(val_samples)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = CodeCompletionDataset(train_samples, tokenizer, max_length)\n",
        "    val_dataset = CodeCompletionDataset(val_samples, tokenizer, max_length)\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=grad_accum_steps,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        logging_steps=10,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        fp16=True,  # Enable mixed-precision training\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Example usage with dummy training data for demonstration\n",
        "# In a real scenario, replace with your actual code examples\n",
        "if __name__ == \"__main__\":\n",
        "    # Dummy training data\n",
        "    training_data = [\n",
        "        {\"code\": \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)\", \"function_name\": \"factorial\"},\n",
        "        {\"code\": \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\", \"function_name\": \"fibonacci\"},\n",
        "        # Add more examples...\n",
        "    ]\n",
        "\n",
        "    # Prepare the training samples\n",
        "    training_samples = prepare_training_examples(training_data, tokenizer)\n",
        "\n",
        "    # Train the model\n",
        "    model, tokenizer = train_code_completion_model(training_samples)"
      ],
      "metadata": {
        "id": "JbRnQ3feh-TQ"
      },
      "id": "JbRnQ3feh-TQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51f05fed622e4cc3bb2ef98534cafde6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7c44ff9be374da5a21d26ba304c7a13",
              "IPY_MODEL_ab765e57d317437b8b7b472b7368f1c9",
              "IPY_MODEL_8d42e2d388984117ba602a321be42796"
            ],
            "layout": "IPY_MODEL_a4b4613c82c64de6a96e6a8183b67b7e"
          }
        },
        "c7c44ff9be374da5a21d26ba304c7a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8913a1b667ce4b709383dc09ede565a6",
            "placeholder": "​",
            "style": "IPY_MODEL_4bb0df3e82a44822b942a3669d73c8d2",
            "value": "config.json: 100%"
          }
        },
        "ab765e57d317437b8b7b472b7368f1c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46ba684ae0b94a07b0ea2d6b7f45b4a2",
            "max": 999,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e9ebceeec0d41a3b9ba2e2ad2000f0e",
            "value": 999
          }
        },
        "8d42e2d388984117ba602a321be42796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a36582bade7e4df9bfda1197ff529a21",
            "placeholder": "​",
            "style": "IPY_MODEL_2f183fa6fadb447b939eb1cac04365a0",
            "value": " 999/999 [00:00&lt;00:00, 102kB/s]"
          }
        },
        "a4b4613c82c64de6a96e6a8183b67b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8913a1b667ce4b709383dc09ede565a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bb0df3e82a44822b942a3669d73c8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46ba684ae0b94a07b0ea2d6b7f45b4a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e9ebceeec0d41a3b9ba2e2ad2000f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a36582bade7e4df9bfda1197ff529a21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f183fa6fadb447b939eb1cac04365a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58889f45b71240b1b7801c9a289ee5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a61729ed4fef4ceba317bef057cb2bf8",
              "IPY_MODEL_ec51d731c3064346a346c6440697b44f",
              "IPY_MODEL_55393c2339ac4e419eff33cc8e73c6f2"
            ],
            "layout": "IPY_MODEL_7037c6d14c2647c7b47e3ba875724229"
          }
        },
        "a61729ed4fef4ceba317bef057cb2bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f829b217a9b443b793d6cf7b46d6193f",
            "placeholder": "​",
            "style": "IPY_MODEL_3c57bc598184440692f7005a4895125d",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "ec51d731c3064346a346c6440697b44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1f6789cd8f14354b69adccaea0fa653",
            "max": 797367631,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8fee14aa5ee4627b5b11eac97fd685a",
            "value": 797367631
          }
        },
        "55393c2339ac4e419eff33cc8e73c6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe89a29eb09c481d86365600dcb4268a",
            "placeholder": "​",
            "style": "IPY_MODEL_64bea273ac0146e5af1addc9d93fc00f",
            "value": " 797M/797M [00:08&lt;00:00, 232MB/s]"
          }
        },
        "7037c6d14c2647c7b47e3ba875724229": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f829b217a9b443b793d6cf7b46d6193f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c57bc598184440692f7005a4895125d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1f6789cd8f14354b69adccaea0fa653": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8fee14aa5ee4627b5b11eac97fd685a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe89a29eb09c481d86365600dcb4268a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64bea273ac0146e5af1addc9d93fc00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c46ececc82f4564a7df9295cb7b45d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3de265bd548243ceae913fbfa7b93969",
              "IPY_MODEL_9954828497cf4d6e8cdd833faba14476",
              "IPY_MODEL_23da245e88b94439946f7f0e8566d93d"
            ],
            "layout": "IPY_MODEL_b00ed2aa4f3649bd9bac4156f2cbe6a7"
          }
        },
        "3de265bd548243ceae913fbfa7b93969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_032d3e8155284182bf2ff217e2f11858",
            "placeholder": "​",
            "style": "IPY_MODEL_9e63e2fa46c94621ae824318ce4815f7",
            "value": "model.safetensors: 100%"
          }
        },
        "9954828497cf4d6e8cdd833faba14476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f461177a41b14dee9e5dab8acbab59fb",
            "max": 797330536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6903557a27f84f29b9f25d6bec0c1876",
            "value": 797330536
          }
        },
        "23da245e88b94439946f7f0e8566d93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc426de78f864b7ba1e37a5555f6a3ba",
            "placeholder": "​",
            "style": "IPY_MODEL_107853bc57a04c639e9f7787238995a8",
            "value": " 797M/797M [00:06&lt;00:00, 166MB/s]"
          }
        },
        "b00ed2aa4f3649bd9bac4156f2cbe6a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "032d3e8155284182bf2ff217e2f11858": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e63e2fa46c94621ae824318ce4815f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f461177a41b14dee9e5dab8acbab59fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6903557a27f84f29b9f25d6bec0c1876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc426de78f864b7ba1e37a5555f6a3ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "107853bc57a04c639e9f7787238995a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}