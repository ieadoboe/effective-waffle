{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is All You Need\n",
    "\n",
    "Coding a transformer from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Shakespeares work from Andrej Karpathy's website\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "filepath = tf.keras.utils.get_file('shakespeare.txt', url)\n",
    "\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first few characters\n",
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in vocabulary: 65\n",
      "Total length of text dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(set(shakespeare_text))\n",
    "char_to_int = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "\n",
    "# How many number of distinct characters has the vocabulary:\n",
    "tokens_len = len(unique_chars)\n",
    "print(f'Number of tokens in vocabulary: {tokens_len}')\n",
    "\n",
    "# How many characters has the dataset:\n",
    "text_length = len(shakespeare_text)\n",
    "print(f'Total length of text dataset: {text_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "(1, 1, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_89 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 10\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(tokens_len, embedding_dim))\n",
    "\n",
    "input_array = np.random.randint(tokens_len, size=(1, 1))\n",
    "model.compile('rmsprop', 'sparse_categorical_crossentropy')\n",
    "\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array.shape)\n",
    "\n",
    "model.summary()\n",
    "# (1, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, max_pos_enc, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_len = max_pos_enc # maximum sequence length that the model can handle\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Create the positional encodings\n",
    "        position = np.arange(max_pos_enc)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, embedding_dim, 2) * -(np.log(10000.0) / embedding_dim))\n",
    "        pe = np.zeros((max_pos_enc, embedding_dim))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension e.g. (max_pos_enc,embedding_dim) -> (1,max_pos_enc,embedding_dim)\n",
    "        self.pe = tf.constant(pe[np.newaxis, :, :], dtype=tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]  # Get sequence length from input\n",
    "        return inputs + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10  # Embedding dimension\n",
    "max_len = 50  # Maximum sequence length\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokens_len, embedding_dim),\n",
    "    PositionalEncoding(embedding_dim, max_len)\n",
    "])\n",
    "\n",
    "input_array = np.random.randint(tokens_len, size=(1, 10)) \n",
    "output_array = model(input_array)\n",
    "\n",
    "print(output_array.shape)  # Should be (1, 10, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding_85          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_90 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m)            │           \u001b[38;5;34m650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding_85          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        q, k, v = inputs\n",
    "        \n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        \n",
    "        # dot product attention\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True) # (bs, q_len, k_len)\n",
    "        \n",
    "        # scale dot product\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        \n",
    "        # apply mask when necessary\n",
    "        if mask is not None:\n",
    "            # adding very large negative values \n",
    "            # so they go to zero after softmax\n",
    "            scaled_attention_logits += (mask * -1e9) \n",
    "        \n",
    "        # apply softmax to attention weights (scores)\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        \n",
    "        # multiply by V (values)\n",
    "        out = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        return out, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention Head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        assert embedding_dim % num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
    "        self.depth = embedding_dim // num_heads # depth per head\n",
    "        \n",
    "        # linear projection layers\n",
    "        self.wq = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.wk = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.wv = tf.keras.layers.Dense(embedding_dim)\n",
    "        \n",
    "        # output projection\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention()\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        q, k, v = inputs\n",
    "        \n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # linear projections\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # reshaping q, k, v\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = self.attention([q, k, v], mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        \n",
    "        out = self.dense(concat_attention)\n",
    "        \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network\n",
    "\n",
    "$$\\text{FFN(x)} = \\text{max}(0,~ xW_1 + b_1)W_2 + b_2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, hidden_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # hidden_dim (dff) - feed forward network hidden \n",
    "        # layer dimension a.k.a inner layer dimensionality\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(embedding_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.dense1(inputs)\n",
    "        \n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_heads, dropout_rate = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.mha = MultiHeadAttention(num_heads, embedding_dim)\n",
    "        self.ffn = PositionwiseFeedForward(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        \n",
    "        # multi-head attention\n",
    "        attention_output, _ = self.mha([inputs, inputs, inputs], mask)\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        out1 = self.layer_norm1(inputs + attention_output)\n",
    "        \n",
    "        # feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layer_norm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The Encoder stacks multiple encoder layer to create the full encoder. It includes the Embedding layer and Positional Encoding layer as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, embedding_dim, hidden_dim, num_heads, \n",
    "                 tokens_len, max_pos_enc, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "            \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(tokens_len, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(max_pos_enc, embedding_dim)\n",
    "        \n",
    "        self.encoding_layers = [\n",
    "            EncoderLayer(embedding_dim, hidden_dim, num_heads, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoding_layers[i](x, training, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Self-attention\n",
    "        self.self_attention = MultiHeadAttention(num_heads, embedding_dim)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Cross-attention (encoder-decoder attention)\n",
    "        self.cross_attention = MultiHeadAttention(num_heads, embedding_dim)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ffn = PositionwiseFeedForward(embedding_dim, hidden_dim)\n",
    "        self.norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, inputs, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        # Self attention with look-ahead mask\n",
    "        self_attn_output, _ = self.self_attention([inputs, inputs, inputs], look_ahead_mask)\n",
    "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
    "        out1 = self.norm1(inputs + self_attn_output)\n",
    "        \n",
    "        # Cross attention with encoder output\n",
    "        cross_attn_output, _ = self.cross_attention([out1, enc_output, enc_output], padding_mask)\n",
    "        cross_attn_output = self.dropout2(cross_attn_output, training=training)\n",
    "        out2 = self.norm2(out1 + cross_attn_output)\n",
    "        \n",
    "        # Feed forward\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.norm3(out2 + ffn_output)\n",
    "        \n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, embedding_dim, hidden_dim, num_heads,\n",
    "                 tokens_len, max_pos_enc, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(tokens_len, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(max_pos_enc, embedding_dim)\n",
    "        \n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(embedding_dim, hidden_dim, num_heads, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # embedding and positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # decoder layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.decoder_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, embedding_dim, hidden_dim, num_heads,\n",
    "                input_vocab_size, target_vocab_size, max_pos_enc,\n",
    "                dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            num_layers, embedding_dim, hidden_dim, num_heads,\n",
    "            input_vocab_size, max_pos_enc, dropout_rate\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            num_layers, embedding_dim, hidden_dim, num_heads,\n",
    "            target_vocab_size, max_pos_enc, dropout_rate\n",
    "        )\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def create_padding_mask(self, seq):\n",
    "        \"\"\"Creates a mask for padding tokens (value 0)\"\"\"\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "    \n",
    "    def create_look_ahead_mask(self, seq_len):\n",
    "        \"\"\"Creates a mask to prevent attention to future tokens\"\"\"\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        inp, tar = inputs\n",
    "        \n",
    "        # Create masks\n",
    "        enc_padding_mask = self.create_padding_mask(inp)\n",
    "        dec_padding_mask = self.create_padding_mask(inp)\n",
    "        \n",
    "        # Look ahead mask for decoder\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = self.create_padding_mask(tar)\n",
    "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "        \n",
    "        # Encoder output\n",
    "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
    "        \n",
    "        # Decoder output\n",
    "        dec_output = self.decoder(\n",
    "            x=tar,\n",
    "            enc_output=enc_output,\n",
    "            training=training,\n",
    "            look_ahead_mask=combined_mask,\n",
    "            padding_mask=dec_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Final output\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/deep12/lib/python3.12/site-packages/keras/src/layers/layer.py:1381: UserWarning: Layer 'transformer_40' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Exception encountered when calling PositionalEncoding.call().\n",
      "\n",
      "\u001b[1mDimensions must be equal, but are 512 and 10000 for '{{node encoder_40_1/positional_encoding_88_1/add}} = AddV2[T=DT_FLOAT](encoder_40_1/embedding_93_1/GatherV2, encoder_40_1/positional_encoding_88_1/strided_slice_1)' with input shapes: [1,10,512], [1,10,10000].\u001b[0m\n",
      "\n",
      "Arguments received by PositionalEncoding.call():\n",
      "  • inputs=tf.Tensor(shape=(1, 10, 512), dtype=float32)''\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/deep12/lib/python3.12/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'transformer_40', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "2025-03-28 16:52:07.364581: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [1,10,512] vs. [1,10,10000]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling PositionalEncoding.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1,10,512] vs. [1,10,10000] [Op:AddV2] name: \u001b[0m\n\nArguments received by PositionalEncoding.call():\n  • inputs=tf.Tensor(shape=(1, 10, 512), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[323], line 29\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Provide a sample input to build the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Fixed sample input - specify maxval for integer type\u001b[39;00m\n\u001b[1;32m     25\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     26\u001b[0m     tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m), maxval\u001b[38;5;241m=\u001b[39minput_vocab_size, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32), \n\u001b[1;32m     27\u001b[0m     tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m), maxval\u001b[38;5;241m=\u001b[39mtarget_vocab_size, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Print model summary\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(transformer\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deep12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[322], line 42\u001b[0m, in \u001b[0;36mTransformer.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     39\u001b[0m combined_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmaximum(dec_target_padding_mask, look_ahead_mask)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Encoder output\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Decoder output\u001b[39;00m\n\u001b[1;32m     45\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m     46\u001b[0m     x\u001b[38;5;241m=\u001b[39mtar,\n\u001b[1;32m     47\u001b[0m     enc_output\u001b[38;5;241m=\u001b[39menc_output,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     padding_mask\u001b[38;5;241m=\u001b[39mdec_padding_mask\n\u001b[1;32m     51\u001b[0m )\n",
      "Cell \u001b[0;32mIn[317], line 24\u001b[0m, in \u001b[0;36mEncoder.call\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(x)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n",
      "Cell \u001b[0;32mIn[310], line 19\u001b[0m, in \u001b[0;36mPositionalEncoding.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     18\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(inputs)[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Get sequence length from input\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling PositionalEncoding.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1,10,512] vs. [1,10,10000] [Op:AddV2] name: \u001b[0m\n\nArguments received by PositionalEncoding.call():\n  • inputs=tf.Tensor(shape=(1, 10, 512), dtype=float32)"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "embedding_dim = 512\n",
    "hidden_dim = 2048\n",
    "num_heads = 8\n",
    "input_vocab_size = 8000\n",
    "target_vocab_size = 8000\n",
    "max_pos_enc = 10000\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create model\n",
    "transformer = Transformer(\n",
    "    num_layers, embedding_dim, hidden_dim, num_heads,\n",
    "    input_vocab_size, target_vocab_size, max_pos_enc, dropout_rate\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "transformer.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Provide a sample input to build the model\n",
    "# Fixed sample input - specify maxval for integer type\n",
    "sample_input = (\n",
    "    tf.random.uniform((1, 10), maxval=input_vocab_size, dtype=tf.int32), \n",
    "    tf.random.uniform((1, 10), maxval=target_vocab_size, dtype=tf.int32)\n",
    ")\n",
    "transformer(sample_input)\n",
    "\n",
    "# Print model summary\n",
    "print(transformer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
