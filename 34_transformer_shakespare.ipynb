{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is All You Need\n",
    "\n",
    "Coding a transformer from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Shakespeares work from Andrej Karpathy's website\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "filepath = tf.keras.utils.get_file('shakespeare.txt', url)\n",
    "\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first few characters\n",
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in vocabulary: 65\n",
      "Total length of text dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(set(shakespeare_text))\n",
    "char_to_int = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "\n",
    "# How many number of distinct characters has the vocabulary:\n",
    "tokens_len = len(unique_chars)\n",
    "print(f'Number of tokens in vocabulary: {tokens_len}')\n",
    "\n",
    "# How many characters has the dataset:\n",
    "text_length = len(shakespeare_text)\n",
    "print(f'Total length of text dataset: {text_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "(1, 1, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_26\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_26\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_115 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_115 (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 10\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(tokens_len, embedding_dim))\n",
    "\n",
    "input_array = np.random.randint(tokens_len, size=(1, 1))\n",
    "model.compile('rmsprop', 'sparse_categorical_crossentropy')\n",
    "\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array.shape)\n",
    "\n",
    "model.summary()\n",
    "# (1, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_pos_enc, embedding_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_len = max_pos_enc # maximum sequence length that the model can handle\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Create the positional encodings\n",
    "        position = np.arange(max_pos_enc)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, embedding_dim, 2) * -(np.log(10000.0) / embedding_dim))\n",
    "        pe = np.zeros((max_pos_enc, embedding_dim))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension e.g. (max_pos_enc,embedding_dim) -> (1,max_pos_enc,embedding_dim)\n",
    "        self.pe = tf.constant(pe[np.newaxis, :, :], dtype=tf.float32)\n",
    "    \n",
    "    def call(self, inputs):           \n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        # Add positional encoding, broadcasting across batch dimension\n",
    "        return inputs + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10  # Embedding dimension\n",
    "max_len = 50  # Maximum sequence length\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokens_len, embedding_dim),\n",
    "    PositionalEncoding(max_pos_enc=max_len, embedding_dim=embedding_dim)\n",
    "])\n",
    "\n",
    "input_array = np.random.randint(tokens_len, size=(1, 10)) \n",
    "output_array = model(input_array)\n",
    "\n",
    "print(output_array.shape)  # Should be (1, 10, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_27\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_27\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_116 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding_108         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_116 (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m)            │           \u001b[38;5;34m650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding_108         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        q, k, v = inputs\n",
    "        \n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        \n",
    "        # dot product attention\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True) # (bs, q_len, k_len)\n",
    "        \n",
    "        # scale dot product\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        \n",
    "        # apply mask when necessary\n",
    "        if mask is not None:\n",
    "            # adding very large negative values \n",
    "            # so they go to zero after softmax\n",
    "            scaled_attention_logits += (mask * -1e9) \n",
    "        \n",
    "        # apply softmax to attention weights (scores)\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        \n",
    "        # multiply by V (values)\n",
    "        out = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        return out, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention Head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        assert embedding_dim % num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
    "        self.depth = embedding_dim // num_heads # depth per head\n",
    "        \n",
    "        # linear projection layers\n",
    "        self.wq = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.wk = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.wv = tf.keras.layers.Dense(embedding_dim)\n",
    "        \n",
    "        # output projection\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention()\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        q, k, v = inputs\n",
    "        \n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # linear projections\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # reshaping q, k, v\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = self.attention([q, k, v], mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        \n",
    "        out = self.dense(concat_attention)\n",
    "        \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network\n",
    "\n",
    "$$\\text{FFN(x)} = \\text{max}(0,~ xW_1 + b_1)W_2 + b_2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, hidden_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # hidden_dim (dff) - feed forward network hidden \n",
    "        # layer dimension a.k.a inner layer dimensionality\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(embedding_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.dense1(inputs)\n",
    "        \n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_heads, dropout_rate = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.mha = MultiHeadAttention(num_heads, embedding_dim)\n",
    "        self.ffn = PositionwiseFeedForward(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        \n",
    "        # multi-head attention\n",
    "        attention_output, _ = self.mha([inputs, inputs, inputs], mask)\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        out1 = self.layer_norm1(inputs + attention_output)\n",
    "        \n",
    "        # feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layer_norm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The Encoder stacks multiple encoder layer to create the full encoder. It includes the Embedding layer and Positional Encoding layer as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, embedding_dim, hidden_dim, num_heads, \n",
    "                 tokens_len, max_pos_enc, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "            \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(tokens_len, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(max_pos_enc, embedding_dim)\n",
    "        \n",
    "        self.encoding_layers = [\n",
    "            EncoderLayer(embedding_dim, hidden_dim, num_heads, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = tf.cast(x, dtype=tf.float32)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoding_layers[i](x, training=training, mask=mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Self-attention\n",
    "        self.self_attention = MultiHeadAttention(num_heads, embedding_dim)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Cross-attention (encoder-decoder attention)\n",
    "        self.cross_attention = MultiHeadAttention(num_heads, embedding_dim)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ffn = PositionwiseFeedForward(embedding_dim, hidden_dim)\n",
    "        self.norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, inputs, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        # Self attention with look-ahead mask\n",
    "        self_attn_output, _ = self.self_attention([inputs, inputs, inputs], mask=look_ahead_mask)\n",
    "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
    "        out1 = self.norm1(inputs + self_attn_output)\n",
    "        \n",
    "        # Cross attention with encoder output\n",
    "        cross_attn_output, _ = self.cross_attention([out1, enc_output, enc_output], mask=padding_mask)\n",
    "        cross_attn_output = self.dropout2(cross_attn_output, training=training)\n",
    "        out2 = self.norm2(out1 + cross_attn_output)\n",
    "        \n",
    "        # Feed forward\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.norm3(out2 + ffn_output)\n",
    "        \n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, embedding_dim, hidden_dim, num_heads,\n",
    "                 tokens_len, max_pos_enc, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(tokens_len, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(max_pos_enc, embedding_dim)\n",
    "        \n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(embedding_dim, hidden_dim, num_heads, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # embedding and positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x = tf.cast(x, dtype=tf.float32)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Ensure correct mask shape\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "        \n",
    "        # decoder layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.decoder_layers[i](x, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, embedding_dim, hidden_dim, num_heads,\n",
    "                input_vocab_size, target_vocab_size, max_pos_enc,\n",
    "                dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            num_layers, embedding_dim, hidden_dim, num_heads,\n",
    "            input_vocab_size, max_pos_enc, dropout_rate\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            num_layers, embedding_dim, hidden_dim, num_heads,\n",
    "            target_vocab_size, max_pos_enc, dropout_rate\n",
    "        )\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def create_padding_mask(self, seq):\n",
    "        \"\"\"Creates a mask for padding tokens (value 0)\"\"\"\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "    \n",
    "    def create_look_ahead_mask(self, seq_len):\n",
    "        \"\"\"Creates a mask to prevent attention to future tokens\"\"\"\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        inp, tar = inputs\n",
    "        \n",
    "        # Create masks\n",
    "        enc_padding_mask = self.create_padding_mask(inp)\n",
    "        dec_padding_mask = self.create_padding_mask(inp)\n",
    "        \n",
    "        # Look ahead mask for decoder\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = self.create_padding_mask(tar)\n",
    "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "        \n",
    "        # Encoder output\n",
    "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
    "        \n",
    "        # Decoder output\n",
    "        dec_output = self.decoder(\n",
    "            x=tar,\n",
    "            enc_output=enc_output,\n",
    "            training=training,\n",
    "            look_ahead_mask=combined_mask,\n",
    "            padding_mask=dec_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Final output\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/deep12/lib/python3.12/site-packages/keras/src/layers/layer.py:1381: UserWarning: Layer 'multi_head_attention_339' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Dimension must be 6 but is 4 for '{{node transpose_3}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](scaled_dot_product_attention_331_1/MatMul_1, transpose_3/perm)' with input shapes: [1,1,1,8,10,64], [4].''\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/deep12/lib/python3.12/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'multi_head_attention_339', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/deep12/lib/python3.12/site-packages/keras/src/layers/layer.py:1381: UserWarning: Layer 'transformer_46' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Exception encountered when calling MultiHeadAttention.call().\n",
      "\n",
      "\u001b[1mDimension must be 6 but is 4 for '{{node decoder_40_1/decoder_layer_109_1/multi_head_attention_339_1/transpose_3}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](decoder_40_1/decoder_layer_109_1/multi_head_attention_339_1/scaled_dot_product_attention_331_1/MatMul_1, decoder_40_1/decoder_layer_109_1/multi_head_attention_339_1/transpose_3/perm)' with input shapes: [1,1,1,8,10,64], [4].\u001b[0m\n",
      "\n",
      "Arguments received by MultiHeadAttention.call():\n",
      "  • inputs=['tf.Tensor(shape=(1, 10, 512), dtype=float32)', 'tf.Tensor(shape=(1, 10, 512), dtype=float32)', 'tf.Tensor(shape=(1, 10, 512), dtype=float32)']\n",
      "  • mask=tf.Tensor(shape=(1, 1, 1, 1, 1, 10), dtype=float32)''\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/deep12/lib/python3.12/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'transformer_46', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "2025-03-28 17:19:58.168105: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at transpose_op.cc:142 : INVALID_ARGUMENT: transpose expects a vector of size 6. But input(1) is a vector of size 4\n",
      "2025-03-28 17:19:58.168366: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: transpose expects a vector of size 6. But input(1) is a vector of size 4\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling MultiHeadAttention.call().\n\n\u001b[1m{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:CPU:0}} transpose expects a vector of size 6. But input(1) is a vector of size 4 [Op:Transpose]\u001b[0m\n\nArguments received by MultiHeadAttention.call():\n  • inputs=['tf.Tensor(shape=(1, 10, 512), dtype=float32)', 'tf.Tensor(shape=(1, 10, 512), dtype=float32)', 'tf.Tensor(shape=(1, 10, 512), dtype=float32)']\n  • mask=tf.Tensor(shape=(1, 1, 1, 1, 1, 10), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[395], line 29\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Provide a sample input to build the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Fixed sample input - specify maxval for integer type\u001b[39;00m\n\u001b[1;32m     25\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     26\u001b[0m     tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m), maxval\u001b[38;5;241m=\u001b[39minput_vocab_size, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32), \n\u001b[1;32m     27\u001b[0m     tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m), maxval\u001b[38;5;241m=\u001b[39mtarget_vocab_size, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Print model summary\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(transformer\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deep12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[394], line 45\u001b[0m, in \u001b[0;36mTransformer.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     42\u001b[0m enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(inp, training\u001b[38;5;241m=\u001b[39mtraining, mask\u001b[38;5;241m=\u001b[39menc_padding_mask)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Decoder output\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43menc_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlook_ahead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_padding_mask\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Final output\u001b[39;00m\n\u001b[1;32m     54\u001b[0m final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(dec_output)\n",
      "Cell \u001b[0;32mIn[393], line 34\u001b[0m, in \u001b[0;36mDecoder.call\u001b[0;34m(self, x, enc_output, training, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlook_ahead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlook_ahead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[392], line 27\u001b[0m, in \u001b[0;36mDecoderLayer.call\u001b[0;34m(self, inputs, enc_output, training, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(inputs \u001b[38;5;241m+\u001b[39m self_attn_output)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Cross attention with encoder output\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m cross_attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mout1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m cross_attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(cross_attn_output, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m     29\u001b[0m out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(out1 \u001b[38;5;241m+\u001b[39m cross_attn_output)\n",
      "Cell \u001b[0;32mIn[388], line 41\u001b[0m, in \u001b[0;36mMultiHeadAttention.call\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(v, batch_size)\n\u001b[1;32m     39\u001b[0m scaled_attention, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention([q, k, v], mask)\n\u001b[0;32m---> 41\u001b[0m scaled_attention \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m concat_attention \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(scaled_attention, (batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim))\n\u001b[1;32m     44\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(concat_attention)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling MultiHeadAttention.call().\n\n\u001b[1m{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:CPU:0}} transpose expects a vector of size 6. But input(1) is a vector of size 4 [Op:Transpose]\u001b[0m\n\nArguments received by MultiHeadAttention.call():\n  • inputs=['tf.Tensor(shape=(1, 10, 512), dtype=float32)', 'tf.Tensor(shape=(1, 10, 512), dtype=float32)', 'tf.Tensor(shape=(1, 10, 512), dtype=float32)']\n  • mask=tf.Tensor(shape=(1, 1, 1, 1, 1, 10), dtype=float32)"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "embedding_dim = 512\n",
    "hidden_dim = 2048\n",
    "num_heads = 8\n",
    "input_vocab_size = 8000\n",
    "target_vocab_size = 8000\n",
    "max_pos_enc = 10000\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create model\n",
    "transformer = Transformer(\n",
    "    num_layers, embedding_dim, hidden_dim, num_heads,\n",
    "    input_vocab_size, target_vocab_size, max_pos_enc, dropout_rate\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "transformer.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Provide a sample input to build the model\n",
    "# Fixed sample input - specify maxval for integer type\n",
    "sample_input = (\n",
    "    tf.random.uniform((1, 10), maxval=input_vocab_size, dtype=tf.int32), \n",
    "    tf.random.uniform((1, 10), maxval=target_vocab_size, dtype=tf.int32)\n",
    ")\n",
    "transformer(sample_input)\n",
    "\n",
    "# Print model summary\n",
    "print(transformer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
