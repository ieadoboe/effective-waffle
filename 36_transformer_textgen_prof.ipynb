{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Note: For mor information about this particular transformer model and a\n",
        "# PyTorch implementation, check out the Youtube channel of Andrej Karpathy!"
      ],
      "metadata": {
        "id": "8z07ZAJlTkFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESdoMF6G0J0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd01a6b-ff11-44bb-8b9c-c0afb6d0f9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-22 19:25:22.772543: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-22 19:25:22.787997: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742680522.805533  263314 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742680522.813892  263314 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1742680522.827100  263314 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1742680522.827126  263314 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1742680522.827127  263314 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1742680522.827128  263314 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-03-22 19:25:22.831079: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data file\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDh4zaRq0RJd",
        "outputId": "30a9ee8c-a0f0-4362-a5e8-2885bf32a321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-22 19:25:25--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.9’\n",
            "\n",
            "input.txt.9         100%[===================>]   1.06M  5.03MB/s    in 0.2s    \n",
            "\n",
            "2025-03-22 19:25:25 (5.03 MB/s) - ‘input.txt.9’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file\n",
        "with open('input.txt') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "rWoKBQAX0TBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the characters used\n",
        "chars = sorted(list(set(text)))\n",
        "n_chars = len(chars)"
      ],
      "metadata": {
        "id": "EjfzYGUy0V3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_string = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "Y98x8jHF0XKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = lambda s: [string_to_int[c] for c in s]\n",
        "decoded = lambda l: ''.join(int_to_string[i] for i in l)"
      ],
      "metadata": {
        "id": "Y6gmSph_0YoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test data\n",
        "TRAIN_SPLIT = 0.9\n",
        "\n",
        "n_train = int(len(text) * TRAIN_SPLIT)\n",
        "train_text = encoded(text[:n_train])\n",
        "val_text = encoded(text[n_train:])"
      ],
      "metadata": {
        "id": "PNv-n0Pi0aQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to break the total sequence into smaller chunks for our prediction model\n",
        "# These chunks are of size \"length\" and are shifted by one character\n",
        "# between input and output.\n",
        "def get_dataset(sequence, length, shuffle=False, batch_size=128):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "\n",
        "  # Create windows of (length + 1) so we can split input/target\n",
        "  dataset = dataset.window(length + 1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(length + 1))\n",
        "\n",
        "  # Only shuffle after creating windows to preserve sequential dependencies\n",
        "  if shuffle:\n",
        "      dataset = dataset.shuffle(buffer_size=10000)  # Adjust buffer size\n",
        "\n",
        "  # Split into (input, target) pairs\n",
        "  dataset = dataset.map(lambda window: (window[:-1], window[1:]))\n",
        "\n",
        "  # Batch after shuffling, so each batch contains intact sequences\n",
        "  dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return dataset.repeat()"
      ],
      "metadata": {
        "id": "2-JyZnN1d_HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the sequence length we consider for training\n",
        "seq_length = 100"
      ],
      "metadata": {
        "id": "QEoH0gYCChxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters of the model\n",
        "batch_size = 32\n",
        "lr = 1e-3"
      ],
      "metadata": {
        "id": "5Is7oRTtCnRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and validation\n",
        "train_dataset = get_dataset(train_text, seq_length, shuffle=True, batch_size=batch_size)\n",
        "val_dataset = get_dataset(val_text, seq_length, shuffle=False, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "4aHacZrrfXsD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab0ecc3a-b404-42ee-ded5-9dee02fc48d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "I0000 00:00:1742680526.739476  263314 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21458 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "  def __init__(self, seq_length, d_embed):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.seq_length = seq_length\n",
        "    self.d_embed = d_embed\n",
        "\n",
        "  def call(self, inputs):\n",
        "    position = tf.range(self.seq_length, dtype=tf.float32)[:, tf.newaxis]\n",
        "    div_term = tf.exp(tf.range(0, self.d_embed, 2, dtype=tf.float32) * (-np.log(10000.0) / self.d_embed))\n",
        "\n",
        "    # Compute sin & cos encoding\n",
        "    pos_encoding = tf.concat([tf.sin(position * div_term), tf.cos(position * div_term)], axis=-1)\n",
        "\n",
        "    # Add batch dimension\n",
        "    pos_encoding = pos_encoding[tf.newaxis, :, :]  # Shape (1, seq_length, d_embed)\n",
        "\n",
        "    return inputs + pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "ea8PYQVa2sX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, head_size, seq_length, dropout=0.0, masked=True, **kwargs):\n",
        "    super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
        "\n",
        "    self.seq_length = seq_length\n",
        "    self.scale = head_size ** -0.5\n",
        "    self.key = tf.keras.layers.Dense(head_size, use_bias=False)\n",
        "    self.query = tf.keras.layers.Dense(head_size, use_bias=False)\n",
        "    self.value = tf.keras.layers.Dense(head_size, use_bias=False)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "    self.masked = masked\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "    q = self.query(inputs)\n",
        "    k = self.key(inputs)\n",
        "    v = self.value(inputs)\n",
        "\n",
        "    # print(f'Size q: {q.shape}')\n",
        "    # print(f'Size k: {k.shape}')\n",
        "    # print(f'Size v: {v.shape}')\n",
        "\n",
        "    # Calculate attention\n",
        "    attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
        "    # print(f'Size attn: {attn.shape}')\n",
        "\n",
        "    # Mask attention (remove future elements when applying the softmax)\n",
        "    if self.masked:\n",
        "      tril = tf.linalg.LinearOperatorLowerTriangular(tf.ones([seq_len, seq_len])).to_dense()\n",
        "      attn = attn + (1.0 - tril) * -1e9\n",
        "    # print(f'Attention mask: {attn}')\n",
        "    # print(f'Size masked attention: {attn.shape}')\n",
        "\n",
        "    # Apply attention\n",
        "    attn = tf.nn.softmax(attn)\n",
        "    # print(f'Size softmax attention: {attn.shape}')\n",
        "\n",
        "    # Dropout\n",
        "    attn = self.dropout(attn)\n",
        "\n",
        "    # Calculate output attention weights\n",
        "    return tf.matmul(attn, v)\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_heads, head_size, dropout=0.0):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    self.heads = [ScaledDotProductAttention(head_size, seq_length, dropout) for _ in range(num_heads)]\n",
        "    self.projection = tf.keras.layers.Dense(head_size*num_heads)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    # Apply all heads (in parallel)\n",
        "    attns = [head(inputs) for head in self.heads]\n",
        "    attns = tf.concat(attns, axis=-1)\n",
        "\n",
        "    out = self.projection(attns)\n",
        "    out = self.dropout(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_ff, head_size, dropout=0.0):\n",
        "    super(FeedForward, self).__init__()\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_ff, activation='relu')\n",
        "    self.linear = tf.keras.layers.Dense(head_size)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    out = self.dense(inputs)\n",
        "    out = self.linear(out)\n",
        "    out = self.dropout(out)\n",
        "    return out\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_embed, num_heads, d_ff, dropout=0.0):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    head_size = d_embed // num_heads\n",
        "    self.attention = MultiHeadAttention(num_heads, head_size, dropout)\n",
        "    self.ff = FeedForward(d_ff, d_embed, dropout)\n",
        "    self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    out = inputs + self.attention(self.norm1(inputs))\n",
        "    out = out + self.ff(self.norm2(out))\n",
        "    return out\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_embed, num_heads, d_ff, n_chars, seq_length,\n",
        "               dropout=0.0):\n",
        "    super(Transformer, self).__init__()\n",
        "    layers = [DecoderLayer(d_embed, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "    self.decoder_stack = tf.keras.models.Sequential(layers)\n",
        "    self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.projection = tf.keras.layers.Dense(n_chars)\n",
        "\n",
        "    # Create embedding layer for the words of the sequence\n",
        "    self.embedding = tf.keras.layers.Embedding(n_chars, d_embed)\n",
        "\n",
        "    # Create positional embedding for the sequence positions\n",
        "    self.pos_encoding = PositionalEncoding(seq_length, d_embed)\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    # Embed input\n",
        "    encoded = self.embedding(inputs)\n",
        "    position_encoded = self.pos_encoding(encoded)\n",
        "    x = encoded + position_encoded\n",
        "\n",
        "    # print(f'Size encoded: {encoded.shape}')\n",
        "    # Apply all layers\n",
        "    x = self.decoder_stack(x)\n",
        "\n",
        "    # Normalize and project the final output\n",
        "    x = self.norm(x)\n",
        "    x = self.projection(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "\n",
        "    # Unpack inputs\n",
        "    xb, yb = inputs\n",
        "\n",
        "    # Training step\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = self(xb, training=True)\n",
        "      B, T, C = logits.shape\n",
        "      logits = tf.reshape(logits, [-1, C])\n",
        "      targets = tf.reshape(yb, [-1])\n",
        "      loss = self.compute_loss(y=targets, y_pred=logits)\n",
        "\n",
        "    grads = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "    # Update metrics (includes the metric that tracks the loss)\n",
        "    for metric in self.metrics:\n",
        "      if metric.name == \"loss\":\n",
        "        metric.update_state(loss)\n",
        "      else:\n",
        "        metric.update_state(targets, logits)\n",
        "    return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      # Evaluate the model\n",
        "      seq_len = tf.shape(idx)[1]\n",
        "      logits = self(idx[:, -tf.minimum(seq_length, seq_len):])\n",
        "\n",
        "      # We create the sequence one character at the time, so focus on the last character here\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      # Sample from the distribution\n",
        "      idx_next = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "      # Add sample to the sequence\n",
        "      idx = tf.concat([idx, idx_next], axis=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "GE2pgeGy1dKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding dimension\n",
        "d_embed = 32\n",
        "\n",
        "# Transformer hyper-parameters\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "d_ff = 4*d_embed\n",
        "dropout = 0.0\n",
        "\n",
        "model = Transformer(num_layers, d_embed, num_heads, d_ff,\n",
        "                    n_chars, seq_length, dropout)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "4B-fEqkbhdNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evaluate the model once to get shapes (could also include a build method)\n",
        "for (xb, yb) in train_dataset.take(1):\n",
        "  logits = model(xb)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "uFDw_cxZd1LE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "85f1adec-ed6e-4164-c895-daa97c3f8a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-22 19:25:39.500982: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │        \u001b[38;5;34m50,432\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_8           │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │            \u001b[38;5;34m64\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_108 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m65\u001b[0m)          │         \u001b[38;5;34m2,145\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ positional_encoding             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)            │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,432</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_8           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,145</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ positional_encoding             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)            │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m54,721\u001b[0m (213.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,721</span> (213.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m54,721\u001b[0m (213.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,721</span> (213.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some text before training the model\n",
        "new_text = decoded(model.generate(idx = np.zeros((1, 1)), max_new_tokens=200)[0].numpy())\n",
        "print(new_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3QieZ3WhdIN",
        "outputId": "0876889f-8d21-4cf8-8417-21575a18aece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "GzqZsiAA-UvAnLUqQ!ZRvLAxIOLLiaGMqqZoW-VmZhcq;Lqio\n",
            "X\n",
            "NfRWhqGpAOA\n",
            "\n",
            "GLAAS!wW\n",
            "ybwYHANih,vgAhAYTgofpQ!\n",
            ".wuVihrCfdwAAEhb?N AfLii.\n",
            "V,!sUaxe\n",
            "oLiqUPvehhZ\n",
            "SIi\n",
            "b!LOjbUpp!gyih\n",
            "?\n",
            "hVdhr.AioxYfAqAHHaYTaTozP;Ehi?\n",
            "Y-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of training and validation samples\n",
        "# Each sample is a sequence of length seq_length\n",
        "n_train_samples = max(0, len(train_text) - seq_length)\n",
        "n_val_samples = max(0, len(val_text) - seq_length)\n",
        "\n",
        "# Calculate steps per epoch and validation steps\n",
        "steps_per_epoch = n_train_samples // batch_size\n",
        "validation_steps = n_val_samples // batch_size\n",
        "\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20,\n",
        "          steps_per_epoch=steps_per_epoch, validation_steps=validation_steps)"
      ],
      "metadata": {
        "id": "yPPSrBnyVfEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new text after training\n",
        "new_text = decoded(model.generate(idx = np.zeros((1, 1)), max_new_tokens=2000)[0].numpy())\n",
        "print(new_text)"
      ],
      "metadata": {
        "id": "qELDhepoqII5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c2e508d-9f8d-48b3-eab5-853a7c046ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SICINIUS:\n",
            "Which says both.\n",
            "\n",
            "Second Servingman:\n",
            "Had heeld not as good instant us Coriolanus,\n",
            "That shall noble reger makes rogligion\n",
            "As if cowards before brold is against, I have\n",
            "Doth not lave o' the strength,--well--e,\n",
            "And peaces, to the buidst in this ever fearful,\n",
            "Coriol he heard--mowen.\n",
            "'Their friends, I have the has heirs, and must not\n",
            "To trade be violer: but a strike!\n",
            "\n",
            "SICINIUS:\n",
            "This Aufidius: our fellow; whets with secation.\n",
            "\n",
            "BRUTUS:\n",
            "You make your design!\n",
            "\n",
            "BRUTUS:\n",
            "Go, your feast, you talked his poor war, continues\n",
            "To fooll's desiren, as wrong undafter'd 'Tis power\n",
            "Unloachsterel insolurses; 'twas in him.\n",
            "\n",
            "BRUTUS:\n",
            "No, let us come to 'em\n",
            "And what of peace, biture! What show he has\n",
            "Thou and fly when fell to keep out of city\n",
            "His trick abouts to state with anciest,\n",
            "Which spirited to go in blood, one diseased\n",
            "A moon ripen!\n",
            "\n",
            "SICINIUS:\n",
            "I am to Marcely in Rome: Could we preignar,\n",
            "And the way. But of this rapt with way.\n",
            "\n",
            "MENENIUS:\n",
            "Do we must not for your such.\n",
            "\n",
            "Third Servingman:\n",
            "But if I think thou stood to use our coldour's\n",
            "All most less ever of the commission it. Coune.\n",
            "You will say an army, he's lojenning face?\n",
            "\n",
            "AUFIDIUS:\n",
            "Air, take, did what's accompy not\n",
            "insiblest themsell,\n",
            "To baw the calast past me hack doing.\n",
            "\n",
            "AUFIDIUS:\n",
            "No, bight Sir; I do his pardon matter'd him.\n",
            "\n",
            "AUFIDIUS:\n",
            "Had violured,\n",
            "This constation How say to the foes of the many\n",
            "Of your surrengest?\n",
            "He did much. Ye speak his truth.\n",
            "\n",
            "SICINIUS:\n",
            "forsul before?\n",
            "\n",
            "Second Servingman:\n",
            "Do do voidue; and to said unfit, would here of him,\n",
            "And bencount our name too. Say,\n",
            "Present as I't fightleain: thou'rsh lim.\n",
            "Whone I conteness of your dhours. The name I peace\n",
            "City'd more formab have dising unto melk darchadant daccarer althood\n",
            "Dry of him to the bare: And some house, loving\n",
            "To give this flows of gods, monts;\n",
            "Who cut of them. In: service: this is to leave\n",
            "In hercuphing-whipk be nakitorable; send him as forgeds\n",
            "And thou wouldst one peace toth 'sowly head\n",
            "To the foot.' Nay, so, I had as\n",
            "den make their incuries; din my mouth\n"
          ]
        }
      ]
    }
  ]
}