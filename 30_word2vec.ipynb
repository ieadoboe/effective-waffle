{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYjcRpAPtDzc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import requests\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj5pjASMESWN"
      },
      "outputs": [],
      "source": [
        "# Loading the Shakespeare dataset from Andrej Karpathy's website\n",
        "def load_shakespeare_corpus():\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    response = requests.get(url)\n",
        "    text = response.text.lower()  # Lowercase for simplicity\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "    corpus = text.split()\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfkrmKS6EuEZ"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the Shakespeare corpus:\n",
        "corpus = load_shakespeare_corpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzfic63YE8w-",
        "outputId": "7066c891-408e-44b9-9c28-25c9f1e57f2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['first',\n",
              " 'citizen',\n",
              " 'before',\n",
              " 'we',\n",
              " 'proceed',\n",
              " 'any',\n",
              " 'further',\n",
              " 'hear',\n",
              " 'me',\n",
              " 'speak']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check how the dataset looks like\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU3YnzFCEbav"
      },
      "outputs": [],
      "source": [
        "# Word2Vec has two model architectures, continuous bag of words and skip-grams\n",
        "# Here we use skip-grams\n",
        "def generate_training_data(corpus, window_size, vocab_size):\n",
        "    word_to_id = {word: i for i, word in enumerate(sorted(set(corpus)))}\n",
        "    id_to_word = {i: word for word, i in word_to_id.items()}\n",
        "    word_ids = [word_to_id[word] for word in corpus]\n",
        "\n",
        "    contexts = []\n",
        "    targets = []\n",
        "\n",
        "    for i, target in enumerate(word_ids):\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(word_ids), i + window_size + 1)\n",
        "\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                contexts.append(word_ids[j])\n",
        "                targets.append(target)\n",
        "\n",
        "    contexts = np.array(contexts, dtype=np.int32)\n",
        "    targets = np.array(targets, dtype=np.int32)\n",
        "\n",
        "    return contexts, targets, word_to_id, id_to_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLvqroPkFPdI",
        "outputId": "6297ead2-d4c1-4fd8-be0a-1bc54a39a271"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contexts (encoded): [ 1953   936  4176   936 12339  4176  1953 12339  8513  1953]\n",
            "Targets (encoded): [ 4176  4176  1953  1953  1953   936   936   936   936 12339]\n",
            "Contexts (decoded): ['citizen', 'before', 'first', 'before', 'we', 'first', 'citizen', 'we', 'proceed', 'citizen']\n",
            "Targets (decoded): ['first', 'first', 'citizen', 'citizen', 'citizen', 'before', 'before', 'before', 'before', 'we']\n"
          ]
        }
      ],
      "source": [
        "# Generate the skip-gram dataset\n",
        "window_size = 2\n",
        "vocab_size = len(set(corpus))\n",
        "\n",
        "contexts, targets, word_to_id, id_to_word = generate_training_data(\n",
        "    corpus, window_size, vocab_size\n",
        ")\n",
        "\n",
        "# This is how our dataset looks\n",
        "print(\"Contexts (encoded):\", contexts[:10])\n",
        "print(\"Targets (encoded):\", targets[:10])\n",
        "print(\"Contexts (decoded):\", [id_to_word[i] for i in contexts[:10]])\n",
        "print(\"Targets (decoded):\", [id_to_word[i] for i in targets[:10]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGeMaujwE06n"
      },
      "outputs": [],
      "source": [
        "# Word2Vec is essentially a one-layer network using an embedding layer to\n",
        "# learn the context of context-target pairs\n",
        "def create_word2vec_model(vocab_size, embedding_dim):\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "            keras.layers.Reshape((embedding_dim,)),\n",
        "            keras.layers.Dense(vocab_size, activation=\"softmax\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEHs4xRFHPUX"
      },
      "outputs": [],
      "source": [
        "# (Training) parameters for our word2vec model\n",
        "embedding_dim = 10\n",
        "epochs = 200\n",
        "batch_size = 1024\n",
        "\n",
        "model = create_word2vec_model(vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA3ipFAtL-9Z",
        "outputId": "89efcfc2-24cc-46c6-e761-0472a0fdc73d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 8.5967\n",
            "Epoch 2/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 7.0392\n",
            "Epoch 3/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.8597\n",
            "Epoch 4/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.7861\n",
            "Epoch 5/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.7336\n",
            "Epoch 6/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.6896\n",
            "Epoch 7/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.6685\n",
            "Epoch 8/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.6381\n",
            "Epoch 9/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.6187\n",
            "Epoch 10/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.6045\n",
            "Epoch 11/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.5890\n",
            "Epoch 12/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.5752\n",
            "Epoch 13/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.5622\n",
            "Epoch 14/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.5543\n",
            "Epoch 15/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.5393\n",
            "Epoch 16/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.5290\n",
            "Epoch 17/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.5238\n",
            "Epoch 18/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.5203\n",
            "Epoch 19/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.5089\n",
            "Epoch 20/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.5014\n",
            "Epoch 21/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4943\n",
            "Epoch 22/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4872\n",
            "Epoch 23/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4872\n",
            "Epoch 24/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4825\n",
            "Epoch 25/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4745\n",
            "Epoch 26/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4703\n",
            "Epoch 27/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4687\n",
            "Epoch 28/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4630\n",
            "Epoch 29/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4534\n",
            "Epoch 30/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4552\n",
            "Epoch 31/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4542\n",
            "Epoch 32/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4484\n",
            "Epoch 33/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4424\n",
            "Epoch 34/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4405\n",
            "Epoch 35/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4379\n",
            "Epoch 36/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.4357\n",
            "Epoch 37/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4319\n",
            "Epoch 38/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4258\n",
            "Epoch 39/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4302\n",
            "Epoch 40/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4237\n",
            "Epoch 41/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4213\n",
            "Epoch 42/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4187\n",
            "Epoch 43/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4184\n",
            "Epoch 44/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4092\n",
            "Epoch 45/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4141\n",
            "Epoch 46/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4066\n",
            "Epoch 47/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.4043\n",
            "Epoch 48/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4049\n",
            "Epoch 49/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.4034\n",
            "Epoch 50/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3963\n",
            "Epoch 51/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3973\n",
            "Epoch 52/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3971\n",
            "Epoch 53/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3994\n",
            "Epoch 54/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3939\n",
            "Epoch 55/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3916\n",
            "Epoch 56/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3901\n",
            "Epoch 57/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3903\n",
            "Epoch 58/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3907\n",
            "Epoch 59/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3867\n",
            "Epoch 60/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3825\n",
            "Epoch 61/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3827\n",
            "Epoch 62/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3815\n",
            "Epoch 63/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3828\n",
            "Epoch 64/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3808\n",
            "Epoch 65/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3804\n",
            "Epoch 66/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3784\n",
            "Epoch 67/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3771\n",
            "Epoch 68/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3816\n",
            "Epoch 69/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3709\n",
            "Epoch 70/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3743\n",
            "Epoch 71/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3697\n",
            "Epoch 72/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3678\n",
            "Epoch 73/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3704\n",
            "Epoch 74/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3659\n",
            "Epoch 75/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3698\n",
            "Epoch 76/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3683\n",
            "Epoch 77/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3656\n",
            "Epoch 78/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3638\n",
            "Epoch 79/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3636\n",
            "Epoch 80/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3590\n",
            "Epoch 81/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3617\n",
            "Epoch 82/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3645\n",
            "Epoch 83/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3568\n",
            "Epoch 84/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3575\n",
            "Epoch 85/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3593\n",
            "Epoch 86/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3547\n",
            "Epoch 87/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3588\n",
            "Epoch 88/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3573\n",
            "Epoch 89/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3536\n",
            "Epoch 90/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3501\n",
            "Epoch 91/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3563\n",
            "Epoch 92/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3552\n",
            "Epoch 93/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3514\n",
            "Epoch 94/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3506\n",
            "Epoch 95/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3542\n",
            "Epoch 96/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3553\n",
            "Epoch 97/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3466\n",
            "Epoch 98/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3531\n",
            "Epoch 99/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3538\n",
            "Epoch 100/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3506\n",
            "Epoch 101/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3486\n",
            "Epoch 102/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3528\n",
            "Epoch 103/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3478\n",
            "Epoch 104/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3482\n",
            "Epoch 105/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3436\n",
            "Epoch 106/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3446\n",
            "Epoch 107/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3427\n",
            "Epoch 108/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3437\n",
            "Epoch 109/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3376\n",
            "Epoch 110/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3453\n",
            "Epoch 111/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3426\n",
            "Epoch 112/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3444\n",
            "Epoch 113/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3387\n",
            "Epoch 114/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3418\n",
            "Epoch 115/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3408\n",
            "Epoch 116/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.3369\n",
            "Epoch 117/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3407\n",
            "Epoch 118/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3376\n",
            "Epoch 119/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3425\n",
            "Epoch 120/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3325\n",
            "Epoch 121/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3349\n",
            "Epoch 122/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3408\n",
            "Epoch 123/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3390\n",
            "Epoch 124/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3326\n",
            "Epoch 125/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3386\n",
            "Epoch 126/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3397\n",
            "Epoch 127/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3370\n",
            "Epoch 128/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3399\n",
            "Epoch 129/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3391\n",
            "Epoch 130/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3351\n",
            "Epoch 131/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3381\n",
            "Epoch 132/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3335\n",
            "Epoch 133/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3330\n",
            "Epoch 134/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3391\n",
            "Epoch 135/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3322\n",
            "Epoch 136/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3341\n",
            "Epoch 137/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3300\n",
            "Epoch 138/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3373\n",
            "Epoch 139/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3353\n",
            "Epoch 140/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3263\n",
            "Epoch 141/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3305\n",
            "Epoch 142/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3311\n",
            "Epoch 143/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3324\n",
            "Epoch 144/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3311\n",
            "Epoch 145/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3295\n",
            "Epoch 146/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3298\n",
            "Epoch 147/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3329\n",
            "Epoch 148/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3285\n",
            "Epoch 149/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3292\n",
            "Epoch 150/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3280\n",
            "Epoch 151/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3309\n",
            "Epoch 152/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3229\n",
            "Epoch 153/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3299\n",
            "Epoch 154/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3281\n",
            "Epoch 155/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3302\n",
            "Epoch 156/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3296\n",
            "Epoch 157/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3217\n",
            "Epoch 158/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3267\n",
            "Epoch 159/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3281\n",
            "Epoch 160/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3272\n",
            "Epoch 161/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3231\n",
            "Epoch 162/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3248\n",
            "Epoch 163/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3254\n",
            "Epoch 164/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3274\n",
            "Epoch 165/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3265\n",
            "Epoch 166/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3250\n",
            "Epoch 167/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3210\n",
            "Epoch 168/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3265\n",
            "Epoch 169/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3275\n",
            "Epoch 170/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3203\n",
            "Epoch 171/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3285\n",
            "Epoch 172/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3229\n",
            "Epoch 173/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3305\n",
            "Epoch 174/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3295\n",
            "Epoch 175/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3229\n",
            "Epoch 176/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3199\n",
            "Epoch 177/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3190\n",
            "Epoch 178/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3175\n",
            "Epoch 179/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3188\n",
            "Epoch 180/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3173\n",
            "Epoch 181/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3227\n",
            "Epoch 182/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3180\n",
            "Epoch 183/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3215\n",
            "Epoch 184/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3217\n",
            "Epoch 185/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3217\n",
            "Epoch 186/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3258\n",
            "Epoch 187/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3208\n",
            "Epoch 188/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.3142\n",
            "Epoch 189/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3189\n",
            "Epoch 190/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3191\n",
            "Epoch 191/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3186\n",
            "Epoch 192/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3226\n",
            "Epoch 193/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3215\n",
            "Epoch 194/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3184\n",
            "Epoch 195/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3147\n",
            "Epoch 196/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.3210\n",
            "Epoch 197/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3168\n",
            "Epoch 198/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3205\n",
            "Epoch 199/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3180\n",
            "Epoch 200/200\n",
            "\u001b[1m792/792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 6.3151\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c0e7dd56cd0>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(contexts, targets, epochs=epochs, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bjNmyMPMjnu"
      },
      "outputs": [],
      "source": [
        "# Get word embeddings by extracting the embedding matrix\n",
        "embeddings = model.layers[0].get_weights()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K0vhMaIurgS"
      },
      "outputs": [],
      "source": [
        "# Now that we have trained the embedding model, we can check the most similar words\n",
        "# for a given input word\n",
        "def get_most_similar(word, embeddings, word_to_id, id_to_word, top_n=5):\n",
        "    word_id = word_to_id.get(word)\n",
        "    if word_id is None:\n",
        "        return \"Word not in vocabulary.\"\n",
        "\n",
        "    word_embedding = embeddings[word_id]\n",
        "    similarities = np.dot(embeddings, word_embedding) / (\n",
        "        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(word_embedding)\n",
        "    )\n",
        "    sorted_ids = np.argsort(similarities)[::-1][1 : top_n + 1]\n",
        "    return [id_to_word[i] for i in sorted_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wMXcGSrMYOD"
      },
      "outputs": [],
      "source": [
        "# Similarly, we can check how similar two given words are (highest similarity possible is 1)\n",
        "def word_similarity(word1, word2, embeddings, word_to_id):\n",
        "    word1_id = word_to_id.get(word1)\n",
        "    word2_id = word_to_id.get(word2)\n",
        "\n",
        "    if word1_id is None or word2_id is None:\n",
        "        return \"One or both words not in vocabulary.\"\n",
        "\n",
        "    word1_embedding = embeddings[word1_id]\n",
        "    word2_embedding = embeddings[word2_id]\n",
        "\n",
        "    similarity = np.dot(word1_embedding, word2_embedding) / (\n",
        "        np.linalg.norm(word1_embedding) * np.linalg.norm(word2_embedding)\n",
        "    )\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpvWrRiCx9WT",
        "outputId": "617f1ced-a717-4a61-a166-37e20c5e9aac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding for 'king': [ 0.53898597  1.6234968  -0.42619652 -1.0969392  -0.34923983  1.3874028\n",
            " -1.3035297   0.60685015 -0.7991151  -0.16748612]\n"
          ]
        }
      ],
      "source": [
        "# Example: Get the embedding for a given word\n",
        "sample_word = \"king\"\n",
        "\n",
        "sample_word_id = word_to_id[sample_word]\n",
        "sample_embedding = embeddings[sample_word_id]\n",
        "print(f\"Embedding for '{sample_word}': {sample_embedding}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxESN8vitKji",
        "outputId": "bcfc70a0-751f-4d7a-98a6-f8f1b651baee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most similar words to 'king': ['exeter', 'northumberland', 'percy', 'warwick', 'parkcorner']\n",
            "Most similar words to 'love': ['assistant', 'friend', 'beggar', 'pronounced', 'tutord']\n",
            "Most similar words to 'romeo': ['juliet', 'godo', 'unsatisfied', 'villain', 'mercutio']\n",
            "Most similar words to 'juliet': ['romeo', 'pilgrim', 'godo', 'unsatisfied', 'mercutio']\n",
            "Most similar words to 'caesar': ['hidest', 'smutchd', 'commitst', 'goddess', 'tumult']\n"
          ]
        }
      ],
      "source": [
        "# Example of getting the most similar words.\n",
        "print(\n",
        "    f\"Most similar words to 'king': {get_most_similar('king', embeddings, word_to_id, id_to_word)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Most similar words to 'love': {get_most_similar('love', embeddings, word_to_id, id_to_word)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Most similar words to 'romeo': {get_most_similar('romeo', embeddings, word_to_id, id_to_word)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Most similar words to 'juliet': {get_most_similar('juliet', embeddings, word_to_id, id_to_word)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Most similar words to 'caesar': {get_most_similar('caesar', embeddings, word_to_id, id_to_word)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWsZSkYKuRbP",
        "outputId": "afbe306c-44ae-4924-b7b2-db44d425ae2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity of ('king' - 'man' + 'woman') to 'queen': 0.7571030259132385\n"
          ]
        }
      ],
      "source": [
        "# Word arithmetic example: \"king\" - \"man\" + \"woman\"\n",
        "if (\n",
        "    \"king\" in word_to_id\n",
        "    and \"man\" in word_to_id\n",
        "    and \"woman\" in word_to_id\n",
        "    and \"queen\" in word_to_id\n",
        "):\n",
        "    king_embedding = embeddings[word_to_id[\"king\"]]\n",
        "    man_embedding = embeddings[word_to_id[\"man\"]]\n",
        "    woman_embedding = embeddings[word_to_id[\"woman\"]]\n",
        "    queen_embedding = embeddings[word_to_id[\"queen\"]]\n",
        "    result_embedding = king_embedding - man_embedding + woman_embedding\n",
        "    similarity_to_queen = np.dot(result_embedding, queen_embedding) / (\n",
        "        np.linalg.norm(result_embedding) * np.linalg.norm(queen_embedding)\n",
        "    )\n",
        "    print(f\"Similarity of ('king' - 'man' + 'woman') to 'queen': {similarity_to_queen}\")\n",
        "else:\n",
        "    print(\n",
        "        \"One or more of the words 'king', 'man', 'woman', or 'queen' not found in vocabulary.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-DGh6STvEXp",
        "outputId": "a5b6bdf4-3ca9-4f09-d79c-f860a97d25a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity of ('king' - 'man' + 'woman') to 'peasant': 0.12192746251821518\n"
          ]
        }
      ],
      "source": [
        "# Word arithmetic example: \"king\" - \"man\" + \"woman\"\n",
        "if (\n",
        "    \"king\" in word_to_id\n",
        "    and \"man\" in word_to_id\n",
        "    and \"woman\" in word_to_id\n",
        "    and \"peasant\" in word_to_id\n",
        "):\n",
        "    king_embedding = embeddings[word_to_id[\"king\"]]\n",
        "    man_embedding = embeddings[word_to_id[\"man\"]]\n",
        "    woman_embedding = embeddings[word_to_id[\"woman\"]]\n",
        "    peasant_embedding = embeddings[word_to_id[\"peasant\"]]\n",
        "    result_embedding = king_embedding - man_embedding + woman_embedding\n",
        "    similarity_to_peasant = np.dot(result_embedding, peasant_embedding) / (\n",
        "        np.linalg.norm(result_embedding) * np.linalg.norm(peasant_embedding)\n",
        "    )\n",
        "    print(\n",
        "        f\"Similarity of ('king' - 'man' + 'woman') to 'peasant': {similarity_to_peasant}\"\n",
        "    )\n",
        "else:\n",
        "    print(\n",
        "        \"One or more of the words 'king', 'man', 'woman', or 'peasant' not found in vocabulary.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaMH_swzO-hb",
        "outputId": "9274babc-ef07-4a4b-c5ba-c08fac94d004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between 'romeo' and 'juliet': 0.9851592183113098\n"
          ]
        }
      ],
      "source": [
        "# A few more examples (just make sure both words are in the training corpus)\n",
        "word1 = \"romeo\"\n",
        "word2 = \"juliet\"\n",
        "\n",
        "assert (\n",
        "    word1 in word_to_id and word2 in word_to_id\n",
        "), \"One or both words not in vocabulary.\"\n",
        "\n",
        "word1_embedding = embeddings[word_to_id[word1]]\n",
        "word2_embedding = embeddings[word_to_id[word2]]\n",
        "\n",
        "# Similarity\n",
        "similarity = np.dot(word1_embedding, word2_embedding) / (\n",
        "    np.linalg.norm(word1_embedding) * np.linalg.norm(word2_embedding)\n",
        ")\n",
        "print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
