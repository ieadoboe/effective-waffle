{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer - Decoder only\n",
    "\n",
    "Coding a transformer from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder-Only Models (e.g., GPT, LLaMA)\n",
    "**Best for generative tasks:**\n",
    "- Text generation and completion\n",
    "- Creative writing and storytelling  \n",
    "- Conversational AI and chatbots\n",
    "- Code generation\n",
    "- Open-ended question answering\n",
    "- Text summarization (though encoder-decoder can be better for longer texts)\n",
    "- Language modeling tasks\n",
    "\n",
    "**Why they excel here:** They're trained to predict the next token in a sequence, making them naturally suited for generating coherent text step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Shakespeares work from Andrej Karpathy's website\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", url)\n",
    "\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first few characters\n",
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in vocabulary: 65\n",
      "Total length of text dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(shakespeare_text))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "# How many number of distinct characters has the vocabulary:\n",
    "tokens_len = len(vocab)\n",
    "print(f\"Number of tokens in vocabulary: {tokens_len}\")\n",
    "\n",
    "# How many characters has the dataset:\n",
    "text_length = len(shakespeare_text)\n",
    "print(f\"Total length of text dataset: {text_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Architecture Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "(1, 1, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 10\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(tokens_len, embedding_dim))\n",
    "\n",
    "input_array = np.random.randint(tokens_len, size=(1, 1))\n",
    "model.compile(\"rmsprop\", \"sparse_categorical_crossentropy\")\n",
    "\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array.shape)\n",
    "\n",
    "model.summary()\n",
    "# (1, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 ... 45  8  0]\n",
      "1115394\n"
     ]
    }
   ],
   "source": [
    "def text_to_indices(text):\n",
    "    return [char_to_idx[char] for char in text]\n",
    "\n",
    "\n",
    "text_as_indices = text_to_indices(shakespeare_text)\n",
    "print(np.array(text_as_indices))\n",
    "print(len(text_to_indices(shakespeare_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def indices_to_text(indices):\n",
    "    return \"\".join([idx_to_char[idx] for idx in indices])\n",
    "\n",
    "\n",
    "indices_to_text([np.random.randint(65)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(text, seq_length=100, batch_size=64):\n",
    "    text_as_indices = text_to_indices(text)\n",
    "    total_seq = len(text_as_indices) - seq_length\n",
    "\n",
    "    # Convert to numpy array for efficient indexing\n",
    "    text_as_indices_np = np.array(text_as_indices)\n",
    "\n",
    "    # Create input and target sequences\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    for i in range(\n",
    "        0, total_seq, seq_length // 4\n",
    "    ):  # Use stride for more efficient data usage\n",
    "        if i + seq_length >= total_seq:\n",
    "            break\n",
    "        input_seqs.append(text_as_indices_np[i : i + seq_length])\n",
    "        target_seqs.append(text_as_indices_np[i + 1 : i + seq_length + 1])\n",
    "\n",
    "    # Convert to TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_seqs, target_seqs))\n",
    "\n",
    "    # Batch and shuffle\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "    return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = create_training_data(shakespeare_text, seq_length=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PositionalEncoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}$$\n",
    "$$PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_seq_len, embedding_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_seq_len = (\n",
    "            max_seq_len  # maximum sequence length that the model can handle\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Create the positional encodings\n",
    "        position = np.arange(max_seq_len)[:, np.newaxis]\n",
    "        div_term = np.exp(\n",
    "            np.arange(0, embedding_dim, 2) * -(np.log(10000.0) / embedding_dim)\n",
    "        )\n",
    "        pe = np.zeros((max_seq_len, embedding_dim))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension e.g. (max_seq_len,embedding_dim) -> (1,max_seq_len,embedding_dim)\n",
    "        self.pe = tf.constant(pe, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get the sequence length from the input shape\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "\n",
    "        # Slice the positional encoding to match the sequence length of the input\n",
    "        positional_encoding = self.pe[:seq_len, :]\n",
    "\n",
    "        # Add the positional encoding to the input embeddings\n",
    "        return inputs + positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10  # Embedding dimension\n",
    "max_len = 50  # Maximum sequence length\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(tokens_len, embedding_dim),\n",
    "        PositionalEncoding(max_seq_len=max_len, embedding_dim=embedding_dim),\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_array = np.random.randint(tokens_len, size=(1, 10))\n",
    "output_array = model(input_array)\n",
    "\n",
    "print(output_array.shape)  # Should be (1, 10, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9 11 10 32 14 22 32 10 25 14]]\n",
      "tf.Tensor(\n",
      "[[[-0.02662571  1.0388145   0.00299688  1.0106592   0.00604727\n",
      "    1.0183191   0.01258196  0.96467817  0.03042505  0.95536995]\n",
      "  [ 0.87552226  0.53942084  0.1771199   1.0033325   0.01393058\n",
      "    1.0387336   0.04905548  0.9666929  -0.00980983  0.9950829 ]\n",
      "  [ 0.8984159  -0.4359485   0.29020625  0.97148544  0.01529079\n",
      "    1.008023   -0.00415967  1.0042366   0.00872468  0.9874757 ]\n",
      "  [ 0.14643323 -0.98223406  0.40964356  0.9035516   0.05341332\n",
      "    1.0009195   0.02402867  1.0336223   0.0194952   0.98353183]\n",
      "  [-0.7200725  -0.69473666  0.5992097   0.803593    0.14542888\n",
      "    0.9633081   0.02189     1.0243728   0.01077902  0.9949299 ]\n",
      "  [-0.9103297   0.25798908  0.7396613   0.6770468   0.11435457\n",
      "    0.9930027   0.03623539  0.9632031   0.00500926  0.96322125]\n",
      "  [-0.27410224  0.9679287   0.7658485   0.5953945   0.1282713\n",
      "    0.99242175  0.03596989  1.0334083   0.02138806  0.98352647]\n",
      "  [ 0.6461051   0.7341006   0.87395203  0.46648026  0.14000161\n",
      "    0.9938661   0.01574217  1.00388     0.01187945  0.9874667 ]\n",
      "  [ 1.0007372  -0.1699569   0.97141415  0.2746589   0.18484461\n",
      "    0.99885666 -0.01432005  1.0480671  -0.01996904  1.0495108 ]\n",
      "  [ 0.4488485  -0.9522233   0.9964655   0.14179447  0.26927143\n",
      "    0.9429064   0.04178837  1.0238577   0.01393378  0.9949169 ]]], shape=(1, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(input_array[:3])\n",
    "print(output_array[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m)            │           \u001b[38;5;34m650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding             │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScaledDotProductAttention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ScaledDotProductAttention](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        # dot product attention\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (bs, q_len, k_len)\n",
    "\n",
    "        # scale dot product\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # apply mask when necessary\n",
    "        if mask is not None:\n",
    "            # adding very large negative values\n",
    "            # so they go to zero after softmax\n",
    "            scaled_attention_logits += mask * -1e9\n",
    "\n",
    "        # apply softmax to attention weights (scores)\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "        # multiply by V (values)\n",
    "        out = tf.matmul(attention_weights, v)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHeadAttention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MultiHeadAttention](image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        assert (\n",
    "            embedding_dim % num_heads == 0\n",
    "        ), \"embedding_dim must be divisible by num_heads\"\n",
    "        self.depth = embedding_dim // num_heads  # depth per head\n",
    "\n",
    "        # linear projection layers\n",
    "        self.wq = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.wk = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.wv = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "        # output projection\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        # linear projections\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        # reshaping q, k, v\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = self.attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention, (batch_size, -1, self.embedding_dim)\n",
    "        )\n",
    "\n",
    "        out = self.dense(concat_attention)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Shape:  (2, 100, 64)\n",
      "Key Shape:  (2, 100, 64)\n",
      "Value Shape:  (2, 100, 64)\n",
      "Output Shape:  (2, 100, 64)\n",
      "Attention Weights Shape:  (2, 8, 100, 100)\n",
      "✅ MultiHeadAttention test passed!\n"
     ]
    }
   ],
   "source": [
    "# Define dummy parameters\n",
    "num_heads = 8\n",
    "embedding_dim = 64\n",
    "batch_size = 2\n",
    "seq_length = 100  # Sequence length\n",
    "\n",
    "# Instantiate MultiHeadAttention\n",
    "mha = MultiHeadAttention(num_heads=num_heads, embedding_dim=embedding_dim)\n",
    "\n",
    "# Create dummy input tensors\n",
    "q = tf.random.uniform((batch_size, seq_length, embedding_dim))  # Queries\n",
    "k = tf.random.uniform((batch_size, seq_length, embedding_dim))  # Keys\n",
    "v = tf.random.uniform((batch_size, seq_length, embedding_dim))  # Values\n",
    "\n",
    "# Run the attention layer\n",
    "output, attention_weights = mha(q, k, v)\n",
    "\n",
    "# Print output shapes\n",
    "print(\"Query Shape: \", q.shape)\n",
    "print(\"Key Shape: \", k.shape)\n",
    "print(\"Value Shape: \", v.shape)\n",
    "print(\"Output Shape: \", output.shape)\n",
    "print(\"Attention Weights Shape: \", attention_weights.shape)\n",
    "\n",
    "# Assertions to check correctness\n",
    "assert output.shape == (\n",
    "    batch_size,\n",
    "    seq_length,\n",
    "    embedding_dim,\n",
    "), \"Output shape is incorrect\"\n",
    "assert attention_weights.shape == (\n",
    "    batch_size,\n",
    "    num_heads,\n",
    "    seq_length,\n",
    "    seq_length,\n",
    "), \"Attention weights shape is incorrect\"\n",
    "print(\"✅ MultiHeadAttention test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Network\n",
    "\n",
    "$$\\text{FFN(x)} = \\text{max}(0,~ xW_1 + b_1)W_2 + b_2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, hidden_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # hidden_dim (dff) - feed forward network hidden\n",
    "        # layer dimension a.k.a inner layer dimensionality\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.dense1(inputs)\n",
    "\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, embedding_dim, num_heads, hidden_dim, dropout_rate=0.1, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.mha = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "\n",
    "        # multi-head attention\n",
    "        attention_output, _ = self.mha(v=x, k=x, q=x, mask=mask)\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        out1 = self.layer_norm1(x + attention_output)\n",
    "\n",
    "        # feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layer_norm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, embedding_dim, num_heads, hidden_dim, dropout_rate=0.1, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.cross_attention = MultiHeadAttention(embedding_dim, num_heads)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(\n",
    "        self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None\n",
    "    ):\n",
    "        # Self attention with look-ahead mask\n",
    "        self_attn_output, _ = self.self_attention(q=x, v=x, k=x, mask=look_ahead_mask)\n",
    "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
    "        out1 = self.layer_norm1(x + self_attn_output)\n",
    "\n",
    "        # Cross attention with encoder output\n",
    "        cross_attn_output, _ = self.cross_attention(\n",
    "            q=out1, v=enc_output, k=enc_output, mask=padding_mask\n",
    "        )\n",
    "        cross_attn_output = self.dropout2(cross_attn_output, training=training)\n",
    "        out2 = self.layer_norm2(out1 + cross_attn_output)\n",
    "\n",
    "        # Feed forward\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layer_norm3(out2 + ffn_output)\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The Encoder stacks multiple encoder layer to create the full encoder. It includes the Embedding layer and Positional Encoding layer as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        num_heads,\n",
    "        input_vocab_size,\n",
    "        max_seq_len,\n",
    "        dropout_rate=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, embedding_dim)\n",
    "\n",
    "        self.encoding_layers = [\n",
    "            EncoderLayer(\n",
    "                embedding_dim, num_heads, hidden_dim, dropout_rate=dropout_rate\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoding_layers[i](x, training=training, mask=mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        num_heads,\n",
    "        target_vocab_size,\n",
    "        max_seq_len,\n",
    "        dropout_rate=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, embedding_dim)\n",
    "\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(embedding_dim, num_heads, hidden_dim, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(\n",
    "        self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None\n",
    "    ):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # Ensure correct mask shape\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "        # decoder layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.decoder_layers[i](\n",
    "                x,\n",
    "                enc_output,\n",
    "                training=training,\n",
    "                look_ahead_mask=look_ahead_mask,\n",
    "                padding_mask=padding_mask,\n",
    "            )\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create look-ahead mask for decoder\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        num_heads,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        pe_input,\n",
    "        pe_target,\n",
    "        dropout_rate=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            num_layers,\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_heads,\n",
    "            input_vocab_size,\n",
    "            pe_input,\n",
    "            dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            num_layers,\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_heads,\n",
    "            target_vocab_size,\n",
    "            pe_target,\n",
    "            dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inp,\n",
    "        tar,\n",
    "        enc_padding_mask=None,\n",
    "        look_ahead_mask=None,\n",
    "        dec_padding_mask=None,\n",
    "        training=False,\n",
    "    ):\n",
    "\n",
    "        # Encoder output\n",
    "        enc_output = self.encoder(x=inp, training=training, mask=enc_padding_mask)\n",
    "\n",
    "        # Decoder output\n",
    "        dec_output = self.decoder(\n",
    "            x=tar,\n",
    "            enc_output=enc_output,\n",
    "            training=training,\n",
    "            look_ahead_mask=look_ahead_mask,\n",
    "            padding_mask=dec_padding_mask,\n",
    "        )\n",
    "\n",
    "        # Final output\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">404,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">537,472</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_134 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,385</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_2 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │       \u001b[38;5;34m404,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_2 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │       \u001b[38;5;34m537,472\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_134 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m65\u001b[0m)            │         \u001b[38;5;34m8,385\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">950,721</span> (3.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m950,721\u001b[0m (3.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">950,721</span> (3.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m950,721\u001b[0m (3.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Architecture parameters\n",
    "num_layers = 2\n",
    "embedding_dim = 128\n",
    "hidden_dim = 512\n",
    "num_heads = 8\n",
    "max_seq_len = 100\n",
    "\n",
    "# Training parameters\n",
    "dropout_rate = 0.1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create model\n",
    "model = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    input_vocab_size=tokens_len,\n",
    "    target_vocab_size=tokens_len,\n",
    "    pe_input=max_seq_len,\n",
    "    pe_target=max_seq_len,\n",
    "    dropout_rate=dropout_rate,\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Sample input\n",
    "inp = tf.random.uniform((1, 10), maxval=tokens_len, dtype=tf.int32)\n",
    "tar = tf.random.uniform((1, 10), maxval=tokens_len, dtype=tf.int32)\n",
    "\n",
    "model(inp, tar)\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate_text(model, start_string, num_generate=1000, temperature=1.0):\n",
    "    # Convert start string to indices\n",
    "    input_indices = text_to_indices(start_string)\n",
    "    input_tensor = tf.expand_dims(input_indices, 0)\n",
    "\n",
    "    # Empty result string\n",
    "    result = start_string\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        # Create look-ahead mask\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(input_tensor)[1])\n",
    "\n",
    "        # Call model with keyword arguments\n",
    "        output = model(\n",
    "            inp=input_tensor,\n",
    "            tar=input_tensor,\n",
    "            training=False,\n",
    "            enc_padding_mask=None,\n",
    "            look_ahead_mask=look_ahead_mask,\n",
    "            dec_padding_mask=None,\n",
    "        )\n",
    "\n",
    "        # Select the last token from the output\n",
    "        output = output[:, -1, :]  # (batch_size, vocab_size)\n",
    "\n",
    "        # Apply temperature\n",
    "        if temperature != 1.0:\n",
    "            output = output / temperature\n",
    "\n",
    "        # Sample from the output distribution\n",
    "        predicted_id = tf.random.categorical(output, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Concatenate the predicted character to the output text\n",
    "        result += idx_to_char[predicted_id]\n",
    "\n",
    "        # Update the input tensor to the decoder\n",
    "        input_indices.append(predicted_id)\n",
    "        input_tensor = tf.expand_dims(input_indices[-max_seq_len:], 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Text generated before training -----\n",
      "\n",
      "ROMEO: pL&fLfezfs!R rHOb:e&bwCIaqvNQQoH!NkLzj?cgfdwIfozUwujQjYvRBHQgf\n",
      "&FUplvoTjzbFAjwqz&ov;duqMBXx&aN-o'zo&R?vj&wP;vccij3dQrUd&SCWQuoC\n",
      "v:jPbUOwBwqN-WAvmFv?\n",
      "CBwKr&B3&J?,&?a&&&&jUoB&?sQ&vbBUP&jwaBjauvjqQtPB Scz & ;&;ccRrYBclQz\n",
      "?Q\n",
      "tcc&c;L&;wUgP3s&  loAVpc'fjr&\n"
     ]
    }
   ],
   "source": [
    "# Generate text before training\n",
    "print(\"\\n----- Text generated before training -----\\n\")\n",
    "start_string = \"ROMEO: \"\n",
    "generated_text = generate_text(model, start_string, num_generate=250, temperature=1.0)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "# Loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Use built-in metrics instead of custom class\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    # Create look-ahead mask\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar_inp)[1])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(\n",
    "            inp=inp,\n",
    "            tar=tar_inp,\n",
    "            training=True,\n",
    "            enc_padding_mask=None,\n",
    "            look_ahead_mask=look_ahead_mask,\n",
    "            dec_padding_mask=None,\n",
    "        )\n",
    "\n",
    "        # Apply mask for padding if needed\n",
    "        mask = tf.math.logical_not(tf.math.equal(tar_real, 0))\n",
    "        loss = loss_object(\n",
    "            tar_real, predictions, sample_weight=tf.cast(mask, dtype=tf.float32)\n",
    "        )\n",
    "\n",
    "    # Get gradients and update weights\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Update metrics\n",
    "    train_loss.update_state(loss)\n",
    "    train_accuracy.update_state(\n",
    "        tar_real, predictions, sample_weight=tf.cast(mask, dtype=tf.float32)\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    # Reset metrics at start of each epoch\n",
    "    train_loss.reset_state()  # Changed from reset_states() to reset_state()\n",
    "    train_accuracy.reset_state()  # Changed from reset_states() to reset_state()\n",
    "\n",
    "    for batch, (inp, tar) in enumerate(dataset):\n",
    "        # The train_step now updates metrics internally\n",
    "        loss = train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} \"\n",
    "                f\"Accuracy {train_accuracy.result():.4f}\"\n",
    "            )\n",
    "\n",
    "    # Print epoch results\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1} Loss {train_loss.result():.4f} \"\n",
    "        f\"Accuracy {train_accuracy.result():.4f}\"\n",
    "    )\n",
    "    print(f\"Time taken for 1 epoch: {time.time() - start:.2f} secs\")\n",
    "\n",
    "    # Generate text every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"\\n----- Text generated after epoch {epoch + 1} -----\\n\")\n",
    "        start_string = \"ROMEO: \"\n",
    "        generated_text = generate_text(\n",
    "            model, start_string, num_generate=250, temperature=0.8\n",
    "        )\n",
    "        print(generated_text)\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Save the model\n",
    "model.save_weights(\"shakespeare_transformer/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final text\n",
    "print(\"\\n----- Text generated after training -----\\n\")\n",
    "start_string = \"ROMEO: \"\n",
    "generated_text = generate_text(model, start_string, num_generate=500, temperature=0.7)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the encoder-decoder architecture is not a good architecture for text generation tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
