{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccesary libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the problem to myself\n",
    "\n",
    "### 1. Function\n",
    "\n",
    "$f(x) = sin2x + asin(4x)$\n",
    "\n",
    "The goal is to minimize this function (find where it outputs the smallest value). In machine learning, this usually means adjusting parameters to reduce the loss.\n",
    "\n",
    "### 2. Gradient Descent\n",
    "\n",
    "$\\theta = \\theta - \\eta  \\cdot \\nabla f(\\theta)$\n",
    "\n",
    "where $\\theta$ is weights and bias and all parameters\n",
    "\n",
    "In ML, this would be updating the weights, $w = w- \\eta  \\cdot \\nabla L(w)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def f(x, a=0.499):\n",
    "    return np.sin(2 * x) + a * np.sin(4 * x)\n",
    "\n",
    "\n",
    "def f_prime(x, a=0.499):\n",
    "    return 2 * np.cos(2 * x) + (a * 4) * np.cos(4 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Implementing an optimizer means writing a Python function (or class) that:\n",
    "\n",
    "1. Takes an initial guess for the parameters.\n",
    "2. Computes the gradient of the given function at each step.\n",
    "3. Updates the parameters using the gradient descent rule.\n",
    "4. Stops after a set number of steps or when the updates become small enough (converges).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.02994003426228467\n",
      "-0.06972122169230724\n",
      "-0.10871590025282594\n",
      "-0.14634726718029928\n",
      "-0.1821332686723779\n",
      "-0.21571416593916687\n",
      "-0.24686094115139826\n",
      "-0.27546691378698396\n",
      "-0.3015281748313631\n",
      "-0.32511910395041105\n"
     ]
    }
   ],
   "source": [
    "initial_guess = weight = 0.01\n",
    "max_iters = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(max_iters):\n",
    "    grad = f_prime(weight) # Compute gradient\n",
    "    weight -= learning_rate * grad\n",
    "    print(weight)\n",
    "    if abs(grad) < 10e-13:\n",
    "        print(f\"Breaks on the {i+1}th iteration\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rates to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of test learning rates\n",
    "learning_rates = [0.1, 0.01, 0.001] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_optimizer(f, f_prime, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        initial_guess = 0.75\n",
    "    weight = initial_guess\n",
    "    pred_1 = f(weight)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        grad = f_prime(weight) # Compute gradient\n",
    "        weight -= learning_rate * grad # update weights/parameters\n",
    "        pred_2 = f(weight)\n",
    "        if abs(pred_2 - pred_1) < 1e-13: # Convergence tolerance\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"Min x (weight):\",weight)\n",
    "    print(\"At x (weight), y =\",f(weight))\n",
    "    print(\"Gradient at x (weight):\",f_prime(weight))\n",
    "    \n",
    "    return weight, f(weight), f_prime(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 151th iteration\n",
      "Min x (weight): 2.617801150048571\n",
      "At x (weight), y = -1.29817227299419\n",
      "Gradient at x (weight): 5.4001912941359365e-09\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 1574th iteration\n",
      "Min x (weight): 2.6178008915128057\n",
      "At x (weight), y = -1.2981722729938445\n",
      "Gradient at x (weight): -2.6772025890631213e-06\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Min x (weight): 1.5610820325228227\n",
      "At x (weight), y = 4.251390869903712e-05\n",
      "Gradient at x (weight): -0.005129212682828843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    vanilla_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Momentum optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_momentum_optimizer(f, f_prime, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        initial_guess = 0.75\n",
    "    weight = initial_guess\n",
    "    pred_1 = f(weight)\n",
    "    beta = 0.9 # set the friction\n",
    "    momentum = 0 # initialize velocity\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        grad = f_prime(weight) # Compute gradient\n",
    "        momentum = beta * momentum - learning_rate * grad\n",
    "        weight += momentum # update weights/parameters\n",
    "        pred_2 = f(weight)\n",
    "        if abs(pred_2 - pred_1) < 1e-13: # Convergence tolerance\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"Min x (weight):\",weight)\n",
    "    print(\"At x (weight), y =\",f(weight))\n",
    "    print(\"Gradient at x (weight):\",f_prime(weight))\n",
    "    \n",
    "    return weight, f(weight), f_prime(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 272th iteration\n",
      "Min x (weight): 2.617801423786491\n",
      "At x (weight), y = -1.2981722729938\n",
      "Gradient at x (weight): 2.845743499957365e-06\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 248th iteration\n",
      "Min x (weight): 2.6177986291129005\n",
      "At x (weight), y = -1.2981722729612328\n",
      "Gradient at x (weight): -2.6152142039492787e-05\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Converged on 1621th iteration\n",
      "Min x (weight): 2.6178012818908893\n",
      "At x (weight), y = -1.2981722729940992\n",
      "Gradient at x (weight): 1.3734145909438666e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    vanilla_momentum_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov accelerated gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov_accelerated_optimizer(f, f_prime, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        initial_guess = 0.75\n",
    "    weight = initial_guess\n",
    "    pred_1 = f(weight)\n",
    "    beta = 0.9 # set the friction\n",
    "    momentum = 0 # initialize velocity\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        grad = f_prime(weight + beta * momentum) # Compute gradient\n",
    "        momentum = beta * momentum - learning_rate * grad\n",
    "        weight += momentum # update weights/parameters\n",
    "        pred_2 = f(weight)\n",
    "        if abs(pred_2 - pred_1) < 1e-13: # Convergence tolerance\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"Min x (weight):\",weight)\n",
    "    print(\"At x (weight), y =\",f(weight))\n",
    "    print(\"Gradient at x (weight):\",f_prime(weight))\n",
    "    \n",
    "    return weight, f(weight), f_prime(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 16th iteration\n",
      "Min x (weight): 2.6178011575928775\n",
      "At x (weight), y = -1.2981722729941898\n",
      "Gradient at x (weight): 8.368096893196508e-08\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 150th iteration\n",
      "Min x (weight): 2.6178011268972963\n",
      "At x (weight), y = -1.2981722729941874\n",
      "Gradient at x (weight): -2.3482066258129208e-07\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Converged on 1606th iteration\n",
      "Min x (weight): 2.6178010096864353\n",
      "At x (weight), y = -1.2981722729940885\n",
      "Gradient at x (weight): -1.4510167992698442e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    nesterov_accelerated_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Gradient Algorithm (AdaGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad_optimizer(f, f_prime, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        initial_guess = 0.75\n",
    "    weight = initial_guess\n",
    "    pred_1 = f(weight)\n",
    "    s = 0  # Initialize s\n",
    "    epsilon = 1e-8 # regularization parameter to avoid division by zero\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        grad = f_prime(weight) # Compute gradient\n",
    "        s += grad * grad\n",
    "        weight = weight - learning_rate * (grad/np.sqrt(s + epsilon)) # update weights/parameters\n",
    "        pred_2 = f(weight)\n",
    "        if abs(pred_2 - pred_1) < 1e-13: # Convergence tolerance\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"Min x (weight):\",weight)\n",
    "    print(\"At x (weight), y =\",f(weight))\n",
    "    print(\"Gradient at x (weight):\",f_prime(weight))\n",
    "    \n",
    "    return weight, f(weight), f_prime(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 1125th iteration\n",
      "Min x (weight): 2.6178008423934918\n",
      "At x (weight), y = -1.2981722729937006\n",
      "Gradient at x (weight): -3.1868712790927844e-06\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Min x (weight): 1.4891060712514816\n",
      "At x (weight), y = 0.0024870379181773927\n",
      "Gradient at x (weight): -0.08298058593144297\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Min x (weight): 0.8946785539161752\n",
      "At x (weight), y = 0.7649671358421188\n",
      "Gradient at x (weight): -2.2419737465025835\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    adagrad_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad only started converging after setting a learning rate of 0.1. With the default learning rate of 0.01, the adjusted learning rate becomes extremely small after a few iterations, leading to very slow updates or no noticeable progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square Propagation (RMSProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop_optimizer(f, f_prime, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        initial_guess = 0.75\n",
    "    \n",
    "    weight = initial_guess # initialize initial guess\n",
    "    pred_1 = f(weight)\n",
    "    \n",
    "    beta = 0.9 # decay rate\n",
    "    s = 0  # Initialize s\n",
    "    epsilon = 1e-8 # regularization parameter to avoid division by zero\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        grad = f_prime(weight) # Compute gradient\n",
    "        s = s * beta + (1 - beta) * grad * grad\n",
    "        weight = weight - learning_rate * (grad/np.sqrt(s + epsilon)) # update weights/parameters\n",
    "        pred_2 = f(weight)\n",
    "        if abs(pred_2 - pred_1) < 1e-13:\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"Min x (weight):\", weight)\n",
    "    print(\"At x (weight), y =\", f(weight))\n",
    "    print(\"Gradient at x (weight):\", f_prime(weight))\n",
    "    \n",
    "    return weight, f(weight), f_prime(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Min x (weight): 2.6663394448293567\n",
      "At x (weight), y = -1.2857500845016225\n",
      "Gradient at x (weight): 0.5153434719103737\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 262th iteration\n",
      "Min x (weight): 2.617801129097367\n",
      "At x (weight), y = -1.298172272994188\n",
      "Gradient at x (weight): -2.1199242150604647e-07\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Converged on 1933th iteration\n",
      "Min x (weight): 2.6178010995757988\n",
      "At x (weight), y = -1.2981722729941771\n",
      "Gradient at x (weight): -5.183123216179197e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    rmsprop_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Moment Estimation (Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepnet12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
