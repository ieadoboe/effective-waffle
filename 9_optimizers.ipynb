{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccesary libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the problem to myself\n",
    "\n",
    "### 1. Function\n",
    "\n",
    "$f(x) = sin2x + asin(4x)$\n",
    "\n",
    "The goal is to minimize this function (find where it outputs the smallest value). This means adjusting parameters (weights) to reduce the loss.\n",
    "\n",
    "### 2. Gradient Descent\n",
    "\n",
    "$\\theta = \\theta - \\eta  \\cdot \\nabla f(\\theta)$\n",
    "\n",
    "where $\\theta$ is weights and bias and all parameters\n",
    "\n",
    "In ML, this would be updating the weights, $w = w- \\eta  \\cdot \\nabla L(w)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def f(x, a=0.499):\n",
    "    return np.sin(2 * x) + a * np.sin(4 * x)\n",
    "\n",
    "\n",
    "def f_prime(x, a=0.499):\n",
    "    return 2 * np.cos(2 * x) + (a * 4) * np.cos(4 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Implementing an optimizer means writing a Python function (or class) that:\n",
    "\n",
    "1. Takes an initial guess for the parameters.\n",
    "2. Computes the gradient of the given function at each step.\n",
    "3. Updates the parameters using the gradient descent rule.\n",
    "4. Stops after a set number of steps or when the updates become small enough (converges).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.02994003426228467\n",
      "-0.06972122169230724\n",
      "-0.10871590025282594\n",
      "-0.14634726718029928\n",
      "-0.1821332686723779\n",
      "-0.21571416593916687\n",
      "-0.24686094115139826\n",
      "-0.27546691378698396\n",
      "-0.3015281748313631\n",
      "-0.32511910395041105\n"
     ]
    }
   ],
   "source": [
    "initial_guess = weight = 0.01\n",
    "max_iters = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(max_iters):\n",
    "    grad = f_prime(weight) # Compute gradient\n",
    "    weight -= learning_rate * grad\n",
    "    print(weight)\n",
    "    if abs(grad) < 10e-13:\n",
    "        print(f\"Breaks on the {i+1}th iteration\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rates to test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of test learning rates\n",
    "learning_rates = [0.1, 0.01, 0.001] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_optimizer(f, f_prime, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        initial_guess = 0.75\n",
    "    weight = initial_guess\n",
    "    pred_1 = f(weight)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        grad = f_prime(weight) # Compute gradient\n",
    "        weight -= learning_rate * grad # update weights/parameters\n",
    "        pred_2 = f(weight)\n",
    "        if abs(pred_2 - pred_1) < 1e-13: # Convergence tolerance\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"Min x (weight):\",weight)\n",
    "    print(\"At x (weight), y =\",f(weight))\n",
    "    print(\"Gradient at x (weight):\",f_prime(weight))\n",
    "    \n",
    "    return weight, f(weight), f_prime(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 151th iteration\n",
      "Min x (weight): 2.617801150048571\n",
      "At x (weight), y = -1.29817227299419\n",
      "Gradient at x (weight): 5.4001912941359365e-09\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 1574th iteration\n",
      "Min x (weight): 2.6178008915128057\n",
      "At x (weight), y = -1.2981722729938445\n",
      "Gradient at x (weight): -2.6772025890631213e-06\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Min x (weight): 1.5610820325228227\n",
      "At x (weight), y = 4.251390869903712e-05\n",
      "Gradient at x (weight): -0.005129212682828843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    vanilla_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Momentum optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_momentum_optimizer(f, f_prime, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        initial_guess = 0.75\n",
    "    weight = initial_guess\n",
    "    pred_1 = f(weight)\n",
    "    beta = 0.9 # set the friction\n",
    "    momentum = 0 # initialize velocity\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        grad = f_prime(weight) # Compute gradient\n",
    "        momentum = beta * momentum - learning_rate * grad\n",
    "        weight += momentum # update weights/parameters\n",
    "        pred_2 = f(weight)\n",
    "        if abs(pred_2 - pred_1) < 1e-13: # Convergence tolerance\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"Min x (weight):\",weight)\n",
    "    print(\"At x (weight), y =\",f(weight))\n",
    "    print(\"Gradient at x (weight):\",f_prime(weight))\n",
    "    \n",
    "    return weight, f(weight), f_prime(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 272th iteration\n",
      "Min x (weight): 2.617801423786491\n",
      "At x (weight), y = -1.2981722729938\n",
      "Gradient at x (weight): 2.845743499957365e-06\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 248th iteration\n",
      "Min x (weight): 2.6177986291129005\n",
      "At x (weight), y = -1.2981722729612328\n",
      "Gradient at x (weight): -2.6152142039492787e-05\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Converged on 1621th iteration\n",
      "Min x (weight): 2.6178012818908893\n",
      "At x (weight), y = -1.2981722729940992\n",
      "Gradient at x (weight): 1.3734145909438666e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    vanilla_momentum_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov accelerated gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov_accelerated_optimizer(f, f_prime, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        initial_guess = 0.75\n",
    "    weight = initial_guess\n",
    "    pred_1 = f(weight)\n",
    "    beta = 0.9 # set the friction\n",
    "    momentum = 0 # initialize velocity\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        grad = f_prime(weight + beta * momentum) # Compute gradient\n",
    "        momentum = beta * momentum - learning_rate * grad\n",
    "        weight += momentum # update weights/parameters\n",
    "        pred_2 = f(weight)\n",
    "        if abs(pred_2 - pred_1) < 1e-13: # Convergence tolerance\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"Min x (weight):\",weight)\n",
    "    print(\"At x (weight), y =\",f(weight))\n",
    "    print(\"Gradient at x (weight):\",f_prime(weight))\n",
    "    \n",
    "    return weight, f(weight), f_prime(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 16th iteration\n",
      "Min x (weight): 2.6178011575928775\n",
      "At x (weight), y = -1.2981722729941898\n",
      "Gradient at x (weight): 8.368096893196508e-08\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 150th iteration\n",
      "Min x (weight): 2.6178011268972963\n",
      "At x (weight), y = -1.2981722729941874\n",
      "Gradient at x (weight): -2.3482066258129208e-07\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Converged on 1606th iteration\n",
      "Min x (weight): 2.6178010096864353\n",
      "At x (weight), y = -1.2981722729940885\n",
      "Gradient at x (weight): -1.4510167992698442e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    nesterov_accelerated_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Gradient Algorithm (AdaGrad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad_optimizer(f, f_prime, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        initial_guess = 0.75\n",
    "    weight = initial_guess\n",
    "    pred_1 = f(weight)\n",
    "    s = 0  # Initialize s\n",
    "    epsilon = 1e-8 # regularization parameter to avoid division by zero\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        grad = f_prime(weight) # Compute gradient\n",
    "        s += grad * grad\n",
    "        weight = weight - learning_rate * (grad/np.sqrt(s + epsilon)) # update weights/parameters\n",
    "        pred_2 = f(weight)\n",
    "        if abs(pred_2 - pred_1) < 1e-13: # Convergence tolerance\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"Min x (weight):\",weight)\n",
    "    print(\"At x (weight), y =\",f(weight))\n",
    "    print(\"Gradient at x (weight):\",f_prime(weight))\n",
    "    \n",
    "    return weight, f(weight), f_prime(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 1125th iteration\n",
      "Min x (weight): 2.6178008423934918\n",
      "At x (weight), y = -1.2981722729937006\n",
      "Gradient at x (weight): -3.1868712790927844e-06\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Min x (weight): 1.4891060712514816\n",
      "At x (weight), y = 0.0024870379181773927\n",
      "Gradient at x (weight): -0.08298058593144297\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Min x (weight): 0.8946785539161752\n",
      "At x (weight), y = 0.7649671358421188\n",
      "Gradient at x (weight): -2.2419737465025835\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    adagrad_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad only started converging after setting a learning rate of 0.1. With the default learning rate of 0.01, the adjusted learning rate becomes extremely small after a few iterations, leading to very slow updates or no noticeable progress.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square Propagation (RMSProp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop_optimizer(f, f_prime, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        initial_guess = 0.75\n",
    "    \n",
    "    weight = initial_guess # initialize initial guess\n",
    "    pred_1 = f(weight)\n",
    "    \n",
    "    beta = 0.9 # decay rate\n",
    "    s = 0  # Initialize s\n",
    "    epsilon = 1e-8 # regularization parameter to avoid division by zero\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        grad = f_prime(weight) # Compute gradient\n",
    "        s = s * beta + (1 - beta) * grad * grad\n",
    "        weight = weight - learning_rate * (grad/np.sqrt(s + epsilon)) # update weights/parameters\n",
    "        pred_2 = f(weight)\n",
    "        if abs(pred_2 - pred_1) < 1e-13:\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"Min x (weight):\", weight)\n",
    "    print(\"At x (weight), y =\", f(weight))\n",
    "    print(\"Gradient at x (weight):\", f_prime(weight))\n",
    "    \n",
    "    return weight, f(weight), f_prime(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Min x (weight): 2.6663394448293567\n",
      "At x (weight), y = -1.2857500845016225\n",
      "Gradient at x (weight): 0.5153434719103737\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 262th iteration\n",
      "Min x (weight): 2.617801129097367\n",
      "At x (weight), y = -1.298172272994188\n",
      "Gradient at x (weight): -2.1199242150604647e-07\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Converged on 1933th iteration\n",
      "Min x (weight): 2.6178010995757988\n",
      "At x (weight), y = -1.2981722729941771\n",
      "Gradient at x (weight): -5.183123216179197e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    rmsprop_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Moment Estimation (Adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer(f, f_prime, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        initial_guess = 0.75\n",
    "    \n",
    "    weight = initial_guess # initialize initial guess\n",
    "    pred_1 = f(weight)\n",
    "    \n",
    "    beta1, beta2 = 0.9, 0.999\n",
    "    s = 0  # Initialize s\n",
    "    t = 0\n",
    "    m = 0 # initialize velocity\n",
    "    epsilon = 1e-8 # regularization parameter to avoid division by zero\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        grad = f_prime(weight) # Compute gradient\n",
    "        \n",
    "        t = t + 1\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        s = s * beta2 + (1 - beta2) * grad * grad\n",
    "        \n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        s_hat = s / (1 - beta2 ** t)\n",
    "        \n",
    "        weight = weight - learning_rate * (m_hat/np.sqrt(s_hat + epsilon)) # update weights/parameters\n",
    "        pred_2 = f(weight) # current prediction\n",
    "        if abs(pred_2 - pred_1) < 1e-13: # stopping criterion\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"x (weight):\", weight)\n",
    "    print(\"At x (weight), y =\", f(weight))\n",
    "    print(\"Gradient at x (weight):\", f_prime(weight))\n",
    "    \n",
    "    return weight, f(weight), f_prime(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 286th iteration\n",
      "x (weight): 2.617801548883668\n",
      "At x (weight), y = -1.2981722729933627\n",
      "Gradient at x (weight): 4.14376977209141e-06\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 1150th iteration\n",
      "x (weight): 2.6178012861852817\n",
      "At x (weight), y = -1.2981722729940932\n",
      "Gradient at x (weight): 1.4179738128117236e-06\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "x (weight): 1.6177244779971784\n",
      "At x (weight), y = -0.0005990929253106014\n",
      "Gradient at x (weight): -0.03025987247138051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    adam_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For a = 501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "SGD\n",
      "===================================================\n",
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 416th iteration\n",
      "Min x (weight): 1.5525581500345254\n",
      "At x (weight), y = 9.714628193220609e-05\n",
      "Gradient at x (weight): -1.1678835676320887e-10\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 3729th iteration\n",
      "Min x (weight): 1.552558147460668\n",
      "At x (weight), y = 9.714630246829975e-05\n",
      "Gradient at x (weight): -1.2451601971719128e-09\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Min x (weight): 1.5478450267011268\n",
      "At x (weight), y = 0.00014001033454779516\n",
      "Gradient at x (weight): -0.0023324427348099253\n",
      "\n",
      "===================================================\n",
      "Vanilla Momentum Optimizer\n",
      "===================================================\n",
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 387th iteration\n",
      "Min x (weight): 2.618186051502352\n",
      "At x (weight), y = -1.2981715042702362\n",
      "Gradient at x (weight): 1.0569375596958253e-08\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 372th iteration\n",
      "Min x (weight): 2.618186054155262\n",
      "At x (weight), y = -1.2981715042596387\n",
      "Gradient at x (weight): 3.8182116357532436e-08\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Converged on 3495th iteration\n",
      "Min x (weight): 1.5525581475779484\n",
      "At x (weight), y = 9.714630153254827e-05\n",
      "Gradient at x (weight): -1.1937448807231021e-09\n",
      "\n",
      "===================================================\n",
      "Nesterov Accelerated Optimizer\n",
      "===================================================\n",
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 22th iteration\n",
      "Min x (weight): 2.618186050489212\n",
      "At x (weight), y = -1.2981715042742834\n",
      "Gradient at x (weight): 2.413913513521493e-11\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 212th iteration\n",
      "Min x (weight): 2.618186050020326\n",
      "At x (weight), y = -1.2981715042761564\n",
      "Gradient at x (weight): -4.856249846696414e-09\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Converged on 3528th iteration\n",
      "Min x (weight): 1.552558147561335\n",
      "At x (weight), y = 9.714630166510196e-05\n",
      "Gradient at x (weight): -1.201028165809248e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"===================================================\")\n",
    "print(\"SGD\")\n",
    "print(f\"===================================================\")\n",
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    vanilla_optimizer(f, lambda x: f_prime(x,a=0.501), learning_rate=lr)\n",
    "    print()\n",
    "    \n",
    "print(f\"===================================================\")\n",
    "print(\"Vanilla Momentum Optimizer\")\n",
    "print(f\"===================================================\")\n",
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    vanilla_momentum_optimizer(f, lambda x: f_prime(x,a=0.501), learning_rate=lr)\n",
    "    print()\n",
    "    \n",
    "print(f\"===================================================\")\n",
    "print(\"Nesterov Accelerated Optimizer\")\n",
    "print(f\"===================================================\")\n",
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    nesterov_accelerated_optimizer(f, lambda x: f_prime(x,a=0.501), learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "Adagrad\n",
      "===================================================\n",
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 2523th iteration\n",
      "Min x (weight): 1.5525581484183708\n",
      "At x (weight), y = 9.7146294827051e-05\n",
      "Gradient at x (weight): -8.253060457263928e-10\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Min x (weight): 1.4866024558023436\n",
      "At x (weight), y = 0.0027008402863072656\n",
      "Gradient at x (weight): -0.08028700849470582\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Min x (weight): 0.8946532326170674\n",
      "At x (weight), y = 0.7650239053615728\n",
      "Gradient at x (weight): -2.2492085789187306\n",
      "\n",
      "===================================================\n",
      "RMSProp\n",
      "===================================================\n",
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 57th iteration\n",
      "Min x (weight): 1.5525581503006016\n",
      "At x (weight), y = 9.71462798092515e-05\n",
      "Gradient at x (weight): -1.4099832412739488e-13\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 166th iteration\n",
      "Min x (weight): 1.552558150300573\n",
      "At x (weight), y = 9.714627980948048e-05\n",
      "Gradient at x (weight): -1.5365486660812167e-13\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "Converged on 898th iteration\n",
      "Min x (weight): 1.5525581503003067\n",
      "At x (weight), y = 9.714627981161073e-05\n",
      "Gradient at x (weight): -2.7045032879868813e-13\n",
      "\n",
      "===================================================\n",
      "Adam\n",
      "===================================================\n",
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 393th iteration\n",
      "x (weight): 2.618186052002122\n",
      "At x (weight), y = -1.2981715042682398\n",
      "Gradient at x (weight): 1.577121722107222e-08\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 1514th iteration\n",
      "x (weight): 1.5525581497706504\n",
      "At x (weight), y = 9.714628403758446e-05\n",
      "Gradient at x (weight): -2.324702652600763e-10\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "x (weight): 1.5524331631160664\n",
      "At x (weight), y = 9.814693151088266e-05\n",
      "Gradient at x (weight): -5.4981372491269553e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"===================================================\")\n",
    "print(\"Adagrad\")\n",
    "print(f\"===================================================\")\n",
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    adagrad_optimizer(f, lambda x: f_prime(x,a=0.501), learning_rate=lr)\n",
    "    print()\n",
    "\n",
    "print(f\"===================================================\")\n",
    "print(\"RMSProp\")\n",
    "print(f\"===================================================\")\n",
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    rmsprop_optimizer(f, lambda x: f_prime(x,a=0.501), learning_rate=lr)\n",
    "    print()\n",
    "    \n",
    "print(f\"===================================================\")\n",
    "print(\"Adam\")\n",
    "print(f\"===================================================\")\n",
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    adam_optimizer(f, lambda x: f_prime(x,a=0.501), learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `tf.GradientTape()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define function in terms of tensorflow\n",
    "def f_tf(x, a=0.499):\n",
    "    return tf.sin(2 * x) + a * tf.sin(4 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: 3.604673\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f_tf(x, a=0.499)\n",
    "    \n",
    "grad = tape.gradient(y, x)\n",
    "\n",
    "print(\"Gradient:\",grad.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: 3.611424\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f_tf(x, a=0.501)\n",
    "    \n",
    "grad = tape.gradient(y, x)\n",
    "\n",
    "print(\"Gradient:\",grad.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_tape(f_tf, max_iters=5000, learning_rate=0.001, initial_guess = None):\n",
    "    if initial_guess is None:\n",
    "        weight = tf.Variable(0.75, dtype=tf.float32)\n",
    "    else:\n",
    "        weight = tf.Variable(initial_guess, dtype=tf.float32) # initialize initial guess\n",
    "    \n",
    "    beta1, beta2, epsilon = 0.9, 0.999, 1e-8  # regularization parameter to avoid division by zero\n",
    "    m, s = 0.0, 0.0\n",
    "    \n",
    "    pred_1 = f_tf(weight)\n",
    "    \n",
    "    for t in range(1, max_iters + 1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = f_tf(weight)\n",
    "            \n",
    "        grad = tape.gradient(y, weight)\n",
    "        \n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        s = beta2 * s + (1 - beta2) * tf.square(grad)\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        s_hat = s / (1 - beta2 ** t)\n",
    "        \n",
    "        weight.assign_sub(learning_rate * m_hat / (tf.sqrt(s_hat + epsilon))) # update weights/parameters\n",
    "        \n",
    "        pred_2 = f_tf(weight).numpy() # current prediction\n",
    "        \n",
    "        if abs(pred_2 - pred_1) < 1e-13: # stopping criterion\n",
    "            print(f\"Converged on {i+1}th iteration\")\n",
    "            break\n",
    "        pred_1 = pred_2\n",
    "\n",
    "    print(\"x (weight):\", float(weight))\n",
    "    print(\"At x (weight), y =\", float(f_tf(weight)))\n",
    "    \n",
    "    return weight.numpy(), f_tf(weight).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged on 10th iteration\n",
      "x (weight): 2.615039110183716\n",
      "At x (weight), y = -1.2981327772140503\n",
      "CPU times: user 1.08 s, sys: 11.8 ms, total: 1.09 s\n",
      "Wall time: 1.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float32(2.615039), np.float32(-1.2981328))"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "adam_tape(f_tf, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 10th iteration\n",
      "x (weight): 2.617943525314331\n",
      "At x (weight), y = -1.2981722354888916\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 10th iteration\n",
      "x (weight): 2.615039110183716\n",
      "At x (weight), y = -1.2981327772140503\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "x (weight): 1.6177161931991577\n",
      "At x (weight), y = -0.0005988404154777527\n",
      "\n",
      "CPU times: user 6.42 s, sys: 36.2 ms, total: 6.46 s\n",
      "Wall time: 6.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    adam_tape(f_tf, learning_rate=lr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Results for learning rate = 0.1---\n",
      "\n",
      "Converged on 286th iteration\n",
      "x (weight): 2.617801548883668\n",
      "At x (weight), y = -1.2981722729933627\n",
      "Gradient at x (weight): 4.14376977209141e-06\n",
      "\n",
      "---Results for learning rate = 0.01---\n",
      "\n",
      "Converged on 1150th iteration\n",
      "x (weight): 2.6178012861852817\n",
      "At x (weight), y = -1.2981722729940932\n",
      "Gradient at x (weight): 1.4179738128117236e-06\n",
      "\n",
      "---Results for learning rate = 0.001---\n",
      "\n",
      "x (weight): 1.6177244779971784\n",
      "At x (weight), y = -0.0005990929253106014\n",
      "Gradient at x (weight): -0.03025987247138051\n",
      "\n",
      "CPU times: user 21.3 ms, sys: 1.11 ms, total: 22.4 ms\n",
      "Wall time: 21.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for lr in learning_rates:\n",
    "    print(f\"---Results for learning rate = {lr}---\\n\")\n",
    "    adam_optimizer(f, f_prime, learning_rate=lr)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepnet12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
