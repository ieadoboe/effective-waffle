{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "from collections import defaultdict, Counter"
      ],
      "metadata": {
        "id": "wfOyib5yYlJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Shakespeares work from Andrej Karpathy's website\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "filepath = tf.keras.utils.get_file('shakespeare.txt', url)\n",
        "\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()"
      ],
      "metadata": {
        "id": "dBkt_Ao2ZWzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdbbfb67-3329-40ce-d1c7-064a5b458c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into words\n",
        "words = shakespeare_text.split()"
      ],
      "metadata": {
        "id": "zflj313ZZmv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract individual words with their frequencies\n",
        "def get_word_counts(text, return_sorted=True):\n",
        "  words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "  # Word counts (unsorted)\n",
        "  word_counts = Counter(words)\n",
        "\n",
        "  if return_sorted:\n",
        "    # Sort the word counts by frequency (value) in descending order\n",
        "    word_counts = dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "  return word_counts"
      ],
      "metadata": {
        "id": "1SJADBUAazrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = get_word_counts(shakespeare_text)"
      ],
      "metadata": {
        "id": "7OachGwDZoou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsygl6CwftRW",
        "outputId": "7deb07f7-1b54-4908-fe8b-29f47951417d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11456"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few items of the dictionary\n",
        "dict(list(corpus.items())[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYnlfZDbbDnK",
        "outputId": "76119136-547b-4092-fbed-7dfb1849f3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 6287,\n",
              " 'and': 5690,\n",
              " 'i': 5111,\n",
              " 'to': 4934,\n",
              " 'of': 3760,\n",
              " 'you': 3211,\n",
              " 'my': 3120,\n",
              " 'a': 3018,\n",
              " 'that': 2664,\n",
              " 'in': 2403,\n",
              " 'is': 2118,\n",
              " 'not': 2015,\n",
              " 'for': 1926,\n",
              " 's': 1859,\n",
              " 'with': 1813,\n",
              " 'it': 1773,\n",
              " 'me': 1769,\n",
              " 'be': 1710,\n",
              " 'your': 1686,\n",
              " 'he': 1606}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizerLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, pairs=50, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.pairs = pairs\n",
        "    self.merges = {}\n",
        "\n",
        "  # Training the tokenizer by going over each word from the corpus\n",
        "  def train(self, corpus):\n",
        "    vocab = {\" \".join(word) + \" </w>\": count for word, count in corpus.items()}\n",
        "\n",
        "    while len(self.merges) < self.pairs+1:\n",
        "      pairs = self.get_stats(vocab)  # Count adjacent pairs\n",
        "      if not pairs:\n",
        "        break  # No more pairs to merge\n",
        "\n",
        "      best_pair = max(pairs, key=pairs.get)  # Most frequent pair\n",
        "\n",
        "      #print(best_pair)\n",
        "      self.merges[best_pair] = \"\".join(best_pair)  # Merge pair\n",
        "      vocab = self.merge_vocab(best_pair, vocab)  # Update vocabulary\n",
        "\n",
        "  # Compute the character pairs\n",
        "  def get_stats(self, vocab):\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "      symbols = word.split()\n",
        "      for i in range(len(symbols) - 1):\n",
        "        pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "    return pairs\n",
        "\n",
        "  # Add the character pairs to the vocabulary\n",
        "  def merge_vocab(self, pair, vocab):\n",
        "    new_vocab = {}\n",
        "    bigram = re.escape(\" \".join(pair))\n",
        "    pattern = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
        "\n",
        "    for word in vocab:\n",
        "      new_word = pattern.sub(\"\".join(pair), word)\n",
        "      new_vocab[new_word] = vocab[word]\n",
        "\n",
        "    return new_vocab\n",
        "\n",
        "  # Tokenization using the learned vocabulary\n",
        "  def tokenize(self, text):\n",
        "    word = list(text) + [\"</w>\"]\n",
        "    while len(word) > 1:\n",
        "      pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]\n",
        "      merge_candidates = [p for p in pairs if p in self.merges]\n",
        "\n",
        "      if not merge_candidates:\n",
        "        break  # No more merges possible\n",
        "\n",
        "      best_pair = min(merge_candidates, key=lambda p: self.merges[p])\n",
        "      new_word = []\n",
        "      i = 0\n",
        "      while i < len(word):\n",
        "        if i < len(word) - 1 and (word[i], word[i + 1]) == best_pair:\n",
        "          new_word.append(\"\".join(best_pair))\n",
        "          i += 2\n",
        "        else:\n",
        "          new_word.append(word[i])\n",
        "          i += 1\n",
        "      word = new_word\n",
        "\n",
        "    return word\n",
        "\n",
        "  # Make sure this works for a batch of strings\n",
        "  def call(self, inputs):\n",
        "    return tf.py_function(self._vectorized_tokenize, [inputs], tf.string)\n",
        "\n",
        "  def _vectorized_tokenize(self, inputs):\n",
        "    tokenized_texts = [\" \".join(self.tokenize(text.numpy().decode())) for text in inputs]\n",
        "    return tf.convert_to_tensor(tokenized_texts)"
      ],
      "metadata": {
        "id": "knW7Sm0nYVLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learn the byte pair encoding\n",
        "bpe_layer = BPETokenizerLayer(pairs=1000)\n",
        "bpe_layer.train(corpus)"
      ],
      "metadata": {
        "id": "UhJd35z3cseW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Keras Layer\n",
        "sample_texts = tf.constant([\"\"\"To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\"\"\".lower()])\n",
        "\n",
        "tokenized_texts = bpe_layer(sample_texts)\n",
        "\n",
        "# Convert to string\n",
        "tokenized_texts = tf.strings.reduce_join(tokenized_texts, separator=' ').numpy().decode()\n",
        "\n",
        "# This is the tokenize output\n",
        "print(\"Tokenized Output:\", tokenized_texts)\n",
        "\n",
        "# This is the number of tokens in the outputted text\n",
        "print(\"Number of Tokens:\", len(tokenized_texts.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A0WdaamNxpc",
        "outputId": "804eab5f-22c7-4f63-9315-8690c3291316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Output: to   be ,   or   no t   to   be ,   th at   is   t he   qu es ti on : \n",
            " w he t her   ' t is   no bl er   in   t he   min d   to   su f f er </w>\n",
            "Number of Tokens: 40\n"
          ]
        }
      ]
    }
  ]
}