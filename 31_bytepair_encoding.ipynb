{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfOyib5yYlJA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBkt_Ao2ZWzY",
        "outputId": "fdbbfb67-3329-40ce-d1c7-064a5b458c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Get Shakespeares work from Andrej Karpathy's website\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", url)\n",
        "\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zflj313ZZmv2"
      },
      "outputs": [],
      "source": [
        "# Split into words\n",
        "words = shakespeare_text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SJADBUAazrp"
      },
      "outputs": [],
      "source": [
        "# Extract individual words with their frequencies\n",
        "def get_word_counts(text, return_sorted=True):\n",
        "    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "\n",
        "    # Word counts (unsorted)\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    if return_sorted:\n",
        "        # Sort the word counts by frequency (value) in descending order\n",
        "        word_counts = dict(\n",
        "            sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "        )\n",
        "\n",
        "    return word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OachGwDZoou"
      },
      "outputs": [],
      "source": [
        "corpus = get_word_counts(shakespeare_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsygl6CwftRW",
        "outputId": "7deb07f7-1b54-4908-fe8b-29f47951417d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11456"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYnlfZDbbDnK",
        "outputId": "76119136-547b-4092-fbed-7dfb1849f3fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'the': 6287,\n",
              " 'and': 5690,\n",
              " 'i': 5111,\n",
              " 'to': 4934,\n",
              " 'of': 3760,\n",
              " 'you': 3211,\n",
              " 'my': 3120,\n",
              " 'a': 3018,\n",
              " 'that': 2664,\n",
              " 'in': 2403,\n",
              " 'is': 2118,\n",
              " 'not': 2015,\n",
              " 'for': 1926,\n",
              " 's': 1859,\n",
              " 'with': 1813,\n",
              " 'it': 1773,\n",
              " 'me': 1769,\n",
              " 'be': 1710,\n",
              " 'your': 1686,\n",
              " 'he': 1606}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the first few items of the dictionary\n",
        "dict(list(corpus.items())[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knW7Sm0nYVLG"
      },
      "outputs": [],
      "source": [
        "class BPETokenizerLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, pairs=50, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pairs = pairs\n",
        "        self.merges = {}\n",
        "\n",
        "    # Training the tokenizer by going over each word from the corpus\n",
        "    def train(self, corpus):\n",
        "        vocab = {\" \".join(word) + \" </w>\": count for word, count in corpus.items()}\n",
        "\n",
        "        while len(self.merges) < self.pairs + 1:\n",
        "            pairs = self.get_stats(vocab)  # Count adjacent pairs\n",
        "            if not pairs:\n",
        "                break  # No more pairs to merge\n",
        "\n",
        "            best_pair = max(pairs, key=pairs.get)  # Most frequent pair\n",
        "\n",
        "            # print(best_pair)\n",
        "            self.merges[best_pair] = \"\".join(best_pair)  # Merge pair\n",
        "            vocab = self.merge_vocab(best_pair, vocab)  # Update vocabulary\n",
        "\n",
        "    # Compute the character pairs\n",
        "    def get_stats(self, vocab):\n",
        "        pairs = defaultdict(int)\n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word.split()\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "        return pairs\n",
        "\n",
        "    # Add the character pairs to the vocabulary\n",
        "    def merge_vocab(self, pair, vocab):\n",
        "        new_vocab = {}\n",
        "        bigram = re.escape(\" \".join(pair))\n",
        "        pattern = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
        "\n",
        "        for word in vocab:\n",
        "            new_word = pattern.sub(\"\".join(pair), word)\n",
        "            new_vocab[new_word] = vocab[word]\n",
        "\n",
        "        return new_vocab\n",
        "\n",
        "    # Tokenization using the learned vocabulary\n",
        "    def tokenize(self, text):\n",
        "        word = list(text) + [\"</w>\"]\n",
        "        while len(word) > 1:\n",
        "            pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]\n",
        "            merge_candidates = [p for p in pairs if p in self.merges]\n",
        "\n",
        "            if not merge_candidates:\n",
        "                break  # No more merges possible\n",
        "\n",
        "            best_pair = min(merge_candidates, key=lambda p: self.merges[p])\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                if i < len(word) - 1 and (word[i], word[i + 1]) == best_pair:\n",
        "                    new_word.append(\"\".join(best_pair))\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            word = new_word\n",
        "\n",
        "        return word\n",
        "\n",
        "    # Make sure this works for a batch of strings\n",
        "    def call(self, inputs):\n",
        "        return tf.py_function(self._vectorized_tokenize, [inputs], tf.string)\n",
        "\n",
        "    def _vectorized_tokenize(self, inputs):\n",
        "        tokenized_texts = [\n",
        "            \" \".join(self.tokenize(text.numpy().decode())) for text in inputs\n",
        "        ]\n",
        "        return tf.convert_to_tensor(tokenized_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhJd35z3cseW"
      },
      "outputs": [],
      "source": [
        "# Learn the byte pair encoding\n",
        "bpe_layer = BPETokenizerLayer(pairs=1000)\n",
        "bpe_layer.train(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A0WdaamNxpc",
        "outputId": "804eab5f-22c7-4f63-9315-8690c3291316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Output: to   be ,   or   no t   to   be ,   th at   is   t he   qu es ti on : \n",
            " w he t her   ' t is   no bl er   in   t he   min d   to   su f f er </w>\n",
            "Number of Tokens: 40\n"
          ]
        }
      ],
      "source": [
        "# Test Keras Layer\n",
        "sample_texts = tf.constant(\n",
        "    [\n",
        "        \"\"\"To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\"\"\".lower()\n",
        "    ]\n",
        ")\n",
        "\n",
        "tokenized_texts = bpe_layer(sample_texts)\n",
        "\n",
        "# Convert to string\n",
        "tokenized_texts = (\n",
        "    tf.strings.reduce_join(tokenized_texts, separator=\" \").numpy().decode()\n",
        ")\n",
        "\n",
        "# This is the tokenize output\n",
        "print(\"Tokenized Output:\", tokenized_texts)\n",
        "\n",
        "# This is the number of tokens in the outputted text\n",
        "print(\"Number of Tokens:\", len(tokenized_texts.split()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
